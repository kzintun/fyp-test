Digital Signal Processing is one of the most powerful technologies that will shape science and engineering in the twenty-first century.
Revolutionary changes have already been made in a broad range of fields: communications, medical imaging, radar & sonar, high fidelity music reproduction, and oil prospecting, to name just a few.
Each of these areas has developed a deep DSP technology, with its own algorithms, mathematics, and specialized techniques.
This combination of breath and depth makes it impossible for any one individual to master all of the DSP technology that has been developed.
DSP education involves two tasks: learning general concepts that apply to the field as a whole, and learning specialized techniques for your particular area of interest.
This chapter starts our journey into the world of Digital Signal Processing by describing the dramatic effect that DSP has made in several diverse fields.
The revolution has begun.
Digital Signal Processing is distinguished from other areas in computer science by the unique type of data it uses: signals.
In most cases, these signals originate as sensory data from the real world: seismic vibrations, visual images, sound waves, etc. DSP is the mathematics, the algorithms, and the techniques used to manipulate these signals after they have been converted into a digital form.
This includes a wide variety of goals, such as: enhancement of visual images, recognition and generation of speech, compression of data for storage and transmission, etc. Suppose we attach an analog-to-digital converter to a computer and use it to acquire a chunk of real world data.
DSP answers the question: What next?
The roots of DSP are in the 1960s and 1970s when digital computers first became available.
Computers were expensive during this era, and DSP was limited to only a few critical applications.
Pioneering efforts were made in four key areas: radar & sonar, where national security was at risk; oil exploration, where large amounts of money could be made; space exploration, where the data are irreplaceable; and medical imaging, where lives could be saved.
The personal computer revolution of the 1980s and 1990s caused DSP to explode with new applications.
Rather than being motivated by military and government needs, DSP was suddenly driven by the commercial marketplace.
Anyone who thought they could make money in the rapidly expanding field was suddenly a DSP vendor.
DSP reached the public in such products as: mobile telephones, compact disc players, and electronic voice mail.
Figure 1 illustrates a few of these varied applications.
This technological revolution occurred from the top-down.
In the early 1980s, DSP was taught as a graduate level course in electrical engineering.
A decade later, DSP had become a standard part of the undergraduate curriculum.
Today, DSP is a basic skill needed by scientists and engineers DSP has revolutionized many areas in science and engineering.
A few of these diverse applications are shown here.
As an analogy, DSP can be compared to a previous technological revolution: electronics.
While still the realm of electrical engineering, nearly every scientist and engineer has some background in basic circuit design.
Without it, they would be lost in the technological world.
DSP has the same future.
This recent history is more than a curiosity; it has a tremendous impact on your ability to learn and use DSP.
Suppose you encounter a DSP problem, and turn to textbooks or other publications to find a solution.
What you will typically find is page after page of equations, obscure mathematical symbols, and unfamiliar terminology.
It's a nightmare!
Much of the DSP literature is baffling even to those experienced in the field.
It's not that there is anything wrong with this material, it is just intended for a very specialized audience.
State-of-the-art researchers need this kind of detailed mathematics to understand the theoretical implications of the work.
A basic premise of this book is that most practical DSP techniques can be learned and used without the traditional barriers of detailed mathematics and theory.
The Scientist and Engineer’s Guide to Digital Signal Processing is written for those who want to use DSP as a tool, not a new career.
The remainder of this chapter illustrates areas where DSP has produced revolutionary changes.
As you go through each application, notice that DSP is very interdisciplinary, relying on the technical work in many adjacent fields.
As Fig. 1-2 suggests, the borders between DSP and other technical disciplines are not sharp and well defined, but rather fuzzy and overlapping.
If you want to specialize in DSP, these are the allied areas you will also need to study.
Digital Signal Processing has fuzzy and overlapping borders with many other areas of science, engineering and mathematics.
Telecommunications Telecommunications is about transferring information from one location to another.
This includes many forms of information: telephone conversations, television signals, computer files, and other types of data.
To transfer the information, you need a channel between the two locations.
This may be a wire pair, radio signal, optical fiber, etc. Telecommunications companies receive payment for transferring their customer's information, while they must pay to establish and maintain the channel.
The financial bottom line is simple: the more information they can pass through a single channel, the more money they make.
DSP has revolutionized the telecommunications industry in many areas: signaling tone generation and detection, frequency band shifting, filtering to remove power line hum, etc.
Three specific examples from the telephone network will be discussed here: multiplexing, compression, and echo control.
Multiplexing There are approximately one billion telephones in the world.
At the press of a few buttons, switching networks allow any one of these to be connected to any other in only a few seconds.
The immensity of this task is mind boggling!
Until the 1960s, a connection between two telephones required passing the analog voice signals through mechanical switches and amplifiers.
One connection required one pair of wires.
In comparison, DSP converts audio signals into a stream of serial digital data.
Since bits can be easily intertwined and later separated, many telephone conversations can be transmitted on a single channel.
For example, a telephone standard known as the T-carrier system can simultaneously transmit 24 voice signals.
Each voice signal is sampled 8000 times per second using an 8 bit companded (logarithmic compressed) analog-to-digital conversion.
This results in each voice signal being represented as 64,000 bits/sec, and all 24 channels being contained in 1.544 megabits/sec.
This signal can be transmitted about feet using ordinary telephone lines of 22 gauge copper wire, a typical interconnection distance.
The financial advantage of digital transmission is enormous.
Wire and analog switches are expensive; digital logic gates are cheap.
Compression When a voice signal is digitized at 8000 samples/sec, most of the digital information is redundant.
That is, the information carried by any one sample is largely duplicated by the neighboring samples.
Dozens of DSP algorithms have been developed to convert digitized voice signals into data streams that require fewer bits/sec.
These are called data compression algorithms.
Matching uncompression algorithms are used to restore the signal to its original form.
These algorithms vary in the amount of compression achieved and the resulting sound quality.
In general, reducing the data rate from 64 kilobits/sec to 32 kilobits/sec results in no loss of sound quality.
When compressed to a data rate of 8 kilobits/sec, the sound is noticeably affected, but still usable for long distance telephone networks.
The highest achievable compression is about 2 kilobits/sec, resulting in sound that is highly distorted, but usable for some applications such as military and undersea communications.
Echo control Echoes are a serious problem in long distance telephone connections.
When you speak into a telephone, a signal representing your voice travels to the connecting receiver, where a portion of it returns as an echo.
If the connection is within a few hundred miles, the elapsed time for receiving the echo is only a few milliseconds.
The human ear is accustomed to hearing echoes with these small time delays, and the connection sounds quite normal.
As the distance becomes larger, the echo becomes increasingly noticeable and irritating.
The delay can be several hundred milliseconds for intercontinental communications, and is particularly objectionable.
Digital Signal Processing attacks this type of problem by measuring the returned signal and generating an appropriate antisignal to cancel the offending echo.
This same technique allows speakerphone users to hear and speak at the same time without fighting audio feedback (squealing).
It can also be used to reduce environmental noise by canceling it with digitally generated antinoise.
Audio Processing The two principal human senses are vision and hearing.
Correspondingly, much of DSP is related to image and audio processing.
People listen to both music and speech.
DSP has made revolutionary changes in both these areas.
The path leading from the musician's microphone to the audiophile's speaker is remarkably long.
Digital data representation is important to prevent the degradation commonly associated with analog storage and manipulation.
This is very familiar to anyone who has compared the musical quality of cassette tapes with compact disks.
In a typical scenario, a musical piece is recorded in a sound studio on multiple channels or tracks.
In some cases, this even involves recording individual instruments and singers separately.
This is done to give the sound engineer greater flexibility in creating the final product.
The complex process of combining the individual tracks into a final product is called mix down.
DSP can provide several important functions during mix down, including: filtering, signal addition and subtraction, signal editing, etc.
One of the most interesting DSP applications in music preparation is artificial reverberation.
If the individual channels are simply added together, the resulting piece sounds frail and diluted, much as if the musicians were playing outdoors.
This is because listeners are greatly influenced by the echo or reverberation content of the music, which is usually minimized in the sound studio.
DSP allows artificial echoes and reverberation to be added during mix down to simulate various ideal listening environments.
Echoes with delays of a few hundred milliseconds give the impression of cathedral like locations.
Adding echoes with delays of 10-20 milliseconds provide the perception of more modest size listening rooms.
Speech generation Speech generation and recognition are used to communicate between humans and machines.
Rather than using your hands and eyes, you use your mouth and ears.
This is very convenient when your hands and eyes should be doing something else, such as: driving a car, performing surgery, or (unfortunately) firing your weapons at the enemy.
Two approaches are used for computer generated speech: digital recording and vocal tract simulation.
In digital recording, the voice of a human speaker is digitized and stored, usually in a compressed form.
During playback, the stored data are uncompressed and converted back into an analog signal.
An entire hour of recorded speech requires only about three megabytes of storage, well within the capabilities of even small computer systems.
This is the most common method of digital speech generation used today.
Vocal tract simulators are more complicated, trying to mimic the physical mechanisms by which humans create speech.
The human vocal tract is an acoustic cavity with resonant frequencies determined by the size and shape of the chambers.
Sound originates in the vocal tract in one of two basic ways, called voiced and fricative sounds.
With voiced sounds, vocal cord vibration produces near periodic pulses of air into the vocal cavities.
In comparison, fricative sounds originate from the noisy air turbulence at narrow constrictions, such as the teeth and lips.
Vocal tract simulators operate by generating digital signals that resemble these two types of excitation.
The characteristics of the resonate chamber are simulated by passing the excitation signal through a digital filter with similar resonances.
This approach was used in one of the very early DSP success stories, the Speak & Spell, a widely sold electronic learning aid for children.
Speech recognition The automated recognition of human speech is immensely more difficult than speech generation.
Speech recognition is a classic example of things that the human brain does well, but digital computers do poorly.
Digital computers can store and recall vast amounts of data, perform mathematical calculations at blazing speeds, and do repetitive tasks without becoming bored or inefficient.
Unfortunately, present day computers perform very poorly when faced with raw sensory data.
Teaching a computer to send you a monthly electric bill is easy.
Teaching the same computer to understand your voice is a major undertaking.
Digital Signal Processing generally approaches the problem of voice recognition in two steps: feature extraction followed by feature matching.
Each word in the incoming audio signal is isolated and then analyzed to identify the type of excitation and resonate frequencies.
These parameters are then compared with previous examples of spoken words to identify the closest match.
Often, these systems are limited to only a few hundred words; can only accept speech with distinct pauses between words; and must be retrained for each individual speaker.
While this is adequate for many commercial applications, these limitations are humbling when compared to the abilities of human hearing.
There is a great deal of work to be done in this area, with tremendous financial rewards for those that produce successful commercial products.
Echo Location A common method of obtaining information about a remote object is to bounce a wave off of it.
For example, radar operates by transmitting pulses of radio waves, and examining the received signal for echoes from aircraft.
In sonar, sound waves are transmitted through the water to detect submarines and other submerged objects.
Geophysicists have long probed the earth by setting off explosions and listening for the echoes from deeply buried layers of rock.
While these applications have a common thread, each has its own specific problems and needs.
Digital Signal Processing has produced revolutionary changes in all three areas.
Radar is an acronym for RAdio Detection And Ranging.
In the simplest radar system, a radio transmitter produces a pulse of radio frequency energy a few microseconds long.
This pulse is fed into a highly directional antenna, where the resulting radio wave propagates away at the speed of light.
Aircraft in the path of this wave will reflect a small portion of the energy back toward a receiving antenna, situated near the transmission site.
The distance to the object is calculated from the elapsed time between the transmitted pulse and the received echo.
The direction to the object is found more simply; you know where you pointed the directional antenna when the echo was received.
The operating range of a radar system is determined by two parameters: how much energy is in the initial pulse, and the noise level of the radio receiver.
Unfortunately, increasing the energy in the pulse usually requires making the pulse longer.
In turn, the longer pulse reduces the accuracy and precision of the elapsed time measurement.
This results in a conflict between two important parameters: the ability to detect objects at long range, and the ability to accurately determine an object's distance.
DSP has revolutionized radar in three areas, all of which relate to this basic problem.
First, DSP can compress the pulse after it is received, providing better distance determination without reducing the operating range.
Second, DSP can filter the received signal to decrease the noise.
This increases the range, without degrading the distance determination.
Third, DSP enables the rapid selection and generation of different pulse shapes and lengths.
Among other things, this allows the pulse to be optimized for a particular detection problem.
Now the impressive part: much of this is done at a sampling rate comparable to the radio frequency used, as high as several hundred megahertz!
When it comes to radar, DSP is as much about high-speed hardware design as it is about algorithms.
Sonar is an acronym for SOund NAvigation and Ranging.
It is divided into two categories, active and passive.
In active sonar, sound pulses between 2 kHz and 40 kHz are transmitted into the water, and the resulting echoes detected and analyzed.
Uses of active sonar include: detection & localization of undersea bodies, navigation, communication, and mapping the sea floor.
A maximum operating range of 10 to 100 kilometers is typical.
In comparison, passive sonar simply listens to underwater sounds, which includes: natural turbulence, marine life, and mechanical sounds from submarines and surface vessels.
Since passive sonar emits no energy, it is ideal for covert operations.
You want to detect the other guy, without him detecting you.
The most important application of passive sonar is in military surveillance systems that detect and track submarines.
Passive sonar typically uses lower frequencies than active sonar because they propagate through the water with less absorption.
Detection ranges can be thousands of kilometers.
DSP has revolutionized sonar in many of the same areas as radar: pulse generation, pulse compression, and filtering of detected signals.
In one view, sonar is simpler than radar because of the lower frequencies involved.
In another view, sonar is more difficult than radar because the environment is much less uniform and stable.
Sonar systems usually employ extensive arrays of transmitting and receiving elements, rather than just a single channel.
By properly controlling and mixing the signals in these many elements, the sonar system can steer the emitted pulse to the desired location and determine the direction that echoes are received from.
To handle these multiple channels, sonar systems require the same massive DSP computing power as radar.
As early as the 1920s, geophysicists discovered that the structure of the earth's crust could be probed with sound.
Prospectors could set off an explosion and record the echoes from boundary layers more than ten kilometers below the surface.
These echo seismograms were interpreted by the raw eye to map the subsurface structure.
The reflection seismic method rapidly became the primary method for locating petroleum and mineral deposits, and remains so today.
In the ideal case, a sound pulse sent into the ground produces a single echo for each boundary layer the pulse passes through.
Unfortunately, the situation is not usually this simple.
Each echo returning to the surface must pass through all the other boundary layers above where it originated.
This can result in the echo bouncing between layers, giving rise to echoes of echoes being detected at the surface.
These secondary echoes can make the detected signal very complicated and difficult to interpret.
Digital Signal Processing has been widely used since the 1960s to isolate the primary from the secondary echoes in reflection seismograms.
How did the early geophysicists manage without DSP?
The answer is simple: they looked in easy places, where multiple reflections were minimized.
DSP allows oil to be found in difficult locations, such as under the ocean.
Images are signals with special characteristics.
First, they are a measure of a parameter over space (distance), while most signals are a measure of a parameter over time.
Second, they contain a great deal of information.
For example, more than 10 megabytes can be required to store one second of television video.
This is more than a thousand times greater than for a similar length voice signal.
Third, the final judge of quality is often a subjective human evaluation, rather than an objective criterion.
These special characteristics have made image processing a distinct subgroup within DSP.
In 1895, Wilhelm Conrad Röntgen discovered that x-rays could pass through substantial amounts of matter.
Medicine was revolutionized by the ability to look inside the living human body.
Medical x-ray systems spread throughout the world in only a few years.
In spite of its obvious success, medical x-ray imaging was limited by four problems until DSP and related techniques came along in the 1970s.
First, overlapping structures in the body can hide behind each other.
For example, portions of the heart might not be visible behind the ribs.
Second, it is not always possible to distinguish between similar tissues.
For example, it may be able to separate bone from soft tissue, but not distinguish a tumor from the liver.
Third, x-ray images show anatomy, the body's structure, and not physiology, the body's operation.
The x-ray image of a living person looks exactly like the x-ray image of a dead one!
Fourth, x-ray exposure can cause cancer, requiring it to be used sparingly and only with proper justification.
The problem of overlapping structures was solved in 1971 with the introduction of the first computed tomography scanner (formerly called computed axial tomography, or CAT scanner).
Computed tomography (CT) is a classic example of Digital Signal Processing.
X-rays from many directions are passed through the section of the patient's body being examined.
Instead of simply forming images with the detected x-rays, the signals are converted into digital data and stored in a computer.
The information is then used to calculate images that appear to be slices through the body.
These images show much greater detail than conventional techniques, allowing significantly better diagnosis and treatment.
The impact of CT was nearly as large as the original introduction of x-ray imaging itself.
Within only a few years, every major hospital in the world had access to a CT scanner.
In 1979, two of CT's principle contributors, Godfrey N. Hounsfield and Allan M. Cormack, shared the Nobel Prize in Medicine.
That's good DSP!
The last three x-ray problems have been solved by using penetrating energy other than x-rays, such as radio and sound waves.
DSP plays a key role in all these techniques.
For example, Magnetic Resonance Imaging (MRI) uses magnetic fields in conjunction with radio waves to probe the interior of the human body.
Properly adjusting the strength and frequency of the fields cause the atomic nuclei in a localized region of the body to resonate between quantum energy states.
This resonance results in the emission of a secondary radio wave, detected with an antenna placed near the body.
The strength and other characteristics of this detected signal provide information about the localized region in resonance.
Adjustment of the magnetic field allows the resonance region to be scanned throughout the body, mapping the internal structure.
This information is usually presented as images, just as in computed tomography.
Besides providing excellent discrimination between different types of soft tissue, MRI can provide information about physiology, such as blood flow through arteries.
MRI relies totally on Digital Signal Processing techniques, and could not be implemented without them.
Sometimes, you just have to make the most out of a bad picture.
This is frequently the case with images taken from unmanned satellites and space exploration vehicles.
No one is going to send a repairman to Mars just to tweak the knobs on a camera!
DSP can improve the quality of images taken under extremely unfavorable conditions in several ways: brightness and contrast adjustment, edge detection, noise reduction, focus adjustment, motion blur reduction, etc. Images that have spatial distortion, such as encountered when a flat image is taken of a spherical planet, can also be warped into a correct representation.
Many individual images can also be combined into a single database, allowing the information to be displayed in unique ways.
For example, a video sequence simulating an aerial flight over the surface of a distant planet.
The large information content in images is a problem for systems sold in mass quantity to the general public.
Commercial systems must be cheap, and this doesn't mesh well with large memories and high data transfer rates.
One answer to this dilemma is image compression.
Just as with voice signals, images contain a tremendous amount of redundant information, and can be run through algorithms that reduce the number of bits needed to represent them.
Television and other moving pictures are especially suitable for compression, since most of the image remain the same from frame-to-frame.
Commercial imaging products that take advantage of this technology include: video telephones, computer programs that display moving pictures, and digital television.
Statistics and probability are used in Digital Signal Processing to characterize signals and the processes that generate them.
For example, a primary use of DSP is to reduce interference, noise, and other undesirable components in acquired data.
These may be an inherent part of the signal being measured, arise from imperfections in the data acquisition system, or be introduced as an unavoidable byproduct of some DSP operation.
Statistics and probability allow these disruptive features to be measured and classified, the first step in developing strategies to remove the offending components.
This chapter introduces the most important concepts in statistics and probability, with emphasis on how they apply to acquired signals.
A signal is a description of how one parameter is related to another parameter.
For example, the most common type of signal in analog electronics is a voltage that varies with time.
Since both parameters can assume a continuous range of values, we will call this a continuous signal.
In comparison, passing this signal through an analog-to-digital converter forces each of the two parameters to be quantized.
For instance, imagine the conversion being done with 12 bits at a sampling rate of 1000 samples per second.
The voltage is curtailed to (212) possible binary levels, and the time is only defined at one millisecond increments.
Signals formed from parameters that are quantized in this manner are said to be discrete signals or digitized signals.
For the most part, continuous signals exist in nature, while discrete signals exist inside computers (although you can find exceptions to both cases).
It is also possible to have signals where one parameter is continuous and the other is discrete.
Since these mixed signals are quite uncommon, they do not have special names given to them, and the nature of the two parameters must be explicitly stated.
Figure 2-1 shows two discrete signals, such as might be acquired with a digital data acquisition system.
The vertical axis may represent voltage, light intensity, sound pressure, or an infinite number of other parameters.
Since we don't know what it represents in this particular case, we will give it the generic label: amplitude.
This parameter is also called several other names: the yaxis, the dependent variable, the range, and the ordinate.
The horizontal axis represents the other parameter of the signal, going by such names as: the x-axis, the independent variable, the domain, and the abscissa.
Time is the most common parameter to appear on the horizontal axis of acquired signals; however, other parameters are used in specific applications.
For example, a geophysicist might acquire measurements of rock density at equally spaced distances along the surface of the earth.
To keep things general, we will simply label the horizontal axis: sample number.
If this were a continuous signal, another label would have to be used, such as: time, distance, x, etc.
The two parameters that form a signal are generally not interchangeable.
The parameter on the y-axis (the dependent variable) is said to be a function of the parameter on the x-axis (the independent variable).
In other words, the independent variable describes how or when each sample is taken, while the dependent variable is the actual measurement.
Given a specific value on the x-axis, we can always find the corresponding value on the y-axis, but usually not the other way around.
Pay particular attention to the word: domain, a very widely used term in DSP.
For instance, a signal that uses time as the independent variable (i.e., the parameter on the horizontal axis), is said to be in the time domain.
Another common signal in DSP uses frequency as the independent variable, resulting in the term, frequency domain.
Likewise, signals that use distance as the independent parameter are said to be in the spatial domain (distance is a measure of space).
The type of parameter on the horizontal axis is the domain of the signal; it's that simple.
What if the x-axis is labeled with something very generic, such as sample number?
Authors commonly refer to these signals as being in the time domain.
This is because sampling at equal intervals of time is the most common way of obtaining signals, and they don't have anything more specific to call it.
Although the signals in Fig. 2-1 are discrete, they are displayed in this figure as continuous lines.
This is because there are too many samples to be distinguishable if they were displayed as individual markers.
In graphs that portray shorter signals, say less than 100 samples, the individual markers are usually shown.
Continuous lines may or may not be drawn to connect the markers, depending on how the author wants you to view the data.
For instance, a continuous line could imply what is happening between samples, or simply be an aid to help the reader's eye follow a trend in noisy data.
The point is, examine the labeling of the horizontal axis to find if you are working with a discrete or continuous signal.
Don't rely on an illustrator's ability to draw dots.
The variable, N, is widely used in DSP to represent the total number of samples in a signal.
For example, N ' 512 for the signals in Fig. 2-1.
Examples of two digitized signals with different means and standard deviations.
These are the numbers that appear along the horizontal axis.
Two notations for assigning sample numbers are commonly used.
In the first notation, the sample indexes run from 1 to N (e.g., 1 to 512).
In the second notation, the sample indexes run from 0 to N & 1 (e.g., 0 to 511).
Mathematicians often use the first method (1 to N), while those in DSP commonly uses the second (0 to N & 1 ).
In this book, we will use the second notation.
Don't dismiss this as a trivial problem.
It will confuse you sometime during your career.
Look out for it!
The mean, indicated by µ (a lower case Greek mu), is the statistician's jargon for the average value of a signal.
It is found just as you would expect: add all of the samples together, and divide by N. It looks like this in mathematical form: Calculation of a signal's mean.
The signal is contained in x0 through xN-1, i is an index that runs through these values, and µ is the mean.
In words, sum the values in the signal, xi, by letting the index, i, run from to N & 1 .
Then finish the calculation by dividing the sum by N.
This is identical to the equation: µ ' (x0% x1% x2% þ% xN & 1) /N .
If you are not already familiar with E (upper case Greek sigma) being used to indicate summation, study these equations carefully, and compare them with the computer program in Table 2-1.
Summations of this type are abundant in DSP, and you need to understand this notation fully.
In electronics, the mean is commonly called the DC (direct current) value.
Likewise, AC (alternating current) refers to how the signal fluctuates around the mean value.
If the signal is a simple repetitive waveform, such as a sine or square wave, its excursions can be described by its peak-to-peak amplitude.
Unfortunately, most acquired signals do not show a well defined peak-to-peak value, but have a random nature, such as the signals in Fig. 2-1.
A more generalized method must be used in these cases, called the standard deviation, denoted by F (a lower case Greek sigma).
As a starting point, the expression,*xi & µ*, describes how far the i th sample deviates (differs) from the mean.
The average deviation of a signal is found by summing the deviations of all the individual samples, and then dividing by the number of samples, N. Notice that we take the absolute value of each deviation before the summation; otherwise the positive and negative terms would average to zero.
The average deviation provides a single number representing the typical distance that the samples are from the mean.
While convenient and straightforward, the average deviation is almost never used in statistics.
This is because it doesn't fit well with the physics of how signals operate.
In most cases, the important parameter is not the deviation from the mean, but the power represented by the deviation from the mean.
For example, when random noise signals combine in an electronic circuit, the resultant noise is equal to the combined power of the individual signals, not their combined amplitude.
The standard deviation is similar to the average deviation, except the averaging is done with power instead of amplitude.
This is achieved by squaring each of the deviations before taking the average (remember, power % voltage2).
To finish, the square root is taken to compensate for the initial squaring.
In equation form, the standard deviation is calculated: Calculation of the standard deviation of a signal.
The signal is stored in xi, µ is the mean found from Eq. 2-1, N is the number of samples, and σ is the standard deviation.
In the alternative notation: F ' (x0& µ)2 % (x1& µ)2 % þ% (xN & 1& µ)2 / (N & 1) .
Notice that the average is carried out by dividing by N & 1 instead of N.
This is a subtle feature of the equation that will be discussed in the next section.
The term, F2, occurs frequently in statistics and is given the name variance.
The standard deviation is a measure of how far the signal fluctuates from the mean.
The variance represents the power of this fluctuation.
Another term you should become familiar with is the rms (root-mean-square) value, frequently used in electronics.
By definition, the standard deviation only measures the AC portion of a signal, while the rms value measures both the AC and DC components.
If a signal has no DC component, its rms value is identical to its standard deviation.
Figure 2-2 shows the relationship between the standard deviation and the peak-to-peak value of several common waveforms.
Ratio of the peak-to-peak amplitude to the standard deviation for several common waveforms.
For the square wave, this ratio is 2; for the triangle wave it is 12 ' 3.46 ; for the sine wave it is 2 2 ' 2.83 .
While random noise has no exact peak-to-peak value, it is approximately 6 to 8 times the standard deviation.
Table 2-1 lists a computer routine for calculating the mean and standard deviation using Eqs.
The programs in this book are intended to convey algorithms in the most straightforward way; all other factors are treated as secondary.
Good programming techniques are disregarded if it makes the program logic more clear.
For instance: a simplified version of BASIC is used, line numbers are included, the only control structure allowed is the FOR-NEXT loop, there are no I/O statements, etc. Think of these programs as an alternative way of understanding the equations used in DSP.
If you can't grasp one, maybe the other will help.
In BASIC, the % character at the end of a variable name indicates it is an integer.
All other variables are floating point.
Chapter 4 discusses these variable types in detail.
This method of calculating the mean and standard deviation is adequate for many applications; however, it has two limitations.
First, if the mean is much larger than the standard deviation, Eq. 2-2 involves subtracting two numbers that are very close in value.
This can result in excessive round-off error in the calculations, a topic discussed in more detail in Chapter 4. Second, it is often desirable to recalculate the mean and standard deviation as new samples are acquired and added to the signal.
We will call this type of calculation: running statistics.
While the method of Eqs.
This is a very inefficient use of computational power and memory.
A solution to these problems can be found by manipulating Eqs.
This equation provides the same result as Eq.
The signal is expressed in terms of three accumulated parameters: N, the total number of samples; sum, the sum of these samples; and sum of squares, the sum of the squares of the samples.
The mean and standard deviation are then calculated from these three accumulated parameters.
While moving through the signal, a running tally is kept of three parameters: (1) the number of samples already processed, (2) the sum of these samples, and (3) the sum of the squares of the samples (that is, square the value of each sample and add the result to the accumulated value).
After any number of samples have been processed, the mean and standard deviation can be efficiently calculated using only the current value of the three parameters.
Table 2-2 shows a program that reports the mean and standard deviation in this manner as each new sample is taken into account.
This is the method used in hand calculators to find the statistics of a sequence of numbers.
Every time you enter a number and press the E (summation) key, the three parameters are updated.
The mean and standard deviation can then be found whenever desired, without having to recalculate the entire sequence.
Before ending this discussion on the mean and standard deviation, two other terms need to be mentioned.
In some situations, the mean describes what is being measured, while the standard deviation represents noise and other interference.
In these cases, the standard deviation is not important in itself, but only in comparison to the mean.
This gives rise to the term: signal-to-noise ratio (SNR), which is equal to the mean divided by the standard deviation.
Another term is also used, the coefficient of variation (CV).
This is defined as the standard deviation divided by the mean, multiplied by 100 percent.
For example, a signal (or other group of measure values) with a CV of 2%, has an SNR of 50.
Better data means a higher value for the SNR and a lower value for the CV.
Signal vs. Underlying Process Statistics is the science of interpreting numerical data, such as acquired signals.
In comparison, probability is used in DSP to understand the processes that generate signals.
Although they are closely related, the distinction between the acquired signal and the underlying process is key to many DSP techniques.
For example, imagine creating a 1000 point signal by flipping a coin times.
If the coin flip is heads, the corresponding sample is made a value of one.
On tails, the sample is set to zero.
The process that created this signal has a mean of exactly 0.5, determined by the relative probability of each possible outcome: 50% heads, 50% tails.
However, it is unlikely that the actual 1000 point signal will have a mean of exactly 0.5.
Random chance will make the number of ones and zeros slightly different each time the signal is generated.
The probabilities of the underlying process are constant, but the statistics of the acquired signal change each time the experiment is repeated.
This random irregularity found in actual data is called by such names as: statistical variation, statistical fluctuation, and statistical noise.
This presents a bit of a dilemma.
When you see the terms: mean and standard deviation, how do you know if the author is referring to the statistics of an actual signal, or the probabilities of the underlying process that created the signal?
Unfortunately, the only way you can tell is by the context.
This is not so for all terms used in statistics and probability.
For example, the histogram and probability mass function (discussed in the next section) are matching concepts that are given separate names.
Now, back to Eq. 2-2, calculation of the standard deviation.
As previously mentioned, this equation divides by N-1 in calculating the average of the squared deviations, rather than simply by N. To understand why this is so, imagine that you want to find the mean and standard deviation of some process that generates signals.
Toward this end, you acquire a signal of N samples from the process, and calculate the mean of the signal via Eq.
You can then use this as an estimate of the mean of the underlying process; however, you know there will be an error due to statistical noise.
In particular, for random signals, the typical error between the mean of the N points, and the mean of the underlying process, is given by: Typical error in calculating the mean of an underlying process by using a finite number of samples, N. The parameter, σ, is the standard deviation.
If N is small, the statistical noise in the calculated mean will be very large.
In other words, you do not have access to enough data to properly characterize the process.
The larger the value of N, the smaller the expected error will become.
A milestone in probability theory, the Strong Law of Large Numbers, guarantees that the error becomes zero as N approaches infinity.
In the next step, we would like to calculate the standard deviation of the acquired signal, and use it as an estimate of the standard deviation of the underlying process.
Herein lies the problem.
Before you can calculate the standard deviation using Eq.
However, you don't know the mean of the underlying process, only the mean of the N point signal, which contains an error due to statistical noise.
This error tends to reduce the calculated value of the standard deviation.
To compensate for this, N is replaced by N-1.
If N is large, the difference doesn't matter.
If N is small, this replacement provides a more accurate Examples of signals generated from nonstationary processes.
In (a), both the mean and standard deviation change.
In (b), the standard deviation remains a constant value of one, while the mean changes from a value of zero to two.
It is a common analysis technique to break these signals into short segments, and calculate the statistics of each segment individually.
In other words, Eq. 2-2 is an estimate of the standard deviation of the underlying process.
If we divided by N in the equation, it would provide the standard deviation of the acquired signal.
As an illustration of these ideas, look at the signals in Fig. 2-3, and ask: are the variations in these signals a result of statistical noise, or is the underlying process changing?
It probably isn't hard to convince yourself that these changes are too large for random chance, and must be related to the underlying process.
Processes that change their characteristics in this manner are called nonstationary.
In comparison, the signals previously presented in Fig. 2- were generated from a stationary process, and the variations result completely from statistical noise.
Figure 2-3b illustrates a common problem with nonstationary signals: the slowly changing mean interferes with the calculation of the standard deviation.
In this example, the standard deviation of the signal, over a short interval, is one.
However, the standard deviation of the entire signal is 1.16.
This error can be nearly eliminated by breaking the signal into short sections, and calculating the statistics for each section individually.
If needed, the standard deviations for each of the sections can be averaged to produce a single value.
Suppose we attach an 8 bit analog-to-digital converter to a computer, and acquire 256,000 samples of some signal.
As an example, Fig. 2-4a shows 128 samples that might be a part of this data set.
The value of each sample will be one of 256 possibilities, 0 through 255.
The histogram displays the number of samples there are in the signal that have each of these possible values.
Figure (b) shows the histogram for the 128 samples in (a).
For Value of sample example, there are 2 samples that have a value of 110, 7 samples that have a value of 131, 0 samples that have a value of 170, etc.
We will represent the histogram by Hi, where i is an index that runs from 0 to M-1, and M is the number of possible values that each sample can take on.
For instance, H50 is the number of samples that have a value of 50. Figure (c) shows the histogram of the signal using the full data set, all 256k points.
As can be seen, the larger number of samples results in a much smoother appearance.
Just as with the mean, the statistical noise (roughness) of the histogram is inversely proportional to the square root of the number of samples used.
From the way it is defined, the sum of all of the values in the histogram must be equal to the number of points in the signal: The sum of all of the values in the histogram is equal to the number of points in the signal.
In this equation, H i is the histogram, N is the number of points in the signal, and M is the number of points in the histogram.
The histogram can be used to efficiently calculate the mean and standard deviation of very large data sets.
This is especially important for images, which can contain millions of samples.
The histogram groups samples together that have the same value.
This allows the statistics to be calculated by working with a few groups, rather than a large number of individual samples.
Using this approach, the mean and standard deviation are calculated from the histogram by the equations: Calculation of the mean from the histogram.
This can be viewed as combining all samples having the same value into groups, and then using Eq.
Calculation of the standard deviation from the histogram.
This is the same concept as Eq.
Table 2-3 contains a program for calculating the histogram, mean, and standard deviation using these equations.
Calculation of the histogram is very fast, since it only requires indexing and incrementing.
In comparison, calculating the mean and standard deviation requires the time consuming operations of addition and multiplication.
The strategy of this algorithm is to use these slow operations only on the few numbers in the histogram, not the many samples in the signal.
This makes the algorithm much faster than the previously described methods.
Think a factor of ten for very long signals with the calculations being performed on a general purpose computer.
The notion that the acquired signal is a noisy version of the underlying process is very important; so important that some of the concepts are given different names.
The histogram is what is formed from an acquired signal.
The corresponding curve for the underlying process is called the probability mass function (pmf).
A histogram is always calculated using a finite number of samples, while the pmf is what would be obtained with an infinite number of samples.
The pmf can be estimated (inferred) from the histogram, or it may be deduced by some mathematical technique, such as in the coin flipping example.
Figure 2-5 shows an example pmf, and one of the possible histograms that could be associated with it.
The key to understanding these concepts rests in the units of the vertical axis.
As previously described, the vertical axis of the histogram is the number of times that a particular value occurs in the signal.
The vertical axis of the pmf contains similar information, except expressed on a fractional basis.
In other words, each value in the histogram is divided by the total number of samples to approximate the pmf.
This means that each value in the pmf must be between zero and one, and that the sum of all of the values in the pmf will be equal to one.
The pmf is important because it describes the probability that a certain value will be generated.
For example, imagine a signal with the pmf of Fig. 2-5b, such as previously shown in Fig. 2-4a.
What is the probability that a sample taken from this signal will have a value of 120? Figure 2-5b provides the answer, 0.03, or about 1 chance in 34.
What is the probability that a randomly chosen sample will have a value greater than 150?
Adding up the values in the pmf for: 151, 152, 153, @@@, 255, provides the answer, 0.0122, or about 1 chance in 82.
Thus, the signal would be expected to have a value exceeding 150 on an average of every 82 points.
What is the probability that any one sample will be between 0 and 255?
Summing all of the values in the pmf produces the probability of 1.00, that is, a certainty that this will occur.
The histogram and pmf can only be used with discrete data, such as a digitized signal residing in a computer.
A similar concept applies to continuous signals, such as voltages appearing in analog electronics.
The probability density function (pdf), also called the probability distribution function, is to continuous signals what the probability mass function is to discrete signals.
For example, imagine an analog signal passing through an analog-to-digital converter, resulting in the digitized signal of Fig. 2-4a.
For simplicity, we will assume that voltages between 0 and 255 millivolts become digitized into digital numbers between 0 and 255.
The pmf of this digital The relationship between (a) the histogram, (b) the probability mass function (pmf), and (c) the probability density function (pdf).
The histogram is calculated from a finite number of samples.
The pmf describes the probabilities of the underlying process.
The pdf is similar to the pmf, but is used with continuous rather than discrete signals.
Even though the vertical axis of (b) and (c) have the same values (0 to 0.06), this is only a coincidence of this example.
The amplitude of these three curves is determined by: (a) the sum of the values in the histogram being equal to the number of samples in the signal; (b) the sum of the values in the pmf being equal to one, and (c) the area under the pdf curve being equal to one.
Probability Density Function (pdf) signal is shown by the markers in Fig. 2-5b.
Similarly, the pdf of the analog signal is shown by the continuous line in (c), indicating the signal can take on a continuous range of values, such as the voltage in an electronic circuit.
The vertical axis of the pdf is in units of probability density, rather than just probability.
For example, a pdf of 0.03 at 120.5 does not mean that the a voltage of 120.5 millivolts will occur 3% of the time.
In fact, the probability of the continuous signal being exactly 120.5 millivolts is infinitesimally small.
This is because there are an infinite number of possible values that the signal needs to divide its time between: 120.49997, 120.49998, 120.49999, etc.
The chance that the signal happens to be exactly 120.50000þ is very remote indeed!
To calculate a probability, the probability density is multiplied by a range of values.
For example, the probability that the signal, at any given instant, will be between the values of 120 and 121 is: (121&120) × 0.03 ' 0.03.
The probability that the signal will be between 120.4 and 120.5 is: (120.5&120.4)
If the pdf is not constant over the range of interest, the multiplication becomes the integral of the pdf over that range.
In other words, the area under the pdf bounded by the specified values.
Since the value of the signal must always be something, the total area under the pdf curve, the integral from &4 to %4, will always be equal to one.
This is analogous to the sum of all of the pmf values being equal to one, and the sum of all of the histogram values being equal to N. The histogram, pmf, and pdf are very similar concepts.
Mathematicians always keep them straight, but you will frequently find them used interchangeably (and therefore, incorrectly) by many scientists and spikes, corresponding to the signal only having two possible values.
The pdf of the triangle wave, (b), has a constant value over a range, and is often called a uniform distribution.
The pdf of random noise, as in (c), is the most interesting of all, a bell shaped curve known as a Gaussian.
Figure 2-6 shows three continuous waveforms and their pdfs.
If these were discrete signals, signified by changing the horizontal axis labeling to "sample number," pmfs would be used.
A problem occurs in calculating the histogram when the number of levels each sample can take on is much larger than the number of samples in the signal.
This is always true for signals represented in floating point notation, where each sample is stored as a fractional value.
For example, integer representation might require the sample value to be 3 or 4, while floating point allows millions of possible fractional values between 3 and 4. The previously described approach for calculating the histogram involves counting the number of samples that have each of the possible quantization levels.
This is not possible with floating point data because there are billions of possible levels that would have to be taken into account.
Even worse, nearly all of these possible levels would have no samples that correspond to them.
For example, imagine a 10,000 sample signal, with each sample having one billion possible values.
The conventional histogram would consist of one billion data points, with all but about 10,000 of them having a value of zero.
The solution to these problems is a technique called binning.
This is done by arbitrarily selecting the length of the histogram to be some convenient number, such as 1000 points, often called bins.
The value of each bin represents the total number of samples in the signal that have a value within a certain range.
For example, imagine a floating point signal that contains values between 0.0 and 10.0, and a histogram with 1000 bins.
Bin 0 in the histogram is the number of samples in the signal with a value between 0 and 0.01, bin 1 is the number of samples with a value between 0.01 and 0.02, and so forth, up to bin 999 containing the number of samples with a value between 9.99 and 10.0.
Table 2-4 presents a program for calculating a binned histogram in this manner.
Example of binned histograms.
As shown in (a), the signal used in this example is samples long, with each sample a floating point number uniformly distributed between 1 and 3. Figures (b) and (c) show binned histograms of this signal, using 601 and 9 bins, respectively.
As shown, a large number of bins results in poor resolution along the vertical axis, while a small number of bins provides poor resolution along the horizontal axis.
Using more samples makes the resolution better in both directions.
How many bins should be used?
This is a compromise between two problems.
As shown in Fig. 2-7, too many bins makes it difficult to estimate the amplitude of the underlying pmf.
This is because only a few samples fall into each bin, making the statistical noise very high.
At the other extreme, too few of bins makes it difficult to estimate the underlying pmf in the horizontal direction.
In other words, the number of bins controls a tradeoff between resolution along the y-axis, and resolution along the x-axis.
Signals formed from random processes usually have a bell shaped pdf.
This is called a normal distribution, a Gauss distribution, or a Gaussian, after the great German mathematician, Karl Friedrich Gauss (1777-1855).
The reason why this curve occurs so frequently in nature will be discussed shortly in conjunction with digital noise generation.
The basic shape of the curve is generated from a negative squared exponent: This raw curve can be converted into the complete Gaussian by adding an adjustable mean, µ, and standard deviation, F. In addition, the equation must be normalized so that the total area under the curve is equal to one, a requirement of all probability distribution functions.
This results in the general form of the normal distribution, one of the most important relations in statistics and probability: Equation for the normal distribution, also called the Gauss distribution, or simply a Gaussian.
In this relation, P(x) is the probability distribution function, µ is the mean, and σ is the standard deviation.
The mean centers the curve over a particular value, while the standard deviation controls the width of the bell shape.
An interesting characteristic of the Gaussian is that the tails drop toward zero very rapidly, much faster than with other common functions such as decaying exponentials or 1/x.
For example, at two, four, and six standard Examples of Gaussian curves.
Figure (a) shows the shape of the raw curve without normalization or the addition of adjustable parameters.
In (b) and (c), the complete Gaussian curve is shown for various means and standard deviations.
This is why normally distributed signals, such as illustrated in Fig. 2-6c, appear to have an approximate peak-to-peak value.
In principle, signals of this type can experience excursions of unlimited amplitude.
In practice, the sharp drop of the Gaussian pdf dictates that these extremes almost never occur.
This results in the waveform having a relatively bounded appearance with an apparent peakto-peak amplitude of about 6-8F.
As previously shown, the integral of the pdf is used to find the probability that a signal will be within a certain range of values.
This makes the integral of the pdf important enough that it is given its own name, the cumulative distribution function (cdf).
An especially obnoxious problem with the Gaussian is that it cannot be integrated using elementary methods.
To get around this, the integral of the Gaussian can be calculated by numerical integration.
This involves sampling the continuous Gaussian curve very finely, say, a few million points between -10F and +10F.
The samples in this discrete signal are then added to simulate integration.
The discrete curve resulting from this simulated integration is then stored in a table for use in calculating probabilities.
The cdf of the normal distribution is shown in Fig. 2-9, with its numeric values listed in Table 2-5.
Since this curve is used so frequently in probability, it is given its own symbol: M(x) (upper case Greek phi).
For example, M(&2) has a value of 0.0228.
This indicates that there is a 2.28% probability that the value of the signal will be between -4 and two standard deviations below the mean, at any randomly chosen time.
Likewise, the value: M(1) ' 0.8413, means there is an 84.13% chance that the value of the signal, at a randomly selected instant, will be between -4 and one standard deviation above the mean.
To calculate the probability that the signal will be will be between two values, it is necessary to subtract the appropriate numbers found in the M(x) table.
For example, the probability that the value of the signal, at some randomly chosen time, will be between two standard deviations below the mean and one standard deviation above the mean, is given by: M(1) & M(&2) ' 0.8185, or 81.85%
Using this method, samples taken from a normally distributed signal will be within ±1F of the mean about 68% of the time.
They will be within ±2F about 95% of the time, and within ±3F about 99.75% of the time.
The probability of the signal being more than 10 standard deviations from the mean is so minuscule, it would be expected to occur for only a few microseconds since the beginning of the universe, about 10 billion years!
Equation 2-8 can also be used to express the probability mass function of normally distributed discrete signals.
In this case, x is restricted to be one of the quantized levels that the signal can take on, such as one of the binary values exiting a 12 bit analog-to-digital converter.
Ignore the 1/ 2BF term, it is only used to make the total area under the pdf curve equal to one.
Instead, you must include whatever term is needed to make the sum of all the values in the pmf equal to one.
In most cases, this is done by M (x), the cumulative distribution function of the normal distribution (mean = 0, standard deviation = 1).
These values are calculated by numerically integrating the normal distribution shown in Fig. 2-8b.
In words, M (x) is the probability that the value of a normally distributed signal, at some randomly chosen time, will be less than x.
In this table, the value of x is expressed in units of standard deviations referenced to the mean.
Random noise is an important topic in both electronics and DSP.
For example, it limits how small of a signal an instrument can measure, the distance a radio system can communicate, and how much radiation is required to produce an xray image.
A common need in DSP is to generate signals that resemble various types of random noise.
This is required to test the performance of algorithms that must work in the presence of noise.
The heart of digital noise generation is the random number generator.
Most programming languages have this as a standard function.
The BASIC statement: X = RND, loads the variable, X, with a new random number each time the command is encountered.
Each random number has a value between zero and one, with an equal probability of being anywhere between these two extremes.
Figure 2-10a shows a signal formed by taking 128 samples from this type of random number generator.
The mean of the underlying process that generated this signal is 0.5, the standard deviation is 1 / 12 ' 0.29, and the distribution is uniform between zero and one.
Algorithms need to be tested using the same kind of data they will encounter in actual operation.
This creates the need to generate digital noise with a Gaussian pdf.
There are two methods for generating such signals using a random number generator.
Figure 2-10 illustrates the first method.
Figure (b) shows a signal obtained by adding two random numbers to form each sample, i.e., X = RND+RND.
Since each of the random numbers can run from zero to one, the sum can run from zero to two.
The mean is now one, and the standard deviation is 1 / 6 (remember, when independent random signals are added, the variances also add).
As shown, the pdf has changed from a uniform d i s t r i b u t i o n t o a triangular distribution.
That is, the signal spends more of its time around a value of one, with less time spent near zero or two. Figure (c) takes this idea a step further by adding twelve random numbers to produce each sample.
The mean is now six, and the standard deviation is one.
What is most important, the pdf has virtually become a Gaussian.
This procedure can be used to create a normally distributed noise signal with an arbitrary mean and standard deviation.
For each sample in the signal: (1) add twelve random numbers, (2) subtract six to make the mean equal to zero, (3) multiply by the standard deviation desired, and (4) add the desired mean.
Theorem, one of the most important concepts in probability.
In its simplest form, the Central Limit Theorem states that a sum of random numbers becomes normally distributed as more and more of the random numbers are added together.
The Central Limit Theorem does not require the individual random numbers be from any particular distribution, or even that the random numbers be from the same distribution.
The Central Limit Theorem provides the reason why normally distributed signals are seen so widely in nature.
Whenever many different random forces are interacting, the resulting pdf becomes a Gaussian.
In the second method for generating normally distributed random numbers, the random number generator is invoked twice, to obtain R1 and R2.
A normally distributed random number, X, can then be found: Generation of normally distributed random numbers.
R 1 and R 2 are random numbers with a uniform distribution between zero and one.
This results in X being normally distributed with a mean of zero, and a standard deviation of one.
The log is base e, and the cosine is in radians.
X ' (& 2 log R1)1/2 cos(2BR2 ) Just as before, this approach can generate normally distributed random signals with an arbitrary mean and standard deviation.
Take each number generated by this equation, multiply it by the desired standard deviation, and add the desired mean.
Converting a uniform distribution to a Gaussian distribution.
Figure (a) shows a signal where each sample is generated by a random number generator.
As indicated by the pdf, the value of each sample is uniformly distributed between zero and one.
Each sample in (b) is formed by adding two values from the random number generator.
In (c), each sample is created by adding twelve values from the random number generator.
The pdf of (c) is very nearly Gaussian, with a mean of six, and a standard deviation of one.
Random number generators operate by starting with a seed, a number between zero and one.
When the random number generator is invoked, the seed is passed through a fixed algorithm, resulting in a new number between zero and one.
This new number is reported as the random number, and is then internally stored to be used as the seed the next time the random number generator is called.
The algorithm that transforms the seed into the new random number is often of the form: Common algorithm for generating uniformly distributed random numbers between zero and one.
In this method, S is the seed, R is the new random number, and a,b,& c are appropriately chosen constants.
In words, the quantity aS+b is divided by c, and the remainder is taken as R.
In this manner, a continuous sequence of random numbers can be generated, all starting from the same seed.
This allows a program to be run multiple times using exactly the same random number sequences.
If you want the random number sequence to change, most languages have a provision for reseeding the random number generator, allowing you to choose the number first used as the seed.
A common technique is to use the time (as indicated by the system's clock) as the seed, thus providing a new sequence each time the program is run.
From a pure mathematical view, the numbers generated in this way cannot be absolutely random since each number is fully determined by the previous number.
The term pseudo-random is often used to describe this situation.
However, this is not something you should be concerned with.
The sequences generated by random number generators are statistically random to an exceedingly high degree.
It is very unlikely that you will encounter a situation where they are not adequate.
Precision and accuracy are terms used to describe systems and methods that measure, estimate, or predict.
In all these cases, there is some parameter you wish to know the value of.
This is called the true value, or simply, truth.
The method provides a measured value, that you want to be as close to the true value as possible.
Precision and accuracy are ways of describing the error that can exist between these two values.
Unfortunately, precision and accuracy are used interchangeably in non-technical settings.
In fact, dictionaries define them by referring to each other!
In spite of this, science and engineering have very specific definitions for each.
You should make a point of using the terms correctly, and quietly tolerate others when they use them incorrectly.
As an example, consider an oceanographer measuring water depth using a sonar system.
Short bursts of sound are transmitted from the ship, reflected from the ocean floor, and received at the surface as an echo.
Sound waves travel at a relatively constant velocity in water, allowing the depth to be found from the elapsed time between the transmitted and received pulses.
As with all empirical measurements, a certain amount of error exists between the measured and true values.
This particular measurement could be affected by many factors: random noise in the electronics, waves on the ocean surface, plant growth on the ocean floor, variations in the water temperature causing the sound velocity to change, etc.
To investigate these effects, the oceanographer takes many successive readings at a location known to be exactly 1000 meters deep (the true value).
These measurements are then arranged as the histogram shown in Fig. 2-11.
As would be expected from the Central Limit Theorem, the acquired data are normally distributed.
The mean occurs at the center of the distribution, and represents the best estimate of the depth based on all of the measured data.
The standard deviation defines the width of the distribution, describing how much variation occurs between successive measurements.
This situation results in two general types of error that the system can experience.
First, the mean may be shifted from the true value.
The amount of this shift is called the accuracy of the measurement.
Second, individual measurements may not agree well with each other, as indicated by the width of the distribution.
This is called the precision of the measurement, and is expressed by quoting the standard deviation, the signal-to-noise ratio, or the CV.
Consider a measurement that has good accuracy, but poor precision; the histogram is centered over the true value, but is very broad.
Although the measurements are correct as a group, each individual reading is a poor measure of the true value.
This situation is said to have poor repeatability; measurements taken in succession don't agree well.
Poor precision results from random errors.
This is the name given to errors that change each mean true value Definitions of accuracy and precision.
Accuracy is the difference between the true value and the mean of the under-lying process that generates the data.
Precision is the spread of the values, specified by the standard deviation, the signal-to-noise ratio, or the CV.
Averaging several measurements will always improve the precision.
In short, precision is a measure of random noise.
Now, imagine a measurement that is very precise, but has poor accuracy.
This makes the histogram very slender, but not centered over the true value.
Successive readings are close in value; however, they all have a large error.
Poor accuracy results from systematic errors.
These are errors that become repeated in exactly the same manner each time the measurement is conducted.
Accuracy is usually dependent on how you calibrate the system.
For example, in the ocean depth measurement, the parameter directly measured is elapsed time.
This is converted into depth by a calibration procedure that relates milliseconds to meters.
This may be as simple as multiplying by a fixed velocity, or as complicated as dozens of second order corrections.
Averaging individual measurements does nothing to improve the accuracy.
In short, accuracy is a measure of calibration.
In actual practice there are many ways that precision and accuracy can become intertwined.
For example, imagine building an electronic amplifier from 1% resistors.
This tolerance indicates that the value of each resistor will be within 1% of the stated value over a wide range of conditions, such as temperature, humidity, age, etc.
This error in the resistance will produce a corresponding error in the gain of the amplifier.
Is this error a problem of accuracy or precision?
The answer depends on how you take the measurements.
For example, suppose you build one amplifier and test it several times over a few minutes.
The error in gain remains constant with each test, and you conclude the problem is accuracy.
In comparison, suppose you build one thousand of the amplifiers.
The gain from device to device will fluctuate randomly, and the problem appears to be one of precision.
Likewise, any one of these amplifiers will show gain fluctuations in response to temperature and other environmental changes.
Again, the problem would be called precision.
When deciding which name to call the problem, ask yourself two questions.
First: Will averaging successive readings provide a better measurement?
If yes, call the error precision; if no, call it accuracy.
Second: Will calibration correct the error?
If yes, call it accuracy; if no, call it precision.
This may require some thought, especially related to how the device will be calibrated, and how often it will be done.
Most of the signals directly encountered in science and engineering are continuous: light intensity that changes with distance; voltage that varies over time; a chemical reaction rate that depends on temperature, etc. Analog-to-Digital Conversion (ADC) and Digital-to-Analog Conversion (DAC) are the processes that allow digital computers to interact with these everyday signals.
Digital information is different from its continuous counterpart in two important respects: it is sampled, and it is quantized.
Both of these restrict how much information a digital signal can contain.
This chapter is about information management: understanding what information you need to retain, and what information you can afford to lose.
In turn, this dictates the selection of the sampling frequency, number of bits, and type of analog filtering needed for converting between the analog and digital realms.
First, a bit of trivia.
As you know, it is a digital computer, not a digit computer.
The information processed is called digital data, not digit data.
Why then, is analog-to-digital conversion generally called: digitize and digitization, rather than digitalize and digitalization?
The answer is nothing you would expect.
When electronics got around to inventing digital techniques, the preferred names had already been snatched up by the medical community nearly a century before.
Digitalize and digitalization mean to administer the heart stimulant digitalis.
Figure 3-1 shows the electronic waveforms of a typical analog-to-digital conversion.
Figure (a) is the analog signal to be digitized.
As shown by the labels on the graph, this signal is a voltage that varies over time.
To make the numbers easier, we will assume that the voltage can vary from 0 to 4. volts, corresponding to the digital numbers between 0 and 4095 that will be produced by a 12 bit digitizer.
Notice that the block diagram is broken into two sections, the sample-and-hold (S/H), and the analog-to-digital converter (ADC).
As you probably learned in electronics classes, the sample-and-hold is required to keep the voltage entering the ADC constant while the conversion is taking place.
However, this is not the reason it is shown here; breaking the digitization into these two stages is an important theoretical model for understanding digitization.
The fact that it happens to look like common electronics is just a fortunate bonus.
As shown by the difference between (a) and (b), the output of the sample-andhold is allowed to change only at periodic intervals, at which time it is made identical to the instantaneous value of the input signal.
Changes in the input signal that occur between these sampling times are completely ignored.
That is, sampling converts the independent variable (time in this example) from continuous to discrete.
As shown by the difference between (b) and (c), the ADC produces an integer value between 0 and 4095 for each of the flat regions in (b).
This introduces an error, since each plateau can be any voltage between 0 and 4.095 volts.
For example, both 2.56000 volts and 2.56001 volts will be converted into digital number 2560.
In other words, quantization converts the dependent variable (voltage in this example) from continuous to discrete.
Notice that we carefully avoid comparing (a) and (c), as this would lump the sampling and quantization together.
It is important that we analyze them separately because they degrade the signal in different ways, as well as being controlled by different parameters in the electronics.
There are also cases where one is used without the other.
For instance, sampling without quantization is used in switched capacitor filters.
First we will look at the effects of quantization.
Any one sample in the digitized signal can have a maximum error of ±½ LSB (Least Significant Bit, jargon for the distance between adjacent quantization levels).
Figure (d) shows the quantization error for this particular example, found by subtracting (b) from (c), with the appropriate conversions.
In other words, the digital output (c), is equivalent to the continuous input (b), plus a quantization error (d).
An important feature of this analysis is that the quantization error appears very much like random noise.
This sets the stage for an important model of quantization error.
In most cases, quantization results in nothing more than the addition of a specific amount of random noise to the signal.
The additive noise is uniformly distributed between ±½ LSB, has a mean of zero, and a standard deviation of 1/ 12 LSB (-0.29 LSB).
For example, passing an analog signal through an 8 bit digitizer adds an rms noise of: 0.29 /256, or about 1/900 of the full scale value.
A bit conversion adds a noise of: 0.29 /4096 . 1 /14,000, while a 16 bit conversion adds: 0.29 /65536 . 1 /227,000 .
Since quantization error is a random noise, the number of bits determines the precision of the data.
For example, you might make the statement: "We increased the precision of the measurement from 8 to 12 bits."
This model is extremely powerful, because the random noise generated by quantization will simply add to whatever noise is already present in the Waveforms illustrating the digitization process.
The conversion is broken into two stages to allow the effects of sampling to be separated from the effects of quantization.
The first stage is the sample-and-hold (S/H), where the only information retained is the instantaneous value of the signal when the periodic sampling takes place.
In the second stage, the ADC converts the voltage to the nearest integer number.
This results in each sample in the digitized signal having an error of up to ±½ LSB, as shown in (d).
As b.
Sampled analog signal analog signal.
For example, imagine an analog signal with a maximum amplitude of 1.0 volt, and a random noise of 1.0 millivolt rms.
Digitizing this signal to 8 bits results in 1.0 volt becoming digital number 255, and 1. millivolt becoming 0.255 LSB.
As discussed in the last chapter, random noise signals are combined by adding their variances.
That is, the signals are added in quadrature: A 2 %B 2 ' C .
The total noise on the digitized signal is therefore given by: 0.2552 % 0.292 ' 0.386 LSB.
This is an increase of about 50% over the noise already in the analog signal.
Digitizing this same signal to 12 bits would produce virtually no increase in the noise, and nothing would be lost due to quantization.
When faced with the decision of how many bits are needed in a system, ask two questions: (1) How much noise is already present in the analog signal?
When isn't this model of quantization valid?
Only when the quantization error cannot be treated as random.
The only common occurrence of this is when the analog signal remains at about the same value for many consecutive samples, as is illustrated in Fig. 3-2a.
The output remains stuck on the same digital number for many samples in a row, even though the analog signal may be changing up to ±½ LSB.
Instead of being an additive random noise, the quantization error now looks like a thresholding effect or weird distortion.
Dithering is a common technique for improving the digitization of these slowly varying signals.
As shown in Fig. 3-2b, a small amount of random noise is added to the analog signal.
In this example, the added noise is normally distributed with a standard deviation of 2/3 LSB, resulting in a peakto-peak amplitude of about 3 LSB.
Figure (c) shows how the addition of this dithering noise has affected the digitized signal.
Even when the original analog signal is changing by less than ±½ LSB, the added noise causes the digital output to randomly toggle between adjacent levels.
To understand how this improves the situation, imagine that the input signal is a constant analog voltage of 3.0001 volts, making it one-tenth of the way between the digital levels 3000 and 3001.
Without dithering, taking 10,000 samples of this signal would produce 10,000 identical numbers, all having the value of 3000.
Next, repeat the thought experiment with a small amount of dithering noise added.
The 10,000 values will now oscillate between two (or more) levels, with about 90% having a value of 3000, and 10% having a value of 3001.
Taking the average of all 10,000 values results in something close to 3000.1.
Even though a single measurement has the inherent ±½ LSB limitation, the statistics of a large number of the samples can do much better.
This is quite a strange situation: adding noise provides more information.
Circuits for dithering can be quite sophisticated, such as using a computer to generate random numbers, and then passing them through a DAC to produce the added noise.
After digitization, the computer can subtract Illustration of dithering.
Figure (a) shows how an analog signal that varies less than ±½ LSB can become stuck on the same quantization level during digitization.
Dithering improves this situation by adding a small amount of random noise to the analog signal, such as shown in (b).
In this example, the added noise is normally distributed with a standard deviation of 2/3 LSB.
As shown in (c), the added noise causes the digitized signal to toggle between adjacent quantization levels, providing more information about the original signal.
Millivolts (or digital number) Millivolts (or digital number) c.
Digitization of dithered signal original analog signal digital signal Time (or sample number) the random numbers from the digital signal using floating point arithmetic.
This elegant technique is called subtractive dither, but is only used in the most elaborate systems.
The simplest method, although not always possible, is to use the noise already present in the analog signal for dithering.
The Sampling Theorem The definition of proper sampling is quite simple.
Suppose you sample a continuous signal in some manner.
If you can exactly reconstruct the analog signal from the samples, you must have done the sampling properly.
Even if the sampled data appears confusing or incomplete, the key information has been captured if you can reverse the process.
Figure 3-3 shows several sinusoids before and after digitization.
The continuous line represents the analog signal entering the ADC, while the square markers are the digital signal leaving the ADC.
In (a), the analog signal is a constant DC value, a cosine wave of zero frequency.
Since the analog signal is a series of straight lines between each of the samples, all of the information needed to reconstruct the analog signal is contained in the digital data.
According to our definition, this is proper sampling.
The sine wave shown in (b) has a frequency of 0.09 of the sampling rate.
This might represent, for example, a 90 cycle/second sine wave being sampled at 1000 samples/second.
Expressed in another way, there are 11.1 samples taken over each complete cycle of the sinusoid.
This situation is more complicated than the previous case, because the analog signal cannot be reconstructed by simply drawing straight lines between the data points.
Do these samples properly represent the analog signal?
The answer is yes, because no other sinusoid, or combination of sinusoids, will produce this pattern of samples (within the reasonable constraints listed below).
These samples correspond to only one analog signal, and therefore the analog signal can be exactly reconstructed.
Again, an instance of proper sampling.
In (c), the situation is made more difficult by increasing the sine wave's frequency to 0.31 of the sampling rate.
This results in only 3.2 samples per sine wave cycle.
Here the samples are so sparse that they don't even appear to follow the general trend of the analog signal.
Do these samples properly represent the analog waveform?
Again, the answer is yes, and for exactly the same reason.
The samples are a unique representation of the analog signal.
All of the information needed to reconstruct the continuous waveform is contained in the digital data.
How you go about doing this will be discussed later in this chapter.
Obviously, it must be more sophisticated than just drawing straight lines between the data points.
As strange as it seems, this is proper sampling according to our definition.
In (d), the analog frequency is pushed even higher to 0.95 of the sampling rate, with a mere 1.05 samples per sine wave cycle.
Do these samples properly represent the data?
No, they don't!
The samples represent a different sine wave from the one contained in the analog signal.
In particular, the original sine wave of 0.95 frequency misrepresents itself as a sine wave of 0.05 frequency in the digital signal.
This phenomenon of sinusoids changing frequency during sampling is called aliasing.
Just as a criminal might take on an assumed name or identity (an alias), the sinusoid assumes another frequency that is not its own.
Since the digital data is no longer uniquely related to a particular analog signal, an unambiguous reconstruction is impossible.
There is nothing in the sampled data to suggest that the original analog signal had a frequency of 0. rather than 0.05.
The sine wave has hidden its true identity completely; the perfect crime has been committed!
According to our definition, this is an example of improper sampling.
This line of reasoning leads to a milestone in DSP, the sampling theorem.
Frequently this is called the Shannon sampling theorem, or the Nyquist sampling theorem, after the authors of 1940s papers on the topic.
The sampling theorem indicates that a continuous signal can be properly sampled, only if it does not contain frequency components above one-half of the sampling rate.
For instance, a sampling rate of 2,000 samples/second requires the analog signal to be composed of frequencies below 1000 cycles/second.
If frequencies above this limit are present in the signal, they will be aliased to frequencies between 0 and 1000 cycles/second, combining with whatever information that was legitimately there.
Illustration of proper and improper sampling.
A continuous signal is sampled properly if the samples contain all the information needed to recreate the original waveform.
Figures (a), (b), and (c) illustrate proper sampling of three sinusoidal waves.
This is certainly not obvious, since the samples in (c) do not even appear to capture the shape of the waveform.
Nevertheless, each of these continuous signals forms a unique one-to-one pair with its pattern of samples.
This guarantees that reconstruction can take place.
In (d), the frequency of the analog sine wave is greater than the Nyquist frequency (one-half of the sampling rate).
This results in aliasing, where the frequency of the sampled data is different from the frequency of the continuous signal.
Since aliasing has corrupted the information, the original signal cannot be reconstructed from the samples.
Two terms are widely used when discussing the sampling theorem: the Nyquist frequency and the Nyquist rate.
Unfortunately, their meaning is not standardized.
To understand this, consider an analog signal composed of frequencies between DC and 3 kHz.
To properly digitize this signal it must be sampled at 6,000 samples/sec (6 kHz) or higher.
Suppose we choose to sample at 8,000 samples/sec (8 kHz), allowing frequencies between DC and kHz to be properly represented.
In this situation there are four important frequencies: (1) the highest frequency in the signal, 3 kHz; (2) twice this frequency, 6 kHz; (3) the sampling rate, 8 kHz; and (4) one-half the sampling rate, 4 kHz.
Which of these four is the Nyquist frequency and which is the Nyquist rate?
It depends who you ask!
All of the possible combinations are used.
Fortunately, most authors are careful to define how they are using the terms.
In this book, they are both used to mean one-half the sampling rate.
Figure 3-4 shows how frequencies are changed during aliasing.
The key point to remember is that a digital signal cannot contain frequencies above one-half the sampling rate (i.e., the Nyquist frequency/rate).
When the frequency of the continuous wave is below the Nyquist rate, the frequency of the sampled data is a match.
However, when the continuous signal's frequency is above the Nyquist rate, aliasing changes the frequency into something that can be represented in the sampled data.
As shown by the zigzagging line in Fig. 3-4, every continuous frequency above the Nyquist rate has a corresponding digital frequency between zero and one-half the sampling rate.
If there happens to be a sinusoid already at this lower frequency, the aliased signal will add to it, resulting in a loss of information.
Aliasing is a double curse; information can be lost about the higher and the lower frequency.
Suppose you are given a digital signal containing a frequency of 0.2 of the sampling rate.
If this signal were obtained by proper sampling, the original analog signal must have had a frequency of 0.2.
If aliasing took place during sampling, the digital frequency of 0.2 could have come from any one of an infinite number of frequencies in the analog signal: 0.2, 0.8, 1.2, 1.8, 2.2, þ .
Just as aliasing can change the frequency during sampling, it can also change the phase.
For example, look back at the aliased signal in Fig. 3-3d.
The aliased digital signal is inverted from the original analog signal; one is a sine wave while the other is a negative sine wave.
In other words, aliasing has changed the frequency and introduced a 180E phase shift.
Only two phase shifts are possible: 0E (no phase shift) and 180E (inversion).
The zero phase shift occurs for analog frequencies of 0 to 0.5, 1.0 to 1.5, 2.0 to 2.5, etc.
An inverted phase occurs for analog frequencies of 0.5 to 1.0, 1.5 to 2.0, 2.5 to 3.0, and so on.
Now we will dive into a more detailed analysis of sampling and how aliasing occurs.
Our overall goal is to understand what happens to the information when a signal is converted from a continuous to a discrete form.
The problem is, these are very different things; one is a continuous waveform while the other is an array of numbers.
This "apples-to-oranges" comparison makes the analysis very difficult.
The solution is to introduce a theoretical concept called the impulse train.
Figure 3-5a shows an example analog signal.
Figure (c) shows the signal sampled by using an impulse train.
The impulse train is a continuous signal consisting of a series of narrow spikes (impulses) that match the original signal at the sampling instants.
Each impulse is infinitesimally narrow, a concept that will be discussed in Chapter 13. Between these sampling times the value of the waveform is zero.
Keep in mind that the impulse train is a theoretical concept, not a waveform that can exist in an electronic circuit.
Since both the original analog signal and the impulse train are continuous waveforms, we can make an "apples-apples" comparison between the two.
Conversion of analog frequency into digital frequency during sampling.
Continuous signals with a frequency less than one-half of the sampling rate are directly converted into the corresponding digital frequency.
Above one-half of the sampling rate, aliasing takes place, resulting in the frequency being misrepresented in the digital data.
Aliasing always changes a higher frequency into a lower frequency between 0 and 0.5.
In addition, aliasing may also change the phase of the signal by degrees.
Now we need to examine the relationship between the impulse train and the discrete signal (an array of numbers).
This one is easy; in terms of information content, they are identical.
If one is known, it is trivial to calculate the other.
Think of these as different ends of a bridge crossing between the analog and digital worlds.
This means we have achieved our overall goal once we understand the consequences of changing the waveform in Fig. 3-5a into the waveform in Fig. 3.5c.
Three continuous waveforms are shown in the left-hand column in Fig. 3-5.
The corresponding frequency spectra of these signals are displayed in the righthand column.
This should be a familiar concept from your knowledge of electronics; every waveform can be viewed as being composed of sinusoids of varying amplitude and frequency.
Later chapters will discuss the frequency domain in detail.
Figure (a) shows an analog signal we wish to sample.
As indicated by its frequency spectrum in (b), it is composed only of frequency components between 0 and about 0.33 fs, where fs is the sampling frequency we intend to use.
For example, this might be a speech signal that has been filtered to remove all frequencies above 3.3 kHz.
Correspondingly, fs would be 10 kHz (10,000 samples/second), our intended sampling rate.
Sampling the signal in (a) by using an impulse train produces the signal shown in (c), and its frequency spectrum shown in (d).
This spectrum is a duplication of the spectrum of the original signal.
Each multiple of the sampling frequency, fs, 2f s, 3f s, 4f s, etc., has received a copy and a left-forright flipped copy of the original frequency spectrum.
The copy is called the upper sideband, while the flipped copy is called the lower sideband.
Sampling has generated new frequencies.
Is this proper sampling?
The answer is yes, because the signal in (c) can be transformed back into the signal in (a) by eliminating all frequencies above ½f s.
That is, an analog low-pass filter will convert the impulse train, (b), back into the original analog signal, (a).
If you are already familiar with the basics of DSP, here is a more technical explanation of why this spectral duplication occurs.
In the time domain, sampling is achieved by multiplying the original signal by an impulse train of unity amplitude spikes.
The frequency spectrum of this unity amplitude impulse train is also a unity amplitude impulse train, with the spikes occurring at multiples of the sampling frequency, f s, 2f s, 3f s, 4f s, etc.
When two time domain signals are multiplied, their frequency spectra are convolved.
This results in the original spectrum being duplicated to the location of each spike in the impulse train's spectrum.
Viewing the original signal as composed of both positive and negative frequencies accounts for the upper and lower sidebands, respectively.
This is the same as amplitude modulation, discussed in Chapter 10. Figure (e) shows an example of improper sampling, resulting from too low of sampling rate.
The analog signal still contains frequencies up to 3. kHz, but the sampling rate has been lowered to 5 kHz.
Notice that fS, 2f S, 3fS þ along the horizontal axis are spaced closer in (f) than in (d).
The frequency spectrum, (f), shows the problem: the duplicated portions of the spectrum have invaded the band between zero and one-half of the sampling frequency.
Although (f) shows these overlapping frequencies as retaining their separate identity, in actual practice they add together forming a single confused mess.
Since there is no way to separate the overlapping frequencies, information is lost, and the original signal cannot be reconstructed.
This overlap occurs when the analog signal contains frequencies greater than one-half the sampling rate, that is, we have proven the sampling theorem.
Digital-to-Analog Conversion In theory, the simplest method for digital-to-analog conversion is to pull the samples from memory and convert them into an impulse train.
This is The sampling theorem in the time and frequency domains.
Figures (a) and (b) show an analog signal composed of frequency components between zero and 0.33 of the sampling frequency, fs.
In (c), the analog signal is sampled by converting it to an impulse train.
In the frequency domain, (d), this results in the spectrum being duplicated into an infinite number of upper and lower sidebands.
Since the original frequencies in (b) exist undistorted in (d), proper sampling has taken place.
In comparison, the analog signal in (e) is sampled at 0. of the sampling frequency, a value exceeding the Nyquist rate.
This results in aliasing, indicated by the sidebands in (f) overlapping.
As just described, the original analog signal can be perfectly reconstructed by passing this impulse train through a low-pass filter, with the cutoff frequency equal to one-half of the sampling rate.
In other words, the original signal and the impulse train have identical frequency spectra below the Nyquist frequency (one-half the sampling rate).
At higher frequencies, the impulse train contains a duplication of this information, while the original analog signal contains nothing (assuming aliasing did not occur).
While this method is mathematically pure, it is difficult to generate the required narrow pulses in electronics.
To get around this, nearly all DACs operate by holding the last value until another sample is received.
This is called a zeroth-order hold, the DAC equivalent of the sample-and-hold used during ADC.
The zeroth-order hold produces the staircase appearance shown in (c).
In the frequency domain, the zeroth-order hold results in the spectrum of the impulse train being multiplied by the dark curve shown in (d), given by the equation: EQUATION 3- High frequency amplitude reduction due to the zeroth-order hold.
This curve is plotted in Fig. 3-6d.
The sampling frequency is represented by fS .
For f ' 0, H ( f ) ' 1 .
Bf /f s This is of the general form: sin (Bx) /(Bx), called the sinc function or sinc(x).
The sinc function is very common in DSP, and will be discussed in more detail in later chapters.
If you already have a background in this material, the zerothorder hold can be understood as the convolution of the impulse train with a rectangular pulse, having a width equal to the sampling period.
This results in the frequency domain being multiplied by the Fourier transform of the rectangular pulse, i.e., the sinc function.
In Fig. (d), the light line shows the frequency spectrum of the impulse train (the "correct" spectrum), while the dark line shows the sinc.
The frequency spectrum of the zeroth order hold signal is equal to the product of these two curves.
The analog filter used to convert the zeroth-order hold signal, (c), into the reconstructed signal, (f), needs to do two things: (1) remove all frequencies above one-half of the sampling rate, and (2) boost the frequencies by the reciprocal of the zeroth-order hold's effect, i.e., 1/sinc(x).
This amounts to an amplification of about 36% at one-half of the sampling frequency.
Figure (e) shows the ideal frequency response of this analog filter.
This 1/sinc(x) frequency boost can be handled in four ways: (1) ignore it and accept the consequences, (2) design an analog filter to include the 1/sinc(x) Analysis of digital-to-analog conversion.
In (a), the digital data are converted into an impulse train, with the spectrum in (b).
This is changed into the reconstructed signal, (f), by using an electronic low-pass filter to remove frequencies above one-half the sampling rate [compare (b) and (g)].
However, most electronic DACs create a zeroth-order hold waveform, (c), instead of an impulse train.
The spectrum of the zeroth-order hold is equal to the spectrum of the impulse train multiplied by the sinc function shown in (d).
To convert the zeroth-order hold into the reconstructed signal, the analog filter must remove all frequencies above the Nyquist rate, and correct for the sinc, as shown in (e).
Before leaving this section on sampling, we need to dispel a common myth about analog versus digital signals.
As this chapter has shown, the amount of information carried in a digital signal is limited in two ways: First, the number of bits per sample limits the resolution of the dependent variable.
That is, small changes in the signal's amplitude may be lost in the quantization noise.
Second, the sampling rate limits the resolution of the independent variable, i.e., closely spaced events in the analog signal may be lost between the samples.
This is another way of saying that frequencies above one-half the sampling rate are lost.
Here is the myth: "Since analog signals use continuous parameters, they have infinitely good resolution in both the independent and the dependent variables."
Not true!
Analog signals are limited by the same two problems as digital signals: noise and bandwidth (the highest frequency allowed in the signal).
The noise in an analog signal limits the measurement of the waveform's amplitude, just as quantization noise does in a digital signal.
Likewise, the ability to separate closely spaced events in an analog signal depends on the highest frequency allowed in the waveform.
To understand this, imagine an analog signal containing two closely spaced pulses.
If we place the signal through a low-pass filter (removing the high frequencies), the pulses will blur into a single blob.
For instance, an analog signal formed from frequencies between DC and 10 kHz will have exactly the same resolution as a digital signal sampled at 20 kHz.
It must, since the sampling theorem guarantees that the two contain the same information.
Analog Filters for Data Conversion Figure 3-7 shows a block diagram of a DSP system, as the sampling theorem dictates it should be.
Before encountering the analog-to-digital converter, antialias filter Analog electronic filters used to comply with the sampling theorem.
The electronic filter placed before an ADC is called an antialias filter.
It is used to remove frequency components above one-half of the sampling rate that would alias during the sampling.
The electronic filter placed after a DAC is called a reconstruction filter.
It also eliminates frequencies above the Nyquist rate, and may include a correction for the zeroth-order hold.
This is done to prevent aliasing during sampling, and is correspondingly called an antialias filter.
On the other end, the digitized signal is passed through a digital-to-analog converter and another low-pass filter set to the Nyquist frequency.
This output filter is called a reconstruction filter, and may include the previously described zeroth-order-hold frequency boost.
Unfortunately, there is a serious problem with this simple model: the limitations of electronic filters can be as bad as the problems they are trying to prevent.
If your main interest is in software, you are probably thinking that you don't need to read this section.
Wrong!
Even if you have vowed never to touch an oscilloscope, an understanding of the properties of analog filters is important for successful DSP.
First, the characteristics of every digitized signal you encounter will depend on what type of antialias filter was used when it was acquired.
If you don't understand the nature of the antialias filter, you cannot understand the nature of the digital signal.
Second, the future of DSP is to replace hardware with software.
For example, the multirate techniques presented later in this chapter reduce the need for antialias and reconstruction filters by fancy software tricks.
If you don't understand the hardware, you cannot design software to replace it.
Third, much of DSP is related to digital filter design.
A common strategy is to start with an equivalent analog filter, and convert it into software.
Later chapters assume you have a basic knowledge of analog filter techniques.
Three types of analog filters are commonly used: Chebyshev, Butterworth, and Bessel (also called a Thompson filter).
Each of these is designed to optimize a different performance parameter.
The complexity of each filter can be adjusted by selecting the number of poles and zeros, mathematical terms that will be discussed in later chapters.
The more poles in a filter, the more electronics it requires, and the better it performs.
Each of these names describe what the filter does, not a particular arrangement of resistors and capacitors.
For example, a six pole Bessel filter can be implemented by many different types of circuits, all of which have the same overall characteristics.
For DSP purposes, the characteristics of these filters are more important than how they are constructed.
Nevertheless, we will start with a short segment on the electronic design of these filters to provide an overall framework.
Figure 3-8 shows a common building block for analog filter design, the modified Sallen-Key circuit.
This is named after the authors of a 1950s paper describing the technique.
The circuit shown is a two pole low-pass filter that can be configured as any of the three basic types.
Table 3-1 provides the necessary information to select the appropriate resistors and capacitors.
For example, to design a 1 kHz, 2 pole Butterworth filter, Table 3-1 provides the parameters: k1 = 0.1592 and k2 = 0.586.
Arbitrarily selecting R1 = 10K and C = 0.01uF (common values for op amp circuits), R and Rf can be calculated as 15.95K and 5.86K, respectively.
Rounding these last two values to the nearest 1% standard resistors, results in R = 15.8K and Rf = 5.90K
All of the components should be 1% precision or better.
The particular op amp used isn't critical, as long as the unity gain frequency is more than 30 to 100 times higher than the filter's cutoff frequency.
This is an easy requirement as long as the filter's cutoff frequency is below about kHz.
Four, six, and eight pole filters are formed by cascading 2,3, and 4 of these circuits, respectively.
For example, Fig. 3-9 shows the schematic of a 6 pole A six pole Bessel filter formed by cascading three Sallen-Key circuits.
This is a low-pass filter with a cutoff frequency of 1 kHz.
Bessel filter created by cascading three stages.
Each stage has different values for k1 and k2 as provided by Table 3-1, resulting in different resistors and capacitors being used.
Need a high-pass filter?
Simply swap the R and C components in the circuits (leaving Rf and R1 alone).
This type of circuit is very common for small quantity manufacturing and R&D applications; however, serious production requires the filter to be made as an integrated circuit.
The problem is, it is difficult to make resistors directly in silicon.
The answer is the switched capacitor filter.
Figure 3-10 illustrates its operation by comparing it to a simple RC network.
If a step function is fed into an RC low-pass filter, the output rises exponentially until it matches the input.
The voltage on the capacitor doesn't change instantaneously, because the resistor restricts the flow of electrical charge.
The switched capacitor filter operates by replacing the basic resistorcapacitor network with two capacitors and an electronic switch.
The newly added capacitor is much smaller in value than the already existing capacitor, say, 1% of its value.
The switch alternately connects the small capacitor between the input and the output at a very high frequency, typically 100 times faster than the cutoff frequency of the filter.
When the switch is connected to the input, the small capacitor rapidly charges to whatever voltage is presently on the input.
When the switch is connected to the output, the charge on the small capacitor is transferred to the large capacitor.
In a resistor, the rate of charge transfer is determined by its resistance.
In a switched capacitor circuit, the rate of charge transfer is determined by the value of the small capacitor and by the switching frequency.
This results in a very useful feature of switched capacitor Switched capacitor filter operation.
Switched capacitor filters use switches and capacitors to mimic resistors.
As shown by the equivalent step responses, two capacitors and one switch can perform the same function as a resistor-capacitor network.
This makes the switched capacitor filter ideal for data acquisition systems that operate with more than one sampling rate.
These are easy-to-use devices; pay ten bucks and have the performance of an eight pole filter inside a single 8 pin IC.
Now for the important part: the characteristics of the three classic filter types.
The first performance parameter we want to explore is cutoff frequency sharpness.
A low-pass filter is designed to block all frequencies above the cutoff frequency (the stopband), while passing all frequencies below (the passband).
Figure 3-11 shows the frequency response of these three filters on a logarithmic (dB) scale.
These graphs are shown for filters with a one hertz cutoff frequency, but they can be directly scaled to whatever cutoff frequency you need to use.
How do these filters rate?
The Chebyshev is clearly the best, the Butterworth is worse, and the Bessel is absolutely ghastly!
As you probably surmised, this is what the Chebyshev is designed to do, roll-off (drop in amplitude) as rapidly as possible.
Unfortunately, even an 8 pole Chebyshev isn't as good as you would like for an antialias filter.
For example, imagine a 12 bit system sampling at 10, samples per second.
The sampling theorem dictates that any frequency above 5 kHz will be aliased, something you want to avoid.
With a little guess work, you decide that all frequencies above 5 kHz must be reduced in amplitude by a factor of 100, insuring that any aliased frequencies will have an amplitude of less than one percent.
Looking at Fig. 3-11c, you find that an 8 pole Chebyshev filter, with a cutoff frequency of 1 hertz, doesn't reach an attenuation (signal reduction) of 100 until about 1.35 hertz.
Scaling this to the example, the filter's cutoff frequency must be set to 3.7 kHz so that everything above 5 kHz will have the required attenuation.
This results in the frequency band between 3.7 kHz and 5 kHz being wasted on the inadequate roll-off of the analog filter.
A subtle point: the attenuation factor of 100 in this example is probably sufficient even though there are 4096 steps in 12 bits.
From Fig. 3-4, hertz will alias to 4900 hertz, 6000 hertz will alias to 4000 hertz, etc.
You don't care what the amplitudes of the signals between 5000 and 6300 hertz are, because they alias into the unusable region between 3700 hertz and 5000 hertz.
In order for a frequency to alias into the filter's passband (0 to 3.7 kHz), it must be greater than 6300 hertz, or 1.7 times the filter's cutoff frequency of 3700 hertz.
As shown in Fig. 3-11c, the attenuation provided by an 8 pole Chebyshev filter at 1.7 times the cutoff frequency is about 1300, much more adequate than the 100 we started the analysis with.
The moral to this story: In most systems, the frequency band between about 0.4 and 0.5 of the sampling frequency is an unusable wasteland of filter roll-off and aliased signals.
This is a direct result of the limitations of analog filters.
The frequency response of the perfect low-pass filter is flat across the entire passband.
All of the filters look great in this respect in Fig. 3-11, but only because the vertical axis is displayed on a logarithmic scale.
Another story is told when the graphs are converted to a linear vertical scale, as is shown Frequency response of the three filters on a linear scale.
The Butterworth filter provides the flattest passband.
Passband ripple can now be seen in the Chebyshev filter (wavy variations in the amplitude of the passed frequencies).
In fact, the Chebyshev filter obtains its excellent roll-off by allowing this passband ripple.
When more passband ripple is allowed in a filter, a faster roll-off can be achieved.
All the Chebyshev filters designed by using Table 3-1 have a passband ripple of about 6% (0.5 dB), a good compromise, and a common choice.
A similar design, the elliptic filter, allows ripple in both the passband and the stopband.
Although harder to design, elliptic filters can achieve an even better tradeoff between roll-off and passband ripple.
In comparison, the Butterworth filter is optimized to provide the sharpest rolloff possible without allowing ripple in the passband.
It is commonly called the maximally flat filter, and is identical to a Chebyshev designed for zero passband ripple.
The Bessel filter has no ripple in the passband, but the rolloff is far worse than the Butterworth.
The last parameter to evaluate is the step response, how the filter responds when the input rapidly changes from one value to another.
Figure 3-13 shows the step response of each of the three filters.
The horizontal axis is shown for filters with a 1 hertz cutoff frequency, but can be scaled (inversely) for higher cutoff frequencies.
For example, a 1000 hertz cutoff frequency would show a step response in milliseconds, rather than seconds.
The Butterworth and Chebyshev filters overshoot and show ringing (oscillations that slowly decreasing in amplitude).
In comparison, the Bessel filter has neither of these nasty problems.
Step response of the three filters.
The times shown on the horizontal axis correspond to a one hertz cutoff frequency.
The Bessel is the optimum filter when overshoot and ringing must be minimized.
Pulse response of the Bessel and Chebyshev filters.
A key property of the Bessel filter is that the rising and falling edges in the filter's output looking similar.
In the jargon of the field, this is called linear phase.
Figure (b) shows the result of passing the pulse waveform in (a) through a pole Bessel filter.
Both edges are smoothed in a similar manner.
Figure (c) shows the result of passing (a) through a 4 pole Chebyshev filter.
The left edge overshoots on the top, while the right edge overshoots on the bottom.
Many applications cannot tolerate this distortion.
Figure 3-14 further illustrates this very favorable characteristic of the Bessel filter.
Figure (a) shows a pulse waveform, which can be viewed as a rising step followed by a falling step.
Figures (b) and (c) show how this waveform would appear after Bessel and Chebyshev filters, respectively.
If this were a video signal, for instance, the distortion introduced by the Chebyshev filter would be devastating!
The overshoot would change the brightness of the edges of objects compared to their centers.
Worse yet, the left side of objects would look bright, while the right side of objects would look dark.
Many applications cannot tolerate poor performance in the step response.
This is where the Bessel filter shines; no overshoot and symmetrical edges.
Selecting The Antialias Filter Table 3-2 summarizes the characteristics of these three filters, showing how each optimizes a particular parameter at the expense of everything else.
The Chebyshev optimizes the roll-off, the Butterworth optimizes the passband flatness, and the Bessel optimizes the step response.
The selection of the antialias filter depends almost entirely on one issue: how information is represented in the signals you intend to process.
While Characteristics of the three classic filters.
The Bessel filter provides the best step response, making it the choice for time domain encoded signals.
The Chebyshev and Butterworth filters are used to eliminate frequencies in the stopband, making them ideal for frequency domain encoded signals.
Values in this table are in the units of seconds and hertz, for a one hertz cutoff frequency.
The difference between these two is critical in DSP, and will be a reoccurring theme throughout this book.
In frequency domain encoding, the information is contained in sinusoidal waves that combine to form the signal.
Audio signals are an excellent example of this.
When a person hears speech or music, the perceived sound depends on the frequencies present, and not on the particular shape of the waveform.
This can be shown by passing an audio signal through a circuit that changes the phase of the various sinusoids, but retains their frequency and amplitude.
The resulting signal looks completely different on an oscilloscope, but sounds identical.
The pertinent information has been left intact, even though the waveform has been significantly altered.
Since aliasing misplaces and overlaps frequency components, it directly destroys information encoded in the frequency domain.
Consequently, digitization of these signals usually involves an antialias filter with a sharp cutoff, such as a Chebyshev, Elliptic, or Butterworth.
What about the nasty step response of these filters?
It doesn't matter; the encoded information isn't affected by this type of distortion.
In contrast, time domain encoding uses the shape of the waveform to store information.
For example, physicians can monitor the electrical activity of a person's heart by attaching electrodes to their chest and arms (an electrocardiogram or EKG).
The shape of the EKG waveform provides the information being sought, such as when the various chambers contract during a heartbeat.
Images are another example of this type of signal.
Rather than a waveform that varies over time, images encode information in the shape of a waveform that varies over distance.
Pictures are formed from regions of brightness and color, and how they relate to other regions of brightness and color.
You don't look at the Mona Lisa and say, "My, what an interesting collection of sinusoids."
Here's the problem: The sampling theorem is an analysis of what happens in the frequency domain during digitization.
This makes it ideal to under-stand the analog-to-digital conversion of signals having their information encoded in the frequency domain.
However, the sampling theorem is little help in understanding how time domain encoded signals should be digitized.
Let's take a closer look.
Figure 3-15 illustrates the choices for digitizing a time domain encoded signal.
Figure (a) is an example analog signal to be digitized.
In this case, the information we want to capture is the shape of the rectangular pulses.
A short burst of a high frequency sine wave is also included in this example signal.
This represents wideband noise, interference, and similar junk that always appears on analog signals.
The other figures show how the digitized signal would appear with different antialias filter options: a Chebyshev filter, a Bessel filter, and no filter.
It is important to understand that none of these options will allow the original signal to be reconstructed from the sampled data.
This is because the original signal inherently contains frequency components greater than one-half of the sampling rate.
Since these frequencies cannot exist in the digitized signal, the reconstructed signal cannot contain them either.
These high frequencies result from two sources: (1) noise and interference, which you would like to eliminate, and (2) sharp edges in the waveform, which probably contain information you want to retain.
The Chebyshev filter, shown in (b), attacks the problem by aggressively removing all high frequency components.
This results in a filtered analog signal that can be sampled and later perfectly reconstructed.
However, the reconstructed analog signal is identical to the filtered signal, not the original signal.
Although nothing is lost in sampling, the waveform has been severely distorted by the antialias filter.
As shown in (b), the cure is worse than the disease!
Don't do it!
The Bessel filter, (c), is designed for just this problem.
Its output closely resembles the original waveform, with only a gentle rounding of the edges.
By adjusting the filter's cutoff frequency, the smoothness of the edges can be traded for elimination of high frequency components in the signal.
Using more poles in the filter allows a better tradeoff between these two parameters.
A common guideline is to set the cutoff frequency at about one-quarter of the sampling frequency.
This results in about two samples along the rising portion of each edge.
Notice that both the Bessel and the Chebyshev filter have removed the burst of high frequency noise present in the original signal.
The last choice is to use no antialias filter at all, as is shown in (d).
This has the strong advantage that the value of each sample is identical to the value of the original analog signal.
In other words, it has perfect edge sharpness; a change in the original signal is immediately mirrored in the digital data.
The disadvantage is that aliasing can distort the signal.
This takes two different forms.
First, high frequency interference and noise, such as the example sinusoidal burst, will turn into meaningless samples, as shown in (d).
That is, any high frequency noise present in the analog signal will appear as aliased noise in the digital signal.
In a more general sense, this is not a problem of the sampling, but a problem of the upstream analog electronics.
It is not the ADC's purpose to reduce noise and interference; this is the responsibility of the analog electronics before the digitization takes place.
It may turn out that a Bessel filter should be placed before the digitizer to control this problem.
However, this means the filter should be viewed as part of the analog processing, not something that is being done for the sake of the digitizer.
The second manifestation of aliasing is more subtle.
When an event occurs in the analog signal (such as an edge), the digital signal in (d) detects the change on the next sample.
There is no information in the digital data to indicate what happens between samples.
Now, compare using no filter with using a Bessel filter for this problem.
For example, imagine drawing straight lines between the samples in (c).
The time when this constructed line crosses one-half the amplitude of the step provides a subsample estimate of when the edge occurred in the analog signal.
When no filter is used, this subsample information is completely lost.
You don't need a fancy theorem to evaluate how this will affect your particular situation, just a good understanding of what you plan to do with the data once is it acquired.
Multirate Data Conversion There is a strong trend in electronics to replace analog circuitry with digital algorithms.
Data conversion is an excellent example of this.
Consider the design of a digital voice recorder, a system that will digitize a voice signal, store the data in digital form, and later reconstruct the signal for playback.
To recreate intelligible speech, the system must capture the frequencies between about 100 and 3000 hertz.
However, the analog signal produced by the microphone also contains much higher frequencies, say to 40 kHz.
The brute force approach is to pass the analog signal through an eight pole low-pass Chebyshev filter at 3 kHz, and then sample at 8 kHz.
On the other end, the DAC reconstructs the analog signal at 8 kHz with a zeroth order hold.
Another Chebyshev filter at 3 kHz is used to produce the final voice signal.
Three antialias filter options for time domain encoded signals.
The goal is to eliminate high frequencies (that will alias during sampling), while simultaneously retaining edge sharpness (that carries information).
Figure (a) shows an example analog signal containing both sharp edges and a high frequency noise burst.
Figure (b) shows the digitized signal using a Chebyshev filter.
While the high frequencies have been effectively removed, the edges have been grossly distorted.
This is usually a terrible solution.
The Bessel filter, shown in (c), provides a gentle edge smoothing while removing the high frequencies.
Figure (d) shows the digitized signal using no antialias filter.
In this case, the edges have retained perfect sharpness; however, the high frequency burst has aliased into several meaningless samples.
There are many useful benefits in sampling faster than this direct analysis.
For example, imagine redesigning the digital voice recorder using a 64 kHz sampling rate.
The antialias filter now has an easier task: pass all freq-uencies below 3 kHz, while rejecting all frequencies above 32 kHz.
A similar simplification occurs for the reconstruction filter.
In short, the higher sampling rate allows the eight pole filters to be replaced with simple resistor-capacitor (RC) networks.
The problem is, the digital system is now swamped with data from the higher sampling rate.
The next level of sophistication involves multirate techniques, using more than one sampling rate in the same system.
It works like this for the digital voice recorder example.
First, pass the voice signal through a simple RC low- pass filter and sample the data at 64 kHz.
The resulting digital data contains the desired voice band between 100 and 3000 hertz, but also has an unusable band between 3 kHz and 32 kHz.
Second, remove these unusable frequencies in software, by using a digital low-pass filter at 3 kHz.
Third, resample the digital signal from 64 kHz to 8 kHz by simply discarding every seven out of eight samples, a procedure called decimation.
The resulting digital data is equivalent to that produced by aggressive analog filtering and direct 8 kHz sampling.
Multirate techniques can also be used in the output portion of our example system.
The 8 kHz data is pulled from memory and converted to a 64 kHz sampling rate, a procedure called interpolation.
This involves placing seven samples, with a value of zero, between each of the samples obtained from memory.
The resulting signal is a digital impulse train, containing the desired voice band between 100 and 3000 hertz, plus spectral duplications between kHz and 32 kHz.
Refer back to Figs. 3-6 a&b to understand why this it true.
Everything above 3 kHz is then removed with a digital low-pass filter.
After conversion to an analog signal through a DAC, a simple RC network is all that is required to produce the final voice signal.
Multirate data conversion is valuable for two reasons: (1) it replaces analog components with software, a clear economic advantage in massproduced products, and (2) it can achieve higher levels of performance in critical applications.
For example, compact disc audio systems use techniques of this type to achieve the best possible sound quality.
This increased performance is a result of replacing analog components (1% precision), with digital algorithms (0.0001% precision from round-off error).
As discussed in upcoming chapters, digital filters outperform analog filters by hundreds of times in key areas.
Single Bit Data Conversion A popular technique in telecommunications and high fidelity music reproduction is single bit ADC and DAC.
These are multirate techniques where a higher sampling rate is traded for a lower number of bits.
In the extreme, only a single bit is needed for each sample.
While there are many different circuit configurations, most are based on the use of delta modulation.
Three example circuits will be presented to give you a flavor of the field.
All of these circuits are implemented in IC's, so don't worry where all of the individual transistors and op amps should go.
No one is going to ask you to build one of these circuits from basic components.
Figure 3-16 shows the block diagram of a typical delta modulator.
The analog input is a voice signal with an amplitude of a few volts, while the output signal is a stream of digital ones and zeros.
A comparator decides which has the greater voltage, the incoming analog signal, or the voltage stored on the capacitor.
This decision, in the form of a digital one or zero, is applied to the input of the latch.
At each clock pulse, typically at a few hundred kilohertz, the latch transfers whatever digital state appears on its Block diagram of a delta modulation circuit.
The input voltage is compared with the voltage stored on the capacitor, resulting in a digital zero or one being applied to the input of the latch.
The output of the latch is updated in synchronization with the clock, and used in a feedback loop to cause the capacitor voltage to track the input voltage.
This latch insures that the output is synchronized with the clock, thereby defining the sampling rate, i.e., the rate at which the 1 bit output can update itself.
A feedback loop is formed by taking the digital output and using it to drive an electronic switch.
If the output is a digital one, the switch connects the capacitor to a positive charge injector.
This is a very loose term for a circuit that increases the voltage on the capacitor by a fixed amount, say 1 millivolt per clock cycle.
This may be nothing more than a resistor connected to a large positive voltage.
If the output is a digital zero, the switch is connected to a negative charge injector.
This decreases the voltage on the capacitor by the same fixed amount.
Figure 3-17 illustrates the signals produced by this circuit.
At time equal zero, the analog input and the voltage on the capacitor both start with a voltage of zero.
As shown in (a), the input signal suddenly increases to 9. volts on the eighth clock cycle.
Since the input signal is now more positive than the voltage on the capacitor, the digital output changes to a one, as shown in (b).
This results in the switch being connected to the positive charge injector, and the voltage on the capacitor increasing by a small amount on each clock cycle.
Although an increment of 1 volt per clock cycle is shown in (a), this is only for illustration, and a value of 1 millivolt is more typical.
This staircase increase in the capacitor voltage continues until it exceeds the voltage of the input signal.
Here the system reached an equilibrium with the output oscillating between a digital one and zero, causing the voltage on the capacitor to oscillate between 9 volts and volts.
In this manner, the feedback of the circuit forces the capacitor voltage to track the voltage of the input signal.
If the input signal changes very rapidly, the voltage on the capacitor changes at a constant rate until a match is obtained.
This constant rate of change is called the slew rate, just as in other electronic devices such as op amps.
Now, consider the characteristics of the delta modulated output signal.
If the analog input is increasing in value, the output signal will consist of more ones than zeros.
Likewise, if the analog input is decreasing in value, the output will consist of more zeros than ones.
If the analog input is constant, the digital output will alternate between zero and one with an equal number of each.
Put in more general terms, the relative number of ones versus zeros is directly proportional to the slope (derivative) of the analog input.
This circuit is a cheap method of transforming an analog signal into a serial stream of ones and zeros for transmission or digital storage.
An especially attractive feature is that all the bits have the same meaning, unlike the conventional serial format: start bit, LSB, @ @ @, MSB, stop bit.
The circuit at the receiver is identical to the feedback portion of the transmitting circuit.
Just as the voltage on the capacitor in the transmitting circuit follows the analog input, so does the voltage on the capacitor in the receiving circuit.
That is, the capacitor voltage shown in (a) also represents how the reconstructed signal would appear.
A critical limitation of this circuit is the unavoidable tradeoff between (1) maximum slew rate, (2) quantization size, and (3) data rate.
In particular, if the maximum slew rate and quantization size are adjusted to acceptable values for voice communication, the data rate ends up in the MHz range.
This is too high to be of commercial value.
For instance, conventional sampling of a voice signal requires only about 64,000 bits per second.
A solution to this problem is shown in Fig. 3-18, the Continuously Variable Slope Delta (CVSD) modulator, a technique implemented in the Motorola MC3518 family.
In this approach, the clock rate and the quantization size are set to something acceptable, say 30 kHz, and 2000 levels.
This results in a terrible slew rate, which you correct with additional circuitry.
In operation, a shift resister continually looks at the last four bits that the system has produced.
If the circuit is in a slew rate limited condition, the last four bits will be all ones (positive slope) or all zeros (negative slope).
A logic circuit detects this situation and produces an analog signal that increases the level of charge produced by the charge injectors.
This boosts the slew rate by increasing the size of the voltage steps being applied to the capacitor.
An analog filter is usually placed between the logic circuitry and the charge injectors.
This allows the step size to depend on how long the circuit has been in a slew limited condition.
As long as the circuit is slew limited, the step size keeps getting larger and larger.
This is often called a syllabic filter, since its characteristics depend on the average length of the syllables making up speech.
With proper optimization (from the chip manufacturer's Example of signals produced by the delta modulator in Fig. 3-16.
Figure (a) shows the analog input signal, and the corresponding voltage on the capacitor.
Figure (b) shows the delta modulated output, a digital stream of ones and zeros.
The continually changing step size makes the digital data difficult to understand, but fortunately, you don't need to.
At the receiver, the analog signal is reconstructed by incorporating a syllabic filter that is identical to the one in the transmission circuit.
If the two filters are matched, little distortion results from the CVSD modulation.
CVSD is probably the easiest way to digitally transmit a voice signal.
While CVSD modulation is great for encoding voice signals, it cannot be used for general purpose analog-to-digital conversion.
Even if you get around the fact that the digital data is related to the derivative of the input signal, the changing step size will confuse things beyond repair.
In addition, the DC level of the analog signal is usually not captured in the digital data.
The delta-sigma converter, shown in Fig. 3-19, eliminates these problems by cleverly combining analog electronics with DSP algorithms.
Notice that the voltage on the capacitor is now being compared with ground potential.
The feedback loop has also been modified so that the voltage on the CVSD modulation block diagram.
A logic circuit is added to the basic delta modulator to improve the slew rate.
As the input signal increases and decreases in voltage, it tries to raise and lower the voltage on the capacitor.
This change in voltage is detected by the comparator, resulting in the charge injectors producing a counteracting charge to keep the capacitor at zero volts.
If the input voltage is positive, the digital output will be composed of more ones than zeros.
The excess number of ones being needed to generate the negative charge that cancels with the positive input signal.
Likewise, if the input voltage is negative, the digital output will be composed of more zeros than ones, providing a net positive charge injection.
If the input signal is equal to zero volts, an equal number of ones and zeros will be generated in the output, providing an overall charge injection of zero.
The relative number of ones and zeros in the output is now related to the level of the input voltage, not the slope as in the previous circuit.
This is much simpler.
For instance, you could form a 12 bit ADC by feeding the digital output into a counter, and counting the number of ones over 4096 clock cycles.
A digital number of 4095 would correspond to the maximum positive input voltage.
Likewise, digital number 0 would correspond to the maximum negative input voltage, and 2048 would correspond to an input voltage of zero.
This also shows the origin of the name, delta-sigma: delta modulation followed by summation (sigma).
The ones and zeros produced by this type of delta modulator are very easy to transform back into an analog signal.
All that is required is an analog lowpass filter, which might be as simple as a single RC network.
The high Block diagram of a delta-sigma analog-to-digital converter.
In the simplest case, the pulses from a delta modulator are counted for a predetermined number of clock cycles.
The output of the counter is then latched to complete the conversion.
In a more sophisticated circuit, the pulses are passed through a digital low-pass filter and then resampled (decimated) to a lower sampling rate.
For example, suppose that the ones and zeros are represented by 5 volts and 0 volts, respectively.
If 80% of the bits in the data stream are ones, and 20% are zeros, the output of the low-pass filter will be 4 volts.
This method of transforming the single bit data stream back into the original waveform is important for several reasons.
First, it describes a slick way to replace the counter in the delta-sigma ADC circuit.
Instead of simply counting the pulses from the delta modulator, the binary signal is passed through a digital low-pass filter, and then decimated to reduce the sampling rate.
For example, this procedure might start by changing each of the ones and zeros in the digital stream into a 12 bit sample; ones become a value of 4095, while zeros become a value of 0. Using a digital low-pass filter on this signal produces a digitized version of the original waveform, just as an analog lowpass filter would form an analog recreation.
Decimation then reduces the sampling rate by discarding most of the samples.
This results in a digital signal that is equivalent to direct sampling of the original waveform.
This approach is used in many commercial ADC's for digitizing voice and other audio signals.
An example is the National Semiconductor ADC16071, which provides 16 bit analog-to-digital conversion at sampling rates up to 192 kHz.
At a sampling rate of 100 kHz, the delta modulator operates with a clock frequency of 6.4 MHz.
The low-pass digital filter is a 246 point FIR, such as described in Chapter 16.
This removes all frequencies in the digital data above 50 kHz, ½ of the eventual sampling rate.
Conceptually, this can be viewed as forming a digital signal at 6.4 MHz, with each sample represented by 16 bits.
The signal is then decimated from 6.4 MHz to 100 kHz, accomplished by deleting every 63 out of 64 samples.
In actual operation, much more goes on inside of this device than described by this simple discussion.
Delta-sigma converters can also be used for digital-to-analog conversion of voice and audio signals.
The digital signal is retrieved from memory, and converted into a delta modulated stream of ones and zeros.
As mentioned above, this single bit signal can easily be changed into the reconstructed analog signal with a simple low-pass analog filter.
As with the antialias filter, usually only a single RC network is required.
This is because the majority of the filtration is handled by the high-performance digital filters.
Delta-sigma ADC's have several quirks that limit their use to specific applications.
For example, it is difficult to multiplex their inputs.
When the input is switched from one signal to another, proper operation is not established until the digital filter can clear itself of data from the previous signal.
Deltasigma converters are also limited in another respect: you don't know exactly when each sample was taken.
Each acquired sample is a composite of the one bit information taken over a segment of the input signal.
This is not a problem for signals encoded in the frequency domain, such as audio, but it is a significant limitation for time domain encoded signals.
To understand the shape of a signal's waveform, you often need to know the precise instant each sample was taken.
Lastly, most of these devices are specifically designed for audio applications, and their performance specifications are quoted accordingly.
For example, a 16 bit ADC used for voice signals does not necessarily mean that each sample has 16 bits of precision.
Much more likely, the manufacturer is stating that voice signals can be digitized to 16 bits of dynamic range.
Don't expect to get a full 16 bits of useful information from this device for general purpose data acquisition.
While these explanations and examples provide an introduction to single bit ADC and DAC, it must be emphasized that they are simplified descriptions of sophisticated DSP and integrated circuit technology.
You wouldn't expect the manufacturer to tell their competitors all the internal workings of their chips, so don't expect them to tell you.
DSP applications are usually programmed in the same languages as other science and engineering tasks, such as: C, BASIC and assembly.
The power and versatility of C makes it the language of choice for computer scientists and other professional programmers.
On the other hand, the simplicity of BASIC makes it ideal for scientists and engineers who only occasionally visit the programming world.
Regardless of the language you use, most of the important DSP software issues are buried far below in the realm of whirling ones and zeros.
This includes such topics as: how numbers are represented by bit patterns, round-off error in computer arithmetic, the computational speed of different types of processors, etc.
This chapter is about the things you can do at the high level to avoid being trampled by the low level internal workings of your computer.
Computer Numbers Digital computers are very proficient at storing and recalling numbers; unfortunately, this process isn't without error.
For example, you instruct your computer to store the number: 1.41421356.
The computer does its best, storing the closest number it can represent: 1.41421354.
In some cases this error is quite insignificant, while in other cases it is disastrous.
As another illustration, a classic computational error results from the addition of two numbers with very different values, for example, 1 and 0.00000001.
We would like the answer to be 1.00000001, but the computer replies with 1.
An understanding of how computers store and manipulate numbers allows you to anticipate and correct these problems before your program spits out meaningless data.
These problems arise because a fixed number of bits are allocated to store each number, usually 8, 16, 32 or 64.
For example, consider the case where eight bits are used to store the value of a variable.
Since there are 2 8 = possible bit patterns, the variable can only take on 256 different values.
This is a fundamental limitation of the situation, and there is nothing we can do about it.
The part we can control is what value we declare each bit pattern to represent.
In the simplest cases, the 256 bit patterns might represent the integers from 0 to 255, 1 to 256, -127 to 128, etc.
In a more unusual scheme, the 256 bit patterns might represent 256 exponentially related numbers: 1, 10, 100, 1000, þ, 10254, 10255.
Everyone accessing the data must understand what value each bit pattern represents.
This is usually provided by an algorithm or formula for converting between the represented value and the corresponding bit pattern, and back again.
While many encoding schemes are possible, only two general formats have become common, fixed point (also called integer numbers) and floating point (also called real numbers).
In this book's BASIC programs, fixed point variables are indicated by the % symbol as the last character in the name, such as: I%, N%, SUM%, etc.
All other variables are floating point, for example: X, Y, MEAN, etc.
When you evaluate the formats presented in the next few pages, try to understand them in terms of their range (the largest and smallest numbers they can represent) and their precision (the size of the gaps between numbers).
Fixed Point (Integers) Fixed point representation is used to store integers, the positive and negative whole numbers: þ &3,&2, &1, 0, 1, 2, 3,þ .
High level programs, such as C and BASIC, usually allocate 16 bits to store each integer.
In the simplest case, the 216 ' 65,536 possible bit patterns are assigned to the numbers 0 through 65,535.
This is called unsigned integer format, and a simplified example is shown in Fig. 4-1 (using only 4 bits per number).
Conversion between the bit pattern and the number being represented is nothing more than changing between base 2 (binary) and base 10 (decimal).
The disadvantage of unsigned integer is that negative numbers cannot be represented.
Offset binary is similar to unsigned integer, except the decimal values are shifted to allow for negative numbers.
In the 4 bit example of Fig. 4-1, the decimal numbers are offset by seven, resulting in the 16 bit patterns corresponding to the integer numbers -7 through 8.
In this same manner, a 16 bit representation would use 32,767 as an offset, resulting in a range between -32,767 and 32,768.
Offset binary is not a standardized format, and you will find other offsets used, such 32,768.
The most important use of offset binary is in ADC and DAC.
For example, the input voltage range of -5v to 5v might be mapped to the digital numbers 0 to 4095, for a 12 bit conversion.
Sign and magnitude is another simple way of representing negative integers.
The far left bit is called the sign bit, and is made a zero for positive numbers, and a one for negative numbers.
The other bits are a standard binary representation of the absolute value of the number.
This results in one wasted bit pattern, since there are two representations for zero, 0000 (positive zero) and 1000 (negative zero).
This encoding scheme results in 16 bit numbers having a range of -32,767 to 32,767.
Common formats for fixed point (integer) representation.
Unsigned integer is a simple binary format, but cannot represent negative numbers.
Offset binary and sign & magnitude allow negative numbers, but they are difficult to implement in hardware.
Two's complement is the easiest to design hardware for, and is the most common format for general purpose computing.
These first three representations are conceptually simple, but difficult to implement in hardware.
Remember, when A=B+C is entered into a computer program, some hardware engineer had to figure out how to make the bit pattern representing B, combine with the bit pattern representing C, to form the bit pattern representing A. Two's complement is the format loved by hardware engineers, and is how integers are usually represented in computers.
To understand the encoding pattern, look first at decimal number zero in Fig. 4-1, which corresponds to a binary zero, 0000.
As we count upward, the decimal number is simply the binary equivalent (0 = 0000, 1 = 0001, 2 = 0010, 3 = 0011, etc.).
Now, remember that these four bits are stored in a register consisting of 4 flip-flops.
If we again start at 0000 and begin subtracting, the digital hardware automatically counts in two's complement: 0 = 0000, -1 = 1111, -2 = 1110, - = 1101, etc.
This is analogous to the odometer in a new automobile.
If driven forward, it changes: 00000, 00001, 00002, 00003, and so on.
When driven backwards, the odometer changes: 00000, 99999, 99998, 99997, etc.
Using 16 bits, two's complement can represent numbers from -32,768 to 32,767.
The left most bit is a 0 if the number is positive or zero, and a 1 if the number is negative.
Consequently, the left most bit is called the sign bit, just as in sign & magnitude representation.
Converting between decimal and two's complement is straightforward for positive numbers, a simple decimal to binary conversion.
For negative numbers, the following algorithm is often used: (1) take the absolute value of the decimal number, (2) convert it to binary, (3) complement all of the bits (ones become zeros and zeros become ones), (4) add 1 to the binary number.
For example: - 1011.
Two's complement is hard for humans, but easy for digital electronics.
Floating Point (Real Numbers) The encoding scheme for floating point numbers is more complicated than for fixed point.
The basic idea is the same as used in scientific notation, where a mantissa is multiplied by ten raised to some exponent.
For instance, 5.4321 × 106, where 5.4321 is the mantissa and 6 is the exponent.
Scientific notation is exceptional at representing very large and very small numbers.
For example: 1.2 × 1050, the number of atoms in the earth, or 2.6 × 10& 23, the distance a turtle crawls in one second, compared to the diameter of our galaxy.
Notice that numbers represented in scientific notation are normalized so that there is only a single nonzero digit left of the decimal point.
This is achieved by adjusting the exponent as needed.
Floating point representation is similar to scientific notation, except everything is carried out in base two, rather than base ten.
While several similar formats are in use, the most common is ANSI/IEEE Std.
This standard defines the format for 32 bit numbers called single precision, as well as 64 bit numbers called double precision.
As shown in Fig. 4-2, the 32 bits used in single precision are divided into three separate groups: bits 0 through 22 form the mantissa, bits 23 through 30 form the exponent, and bit 31 is the sign bit.
These bits form the floating point number, v, by the following relation: EQUATION 4- Equation for converting a bit pattern into a floating point number.
The number is represented by v, S is the value of the sign bit, M is the value of the mantissa, and E is the value of the exponent.
The variable, E, is the number between 0 and 255 represented by the eight exponent bits.
Subtracting 127 from this number allows the exponent term to run from 2& 127 to 2128.
In other words, the exponent is stored in offset binary with an offset of 127.
The mantissa, M, is formed from the 23 bits as a binary fraction.
For example, the decimal fraction: 2.783, is interpreted: 2 % 7/10 % 8/100 % 3/1000 .
The binary fraction: 1.0101, means: 1 % 0/2 % 1/4 % 0/8 % 1/16 .
Floating point numbers are normalized in the same way as scientific notation, that is, there is only one nonzero digit left of the decimal point (called a binary point in Single precision floating point storage format.
The 32 bits are broken into three separate parts, the sign bit, the exponent and the mantissa.
Equations 4-1 and 4-2 shows how the represented number is found from these three parts.
MSB and LSB refer to “most significant bit” and “least significant bit,” respectively.
Since the only nonzero number that exists in base two is 1, the leading digit in the mantissa will always be a 1, and therefore does not need to be stored.
Removing this redundancy allows the number to have an additional one bit of precision.
The 23 stored bits, referred to by the notation: m22, m21, m21,þ, m0, form the mantissa according to: EQUATION 4- Algorithm for converting the bit pattern into the mantissa, M, used in Eq. 4-1.
M ' 1.m22m21m20m19 @@@ m2m1m In other words, M ' 1 % m22 2& 1 % m21 2& 2 % m20 2& 3@@@ .
If bits 0 through 22 are all zeros, M takes on the value of one.
If bits 0 through 22 are all ones, M is just a hair under two, i.e., 2 &2& 23.
Using this encoding scheme, the largest number that can be represented is: ±(2&2& 23) × 2128 ' ±6.8 × 1038.
Likewise, the smallest number that can be represented is: ±1.0 × 2& 127 ' ±5.9 × 10& 39.
The IEEE standard reduces this range slightly to free bit patterns that are assigned special meanings.
In particular, the largest and smallest numbers allowed in the standard are ±3.4 × 1038 and ±1.2 × 10& 38, respectively.
The freed bit patterns allow three special classes of numbers: (1) ±0 is defined as all of the mantissa and exponent bits being zero.
These are lower precision numbers obtained by removing the requirement that the leading digit in the mantissa be a one.
Besides these three special classes, there are bit patterns that are not assigned a meaning, commonly referred to as NANs (Not A Number).
The IEEE standard for double precision simply adds more bits to both the mantissa and exponent.
Of the 64 bits used to store a double precision number, bits 0 through 51 are the mantissa, bits 52 through 62 are the exponent, and bit 63 is the sign bit.
As before, the mantissa is between one and just under two, i.e., M ' 1 % m51 2& 1 % m50 2& 2 % m49 2& 3@@@ .
The 11 exponent bits form a number between 0 and 2047, with an offset of 1023, allowing exponents from 2& to 21024 .
The largest and smallest numbers allowed are ±1.8 × 10308 and ±2.2 × 10& 308, respectively.
These are incredibly large and small numbers!
It is quite uncommon to find an application where single precision is not adequate.
You will probably never find a case where double precision limits what you want to accomplish.
Number Precision The errors associated with number representation are very similar to quantization errors during ADC.
You want to store a continuous range of values; however, you can represent only a finite number of quantized levels.
Every time a new number is generated, after a math calculation for example, it must be rounded to the nearest value that can be stored in the format you are using.
As an example, imagine that you allocate 32 bits to store a number.
Since there are exactly 232 ' 4,294,967,296 different bit patterns possible, you can represent exactly 4,294,967,296 different numbers.
Some programming languages allow a variable called a long integer, stored as 32 bits, fixed point, two's complement.
This means that the 4,294,967,296 possible bit patterns represent the integers between -2,147,483,648 and 2,147,483,647.
In comparison, single precision floating point spreads these 4,294,967,296 bit patterns over the much larger range: &3.4 × 1038 to 3.4 × 1038 .
With fixed point variables, the gaps between adjacent numbers are always exactly one.
In floating point notation, the gaps between adjacent numbers vary over the represented number range.
If we randomly pick a floating point number, the gap next to that number is approximately ten million times smaller than the number itself (to be exact, 2& 24 to 2& 23 times the number).
This is a key concept of floating point notation: large numbers have large gaps between them, while small numbers have small gaps.
Figure 4-3 illustrates this by showing consecutive floating point numbers, and the gaps that separate them.
Examples of the spacing between single precision floating point numbers.
The spacing between adjacent numbers is always between about 1 part in 8 million and 1 part in 17 million of the value of the number.
The program in Table 4-1 illustrates how round-off error (quantization error in math calculations) causes problems in DSP.
Within the program loop, two random numbers are added to the floating point variable X, and then subtracted back out again.
Ideally, this should do nothing.
In reality, the round-off error from each of the arithmetic operations causes the value of X to gradually drift away from its initial value.
This drift can take one of two forms depending on how the errors add together.
If the round-off errors are randomly positive and negative, the value of the variable will randomly increase and decrease.
If the errors are predominately of the same sign, the value of the variable will drift away much more rapidly and uniformly.
TABLE 4- Program for demonstrating floating point error accumulation.
This program initially sets the value of X to 1.000000, and then runs through a loop that should ideally do nothing.
During each loop, two random numbers, A and B, are added to X, and then subtracted back out.
The accumulated error from these additions and subtraction causes X to wander from its initial value.
As Fig. 44 shows, the error may be random or additive.
Accumulation of round-off error in floating point variables.
These curves are generated by the program shown in Table 4-1.
When a floating point variable is repeatedly used in arithmetic operations, accumulated round-off error causes the variable's value to drift.
If the errors are both positive and negative, the value will increase and decrease in a random fashion.
If the round-off errors are predominately of the same sign, the value will change in a much more rapid and uniform manner.
Random error Number of loops Figure 4-4 shows how the variable, X, in this example program drifts in value.
An obvious concern is that additive error is much worse than random error.
This is because random errors tend to cancel with each other, while the additive errors simply accumulate.
The additive error is roughly equal to the round-off error from a single operation, multiplied by the total number of operations.
In comparison, the random error only increases in proportion to the square root of the number of operations.
As shown by this example, additive error can be hundreds of times worse than random error for common DSP algorithms.
Unfortunately, it is nearly impossible to control or predict which of these two behaviors a particular algorithm will experience.
For example, the program in Table 4-1 generates an additive error.
This can be changed to a random error by merely making a slight modification to the numbers being added and subtracted.
In particular, the random error curve in Fig. 4-4 was generated by defining: A ' EXP(RND) and B ' EXP(RND), rather than: A ' RND and B ' RND .
Instead of A and B being randomly distributed numbers between 0 and 1, they become exponentially distributed values between 1 and 2.718.
Even this small change is sufficient to toggle the mode of error accumulation.
Since we can't control which way the round-off errors accumulate, keep in mind the worse case scenario.
Expect that every single precision number will have an error of about one part in forty million, multiplied by the number of operations it has been through.
This is based on the assumption of additive error, and the average error from a single operation being one-quarter of a quantization level.
Through the same analysis, every double precision number has an error of about one part in forty quadrillion, multiplied by the number of operations.
Comparison of floating point and integer variables for loop control.
The left hand program controls the FOR-NEXT loop with a floating point variable, X.
This results in an accumulated round-off error of 0.000133 by the end of the program, causing the last loop value, X = 10.0, to be omitted.
In comparison, the right hand program uses an integer, I%, for the loop index.
This provides perfect precision, and guarantees that the proper number of loop cycles will be completed.
Table 4-2 illustrates a particularly annoying problem of round-off error.
Each of the two programs in this table perform the same task: printing numbers equally spaced between 0 and 10.
The left-hand program uses the floating point variable, X, as the loop index.
When instructed to execute a loop, the computer begins by setting the index variable to the starting value of the loop (0 in this example).
At the end of each loop cycle, the step size (0.01 in the case) is added to the index.
A decision is then made: are more loops cycles required, or is the loop completed?
The loop ends when the computer finds that the value of the index is greater than the termination value (in this example, 10.0).
As shown by the generated output, round-off error in the additions cause the value of X to accumulate a significant discrepancy over the course of the loop.
In fact, the accumulated error prevents the execution of the last loop cycle.
Instead of X having a value of 10.0 on the last cycle, the error makes the last value of X equal to 10.000133.
Since X is greater than the termination value, the computer thinks its work is done, and the loop prematurely ends.
This missing last value is a common bug in many computer programs.
In comparison, the program on the right uses an integer variable, I%, to control the loop.
The addition, subtraction, or multiplication of two integers always produces another integer.
This means that fixed point notation has absolutely no round-off error with these operations.
Integers are ideal for controlling loops, as well as other variables that undergo multiple mathematical operations.
The last loop cycle is guaranteed to execute!
Unless you have some strong motivation to do otherwise, always use integers for loop indexes and counters.
A useful fact to remember: single precision floating point has an exact binary representation for every whole number between ±16.8 million (to be exact, ±224).
Above this value, the gaps between the levels are larger than one, causing some whole number values to be missed.
This allows floating point whole numbers (between ±16.8 million) to be added, subtracted and multiplied, with no round-off error.
Execution Speed: Program Language DSP programming can be loosely divided into three levels of sophistication: Assembly, Compiled, and Application Specific.
To understand the difference between these three, we need to start with the very basics of digital electronics.
All microprocessors are based around a set of internal binary registers, that is, a group of flip-flops that can store a series of ones and zeros.
For example, the 8088 microprocessor, the core of the original IBM PC, has four general purpose registers, each consisting of 16 bits.
These are identified by the names: AX, BX, CX, and DX.
There are also nine additional registers with special purposes, called: SI, DI, SP, BP, CS, DS, SS, ES, and IP.
For example, IP, the Instruction Pointer, keeps track of where in memory the next instruction resides.
Suppose you write a program to add the numbers: 1234 and 4321.
When the program begins, IP contains the address of a section of memory that contains a pattern of ones and zeros, as shown in Table 4-3.
Although it looks meaningless to most humans, this pattern of ones and zeros contains all of the commands and data required to complete the task.
For example, when the microprocessor encounters the bit pattern: 00000011 11000011, it interpreters it as a command to take the 16 bits stored in the BX register, add them in binary to the 16 bits stored in the AX register, and store the result in the AX register.
This level of programming is called machine code, and is only a hair above working with the actual electronic circuits.
Since working in binary will eventually drive even the most patient engineer crazy, these patterns of ones and zeros are assigned names according to the function they perform.
This level of programming is called assembly, and an example is shown in Table 4-4.
Although an assembly program is much easier to understand, it is fundamentally the same as programming in TABLE 4- A machine code program for adding and 4321.
This is the lowest level of programming: direct manipulation of the digital electronics.
For example: ADD AX, BX translates to: 00000011 11000011.
A program called an assembler is used to convert the assembly code in Table 4- (called the source code) into the patterns of ones and zeros shown in Table 4-3 (called the object code or executable code).
This executable code can be directly run on the microprocessor.
Obviously, assembly programming requires an extensive understanding of the internal construction of the particular microprocessor you intend to use.
An assembly program for adding 1234 and 4321.
An assembler is a program that converts an assembly program into machine code.
MOV CX, MOV DS:[0],CX Assembly programming involves the direct manipulation of the digital electronics: registers, memory locations, status bits, etc.
The next level of sophistication can manipulate abstract variables without any reference to the particular hardware.
These are called compiled or high-level languages.
A dozen or so are in common use, such as: C, BASIC, FORTRAN, PASCAL, APL, COBOL, LISP, etc. Table 4-5 shows a BASIC program for adding and 4321.
The programmer only knows about the variables A, B, and C, and nothing about the hardware.
TABLE 4- A BASIC program for adding and 4321.
A compiler is a program that converts this type of high-level source code into machine code.
C = A+B A program called a compiler is used to transform the high-level source code directly into machine code.
This requires the compiler to assign hardware memory locations to each of the abstract variables being referenced.
For example, the first time the compiler encounters the variable A in Table 4- (line 100), it understands that the programmer is using this symbol to mean a single precision floating point variable.
Correspondingly, the compiler designates four bytes of memory that will be used for nothing but to hold the value of this variable.
Each subsequent time that an A appears in the program, the computer knows to update the value of the four bytes as needed.
The compiler also breaks complicated mathematical expressions, such as: Y = LOG(XCOS(Z)), into more basic arithmetic.
Microprocessors only know how to add, subtract, multiply and divide.
Anything more complicated must be done as a series of these four elementary operations.
High-level languages isolate the programmer from the hardware.
This makes the programming much easier and allows the source code to be transported between different types of microprocessors.
Most important, the programmer who uses a compiled language needs to know nothing about the internal workings of the computer.
Another programmer has assumed this responsibility, the one who wrote the compiler.
Most compilers operate by converting the entire program into machine code before it is executed.
An exception to this is a type of compiler called an interpreter, of which interpreter BASIC is the most common example.
An interpreter converts a single line of source code into machine code, executes that machine code, and then goes on to the next line of source code.
This provides an interactive environment for simple programs, although the execution speed is extremely slow (think a factor of 100).
The highest level of programming sophistication is found in applications packages for DSP.
These come in a variety of forms, and are often provided to support specific hardware.
Suppose you buy a newly developed DSP microprocessor to embed in your current project.
These devices often have lots of built-in features for DSP: analog inputs, analog outputs, digital I/O, antialias and reconstruction filters, etc.
The question is: how do you program it?
In the worst case, the manufacturer will give you an assembler, and expect you to learn the internal architecture of the device.
In a more typical scenario, a C compiler will be provided, allowing you to program without being bothered by how the microprocessor actually operates.
In the best case, the manufacturer will provide a sophisticated software package to help in the programming: libraries of algorithms, prewritten routines for I/O, debugging tools, etc.
You might simply connect icons to form the desired system in an easy-to-use graphical display.
The things you manipulate are signal pathways, algorithms for processing signals, analog I/O parameters, etc.
When you are satisfied with the design, it is transformed into suitable machine code for execution in the hardware.
Other types of applications packages are used with image processing, spectral analysis, instrumentation and control, digital filter design, etc.
This is the shape of the future.
The distinction between these three levels can be very fuzzy.
For example, most complied languages allow you to directly manipulate the hardware.
Likewise, a high-level language with a well stocked library of DSP functions is very close to being an applications package.
The point of these three catagories is understand what you are manipulating: (1) hardware, (2) abstract variables, or (3) entire procedures and algorithms.
There is also another important concept behind these classifications.
When you use a high-level language, you are relying on the programmer who wrote the compiler to understand the best techniques for hardware manipulation.
Similarly, when you use an applications package, you are relying on the programmer who wrote the package to understand the best DSP techniques.
Here's the rub: these programmers have never seen the particular problem you are dealing with.
Therefore, they cannot always provide you with an optimal solution.
As you operate on a higher level, expect that the final machine code will be less efficient in terms of memory usage, speed, and precision.
Which programming language should you use?
That depends on who you are and what you plan to do.
Most computer scientists and programmers use C (or the more advanced C++).
Power, flexibility, modularity; C has it all.
C is so popular, the question becomes: Why would anyone program their DSP application in something other than C? Three answers come to mind.
First, DSP has grown so rapidly that some organizations and individuals are stuck in the mode of other languages, such as FORTRAN and PASCAL.
This is especially true of military and government agencies that are notoriously slow to change.
Second, some applications require the utmost efficiency, only achievable by assembly programming.
This falls into the category of "a little more speed for a lot more work."
Third, C is not an especially easy language to master, especially for part time programmers.
This includes a wide range of engineers and scientists who occasionally need DSP techniques to assist in their research or design activities.
This group often turns to BASIC because of its simplicity.
Why was BASIC chosen for this book?
This book is about algorithms, not programming style.
You should be concentrating on DSP techniques, and not be distracted by the quirks of a particular language.
For instance, all the programs in this book have line numbers.
This makes it easy to describe how the program operates: "line 100 does such-and-such, line 110 does this-and that," etc.
Of course, you will probably never use line numbers in your actual programs.
The point is, learning DSP has different requirements than using DSP.
There are many books on the market that provide exquisite source code for DSP algorithms.
If you are simply looking for prewritten code to copy into your program, you are in the wrong place.
Comparing the execution speed of hardware or software is a thankless task; no matter what the result, the loser will cry that the match was unfair!
Programmers who like high-level languages (such as traditional computer scientists), will argue that assembly is only 50% faster than compiled code, but five times more trouble.
Those who like assembly (typically, scientists and hardware engineers) will claim the reverse: assembly is five times faster, but only 50% more difficult to use.
As in most controversies, both sides can provide selective data to support their claims.
As a rule-of-thumb, expect that a subroutine written in assembly will be between 1.5 and 3.0 times faster than the comparable high-level program.
The only way to know the exact value is to write the code and conduct speed tests.
Since personal computers are increasing in speed about 40% every year, writing a routine in assembly is equivalent to about a two year jump in hardware technology.
Most professional programmers are rather offended at the idea of using assembly, and gag if you suggest BASIC.
Their rational is quite simple: assembly and BASIC discourage the use of good software practices.
Good code should be portable (able to move from one type of computer to another), modular (broken into a well defined subroutine structure), and easy to understand (lots of comments and descriptive variable names).
The weak structure of assembly and BASIC makes it difficult to achieve these standards.
This is compounded by the fact that the people who are attracted to assembly and BASIC often have little formal training in proper software structure and documentation.
Assembly lovers respond to this attack with a zinger of their own.
Suppose you write a program in C, and your competitor writes the same program in assembly.
The end user's first impression will be that your program is junk because it is twice as slow.
No one would suggest that you write large programs in assembly, only those portions of the program that need rapid execution.
For example, many functions in DSP software libraries are written in assembly, and then accessed from larger programs written in C.
Even the staunchest software purist will use assembly code, as long as they don't have to write it.
Execution Speed: Hardware Computing power is increasing so rapidly, any book on the subject will be obsolete before it is published.
It's an author's nightmare!
The original IBM PC was introduced in 1981, based around the 8088 microprocessor with a 4. MHz clock and an 8 bit data bus.
This was followed by a new generation of personal computers being introduced every 3-4 years: 8088 !
Each of these new systems boosted the computing speed by a factor of about five over the previous technology.
By 1996, the clock speed had increased to 200 MHz, and the data bus to 32 bits.
With other improvements, this resulted in an increase in computing power of nearly one thousand in only 15 years!
You should expect another factor of one thousand in the next 15 years.
The only way to obtain up-to-date information in this rapidly changing field is directly from the manufacturers: advertisements, specification sheets, price lists, etc. Forget books for performance data, look in magazines and your daily newspaper.
Expect that raw computational speed will more than double each two years.
Learning about the current state of computer power is simply not enough; you need to understand and track how it is evolving.
Keeping this in mind, we can jump into an overview of how execution speed is limited by computer hardware.
Since computers are composed of many subsystems, the time required to execute a particular task will depend on two primary factors: (1) the speed of the individual subsystems, and (2) the time it takes to transfer data between these blocks.
Figure 4-5 shows a simplified diagram of the most important speed limiting components in a typical personal computer.
The Central Processing Unit (CPU) is the heart of the system.
As previously described, it consists of a dozen or so registers, each capable of holding 32 bits (in present generation personal computers).
Also included in the CPU is the digital electronics needed for rudimentary operations, such as moving bits around and fixed point arithmetic.
More involved mathematics is handled by transferring the data to a special hardware circuit called a math coprocessor (also called an arithmetic logic unit, or ALU).
The math coprocessor may be contained in the same chip as the CPU, or it may be a separate electronic device.
For example, the addition of two floating point numbers would require the CPU to transfer bytes (4 for each number) to the math coprocessor, and several bytes that describe what to do with the data.
After a short computational time, the math coprocessor would pass four bytes back to the CPU, containing the floating point number that is the sum.
The most inexpensive computer systems don't have a math coprocessor, or provide it only as an option.
For example, the 80486DX microprocessor has an internal math coprocessor, while the 80486SX does not.
These lower performance systems replace hardware with software.
Each of the mathematical functions is broken into Architecture of a typical computer system.
The computational speed is limited by: (1) the speed of the individual subsystems, and (2) the rate at which data can be transferred between these subsystems.
Central Processing Unit (CPU) Main Memory (program and data) elementary binary operations that can be handled directly within the CPU.
While this provides the same result, the execution time is much slower, say, a factor of 10 to 20.
Most personal computer software can be used with or without a math coprocessor.
This is accomplished by having the compiler generate machine code to handle both cases, all stored in the final executable program.
If a math coprocessor is present on the particular computer being used, one section of the code will be run.
If a math coprocessor is not present, the other section of the code will be used.
The compiler can also be directed to generate code for only one of these situations.
For example, you will occasionally find a program that requires that a math coprocessor be present, and will crash if run on a computer that does not have one.
Applications such as word processing usually do not benefit from a math coprocessor.
This is because they involve moving data around in memory, not the calculation of mathematical expressions.
Likewise, calculations involving fixed point variables (integers) are unaffected by the presence of a math coprocessor, since they are handled within the CPU.
On the other hand, the execution speed of DSP and other computational programs using floating point calculations can be an order of magnitude different with and without a math coprocessor.
The CPU and main memory are contained in separate chips in most computer systems.
For obvious reasons, you would like the main memory to be very large and very fast.
Unfortunately, this makes the memory very expensive.
The transfer of data between the main memory and the CPU is a very common bottleneck for speed.
The CPU asks the main memory for the binary information at a particular memory address, and then must wait to receive the information.
A common technique to get around this problem is to use a memory cache.
This is a small amount of very fast memory used as a buffer between the CPU and the main memory.
A few hundred kilobytes is typical.
When the CPU requests the main memory to provide the binary data at a particular address, high speed digital electronics copies a section of the main memory around this address into the memory cache.
The next time that the CPU requests memory information, it is very likely that it will already be contained in the memory cache, making the retrieval very rapid.
This is based on the fact that programs tend to access memory locations that are nearby neighbors of previously accessed data.
In typical personal computer applications, the addition of a memory cache can improve the overall speed by several times.
The memory cache may be in the same chip as the CPU, or it may be an external electronic device.
The rate at which data can be transferred between subsystems depends on the number of parallel data lines provided, and the maximum rate that digital signals that can be passed along each line.
Digital data can generally be transferred at a much higher rate within a single chip as compared to transferring data between chips.
Likewise, data paths that must pass through electrical connectors to other printed circuit boards (i.e., a bus structure) will be slower still.
This is a strong motivation for stuffing as much electronics as possible inside the CPU.
A particularly nasty problem for computer speed is backward compatibility.
When a computer company introduces a new product, say a data acquisition card or a software program, they want to sell it into the largest possible market.
This means that it must be compatible with most of the computers currently in use, which could span several generations of technology.
This frequently limits the performance of the hardware or software to that of a much older system.
For example, suppose you buy an I/O card that plugs into the bus of your MHz Pentium personal computer, providing you with eight digital lines that can transmit and receive data one byte at a time.
You then write an assembly program to rapidly transfer data between your computer and some external device, such as a scientific experiment or another computer.
Much to your surprise, the maximum data transfer rate is only about 100,000 bytes per second, more than one thousand times slower than the microprocessor clock rate!
The villain is the ISA bus, a technology that is backward compatible to the computers of the early 1980s.
Table 4-6 provides execution times for several generations of computers.
Obviously, you should treat these as very rough approximations.
If you want to understand your system, take measurements on your system.
It's quite easy; write a loop that executes a million of some operation, and use your watch to time how long it takes.
The first three systems, the 80286, 80486, and Pentium, are the standard desk-top personal computers of 1986, 1993 and 1996, respectively.
The fourth is a 1994 microprocessor designed especially for DSP tasks, the Texas Instruments TMS320C40.
Measured execution times for various computers.
Times are in microseconds.
The 80286, 80486, and Pentium are three generations of personal computers, while the TMS320C40 is a microprocessor specifically designed for DSP tasks.
All of the personal computers include a math coprocessor.
Use these times only as a general estimate; times on your computer will vary according to the particular hardware and software used.
The Pentium is faster than the 80286 system for four reasons, (1) the greater clock speed, (2) more lines in the data bus, (3) the addition of a memory cache, and (4) a more efficient internal design, requiring fewer clock cycles per instruction.
If the Pentium was a Cadillac, the TMS320C40 would be a Ferrari: less comfort, but blinding speed.
This chip is representative of several microprocessors specifically designed to decrease the execution time of DSP algorithms.
Others in this category are the Intel i860, AT&T DSP3210, Motorola DSP96002, and the Analog Devices ADSP-2171.
These often go by the names: DSP microprocessor, Digital Signal Processor, and RISC (Reduced Instruction Set Computer).
This last name reflects that the increased speed results from fewer assembly level instructions being made available to the programmer.
In comparison, more traditional microprocessors, such as the Pentium, are called CISC (Complex Instruction Set Computer).
DSP microprocessors are used in two ways: as slave modules under the control of a more conventional computer, or as an imbedded processor in a dedicated application, such as a cellular telephone.
Some models only handle fixed point numbers, while others can work with floating point.
The internal architecture used to obtain the increased speed includes: (1) lots of very fast cache memory contained within the chip, (2) separate buses for the program and data, allowing the two to be accessed simultaneously (called a Harvard Architecture), (3) fast hardware for math calculations contained directly in the microprocessor, and (4) a pipeline design.
A pipeline architecture breaks the hardware required for a certain task into several successive stages.
For example, the addition of two numbers may be done in three pipeline stages.
The first stage of the pipeline does nothing but fetch the numbers to be added from memory.
The only task of the second stage is to add the two numbers together.
The third stage does nothing but store the result in memory.
If each stage can complete its task in a single clock cycle, the entire procedure will take three clock cycles to execute.
The key feature of the pipeline structure is that another task can be started before the previous task is completed.
In this example, we could begin the addition of another two numbers as soon as the first stage is idle, at the end of the first clock cycle.
For a large number of operations, the speed of the system will be quoted as one addition per clock cycle, even though the addition of any two numbers requires three clock cycles to complete.
Pipelines are great for speed, but they can be difficult to program.
The algorithm must allow a new calculation to begin, even though the results of previous calculations are unavailable (because they are still in the pipeline).
Chapters 28 and 29 discuss DSP microprocessors in much more detail.
These are amazing devices; their high-power and low-cost will bring DSP to a wide range of consumer and scientific applications.
This is one of the technologies that will shape the twenty-first century.
Execution Speed: Programming Tips While computer hardware and programming languages are important for maximizing execution speed, they are not something you change on a day-to day basis.
In comparison, how you program can be changed at any time, and will drastically affect how long the program will require to execute.
Here are three suggestions.
First, use integers instead of floating point variables whenever possible.
Conventional microprocessors, such as used in personal computers, process integers 10 to 20 times faster than floating point numbers.
On systems without a math coprocessor, the difference can be 200 to 1.
An exception to this is integer division, which is often accomplished by converting the values into floating point.
This makes the operation ghastly slow compared to other integer calculations.
See Table 4-6 for details.
Second, avoid using functions such as: sin(x ), log(x ), y x, etc.
These transcendental functions are calculated as a series of additions, subtractions and multiplications.
For example, the Maclaurin power series provides: sin(x ) ' x & Calculating rarely used functions from more common ones.
All angles are in radians, ATN(X) is the arctangent, LOG(X) is the natural logarithm, SGN(X) is the sign of X (i.e., -1 for X#0, 1 for X>0), EXP(X) is eX.
Another option is to precalculate these slow functions, and store the values in a look-up table (LUT).
For example, imagine an 8 bit data acquisition system used to continually monitor the voltage across a resistor.
If the parameter of interest is the power being dissipated in the resistor, the measured voltage can be used to calculate: P ' V 2/R .
As a faster alternative, the power corresponding to each of the possible 256 voltage measurements can be calculated beforehand, and stored in a LUT.
When the system is running, the measured voltage, a digital number between 0 and 255, becomes an index in the LUT to find the corresponding power.
Look-up tables can be hundreds of times faster than direct calculation.
Third, learn what is fast and what is slow on your particular system.
This comes with experience and testing, and there will always be surprises.
Pay particular attention to graphics commands and I/O.
There are usually several ways to handle these requirements, and the speeds can be tremendously different.
For example, the BASIC command: BLOAD, transfers a data file directly into a section of memory.
Reading the same file into memory byte-bybyte (in a loop) can be 100 times as slow.
As another example, the BASIC command: LINE, can be used to draw a colored box on the video screen.
Drawing the same box pixel-by-pixel can also take 100 times as long.
Even putting a print statement within a loop (to keep track of what it is doing) can slow the operation by thousands!
Most DSP techniques are based on a divide-and-conquer strategy called superposition.
The signal being processed is broken into simple components, each component is processed individually, and the results reunited.
This approach has the tremendous power of breaking a single complicated problem into many easy ones.
Superposition can only be used with linear systems, a term meaning that certain mathematical rules apply.
Fortunately, most of the applications encountered in science and engineering fall into this category.
This chapter presents the foundation of DSP: what it means for a system to be linear, various ways for breaking signals into simpler components, and how superposition provides a variety of signal processing techniques.
Signals and Systems A signal is a description of how one parameter varies with another parameter.
For instance, voltage changing over time in an electronic circuit, or brightness varying with distance in an image.
A system is any process that produces an output signal in response to an input signal.
This is illustrated by the block diagram in Fig. 5-1.
Continuous systems input and output continuous signals, such as in analog electronics.
Discrete systems input and output discrete signals, such as computer programs that manipulate the values stored in arrays.
Several rules are used for naming signals.
These aren't always followed in DSP, but they are very common and you should memorize them.
The mathematics is difficult enough without a clear notation.
First, continuous signals use parentheses, such as: x(t) and y(t), while discrete signals use brackets, as in: x[n] and y[n] .
Second, signals use lower case letters.
Upper case letters are reserved for the frequency domain, discussed in later chapters.
Third, the name given to a signal is usually descriptive of the parameters it represents.
For example, a voltage depending on time might be called: v(t), or a stock market price measured each day could be: p[d] .
Terminology for signals and systems.
A system is any process that generates an output signal in response to an input signal.
Continuous signals are usually represented with parentheses, while discrete signals use brackets.
All signals use lower case letters, reserving the upper case for the frequency domain (presented in later chapters).
Unless there is a better name available, the input signal is called: x(t) or x[n], while the output is called: y(t) or y[n].
Signals and systems are frequently discussed without knowing the exact parameters being represented.
This is the same as using x and y in algebra, without assigning a physical meaning to the variables.
This brings in a fourth rule for naming signals.
If a more descriptive name is not available, the input signal to a discrete system is usually called: x[n], and the output signal: y[n] .
For continuous systems, the signals: x(t) and y(t) are used.
There are many reasons for wanting to understand a system.
For example, you may want to design a system to remove noise in an electrocardiogram, sharpen an out-of-focus image, or remove echoes in an audio recording.
In other cases, the system might have a distortion or interfering effect that you need to characterize or measure.
For instance, when you speak into a telephone, you expect the other person to hear something that resembles your voice.
Unfortunately, the input signal to a transmission line is seldom identical to the output signal.
If you understand how the transmission line (the system) is changing the signal, maybe you can compensate for its effect.
In still other cases, the system may represent some physical process that you want to study or analyze.
Radar and sonar are good examples of this.
These methods operate by comparing the transmitted and reflected signals to find the characteristics of a remote object.
In terms of system theory, the problem is to find the system that changes the transmitted signal into the received signal.
At first glance, it may seem an overwhelming task to understand all of the possible systems in the world.
Fortunately, most useful systems fall into a category called linear systems.
This fact is extremely important.
Without the linear system concept, we would be forced to examine the individual characteristics of many unrelated systems.
With this approach, we can focus on the traits of the linear system category as a whole.
Our first task is to identify what properties make a system linear, and how they fit into the everyday notion of electronics, software, and other signal processing systems.
Requirements for Linearity A system is called linear if it has two mathematical properties: homogeneity (hÇma-gen-~-ity) and additivity.
If you can show that a system has both properties, then you have proven that the system is linear.
Likewise, if you can show that a system doesn't have one or both properties, you have proven that it isn't linear.
A third property, shift invariance, is not a strict requirement for linearity, but it is a mandatory property for most DSP techniques.
When you see the term linear system used in DSP, you should assume it includes shift invariance unless you have reason to believe otherwise.
These three properties form the mathematics of how linear system theory is defined and used.
Later in this chapter we will look at more intuitive ways of understanding linearity.
For now, let's go through these formal mathematical properties.
As illustrated in Fig. 5-2, homogeneity means that a change in the input signal's amplitude results in a corresponding change in the output signal's amplitude.
In mathematical terms, if an input signal of x[n] results in an output signal of y[n], an input of k x[n] results in an output of k y[n], for any input signal and constant, k.
Definition of homogeneity.
A system is said to be homogeneous if an amplitude change in the input results in an identical amplitude change in the output.
That is, if x[n] results in y[n], then kx[n] results in ky[n], for any signal, x[n], and any constant, k.
A simple resistor provides a good example of both homogenous and nonhomogeneous systems.
If the input to the system is the voltage across the resistor, v(t), and the output from the system is the current through the resistor, i(t), the system is homogeneous.
Ohm's law guarantees this; if the voltage is increased or decreased, there will be a corresponding increase or decrease in the current.
Now, consider another system where the input signal is the voltage across the resistor, v(t), but the output signal is the power being dissipated in the resistor, p(t) .
Since power is proportional to the square of the voltage, if the input signal is increased by a factor of two, the output signal is increase by a factor of four.
This system is not homogeneous and therefore cannot be linear.
The property of additivity is illustrated in Fig. 5-3.
Consider a system where an input of x1[n] produces an output of y1[n] .
Further suppose that a different input, x2[n], produces another output, y2[n] .
The system is said to be additive, if an input of x1[n] % x2[n] results in an output of y1[n] % y2[n], for all possible input signals.
In words, signals added at the input produce signals that are added at the output.
Definition of additivity.
A system is said to be additive if added signals pass through it without interacting.
Formally, if x1[n] results in y1[n], and if x2[n] results in y2[n], then x1[n]+x2[n] results in y1[n]+y2[n].
The important point is that added signals pass through the system without interacting.
As an example, think about a telephone conversation with your Aunt Edna and Uncle Bernie.
Aunt Edna begins a rather lengthy story about how well her radishes are doing this year.
In the background, Uncle Bernie is yelling at the dog for having an accident in his favorite chair.
The two voice signals are added and electronically transmitted through the telephone network.
Since this system is additive, the sound you hear is the sum of the two voices as they would sound if transmitted individually.
You hear Edna and Bernie, not the creature, Ednabernie.
A good example of a nonadditive circuit is the mixer stage in a radio transmitter.
Two signals are present: an audio signal that contains the voice or music, and a carrier wave that can propagate through space when applied to an antenna.
The two signals are added and applied to a nonlinearity, such as a pn junction diode.
This results in the signals merging to form a third signal, a modulated radio wave capable of carrying the information over great distances.
As shown in Fig. 5-4, shift invariance means that a shift in the input signal will result in nothing more than an identical shift in the output signal.
In more formal terms, if an input signal of x[n] results in an output of y [n], an input signal of x[n % s] results in an output of y[n % s], for any input signal and any constant, s.
Pay particular notice to how the mathematics of this shift is written, it will be used in upcoming chapters.
By adding a constant, s, to the independent variable, n, the waveform can be advanced or retarded in the horizontal direction.
For example, when s ' 2, the signal is shifted left by two samples; when s ' & 2, the signal is shifted right by two samples.
Definition of shift invariance.
A system is said to be shift invariant if a shift in the input signal causes an identical shift in the output signal.
In mathematical terms, if x[n] produces y[n], then x[n+s] produces y[n+s], for any signal, x[n], and any constant, s.
Shift invariance is important because it means the characteristics of the system do not change with time (or whatever the independent variable happens to be).
If a blip in the input causes a blop in the output, you can be assured that another blip will cause an identical blop.
Most of the systems you encounter will be shift invariant.
This is fortunate, because it is difficult to deal with systems that change their characteristics while in operation.
For example, imagine that you have designed a digital filter to compensate for the degrading effects of a telephone transmission line.
Your filter makes the voices sound more natural and easier to understand.
Much to your surprise, along comes winter and you find the characteristics of the telephone line have changed with temperature.
Your compensation filter is now mismatched and doesn't work especially well.
This situation may require a more sophisticated algorithm that can adapt to changing conditions.
Why do homogeneity and additivity play a critical role in linearity, while shift invariance is something on the side?
This is because linearity is a very broad concept, encompassing much more than just signals and systems.
For example, consider a farmer selling oranges for $2 per crate and apples for $5 per crate.
If the farmer sells only oranges, he will receive $20 for 10 crates, and $40 for 20 crates, making the exchange homogenous.
If he sells 20 crates of oranges and 10 crates of apples, the farmer will receive: 20 ×$2 % 10 ×$5 ' $90 .
This is the same amount as if the two had been sold individually, making the transaction additive.
Being both homogenous and additive, this sale of goods is a linear process.
However, since there are no signals involved, this is not a system, and shift invariance has no meaning.
Shift invariance can be thought of as an additional aspect of linearity needed when signals and systems are involved.
Static Linearity and Sinusoidal Fidelity Homogeneity, additivity, and shift invariance are important because they provide the mathematical basis for defining linear systems.
Unfortunately, these properties alone don't provide most scientists and engineers with an intuitive feeling of what linear systems are about.
The properties of static linearity and sinusoidal fidelity are often of help here.
These are not especially important from a mathematical standpoint, but relate to how humans think about and understand linear systems.
You should pay special attention to this section.
Static linearity defines how a linear system reacts when the signals aren't changing, i.e., when they are DC or static.
The static response of a linear system is very simple: the output is the input multiplied by a constant.
That is, a graph of the possible input values plotted against the corresponding output values is a straight line that passes through the origin.
This is shown in Fig. 5-5 for two common linear systems: Ohm's law for resistors, and Hooke's law for springs.
For comparison, Fig. 5-6 shows the static relationship for two nonlinear systems: a pn junction diode, and the magnetic properties of iron.
Two examples of static linearity.
In (a), Ohm's law: the current through a resistor is equal to the voltage across the resistor divided by the resistance.
In (b), Hooke's law: The elongation of a spring is equal to the applied force multiplied by the spring stiffness coefficient.
All linear systems have the property of static linearity.
The opposite is usually true, but not always.
There are systems that show static linearity, but are not linear with respect to changing signals.
However, a very common class of systems can be completely understood with static linearity alone.
In these systems it doesn't matter if the input signal is static or changing.
These are called memoryless systems, because the output depends only on the present state of the input, and not on its history.
For example, the instantaneous current in a resistor depends only on the instantaneous voltage across it, and not on how the signals came to be the value they are.
If a system has static linearity, and is memoryless, then the system must be linear.
This provides an important way to understand (and prove) the linearity of these simple systems.
Two examples of DC nonlinearity.
In (a), a silicon diode has an exponential relationship between voltage and current.
In (b), the relationship between magnetic intensity, H, and flux density, B, in iron depends on the history of the sample, a behavior called hysteresis.
An important characteristic of linear systems is how they behave with sinusoids, a property we will call sinusoidal fidelity: If the input to a linear system is a sinusoidal wave, the output will also be a sinusoidal wave, and at exactly the same frequency as the input.
Sinusoids are the only waveform that have this property.
For instance, there is no reason to expect that a square wave entering a linear system will produce a square wave on the output.
Although a sinusoid on the input guarantees a sinusoid on the output, the two may be different in amplitude and phase.
This should be familiar from your knowledge of electronics: a circuit can be described by its frequency response, graphs of how the circuit's gain and phase vary with frequency.
Now for the reverse question: If a system always produces a sinusoidal output in response to a sinusoidal input, is the system guaranteed to be linear?
The answer is no, but the exceptions are rare and usually obvious.
For example, imagine an evil demon hiding inside a system, with the goal of trying to mislead you.
The demon has an oscilloscope to observe the input signal, and a sine wave generator to produce an output signal.
When you feed a sine wave into the input, the demon quickly measures the frequency and adjusts his signal generator to produce a corresponding output.
Of course, this system is not linear, because it is not additive.
To show this, place the sum of two sine waves into the system.
The demon can only respond with a single sine wave for the output.
This example is not as contrived as you might think; phase lock loops operate in much this way.
To get a better feeling for linearity, think about a technician trying to determine if an electronic device is linear.
The technician would attach a sine wave generator to the input of the device, and an oscilloscope to the output.
With a sine wave input, the technician would look to see if the output is also a sine wave.
For example, the output cannot be clipped on the top or bottom, the top half cannot look different from the bottom half, there must be no distortion where the signal crosses zero, etc. Next, the technician would vary the amplitude of the input and observe the effect on the output signal.
If the system is linear, the amplitude of the output must track the amplitude of the input.
Lastly, the technician would vary the input signal's frequency, and verify that the output signal's frequency changes accordingly.
As the frequency is changed, there will likely be amplitude and phase changes seen in the output, but these are perfectly permissible in a linear system.
At some frequencies, the output may even be zero, that is, a sinusoid with zero amplitude.
If the technician sees all these things, he will conclude that the system is linear.
While this conclusion is not a rigorous mathematical proof, the level of confidence is justifiably high.
Examples of Linear and Nonlinear Systems Table 5-1 provides examples of common linear and nonlinear systems.
As you go through the lists, keep in mind the mathematician's view of linearity (homogeneity, additivity, and shift invariance), as well as the informal way most scientists and engineers use (static linearity and sinusoidal fidelity).
Examples of Linear Systems Wave propagation such as sound and electromagnetic waves Electrical circuits composed of resistors, capacitors, and inductors Electronic circuits, such as amplifiers and filters Mechanical motion from the interaction of masses, springs, and dashpots (dampeners) Systems described by differential equations such as resistor-capacitor-inductor networks Multiplication by a constant, that is, amplification or attenuation of the signal Signal changes, such as echoes, resonances, and image blurring The unity system where the output is always equal to the input The null system where the output is always equal to the zero, regardless of the input Differentiation and integration, and the analogous operations of first difference and running sum for discrete signals Small perturbations in an otherwise nonlinear system, for instance, a small signal being amplified by a properly biased transistor Convolution, a mathematical operation where each value in the output is expressed as the sum of values in the input multiplied by a set of weighing coefficients.
Recursion, a technique similar to convolution, except previously calculated values in the output are used in addition to values from the input Examples of Nonlinear Systems Systems that do not have static linearity, for instance, the voltage and power in a resistor: P ' V 2R, the radiant energy emission of a hot object depending on its temperature: R ' kT 4, the intensity of light transmitted through a thickness of translucent material: I ' e & "T, etc. Systems that do not have sinusoidal fidelity, such as electronics circuits for: peak detection, squaring, sine wave to square wave conversion, frequency doubling, etc. Common electronic distortion, such as clipping, crossover distortion and slewing Multiplication of one signal by another signal, such as in amplitude modulation and automatic gain controls Hysteresis phenomena, such as magnetic flux density versus magnetic intensity in iron, or mechanical stress versus strain in vulcanized rubber Saturation, such as electronic amplifiers and transformers driven too hard Systems with a threshold, for example, digital logic gates, or seismic vibrations that are strong enough to pulverize the intervening rock Table 5- Examples of linear and nonlinear systems.
Formally, linear systems are defined by the properties of homogeneity, additivity, and shift invariance.
Informally, most scientists and engineers think of linear systems in terms of static linearity and sinusoidal fidelity.
Special Properties of Linearity Linearity is commutative, a property involving the combination of two or more systems.
Figure 5-10 shows the general idea.
Imagine two systems combined in a cascade, that is, the output of one system is the input to the next.
If each system is linear, then the overall combination will also be linear.
The commutative property states that the order of the systems in the cascade can be rearranged without affecting the characteristics of the overall combination.
You probably have used this principle in electronic circuits.
For example, imagine a circuit composed of two stages, one for amplification, and one for filtering.
Which is best, amplify and then filter, or filter and then amplify?
If both stages are linear, the order doesn't make any difference and the overall result is the same.
Keep in mind that actual electronics has nonlinear effects that may make the order important, for instance: interference, DC offsets, internal noise, slew rate distortion, etc.
The commutative property for linear systems.
When two or more linear systems are arranged in a cascade, the order of the systems does not affect the characteristics of the overall combination.
Figure 5-8 shows the next step in linear system theory: multiple inputs and outputs.
A system with multiple inputs and/or outputs will be linear if it is composed of linear subsystems and additions of signals.
The complexity does not matter, only that nothing nonlinear is allowed inside of the system.
To understand what linearity means for systems with multiple inputs and/or outputs, consider the following thought experiment.
Start by placing a signal on one input while the other inputs are held at zero.
This will cause the multiple outputs to respond with some pattern of signals.
Next, repeat the procedure by placing another signal on a different input.
Just as before, keep all of the other inputs at zero.
This second input signal will result in another pattern of signals appearing on the multiple outputs.
To finish the experiment, place both signals on their respective inputs simultaneously.
The signals appearing on the outputs will simply be the superposition (sum) of the output signals produced when the input signals were applied separately.
Any system with multiple inputs and/or outputs will be linear if it is composed of linear systems and signal additions.
The use of multiplication in linear systems is frequently misunderstood.
This is because multiplication can be either linear or nonlinear, depending on what the signal is multiplied by. Figure 5-9 illustrates the two cases.
A system that multiplies the input signal by a constant, is linear.
This system is an amplifier or an attenuator, depending if the constant is greater or less than one, respectively.
In contrast, multiplying a signal by another signal is nonlinear.
Imagine a sinusoid multiplied by another sinusoid of a different frequency; the resulting waveform is clearly not sinusoidal.
Another commonly misunderstood situation relates to parasitic signals added in electronics, such as DC offsets and thermal noise.
Is the addition of these extraneous signals linear or nonlinear?
The answer depends on where the contaminating signals are viewed as originating.
If they are viewed as coming from within the system, the process is nonlinear.
This is because a sinusoidal input does not produce a pure sinusoidal output.
Conversely, the extraneous signal can be viewed as externally entering the system on a separate input of a multiple input system.
This makes the process linear, since only a signal addition is required within the system.
Linearity of multiplication.
Multiplying a signal by a constant is a linear operation.
In contrast, the multiplication of two signals is nonlinear.
Superposition: the Foundation of DSP When we are dealing with linear systems, the only way signals can be combined is by scaling (multiplication of the signals by constants) followed by addition.
For instance, a signal cannot be multiplied by another signal.
Figure 5-10 shows an example: three signals: x0[n], x1[n], and x2[n] are added to form a fourth signal, x [n] .
This process of combining signals through scaling and addition is called synthesis.
Decomposition is the inverse operation of synthesis, where a single signal is broken into two or more additive components.
This is more involved than synthesis, because there are infinite possible decompositions for any given signal.
For example, the numbers 15 and 25 can only be synthesized (added) into the number 40.
In comparison, the number 40 can be decomposed into: 1 % 39 or 2 % 38 or & 30.5 % 60 % 10.5, etc. Now we come to the heart of DSP: superposition, the overall strategy for understanding how signals and systems can be analyzed.
Consider an input Here is the important part: the output signal obtained by this method is identical to the one produced by directly passing the input signal through the system.
This is a very powerful idea.
Instead of trying to understanding how complicated signals are changed by a system, all we need to know is how simple signals are modified.
In the jargon of signal processing, the input and output signals are viewed as a superposition (sum) of simpler waveforms.
This is the basis of nearly all signal processing techniques.
As a simple example of how superposition is used, multiply the number by the number 4, in your head.
How did you do it?
You might have imagined 2041 match sticks floating in your mind, quadrupled the mental image, and started counting.
Much more likely, you used superposition to simplify the problem.
The number 2041 can be decomposed into: 2000 % 40 % 1 .
Each of these components can be multiplied by 4 and then synthesized to find the final answer, i.e., 8000 % 160 % 4 ' 8164 .
Common Decompositions Keep in mind that the goal of this method is to replace a complicated problem with several easy ones.
If the decomposition doesn't simplify the situation in some way, then nothing has been gained.
There are two main ways to decompose signals in signal processing: impulse decomposition and Fourier decomposition.
They are described in detail in the next several chapters.
In addition, several minor decompositions are occasionally used.
Here are brief descriptions of the two major decompositions, along with three of the minor ones.
Impulse Decomposition As shown in Fig. 5-12, impulse decomposition breaks an N samples signal into N component signals, each containing N samples.
Each of the component signals contains one point from the original signal, with the remainder of the values being zero.
A single nonzero point in a string of zeros is called an impulse.
Impulse decomposition is important because it allows signals to be examined one sample at a time.
Similarly, systems are characterized by how they respond to impulses.
By knowing how a system responds to an impulse, the system's output can be calculated for any given input.
This approach is called convolution, and is the topic of the next two chapters.
Step Decomposition Step decomposition, shown in Fig. 5-13, also breaks an N sample signal into N component signals, each composed of N samples.
Each component signal is a step, that is, the first samples have a value of zero, while the last samples are some constant value.
Consider the decomposition of an N point signal, x[n], into the components: x0[n], x1[n], x2[n], þ xN& 1[n] .
The k th component signal, xk[n], is composed of zeros for points 0 through k& 1, while the remaining points have a value of: x[k] & x[k& 1] .
For example, the 5 th component signal, x5[n], is composed of zeros for points 0 through 4, while the remaining samples have a value of: x[5] & x[4] (the difference between Example of impulse decomposition.
An N point signal is broken into N components, each consisting of a single nonzero point.
Example of step decomposition.
An N point signal is broken into N signals, each consisting of a step function sample 4 and 5 of the original signal).
As a special case, x0[n] has all of its samples equal to x[0] .
Just as impulse decomposition looks at signals one point at a time, step decomposition characterizes signals by the difference between adjacent samples.
Likewise, systems are characterized by how they respond to a change in the input signal.
Even/Odd Decomposition The even/odd decomposition, shown in Fig. 5-14, breaks a signal into two component signals, one having even symmetry and the other having odd symmetry.
An N point signal is said to have even symmetry if it is a mirror image around point N/2 .
That is, sample x[N/2 % 1] must equal x[N/2 & 1], sample x[N/2 % 2] must equal x[N/2 & 2], etc. Similarly, odd symmetry occurs when the matching points have equal magnitudes but are opposite in sign, such as: x[N/2 % 1] ' & x[N/2 & 1], x[N/2 % 2] ' & x[N/2 & 2], etc.
These definitions assume that the signal is composed of an even number of samples, and that the indexes run from 0 to N& 1 .
The decomposition is calculated from the relations: Equations for even/odd decomposition.
These equations separate a signal, x [n], into its even part, xE [n], and its odd part, xO [n] .
Since this decomposition is based on circularly symmetry, the zeroth samples in the even and odd signals are calculated: xE [0] ' x [0], and xO [0] ' 0 .
All of the signals are N samples long, with indexes running from 0 to N- This may seem a strange definition of left-right symmetry, since N/2 & ½ (between two samples) is the exact center of the signal, not N/2 .
Likewise, this off-center symmetry means that sample zero needs special handling.
What's this all about?
This decomposition is part of an important concept in DSP called circular symmetry.
It is based on viewing the end of the signal as connected to the beginning of the signal.
Just as point x[4] is next to point x[5], point x[N& 1] is next to point x[0] .
Picture a snake biting its own tail.
When even and odd signals are viewed in this circular manner, there are actually two lines of symmetry, one at point x[N/2] and another at point x[0] .
For example, in an even signal, this symmetry around x[0] means that point x[1] equals point x[N& 1], point x[2] equals point x[N& 2], etc.
In an odd signal, point 0 and point N/2 always have a value of zero.
In an even signal, point 0 and point N /2 are equal to the corresponding points in the original signal.
What is the motivation for viewing the last sample in a signal as being next to the first sample?
There is nothing in conventional data acquisition to support this circular notion.
In fact, the first and last samples generally have less in common than any other two points in the sequence.
It's common sense!
The missing piece to this puzzle is a DSP technique called Fourier analysis.
The mathematics of Fourier analysis inherently views the signal as being circular, although it usually has no physical meaning in terms of where the data came from.
We will look at this in more detail in Chapter 10.
For now, the important thing to understand is that Eq. 5-1 provides a valid decomposition, simply because the even and odd parts can be added together to reconstruct the original signal.
Example of even/odd decomposition.
An N point signal is broken into two N point signals, one with even symmetry, and the other with odd symmetry.
Example of interlaced decomposition.
An N point signal is broken into two N point signals, one with the odd samples set to zero, the other with the even samples set to zero.
Interlaced Decomposition As shown in Fig. 5-15, the interlaced decomposition breaks the signal into two component signals, the even sample signal and the odd sample signal (not to be confused with even and odd symmetry signals).
To find the even sample signal, start with the original signal and set all of the odd numbered samples to zero.
To find the odd sample signal, start with the original signal and set all of the even numbered samples to zero.
It's that simple.
At first glance, this decomposition might seem trivial and uninteresting.
This is ironic, because the interlaced decomposition is the basis for an extremely important algorithm in DSP, the Fast Fourier Transform (FFT).
The procedure for calculating the Fourier decomposition has been known for several hundred years.
Unfortunately, it is frustratingly slow, often requiring minutes or hours to execute on present day computers.
The FFT is a family of algorithms developed in the 1960s to reduce this computation time.
The strategy is an exquisite example of DSP: reduce the signal to elementary components by repeated use of the interlace transform; calculate the Fourier decomposition of the individual components; synthesize the results into the final answer.
The results are dramatic; it is common for the speed to be improved by a factor of hundreds or thousands.
Fourier Decomposition Fourier decomposition is very mathematical and not at all obvious.
Figure 5-16 shows an example of the technique.
Any N point signal can be decomposed into N% 2 signals, half of them sine waves and half of them cosine waves.
The lowest frequency cosine wave (called xC0 [n] in this illustration), makes zero complete cycles over the N samples, i.e., it is a DC signal.
The next cosine components: xC1 [n], xC2 [n], and xC3 [n], make 1, 2, and 3 complete cycles over the N samples, respectively.
This pattern holds for the remainder of the cosine waves, as well as for the sine wave components.
Since the frequency of each component is fixed, the only thing that changes for different signals being decomposed is the amplitude of each of the sine and cosine waves.
Fourier decomposition is important for three reasons.
First, a wide variety of signals are inherently created from superimposed sinusoids.
Audio signals are a good example of this.
Fourier decomposition provides a direct analysis of the information contained in these types of signals.
Second, linear systems respond to sinusoids in a unique way: a sinusoidal input always results in a sinusoidal output.
In this approach, systems are characterized by how they change the amplitude and phase of sinusoids passing through them.
Since an input signal can be decomposed into sinusoids, knowing how a system will react to sinusoids allows the output of the system to be found.
Third, the Fourier decomposition is the basis for a broad and powerful area of mathematics called Fourier analysis, and the even more advanced Laplace and z-transforms.
Most cutting-edge DSP algorithms are based on some aspect of these techniques.
Why is it even possible to decompose an arbitrary signal into sine and cosine waves?
How are the amplitudes of these sinusoids determined for a particular signal?
What kinds of systems can be designed with this technique?
These are the questions to be answered in later chapters.
The details of the Fourier decomposition are too involved to be presented in this brief overview.
For now, the important idea to understand is that when all of the component sinusoids are added together, the original signal is exactly reconstructed.
Much more on this in Chapter 8. Alternatives to Linearity To appreciate the importance of linear systems, consider that there is only one major strategy for analyzing systems that are nonlinear.
That strategy is to make the nonlinear system resemble a linear system.
There are three common ways of doing this: First, ignore the nonlinearity.
If the nonlinearity is small enough, the system can be approximated as linear.
Errors resulting from the original assumption are tolerated as noise or simply ignored.
Illustration of Fourier decomposition.
An N point signal is decomposed into N+2 signals, each having N points.
Half of these signals are cosine waves, and half are sine waves.
The frequencies of the sinusoids are fixed; only the amplitudes can change.
Second, keep the signals very small.
Many nonlinear systems appear linear if the signals have a very small amplitude.
For instance, transistors are very nonlinear over their full range of operation, but provide accurate linear amplification when the signals are kept under a few millivolts.
Operational amplifiers take this idea to the extreme.
By using very high open-loop gain together with negative feedback, the input signal to the op amp (i.e., the difference between the inverting and noninverting inputs) is kept to only a few microvolts.
This minuscule input signal results in excellent linearity from an otherwise ghastly nonlinear circuit.
Third, apply a linearizing transform.
For example, consider two signals being multiplied to make a third: a[n] ' b[n] × c[n] .
Taking the logarithm of the signals changes the nonlinear process of multiplication into the linear process of addition: log( a[n]) ' log( b[n]) % log(c[n] ) .
The fancy name for this approach is homomorphic signal processing.
For example, a visual image can be modeled as the reflectivity of the scene (a two-dimensional signal) being multiplied by the ambient illumination (another two-dimensional signal).
Homomorphic techniques enable the illumination signal to be made more uniform, thereby improving the image.
In the next chapters we examine the two main techniques of signal processing: convolution and Fourier analysis.
Both are based on the strategy presented in this chapter: (1) decompose signals into simple additive components, (2) process the components in some useful manner, and (3) synthesize the components into a final result.
This is DSP.
Convolution is a mathematical way of combining two signals to form a third signal.
It is the single most important technique in Digital Signal Processing.
Using the strategy of impulse decomposition, systems are described by a signal called the impulse response.
Convolution is important because it relates the three signals of interest: the input signal, the output signal, and the impulse response.
This chapter presents convolution from two different viewpoints, called the input side algorithm and the output side algorithm.
Convolution provides the mathematical framework for DSP; there is nothing more important in this book.
The Delta Function and Impulse Response The previous chapter describes how a signal can be decomposed into a group of components called impulses.
An impulse is a signal composed of all zeros, except a single nonzero point.
In effect, impulse decomposition provides a way to analyze signals one sample at a time.
The previous chapter also presented the fundamental concept of DSP: the input signal is decomposed into simple additive components, each of these components is passed through a linear system, and the resulting output components are synthesized (added).
The signal resulting from this divide-and-conquer procedure is identical to that obtained by directly passing the original signal through the system.
While many different decompositions are possible, two form the backbone of signal processing: impulse decomposition and Fourier decomposition.
When impulse decomposition is used, the procedure can be described by a mathematical operation called convolution.
In this chapter (and most of the following ones) we will only be dealing with discrete signals.
Convolution also applies to continuous signals, but the mathematics is more complicated.
We will look at how continious signals are processed in Chapter 13. Figure 6-1 defines two important terms used in DSP.
The first is the delta function, symbolized by the Greek letter delta, *[n] .
The delta function is a normalized impulse, that is, sample number zero has a value of one, while all other samples have a value of zero.
For this reason, the delta function is frequently called the unit impulse.
The second term defined in Fig. 6-1 is the impulse response.
As the name suggests, the impulse response is the signal that exits a system when a delta function (unit impulse) is the input.
If two systems are different in any way, they will have different impulse responses.
Just as the input and output signals are often called x[n] and y[n], the impulse response is usually given the symbol, h[n] .
Of course, this can be changed if a more descriptive name is available, for instance, f [n] might be used to identify the impulse response of a filter.
Any impulse can be represented as a shifted and scaled delta function.
Consider a signal, a[n], composed of all zeros except sample number 8, which has a value of -3.
This is the same as a delta function shifted to the right by 8 samples, and multiplied by -3.
In equation form: a[n] ' & 3*[n& 8] .
Make sure you understand this notation, it is used in nearly all DSP equations.
If the input to a system is an impulse, such as & 3*[n& 8], what is the system's output?
This is where the properties of homogeneity and shift invariance are used.
Scaling and shifting the input results in an identical scaling and shifting of the output.
If *[n] results in h[n], it follows that & 3*[n& 8] results in & 3h[n& 8] .
In words, the output is a version of the impulse response that has been shifted and scaled by the same amount as the delta function on the input.
If you know a system's impulse response, you immediately know how it will react to any impulse.
Convolution Let's summarize this way of understanding how a system changes an input signal into an output signal.
First, the input signal can be decomposed into a set of impulses, each of which can be viewed as a scaled and shifted delta function.
Second, the output resulting from each impulse is a scaled and shifted version of the impulse response.
Third, the overall output signal can be found by adding these scaled and shifted impulse responses.
In other words, if we know a system's impulse response, then we can calculate what the output will be for any possible input signal.
This means we know everything about the system.
There is nothing more that can be learned about a linear system's characteristics.
The impulse response goes by a different name in some applications.
If the system being considered is a filter, the impulse response is called the filter kernel, the convolution kernel, or simply, the kernel.
In image processing, the impulse response is called the point spread function.
While these terms are used in slightly different ways, they all mean the same thing, the signal produced by a system when the input is a delta function.
Definition of delta function and impulse response.
The delta function is a normalized impulse.
All of its samples have a value of zero, except for sample number zero, which has a value of one.
The Greek letter delta, *[n], is used to identify the delta function.
The impulse response of a linear system, usually denoted by h[n], is the output of the system when the input is a delta function.
Convolution is a formal mathematical operation, just as multiplication, addition, and integration.
Addition takes two numbers and produces a third number, while convolution takes two signals and produces a third signal.
Convolution is used in the mathematics of many fields, such as probability and statistics.
In linear systems, convolution is used to describe the relationship between three signals of interest: the input signal, the impulse response, and the output signal.
Figure 6-2 shows the notation when convolution is used with linear systems.
An input signal, x[n], enters a linear system with an impulse response, h[n], resulting in an output signal, y[n] .
In equation form: x[n] t h[n] ' y[n] .
Expressed in words, the input signal convolved with the impulse response is equal to the output signal.
Just as addition is represented by the plus, +, and multiplication by the cross, ×, convolution is represented by the star, t.
It is unfortunate that most programming languages also use the star to indicate multiplication.
A star in a computer program means multiplication, while a star in an equation means convolution.
How convolution is used in DSP.
The output signal from a linear system is equal to the input signal convolved with the system's impulse response.
Convolution is denoted by a star when writing equations.
Examples of low-pass and high-pass filtering using convolution.
In this example, the input signal is a few cycles of a sine wave plus a slowly rising ramp.
These two components are separated by using properly selected impulse responses.
Figure 6-3 shows convolution being used for low-pass and high-pass filtering.
The example input signal is the sum of two components: three cycles of a sine wave (representing a high frequency), plus a slowly rising ramp (composed of low frequencies).
In (a), the impulse response for the low-pass filter is a smooth arch, resulting in only the slowly changing ramp waveform being passed to the output.
Similarly, the high-pass filter, (b), allows only the more rapidly changing sinusoid to pass. Figure 6-4 illustrates two additional examples of how convolution is used to process signals.
The inverting attenuator, (a), flips the signal top-for-bottom, and reduces its amplitude.
The discrete derivative (also called the first difference), shown in (b), results in an output signal related to the slope of the input signal.
Notice the lengths of the signals in Figs.
The input signals are 81 samples long, while each impulse response is composed of 31 samples.
In most DSP applications, the input signal is hundreds, thousands, or even millions of samples in length.
The impulse response is usually much shorter, say, a few points to a few hundred points.
The mathematics behind convolution doesn't restrict how long these signals are.
It does, however, specify the length of the output signal.
The length of the output signal is Examples of signals being processed using convolution.
Many signal processing tasks use very simple impulse responses.
As shown in these examples, dramatic changes can be achieved with only a few nonzero points.
For the signals in Figs.
The input signal runs from sample 0 to 80, the impulse response from sample 0 to 30, and the output signal from sample 0 to 110.
Now we come to the detailed mathematics of convolution.
As used in Digital Signal Processing, convolution can be understood in two separate ways.
The first looks at convolution from the viewpoint of the input signal.
This involves analyzing how each sample in the input signal contributes to many points in the output signal.
The second way looks at convolution from the viewpoint of the output signal.
This examines how each sample in the output signal has received information from many points in the input signal.
Keep in mind that these two perspectives are different ways of thinking about the same mathematical operation.
The first viewpoint is important because it provides a conceptual understanding of how convolution pertains to DSP.
The second viewpoint describes the mathematics of convolution.
This typifies one of the most difficult tasks you will encounter in DSP: making your conceptual understanding fit with the jumble of mathematics used to communicate the ideas.
The Input Side Algorithm Figure 6-5 shows a simple convolution problem: a 9 point input signal, x[n], is passed through a system with a 4 point impulse response, h[n], resulting in a 9 % 4 & 1 ' 12 point output signal, y[n] .
In mathematical terms, x[n] is convolved with h[n] to produce y[n] .
This first viewpoint of convolution is based on the fundamental concept of DSP: decompose the input, pass the components through the system, and synthesize the output.
In this example, each of the nine samples in the input signal will contribute a scaled and shifted version of the impulse response to the output signal.
These nine signals are shown in Fig. 6-6.
Adding these nine signals produces the output signal, y[n] .
Let's look at several of these nine signals in detail.
We will start with sample number four in the input signal, i.e., x[4] .
This sample is at index number four, and has a value of 1.4.
When the signal is decomposed, this turns into an impulse represented as: 1.4 *[n& 4] .
After passing through the system, the resulting output component will be: 1.4 h[n& 4] .
This signal is shown in the center box of the nine signals in Fig. 6-6.
Notice that this is the impulse response, h[n], multiplied by 1.4, and shifted four samples to the right.
Zeros have been added at samples 0-3 and at samples 8-11 to serve as place holders.
To make this more clear, Fig. 6-6 uses squares to represent the data points that come from the shifted and scaled impulse response, and diamonds for the added zeros.
Now examine sample x[8], the last point in the input signal.
This sample is at index number eight, and has a value of -0.5.
As shown in the lower-right graph of Fig. 6-6, x[8] results in an impulse response that has been shifted to the right by eight points and multiplied by -0.5.
Place holding zeros have been added at points 0-7.
Lastly, examine the effect of points x[0] and x[7] .
Both these samples have a value of zero, and therefore produce output components consisting of all zeros.
Example convolution problem.
A nine point input signal, convolved with a four point impulse response, results in a twelve point output signal.
Each point in the input signal contributes a scaled and shifted impulse response to the output signal.
These nine scaled and shifted impulse responses are shown in Fig. 6-6.
Output signal components for the convolution in Fig. 6-5.
In these signals, each point that results from a scaled and shifted impulse response is represented by a square marker.
The remaining data points, represented by diamonds, are zeros that have been added as place holders.
In this example, x[n] is a nine point signal and h[n] is a four point signal.
In our next example, shown in Fig. 6-7, we will reverse the situation by making x[n] a four point signal, and h[n] a nine point signal.
The same two waveforms are used, they are just swapped.
As shown by the output signal components, the four samples in x[n] result in four shifted and scaled versions of the nine point impulse response.
Just as before, leading and trailing zeros are added as place holders.
But wait just one moment!
The output signal in Fig. 6-7 is identical to the output signal in Fig. 6-5.
This isn't a mistake, but an important property.
Convolution is commutative: a[n] t b[n] ' b[n] t a[n] .
The mathematics does not care which is the input signal and which is the impulse response, only that two signals are convolved with each other.
Although the mathematics may allow it, exchanging the two signals has no physical meaning in system theory.
The input signal and impulse response are two totally different things and exchanging them doesn't make sense.
What the commutative property provides is a mathematical tool for manipulating equations to achieve various results.
A program for calculating convolutions using the input side algorithm is shown in Table 6-1.
Remember, the programs in this book are meant to convey algorithms in the simplest form, even at the expense of good programming style.
For instance, all of the input and output is handled in mythical subroutines (lines 160 and 280), meaning we do not define how these operations are conducted.
Do not skip over these programs; they are a key part of the material and you need to understand them in detail.
The program convolves an 81 point input signal, held in array X[ ], with a point impulse response, held in array H[ ], resulting in a 111 point output signal, held in array Y[ ].
These are the same lengths shown in Figs.
Notice that the names of these arrays use upper case letters.
This is a violation of the naming conventions previously discussed, because upper case letters are reserved for frequency domain signals.
Unfortunately, the simple BASIC used in this book does not allow lower case variable names.
Also notice that line 240 uses a star for multiplication.
Remember, a star in a program means multiplication, while a star in an equation means convolution.
A star in text (such as documentation or program comments) can mean either.
The mythical subroutine in line 160 places the input signal into X[ ] and the impulse response into H[ ].
Lines 180-200 set all of the values in Y[ ] to zero.
This is necessary because Y[ ] is used as an accumulator to sum the output components as they are calculated.
Lines 220 to 260 are the heart of the program.
The FOR statement in line 220 controls a loop that steps through each point in the input signal, X[ ].
For each sample in the input signal, an inner loop (lines 230-250) calculates a scaled and shifted version of the impulse response, and adds it to the array accumulating the output signal, Y[ ].
This nested loop structure (one loop within another loop) is a key characteristic of convolution programs; become familiar with it.
A second example of convolution.
The waveforms for the input signal and impulse response are exchanged from the example of Fig. 6-5.
Since convolution is commutative, the output signals for the two examples are identical.
Keeping the indexing straight in line 240 can drive you crazy!
Let's say we are halfway through the execution of this program, so that we have just begun action on sample X[40], i.e., I% = 40.
The inner loop runs through each point in the impulse response doing three things.
First, the impulse response is scaled by multiplying it by the value of the input sample.
If this were the only action taken by the inner loop, line 240 could be written, Y[J%] = X[40] t H[J%].
Second, the scaled impulse is shifted 40 samples to the right by adding this number to the index used in the output signal.
This second action would change line 240 to: Y[40+J%] = X[40] t H[J%].
Third, Y[ ] must accumulate (synthesize) all the signals resulting from each sample in the input signal.
Therefore, the new information must be added to the information that is already in the array.
This results in the final command: Y[40+J%] = Y[40+J%] + X[40] t H[J%].
Study this carefully; it is very confusing, but very important.
The Output Side Algorithm The first viewpoint of convolution analyzes how each sample in the input signal affects many samples in the output signal.
In this second viewpoint, we reverse this by looking at individual samples in the output signal, and finding the contributing points from the input.
This is important from both mathematical and practical standpoints.
Suppose that we are given some input signal and impulse response, and want to find the convolution of the two.
The most straightforward method would be to write a program that loops through the output signal, calculating one sample on each loop cycle.
Likewise, equations are written in the form: y[n] ' some combination of other variables.
That is, sample n in the output signal is equal to some combination of the many values in the input signal and impulse response.
This requires a knowledge of how each sample in the output signal can be calculated independently of all other samples in the output signal.
The output side algorithm provides this information.
Let's look at an example of how a single point in the output signal is influenced by several points from the input.
The example point we will use is y[6] in Fig. 6-5.
This point is equal to the sum of all the sixth points in the nine output components, shown in Fig. 6-6.
Now, look closely at these nine output components and identify which can affect y[6] .
That is, find which of these nine signals contains a nonzero sample at the sixth position.
Five of the output components only have added zeros (the diamond markers) at the sixth sample, and can therefore be ignored.
Only four of the output components are capable of having a nonzero value in the sixth position.
These are the output components generated from the input samples: x[3], x[4], x[5], and x[6] .
By adding the sixth sample from each of these output components, y[6] is determined as: y[6] ' x[3] h[3] % x[4 ]h[2] % x[5] h[1] % x[6]h[0] .
That is, four samples from the input signal are multiplied by the four samples in the impulse response, and the products added.
Figure 6-8 illustrates the output side algorithm as a convolution machine, a flow diagram of how convolution occurs.
Think of the input signal, x[n], and the output signal, y[n], as fixed on the page.
The convolution machine, everything inside the dashed box, is free to move left and right as needed.
The convolution machine is positioned so that its output is aligned with the output sample being calculated.
Four samples from the input signal fall into the inputs of the convolution machine.
These values are multiplied by the indicated samples in the impulse response, and the products are added.
This produces the value for the output signal, which drops into its proper place.
For example, To calculate y[7], the convolution machine moves one sample to the right.
This results in another four samples entering the machine, x[4] through x[7], and the value for y[7] dropping into the proper place.
This process is repeated for all points in the output signal needing to be calculated.
The convolution machine.
This is a flow diagram showing how each sample in the output signal is influenced by the input signal and impulse response.
See the text for details.
The arrangement of the impulse response inside the convolution machine is very important.
The impulse response is flipped left-for-right.
This places sample number zero on the right, and increasingly positive sample numbers running to the left.
Compare this to the normal impulse response in Fig. 6- to understand the geometry of this flip.
Why is this flip needed?
It simply falls out of the mathematics.
The impulse response describes how each point in the input signal affects the output signal.
This results in each point in the output signal being affected by points in the input signal weighted by a flipped impulse response.
The convolution machine in action.
Figures (a) through (d) show the convolution machine set to calculate four different output signal samples, y[0], y[3], y[8], and y[11].
Figure 6-9 shows the convolution machine being used to calculate several samples in the output signal.
This diagram also illustrates a real nuisance in convolution.
In (a), the convolution machine is located fully to the left with its output aimed at y[0] .
In this position, it is trying to receive input from samples: x[& 3], x[& 2], x[& 1], and x[0] .
The problem is, three of these samples: x[& 3], x[& 2], and x[& 1], do not exist!
This same dilemma arises in (d), where the convolution machine tries to accept samples to the right of the defined input signal, points x[9], x[10], and x[11] .
One way to handle this problem is by inventing the nonexistent samples.
This involves adding samples to the ends of the input signal, with each of the added samples having a value of zero.
This is called padding the signal with zeros.
Instead of trying to access a nonexistent value, the convolution machine receives a sample that has a value of zero.
Since this zero is eliminated during the multiplication, the result is mathematically the same as ignoring the nonexistent inputs.
The important part is that the far left and far right samples in the output signal are based on incomplete information.
In DSP jargon, the impulse response is not fully immersed in the input signal.
If the impulse response is M points in length, the first and last M& 1 samples in the output signal are based on less information than the samples between.
This is analogous to an electronic circuit requiring a certain amount of time to stabilize after the power is applied.
The difference is that this transient is easy to ignore in electronics, but very prominent in DSP. Figure 6-10 shows an example of the trouble these end effects can cause.
The input signal is a sine wave plus a DC component.
The desire is to remove the DC part of the signal, while leaving the sine wave intact.
This calls for a highpass filter, such as the impulse response shown in the figure.
The problem is, the first and last 30 points are a mess!
The shape of these end regions can be understood by imagining the input signal padded with 30 zeros on the left side, samples x[& 1] through x[& 30], and 30 zeros on the right, samples x[81] through x[110] .
The output signal can then be viewed as a filtered version of this longer waveform.
These "end effect" problems are widespread in DSP.
As a general rule, expect that the beginning and ending samples in processed signals will be quite useless.
Now the math.
Using the convolution machine as a guideline, we can write the standard equation for convolution.
If x[n] is an N point signal running from to N-1, and h[n] is an M point signal running from 0 to M-1, the convolution of the two: y[n] ' x[n] t h[n], is an N+M-1 point signal running from 0 to This equation is called the convolution sum.
It allows each point in the output signal to be calculated independently of all other points in the output signal.
The index, i, determines which sample in the output signal is being calculated, and therefore corresponds to the left-right position of the convolution machine.
In computer programs performing convolution, a loop makes this index run through each sample in the output signal.
To calculate one of the output samples, the index, j, is used inside of the convolution machine.
As j runs through 0 to M-1, each sample in the impulse response, h[ j], is multiplied by the proper sample from the input signal, x[i& j].
All these products are added to produce the output sample being calculated.
Study Eq. 6-1 until you fully understand how it is implemented by the convolution machine.
Much of DSP is based on this equation.
This is merely a place holder to indicate that some variable is the index into the array.
Sometimes the equations are written: y[ ] ' x[ ] t h[ ], just to avoid having to bring in a meaningless symbol).
Table 6-2 shows a program for performing convolutions using the output side algorithm, a direct use of Eq. 6-1.
This program produces the same output signal as the program for the input side algorithm, shown previously in Table 6-1.
Notice the main difference between these two programs: the input side algorithm loops through each sample in the input signal (line 220 of Table 61), while the output side algorithm loops through each sample in the output signal (line 180 of Table 6-2).
Here is a detailed operation of this program.
The FOR-NEXT loop in lines to 250 steps through each sample in the output signal, using I% as the index.
For each of these values, an inner loop, composed of lines 200 to 230, calculates the value of the output sample, Y[I%].
The value of Y[I%] is set to zero in line 190, allowing it to accumulate the products inside of the convolution machine.
The FOR-NEXT loop in lines 200 to 240 provide a direct implementation of Eq. 6-1.
The index, J%, steps through each End effects in convolution.
When an input signal is convolved with an M point impulse response, the first and last M-1 points in the output signal may not be usable.
In this example, the impulse response is a high-pass filter used to remove the DC component from the input signal.
Line 230 provides the multiplication of each sample in the impulse response, H[J%], with the appropriate sample from the input signal, X[I%-J%], and adds the result to the accumulator.
In line 230, the sample taken from the input signal is: X[I%-J%].
Lines and 220 prevent this from being outside the defined array, X[0] to X[80].
In other words, this program handles undefined samples in the input signal by ignoring them.
Another alternative would be to define the input signal's array from X[-30] to X[110], allowing 30 zeros to be padded on each side of the true data.
As a third alternative, the FOR-NEXT loop in line 180 could be changed to run from 30 to 80, rather than 0 to 110.
That is, the program would only calculate the samples in the output signal where the impulse response is fully immersed in the input signal.
The important thing is that you must use one of these three techniques.
If you don't, the program will crash when it tries to read the out-of-bounds data.
The characteristics of a linear system are completely described by its impulse response.
This is the basis of the input side algorithm: each point in the input signal contributes a scaled and shifted version of the impulse response to the output signal.
The mathematical consequences of this lead to the output side algorithm: each point in the output signal receives a contribution from many points in the input signal, multiplied by a flipped impulse response.
While this is all true, it doesn't provide the full story on why convolution is important in signal processing.
Look back at the convolution machine in Fig. 6-8, and ignore that the signal inside the dotted box is an impulse response.
Think of it as a set of weighing coefficients that happen to be embedded in the flow diagram.
In this view, each sample in the output signal is equal to a sum of weighted inputs.
Each sample in the output is influenced by a region of samples in the input signal, as determined by what the weighing coefficients are chosen to be.
For example, imagine there are ten weighing coefficients, each with a value of onetenth.
This makes each sample in the output signal the average of ten samples from the input.
Taking this further, the weighing coefficients do not need to be restricted to the left side of the output sample being calculated.
For instance, Fig. 6-8 shows y[6] being calculated from: x[3], x[4], x[5], and x[6] .
Viewing the convolution machine as a sum of weighted inputs, the weighing coefficients could be chosen symmetrically around the output sample.
For example, y[6] might receive contributions from: x[4], x[5], x[6], x[7], and x[8] .
Using the same indexing notation as in Fig. 6-8, the weighing coefficients for these five inputs would be held in: h[2], h[1], h[0], h[& 1], and h[& 2] .
In other words, the impulse response that corresponds to our selection of symmetrical weighing coefficients requires the use of negative indexes.
We will return to this in the next chapter.
Mathematically, there is only one concept here: convolution as defined by Eq. 6-1.
However, science and engineering problems approach this single concept from two distinct directions.
Sometimes you will want to think of a system in terms of what its impulse response looks like.
Other times you will understand the system as a set of weighing coefficients.
You need to become familiar with both views, and how to toggle between them.
A linear system's characteristics are completely specified by the system's impulse response, as governed by the mathematics of convolution.
This is the basis of many signal processing techniques.
For example: Digital filters are created by designing an appropriate impulse response.
Enemy aircraft are detected with radar by analyzing a measured impulse response.
Echo suppression in long distance telephone calls is accomplished by creating an impulse response that counteracts the impulse response of the reverberation.
The list goes on and on.
This chapter expands on the properties and usage of convolution in several areas.
First, several common impulse responses are discussed.
Second, methods are presented for dealing with cascade and parallel combinations of linear systems.
Third, the technique of correlation is introduced.
Fourth, a nasty problem with convolution is examined, the computation time can be unacceptably long using conventional algorithms and computers.
Common Impulse Responses Delta Function The simplest impulse response is nothing more that a delta function, as shown in Fig. 7-1a.
That is, an impulse on the input produces an identical impulse on the output.
This means that all signals are passed through the system without change.
Convolving any signal with a delta function results in exactly the same signal.
Mathematically, this is written: EQUATION 7- The delta function is the identity for convolution.
Any signal convolved with a delta function is left unchanged.
This is analogous to zero being the identity for addition ( a % 0 ' a ), and one being the identity for multiplication ( a × 1 ' a ) .
At first glance, this type of system may seem trivial and uninteresting.
Not so!
Such systems are the ideal for data storage, communication and measurement.
Much of DSP is concerned with passing information through systems without change or degradation.
Figure 7-1b shows a slight modification to the delta function impulse response.
If the delta function is made larger or smaller in amplitude, the resulting system is an amplifier or attenuator, respectively.
In equation form, amplification results if k is greater than one, and attenuation results if k is less than one: EQUATION 7- A system that amplifies or attenuates has a scaled delta function for an impulse response.
In this equation, k determines the amplification or attenuation.
This results in a system that introduces an identical shift between the input and output signals.
This could be described as a signal delay, or a signal advance, depending on the direction of the shift.
Letting the shift be represented by the parameter, s, this can be written as the equation: EQUATION 7- A relative shift between the input and output signals corresponds to an impulse response that is a shifted delta function.
The variable, s, determines the amount of shift in this equation.
For example, consider a radio signal transmitted from a remote space probe, and the corresponding signal received on the earth.
The time it takes the radio wave to propagate over the distance causes a delay between the transmitted and received signals.
In biology, the electrical signals in adjacent nerve cells are shifted versions of each other, as determined by the time it takes an action potential to cross the synaptic junction that connects the two.
Figure 7-1d shows an impulse response composed of a delta function plus a shifted and scaled delta function.
By superposition, the output of this system is the input signal plus a delayed version of the input signal, i.e., an echo.
Echoes are important in many DSP applications.
The addition of echoes is a key part in making audio recordings sound natural and pleasant.
Radar and sonar analyze echoes to detect aircraft and submarines.
Geophysicists use echoes to find oil.
Echoes are also very important in telephone networks, because you want to avoid them.
Convolving a signal with the delta function leaves the signal unchanged.
This is the goal of systems that transmit or store signals.
Amplitude Sample number b.
Amplification & Attenuation Increasing or decreasing the amplitude of the delta function forms an impulse response that amplifies or attenuates, respectively.
This impulse response will amplify the signal by 1.6.
Simple impulse responses using shifted and scaled delta functions.
Calculus-like Operations Convolution can change discrete signals in ways that resemble integration and differentiation.
Since the terms "derivative" and "integral" specifically refer to operations on continuous signals, other names are given to their discrete counterparts.
The discrete operation that mimics the first derivative is called the first difference.
Likewise, the discrete form of the integral is called the running sum.
It is also common to hear these operations called the discrete derivative and the discrete integral, although mathematicians frown when they hear these informal terms used.
Figure 7-2 shows the impulse responses that implement the first difference and the running sum. Figure 7-3 shows an example using these operations.
In 73a, the original signal is composed of several sections with varying slopes.
Convolving this signal with the first difference impulse response produces the signal in Fig. 7-3b.
Just as with the first derivative, the amplitude of each point in the first difference signal is equal to the slope at the corresponding location in the original signal.
The running sum is the inverse operation of the first difference.
That is, convolving the signal in (b), with the running sum's impulse response, produces the signal in (a).
These impulse responses are simple enough that a full convolution program is usually not needed to implement them.
Rather, think of them in the alternative mode: each sample in the output signal is a sum of weighted samples from the input.
For instance, the first difference can be calculated: EQUATION 7- Calculation of the first difference.
In this relation, x [n] is the original signal, and y [n] is the first difference.
For instance, y[40] ' x[40] & x[39] .
It should be mentioned that this is not the only way to define a discrete derivative.
Another common method is to define the slope symmetrically around the point being examined, such as: y[n] ' ( x[n% 1] & x[n& 1] )/ 2 .
Amplitude a.
First Difference This is the discrete version of the first derivative.
Each sample in the output signal is equal to the difference between adjacent samples in the input signal.
In other words, the output signal is the slope of the input signal.
Impulse responses that mimic calculus operations.
Example of calculus-like operations.
The signal in (b) is the first difference of the signal in (a).
Correspondingly, the signal is (a) is the running sum of the signal in (b).
These processing methods are used with discrete signals the same as differentiation and integration are used with continuous signals.
Using this same approach, each sample in the running sum can be calculated by summing all points in the original signal to the left of the sample's location.
For instance, if y[n] is the running sum of x[n], then sample y[40] is found by adding samples x[0] through x[40] .
Likewise, sample y[41] is found by adding samples x[0] through x[41] .
Of course, it would be very inefficient to calculate the running sum in this manner.
For example, if y[40] has already been Calculation of the running sum.
In this relation, x [n] is the original signal, and y [n] is the running sum.
We will revisit them in Chapter 19.
For now, the important idea to understand is that these relations are identical to convolution using the impulse responses of Fig. 7-2.
Table 7-1 provides computer programs that implement these calculus-like operations.
The original signal is held in X[ ], and the processed signal (the first difference or running sum) is held in Y[ ].
Both arrays run from 0 to N%-1.
Low-pass and High-pass Filters The design of digital filters is covered in detail in later chapters.
For now, be satisfied to understand the general shape of low-pass and high-pass filter kernels (another name for a filter's impulse response).
Figure 7-4 shows several common low-pass filter kernels.
In general, low-pass filter kernels are composed of a group of adjacent positive points.
This results in each sample in the output signal being a weighted average of many adjacent points from the input signal.
This averaging smoothes the signal, thereby removing highfrequency components.
As shown by the sinc function in (c), some low-pass filter kernels include a few negative valued samples in the tails.
Just as in analog electronics, digital low-pass filters are used for noise reduction, signal separation, wave shaping, etc.
Typical low-pass filter kernels.
Low-pass filter kernels are formed from a group of adjacent positive points that provide an averaging (smoothing) of the signal.
As discussed in later chapters, each of these filter kernels is best for a particular purpose.
The exponential, (a), is the simplest recursive filter.
The rectangular pulse, (b), is best at reducing noise while maintaining edge sharpness.
The sinc function in (c), a curve of the form: sin(x)/(x), is used to separate one band of frequencies from another.
Typical high-pass filter kernels.
These are formed by subtracting the corresponding lowpass filter kernels in Fig. 7-4 from a delta function.
The distinguishing characteristic of high-pass filter kernels is a spike surrounded by many adjacent negative samples.
The cutoff frequency of the filter is changed by making filter kernel wider or narrower.
If a low-pass filter has a gain of one at DC (zero frequency), then the sum of all of the points in the impulse response must be equal to one.
As illustrated in (a) and (c), some filter kernels theoretically extend to infinity without dropping to a value of zero.
In actual practice, the tails are truncated after a certain number of samples, allowing it to be represented by a finite number of points.
How else could it be stored in a computer?
Figure 7-5 shows three common high-pass filter kernels, derived from the corresponding low-pass filter kernels in Fig. 7-4.
This is a common strategy in filter design: first devise a low-pass filter and then transform it to what you need, high-pass, band-pass, band-reject, etc.
To understand the low-pass to high-pass transform, remember that a delta function impulse response passes the entire signal, while a low-pass impulse response passes only the lowfrequency components.
By superposition, a filter kernel consisting of a delta function minus the low-pass filter kernel will pass the entire signal minus the low-frequency components.
A high-pass filter is born!
As shown in Fig. 7-5, the delta function is usually added at the center of symmetry, or sample zero if the filter kernel is not symmetrical.
High-pass filters have zero gain at DC (zero frequency), achieved by making the sum of all the points in the filter kernel equal to zero.
Imagine a simple analog electronic circuit.
If you apply a short pulse to the input, you will see a response on the output.
This is the kind of cause and effect that our universe is based on.
One thing we definitely know: any effect must happen after the cause.
This is a basic characteristic of what we call time.
Now compare this to a DSP system that changes an input signal into an output signal, both stored in arrays in a computer.
If this mimics a real world system, it must follow the same principle of causality as the real world does.
For example, the value at sample number eight in the input signal can only affect sample number eight or greater in the output signal.
Systems that operate in this manner are said to be causal.
Of course, digital processing doesn't necessarily have to function this way.
Since both the input and output signals are arrays of numbers stored in a computer, any of the input signal values can affect any of the output signal values.
As shown by the examples in Fig. 7-6, the impulse response of a causal system must have a value of zero for all negative numbered samples.
Think of this from the input side view of convolution.
To be causal, an impulse in the input signal at sample number n must only affect those points in the output signal with a sample number of n or greater.
In common usage, the term causal is applied to any signal where all the negative numbered samples have a value of zero, whether it is an impulse response or not.
Examples of phase linearity.
Signals that have a left-right symmetry are said to be linear phase.
If the axis of symmetry occurs at sample number zero, they are additionally said to be zero phase.
Any linear phase signal can be transformed into a zero phase signal simply by shifting.
Signals that do not have a leftright symmetry are said to be nonlinear phase.
Do not confuse these terms with the linear in linear systems.
They are completely different concepts.
As shown in Fig. 7-7, a signal is said to be zero phase if it has left-right symmetry around sample number zero.
A signal is said to be linear phase if it has left-right symmetry, but around some point other than zero.
This means that any linear phase signal can be changed into a zero phase signal simply by shifting left or right.
Lastly, a signal is said to be nonlinear phase if it does not have left-right symmetry.
You are probably thinking that these names don't seem to follow from their definitions.
What does phase have to do with symmetry?
The answer lies in the frequency spectrum, and will be discussed in more detail in later chapters.
Briefly, the frequency spectrum of any signal is composed of two parts, the magnitude and the phase.
The frequency spectrum of a signal that is symmetrical around zero has a phase that is zero.
Likewise, the frequency spectrum of a signal that is symmetrical around some nonzero point has a phase that is a straight line, i.e., a linear phase.
Lastly, the frequency spectrum of a signal that is not symmetrical has a phase that is not a straight line, i.e., it has a nonlinear phase.
A special note about the potentially confusing terms: linear and nonlinear phase.
What does this have to do the concept of system linearity discussed in previous chapters?
Absolutely nothing!
System linearity is the broad concept that nearly all of DSP is based on (superposition, homogeneity, additivity, etc).
Linear and nonlinear phase mean that the phase is, or is not, a straight line.
In fact, a system must be linear even to say that the phase is zero, linear, or nonlinear.
Mathematical Properties Commutative Property The commutative property for convolution is expressed in mathematical form: EQUATION 7- The commutative property of convolution.
This states that the order in which signals are convolved can be exchanged.
As shown in Fig. 7-8, this has a strange meaning for system theory.
In any linear system, the input signal and the system's impulse response can be exchanged without changing the output signal.
This is interesting, but usually doesn't have any physical meaning.
The input signal and the impulse response are very different things.
Just because the mathematics allows you to do something, doesn't mean that it makes sense to do it.
For example, suppose you make: $10/hour × 2,000 hours/year = $20,000/year.
The commutative property for multiplication provides that you can make the same annual salary by only working 10 hours/year at $2000/hour.
Let's see you convince your boss that this is meaningful!
In spite of this, the commutative property sees great use in DSP for manipulating equations, just as in ordinary algebra.
The commutative property in system theory.
The commutative property of convolution allows the input signal and the impulse response of a system to be exchanged without changing the output.
While interesting, this usually has no physical significance.
Is it possible to convolve three or more signals?
The answer is yes, and the associative property describes how: convolve two of the signals to produce an intermediate signal, then convolve the intermediate signal with the third signal.
The associative property provides that the order of the convolutions doesn't matter.
As an equation: EQUATION 7- The associative property of convolution describes how three or more signals are convolved.
The associative property is used in system theory to describe how cascaded systems behave.
As shown in Fig. 7-9, two or more systems are said to be in a cascade if the output of one system is used as the input for the next system.
From the associative property, the order of the systems can be rearranged without changing the overall response of the cascade.
Further, any number of cascaded systems can be replaced with a single system.
The impulse response of the replacement system is found by convolving the impulse responses of all of the original systems.
The associative property in system theory.
The associative property provides two important characteristics of cascaded linear systems.
First, the order of the systems can be rearranged without changing the overall operation of the cascade.
Second, two or more systems in a cascade can be replaced by a single system.
The impulse response of the replacement system is found by convolving the impulse responses of the stages being replaced.
Distributive Property In equation form, the distributive property is written: EQUATION 7- The distributive property of convolution describes how parallel systems are analyzed.
The distributive property describes the operation of parallel systems with added outputs.
As shown in Fig. 7-10, two or more systems can share the same input, x[n], and have their outputs added to produce y[n] .
The distributive property allows this combination of systems to be replaced with a single system, having an impulse response equal to the sum of the impulse responses of the original systems.
The distributive property in system theory.
The distributive property shows that parallel systems with added outputs can be replaced with a single system.
The impulse response of the replacement system is equal to the sum of the impulse responses of all the original systems.
Transference between the Input and Output Rather than being a formal mathematical property, this is a way of thinking about a common situation in signal processing.
As illustrated in Fig. 7-11, imagine a linear system receiving an input signal, x[n], and generating an output signal, y[n] .
Now suppose that the input signal is changed in some linear way, resulting in a new input signal, which we will call x 3[n] .
This results in a new output signal, y 3[n] .
The question is, how does the change in the input signal relate to the change in the output signal?
The answer is: the output signal is changed in exactly the same linear way that the input signal was changed.
For example, if the input signal is amplified by a factor of two, the output signal will also be amplified by a factor of two.
If the derivative is taken of the input signal, the derivative will also be taken of the output signal.
If the input is filtered in some way, the output will be filtered in an identical manner.
This can easily be proven by using the associative property.
Tranference between the input and output.
This is a way of thinking about a common situation in signal processing.
A linear change made to the input signal results in the same linear change being made to the output signal.
The Central Limit Theorem The Central Limit Theorem is an important tool in probability theory because it mathematically explains why the Gaussian probability distribution is observed so commonly in nature.
For example: the amplitude of thermal noise in electronic circuits follows a Gaussian distribution; the cross-sectional intensity of a laser beam is Gaussian; even the pattern of holes around a dart board bull's eye is Gaussian.
In its simplest form, the Central Limit Theorem states that a Gaussian distribution results when the observed variable is the sum of many random processes.
Even if the component processes do not have a Gaussian distribution, the sum of them will.
The Central Limit Theorem has an interesting implication for convolution.
If a pulse-like signal is convolved with itself many times, a Gaussian is produced.
Figure 7-12 shows an example of this.
The signal in (a) is an Example of convolving a pulse waveform with itself.
The Central Limit Theorem shows that a Gaussian waveform is produced when an arbitrary shaped pulse is convolved with itself many times.
Figure (a) is an example pulse.
In (b), the pulse is convolved with itself once, and begins to appear smooth and regular.
In (c), the pulse is convolved with itself three times, and closely approximates a Gaussian.
Figure (b) shows the result of convolving this signal with itself one time.
Figure (c) shows the result of convolving this signal with itself three times.
Even with only three convolutions, the waveform looks very much like a Gaussian.
In mathematics jargon, the procedure converges to a Gaussian very quickly.
The width of the resulting Gaussian (i.e., F in Eq. 2-7 or 2-8) is equal to the width of the original pulse (expressed as F in Eq. 2-7) multiplied by the square root of the number of convolutions.
Correlation The concept of correlation can best be presented with an example.
Figure 7- shows the key elements of a radar system.
A specially designed antenna transmits a short burst of radio wave energy in a selected direction.
If the propagating wave strikes an object, such as the helicopter in this illustration, a small fraction of the energy is reflected back toward a radio receiver located near the transmitter.
The transmitted pulse is a specific shape that we have selected, such as the triangle shown in this example.
The received signal will consist of two parts: (1) a shifted and scaled version of the transmitted pulse, and (2) random noise, resulting from interfering radio waves, thermal noise in the electronics, etc.
Since radio signals travel at a known rate, the speed of light, the shift between the transmitted and received pulse is a direct measure of the distance to the object being detected.
This is the problem: given a signal of some known shape, what is the best way to determine where (or if) the signal occurs in another signal.
Correlation is the answer.
Correlation is a mathematical operation that is very similar to convolution.
Just as with convolution, correlation uses two signals to produce a third signal.
This third signal is called the cross-correlation of the two input signals.
If a signal is correlated with itself, the resulting signal is instead called the autocorrelation.
The convolution machine was presented in the last chapter to show how convolution is performed.
Figure 7-14 is a similar Key elements of a radar system.
Like other echo location systems, radar transmits a short pulse of energy that is reflected by objects being examined.
This makes the received waveform a shifted version of the transmitted waveform, plus random noise.
Detection of a known waveform in a noisy signal is the fundamental problem in echo location.
The answer to this problem is correlation.
TRANSMIT Transmitted amplitude Sample number (or time) Received amplitude -0.
Sample number (or time) RECEIVE illustration of a correlation machine.
The received signal, x[n], and the cross-correlation signal, y[n], are fixed on the page.
The waveform we are looking for, t[n], commonly called the target signal, is contained within the correlation machine.
Each sample in y[n] is calculated by moving the correlation machine left or right until it points to the sample being worked on.
Next, the indicated samples from the received signal fall into the correlation machine, and are multiplied by the corresponding points in the target signal.
The sum of these products then moves into the proper sample in the crosscorrelation signal.
The amplitude of each sample in the cross-correlation signal is a measure of how much the received signal resembles the target signal, at that location.
This means that a peak will occur in the cross-correlation signal for every target signal that is present in the received signal.
In other words, the value of the cross-correlation is maximized when the target signal is aligned with the same features in the received signal.
What if the target signal contains samples with a negative value?
Nothing changes.
Imagine that the correlation machine is positioned such that the target signal is perfectly aligned with the matching waveform in the received signal.
As samples from the received signal fall into the correlation machine, they are multiplied by their matching samples in the target signal.
Neglecting noise, a positive sample will be multiplied by itself, resulting in a positive number.
Likewise, a negative sample will be multiplied by itself, also resulting in a positive number.
Even if the target signal is completely negative, the peak in the cross-correlation will still be positive.
If there is noise on the received signal, there will also be noise on the crosscorrelation signal.
It is an unavoidable fact that random noise looks a certain amount like any target signal you can choose.
The noise on the cross-correlation signal is simply measuring this similarity.
Except for this noise, the peak generated in the cross-correlation signal is symmetrical between its left and right.
This is true even if the target signal isn't symmetrical.
In addition, the width of the peak is twice the width of the target signal.
Remember, the cross-correlation is trying to detect the target signal, not recreate it.
There is no reason to expect that the peak will even look like the target signal.
Correlation is the optimal technique for detecting a known waveform in random noise.
That is, the peak is higher above the noise using correlation than can be produced by any other linear system.
Using correlation to detect a known waveform is frequently called matched filtering.
More on this in Chapter 17.
The correlation machine and convolution machine are identical, except for one small difference.
As discussed in the last chapter, the signal inside of the convolution machine is flipped left-for-right.
This means that samples numbers: 1, 2, 3 þ run from the right to the left.
In the correlation machine this flip doesn't take place, and the samples run in the normal direction.
The correlation machine.
This is a flowchart showing how the cross-correlation of two signals is calculated.
In this example, y [n] is the cross-correlation of x [n] and t [n] .
The dashed box is moved left or right so that its output points at the sample being calculated in y [n] .
The indicated samples from x [n] are multiplied by the corresponding samples in t [n], and the products added.
The correlation machine is identical to the convolution machine (Figs.
In this illustration, the only samples calculated in y [n] are where t [n] is fully immersed in x [n] .
Since this signal reversal is the only difference between the two operations, it is possible to represent correlation using the same mathematics as convolution.
This requires preflipping one of the two signals being correlated, so that the left-for-right flip inherent in convolution is canceled.
For instance, when a[n] a n d b[n], are convolved to produce c[n], the equation is written: a[n ]( b [n] ' c [n] .
In comparison, the cross-correlation of a[n] and b[n] can be written: a[n ]( b [& n] ' c [n] .
That is, flipping b [n] left-for-right is accomplished by reversing the sign of the index, i.e., b [& n] .
Don't let the mathematical similarity between convolution and correlation fool you; they represent very different DSP procedures.
Convolution is the relationship between a system's input signal, output signal, and impulse response.
Correlation is a way to detect a known waveform in a noisy background.
The similar mathematics is only a convenient coincidence.
Writing a program to convolve one signal by another is a simple task, only requiring a few lines of code.
Executing the program may be more painful.
The problem is the large number of additions and multiplications required by the algorithm, resulting in long execution times.
As shown by the programs in the last chapter, the time-consuming operation is composed of multiplying two numbers and adding the result to an accumulator.
Other parts of the algorithm, such as indexing the arrays, are very quick.
The multiply-accumulate is a basic building block in DSP, and we will see it repeated in several other important algorithms.
In fact, the speed of DSP computers is often specified by how long it takes to preform a multiply-accumulate operation.
If a signal composed of N samples is convolved with a signal composed of M samples, N ×M multiply-accumulations must be preformed.
This can be seen from the programs of the last chapter.
Personal computers of the mid 1990's requires about one microsecond per multiply-accumulation (100 MHz Pentium using single precision floating point, see Table 4-6).
Therefore, convolving a 10,000 sample signal with a 100 sample signal requires about one second.
To process a one million point signal with a 3000 point impulse response requires nearly an hour.
A decade earlier (80286 at 12 MHz), this calculation would have required three days!
The problem of excessive execution time is commonly handled in one of three ways.
First, simply keep the signals as short as possible and use integers instead of floating point.
If you only need to run the convolution a few times, this will probably be the best trade-off between execution time and programming effort.
Second, use a computer designed for DSP.
DSP microprocessors are available with multiply-accumulate times of only a few tens of nanoseconds.
This is the route to go if you plan to perform the convolution many times, such as in the design of commercial products.
The third solution is to use a better algorithm for implementing the convolution.
Chapter 17 describes a very sophisticated algorithm called FFT convolution.
FFT convolution produces exactly the same result as the convolution algorithms presented in the last chapter; however, the execution time is dramatically reduced.
For signals with thousands of samples, FFT convolution can be hundreds of times faster.
The disadvantage is program complexity.
Even if you are familiar with the technique, expect to spend several hours getting the program to run.
Fourier analysis is a family of mathematical techniques, all based on decomposing signals into sinusoids.
The discrete Fourier transform (DFT) is the family member used with digitized signals.
This is the first of four chapters on the real DFT, a version of the discrete Fourier transform that uses real numbers to represent the input and output signals.
The complex DFT, a more advanced technique that uses complex numbers, will be discussed in Chapter 31.
In this chapter we look at the mathematics and algorithms of the Fourier decomposition, the heart of the DFT.
The Family of Fourier Transform Fourier analysis is named after Jean Baptiste Joseph Fourier (1768-1830), a French mathematician and physicist.
While many contributed to the field, Fourier is honored for his mathematical discoveries and insight into the practical usefulness of the techniques.
Fourier was interested in heat propagation, and presented a paper in 1807 to the Institut de France on the use of sinusoids to represent temperature distributions.
The paper contained the controversial claim that any continuous periodic signal could be represented as the sum of properly chosen sinusoidal waves.
Among the reviewers were two of history's most famous mathematicians, Joseph Louis Lagrange (1736-1813), and Pierre Simon de Laplace (1749-1827).
While Laplace and the other reviewers voted to publish the paper, Lagrange adamantly protested.
For nearly 50 years, Lagrange had insisted that such an approach could not be used to represent signals with corners, i.e., discontinuous slopes, such as in square waves.
The Institut de France bowed to the prestige of Lagrange, and rejected Fourier's work.
It was only after Lagrange died that the paper was finally published, some 15 years later.
Luckily, Fourier had other things to keep him busy, political activities, expeditions to Egypt with Napoleon, and trying to avoid the guillotine after the French Revolution (literally!).
Who was right?
It's a split decision.
Lagrange was correct in his assertion that a summation of sinusoids cannot form a signal with a corner.
However, you can get very close.
So close that the difference between the two has zero energy.
In this sense, Fourier was right, although 18th century science knew little about the concept of energy.
This phenomenon now goes by the name: Gibbs Effect, and will be discussed in Chapter 11. Figure 8-1 illustrates how a signal can be decomposed into sine and cosine waves.
Figure (a) shows an example signal, 16 points long, running from sample number 0 to 15. Figure (b) shows the Fourier decomposition of this signal, nine cosine waves and nine sine waves, each with a different frequency and amplitude.
Although far from obvious, these 18 sinusoids Sample number add to produce the waveform in (a).
It should be noted that the objection made by Lagrange only applies to continuous signals.
For discrete signals, this decomposition is mathematically exact.
There is no difference between the signal in (a) and the sum of the signals in (b), just as there is no difference between 7 and 3+4.
Why are sinusoids used instead of, for instance, square or triangular waves?
Remember, there are an infinite number of ways that a signal can be decomposed.
The goal of decomposition is to end up with something easier to deal with than the original signal.
For example, impulse decomposition allows signals to be examined one point at a time, leading to the powerful technique of convolution.
The component sine and cosine waves are simpler than the original signal because they have a property that the original signal does not have: sinusoidal fidelity.
As discussed in Chapter 5, a sinusoidal input to a system is guaranteed to produce a sinusoidal output.
Only the amplitude and phase of the signal can change; the frequency and wave shape must remain the same.
Sinusoids are the only waveform that have this useful property.
While square and triangular decompositions are possible, there is no general reason for them to be useful.
The general term: Fourier transform, can be broken into four categories, resulting from the four basic types of signals that can be encountered.
A signal can be either continuous or discrete, and it can be either periodic or aperiodic.
The combination of these two features generates the four categories, described below and illustrated in Fig. 8-2.
Aperiodic-Continuous This includes, for example, decaying exponentials and the Gaussian curve.
These signals extend to both positive and negative infinity without repeating in a periodic pattern.
The Fourier Transform for this type of signal is simply called the Fourier Transform.
Periodic-Continuous Here the examples include: sine waves, square waves, and any waveform that repeats itself in a regular pattern from negative to positive infinity.
This version of the Fourier transform is called the Fourier Series.
Aperiodic-Discrete These signals are only defined at discrete points between positive and negative infinity, and do not repeat themselves in a periodic fashion.
This type of Fourier transform is called the Discrete Time Fourier Transform.
Periodic-Discrete These are discrete signals that repeat themselves in a periodic fashion from negative to positive infinity.
This class of Fourier Transform is sometimes called the Discrete Fourier Series, but is most often called the Discrete Fourier Transform.
You might be thinking that the names given to these four types of Fourier transforms are confusing and poorly organized.
You're right; the names have evolved rather haphazardly over 200 years.
There is nothing you can do but memorize them and move on.
These four classes of signals all extend to positive and negative infinity.
Hold on, you say!
What if you only have a finite number of samples stored in your computer, say a signal formed from 1024 points.
Isn't there a version of the Fourier Transform that uses finite length signals?
No, there isn't.
Sine and cosine waves are defined as extending from negative infinity to positive infinity.
You cannot use a group of infinitely long signals to synthesize something finite in length.
The way around this dilemma is to make the finite data look like an infinite length signal.
This is done by imagining that the signal has an infinite number of samples on the left and right of the actual points.
If all these “imagined” samples have a value of zero, the signal looks discrete and aperiodic, and the Discrete Time Fourier Transform applies.
As an alternative, the imagined samples can be a duplication of the actual points.
In this case, the signal looks discrete and periodic, with a period of 1024 samples.
This calls for the Discrete Fourier Transform to be used.
As it turns out, an infinite number of sinusoids are required to synthesize a signal that is aperiodic.
This makes it impossible to calculate the Discrete Time Fourier Transform in a computer algorithm.
By elimination, the only Illustration of the four Fourier transforms.
A signal may be continuous or discrete, and it may be periodic or aperiodic.
Together these define four possible combinations, each having its own version of the Fourier transform.
The names are not well organized; simply memorize them.
In other words, digital computers can only work with information that is discrete and finite in length.
When you struggle with theoretical issues, grapple with homework problems, and ponder mathematical mysteries, you may find yourself using the first three members of the Fourier transform family.
When you sit down to your computer, you will only use the DFT.
We will briefly look at these other Fourier transforms in future chapters.
For now, concentrate on understanding the Discrete Fourier Transform.
Look back at the example DFT decomposition in Fig. 8-1.
On the face of it, it appears to be a 16 point signal being decomposed into 18 sinusoids, each consisting of 16 points.
In more formal terms, the 16 point signal, shown in (a), must be viewed as a single period of an infinitely long periodic signal.
Likewise, each of the 18 sinusoids, shown in (b), represents a 16 point segment from an infinitely long sinusoid.
Does it really matter if we view this as a point signal being synthesized from 16 point sinusoids, or as an infinitely long periodic signal being synthesized from infinitely long sinusoids?
The answer is: usually no, but sometimes, yes.
In upcoming chapters we will encounter properties of the DFT that seem baffling if the signals are viewed as finite, but become obvious when the periodic nature is considered.
The key point to understand is that this periodicity is invoked in order to use a mathematical tool, i.e., the DFT.
It is usually meaningless in terms of where the signal originated or how it was acquired.
Each of the four Fourier Transforms can be subdivided into real and complex versions.
The real version is the simplest, using ordinary numbers and algebra for the synthesis and decomposition.
For instance, Fig. 8-1 is an example of the real DFT.
The complex versions of the four Fourier transforms are immensely more complicated, requiring the use of complex numbers.
These are numbers such as: 3 %4 j, where j is equal to & (electrical engineers use the variable j, while mathematicians use the variable, i).
Complex mathematics can quickly become overwhelming, even to those that specialize in DSP.
In fact, a primary goal of this book is to present the fundamentals of DSP without the use of complex math, allowing the material to be understood by a wider range of scientists and engineers.
The complex Fourier transforms are the realm of those that specialize in DSP, and are willing to sink to their necks in the swamp of mathematics.
If you are so inclined, Chapters 30-33 will take you there.
The mathematical term: transform, is extensively used in Digital Signal Processing, such as: Fourier transform, Laplace transform, Z transform, Hilbert transform, Discrete Cosine transform, etc.
Just what is a transform?
To answer this question, remember what a function is.
A function is an algorithm or procedure that changes one value into another value.
For example, y ' 2 x %1 is a function.
You pick some value for x, plug it into the equation, and out pops a value for y.
Functions can also change several values into a single value, such as: y ' 2 a % 3 b % 4 c, where a, b, and c are changed into y.
Transforms are a direct extension of this, allowing both the input and output to have multiple values.
Suppose you have a signal composed of 100 samples.
If you devise some equation, algorithm, or procedure for changing these samples into another 100 samples, you have yourself a transform.
If you think it is useful enough, you have the perfect right to attach your last name to it and expound its merits to your colleagues.
Transforms are not limited to any specific type or number of data.
For example, you might have 100 samples of discrete data for the input and 200 samples of discrete data for the output.
Likewise, you might have a continuous signal for the input and a continuous signal for the output.
Mixed signals are also allowed, discrete in and continuous out, and vice versa.
In short, a transform is any fixed procedure that changes one chunk of data into another chunk of data.
Let's see how this applies to the topic at hand: the Discrete Fourier transform.
Notation and Format of the Real DFT As shown in Fig. 8-3, the discrete Fourier transform changes an N point input signal into two N/2 %1 point output signals.
The input signal contains the signal being decomposed, while the two output signals contain the amplitudes of the component sine and cosine waves (scaled in a way we will discuss shortly).
The input signal is said to be in the time domain.
This is because the most common type of signal entering the DFT is composed of DFT terminology.
In the time domain, x[ ] consists of N points running from 0 to N& 1 .
In the frequency domain, the DFT produces two signals, the real part, written: Re X[ ], and the imaginary part, written: Im X [ ] .
Each of these frequency domain signals are N/2 % 1 points long, and run from 0 to N/2 .
The Forward DFT transforms from the time domain to the frequency domain, while the Inverse DFT transforms from the frequency domain to the time domain.
The complex DFT, discussed in Chapter 31, changes N complex points into another set of N complex points).
Of course, any kind of sampled data can be fed into the DFT, regardless of how it was acquired.
When you see the term "time domain" in Fourier analysis, it may actually refer to samples taken over time, or it might be a general reference to any discrete signal that is being decomposed.
The term frequency domain is used to describe the amplitudes of the sine and cosine waves (including the special scaling we promised to explain).
The frequency domain contains exactly the same information as the time domain, just in a different form.
If you know one domain, you can calculate the other.
Given the time domain signal, the process of calculating the frequency domain is called decomposition, analysis, the forward DFT, or simply, the DFT.
If you know the frequency domain, calculation of the time domain is called synthesis, or the inverse DFT.
Both synthesis and analysis can be represented in equation form and computer algorithms.
The number of samples in the time domain is usually represented by the variable N.
While N can be any positive integer, a power of two is usually chosen, i.e., 128, 256, 512, 1024, etc.
There are two reasons for this.
First, digital data storage uses binary addressing, making powers of two a natural signal length.
Second, the most efficient algorithm for calculating the DFT, the Fast Fourier Transform (FFT), usually operates with N that is a power of two.
Typically, N is selected between 32 and 4096.
In most cases, the samples run from 0 to N&1, rather than 1 to N. Standard DSP notation uses lower case letters to represent time domain signals, such as x[ ], y[ ], and z[ ] .
The corresponding upper case letters are used to represent their frequency domains, that is, X [ ], Y[ ], and Z [ ] .
For illustration, assume an N point time domain signal is contained in x[ ] .
The frequency domain of this signal is called X [ ], and consists of two parts, each an array of N/2 %1 samples.
These are called the Real part of X [ ], written as: Re X [ ], and the Imaginary part of X [ ], written as: Im X [ ] .
The values in Re X [ ] are the amplitudes of the cosine waves, while the values in Im X [ ] are the amplitudes of the sine waves (not worrying about the scaling factors for the moment).
Just as the time domain runs from x[0] to x[N&1], the frequency domain signals run from Re X[0] to Re X[N/2], and from Im X[0] to Im X [N/2] .
Study these notations carefully; they are critical to understanding the equations in DSP.
Unfortunately, some computer languages don't distinguish between lower and upper case, making the variable names up to the individual programmer.
The programs in this book use the array XX[ ] to hold the time domain signal, and the arrays REX[ ] and IMX[ ] to hold the frequency domain signals.
The names real part and imaginary part originate from the complex DFT, where they are used to distinguish between real and imaginary numbers.
Nothing so complicated is required for the real DFT.
Until you get to Chapter 31, simply think that "real part" means the cosine wave amplitudes, while "imaginary part" means the sine wave amplitudes.
Don't let these suggestive names mislead you; everything here uses ordinary numbers.
Likewise, don't be misled by the lengths of the frequency domain signals.
It is common in the DSP literature to see statements such as: "The DFT changes an N point time domain signal into an N point frequency domain signal."
This is referring to the complex DFT, where each "point" is a complex number (consisting of real and imaginary parts).
For now, focus on learning the real DFT, the difficult math will come soon enough.
The Frequency Domain's Independent Variable Figure 8-4 shows an example DFT with N ' 128 .
The time domain signal is contained in the array: x[0] to x[127] .
The frequency domain signals are contained in the two arrays: Re X[0] to Re X[64], and Im X [0] to Im X [64] .
Notice that 128 points in the time domain corresponds to 65 points in each of the frequency domain signals, with the frequency indexes running from 0 to 64.
That is, N points in the time domain corresponds to N/2 %1 points in the frequency domain (not N/2 points).
Forgetting about this extra point is a common bug in DFT programs.
The horizontal axis of the frequency domain can be referred to in four different ways, all of which are common in DSP.
In the first method, the horizontal axis is labeled from 0 to 64, corresponding to the 0 to N/ samples in the arrays.
When this labeling is used, the index for the frequency domain is an integer, for example, Re X [k] and Im X [k], where k runs from 0 to N/2 in steps of one.
Programmers like this method because it is how they write code, using an index to access array locations.
This notation is used in Fig. 8-4b.
Example of the DFT.
The DFT converts the time domain signal, x[ ], into the frequency domain signals, Re X[ ] a n d Im X [ ] .
The horizontal axis of the frequency domain can be labeled in one of three ways: (1) as an array index that runs between 0 and N/2, (2) as a fraction of the sampling frequency, running between 0 and 0.5, (3) as a natural frequency, running between 0 and B. In the example shown here, (b) uses the first method, while (c) use the second method.
Frequency (sample number) c.
Im X[ ] Amplitude Amplitude Frequency (fraction of sampling rate) In the second method, used in (c), the horizontal axis is labeled as a fraction of the sampling rate.
This means that the values along the horizonal axis always run between 0 and 0.5, since discrete data can only contain frequencies between DC and one-half the sampling rate.
The index used with this notation is f, for frequency.
The real and imaginary parts are written: Re X [f ] and Im X [f ], where f takes on N/2 %1 equally spaced values between 0 and 0.5.
To convert from the first notation, k, to the second notation, f, divide the horizontal axis by N.
That is, f ' k/N .
Most of the graphs in this book use this second method, reinforcing that discrete signals only contain frequencies between 0 and 0.5 of the sampling rate.
The third style is similar to the second, except the horizontal axis is multiplied by 2 B .
The index used with this labeling is T, a lower case Greek omega.
In this notation, the real and imaginary parts are written: Re X [T] and Im X [T], where T takes on N/2 %1 equally spaced values between 0 and B .
The parameter, T, is called the natural frequency, and has the units of radians.
This is based on the idea that there are 2 B radians in a circle.
Mathematicians like this method because it makes the equations shorter.
For instance, consider how a cosine wave is written in each of The fourth method is to label the horizontal axis in terms of the analog frequencies used in a particular application.
For instance, if the system being examined has a sampling rate of 10 kHz (i.e., 10,000 samples per second), graphs of the frequency domain would run from 0 to 5 kHz.
This method has the advantage of presenting the frequency data in terms of a real world meaning.
The disadvantage is that it is tied to a particular sampling rate, and is therefore not applicable to general DSP algorithm development, such as designing digital filters.
All of these four notations are used in DSP, and you need to become comfortable with converting between them.
This includes both graphs and mathematical equations.
To find which notation is being used, look at the independent variable and its range of values.
You should find one of four notations: k (or some other integer index), running from 0 to N/2 ; f, running from 0 to 0.5; T, running from 0 to B ; or a frequency expressed in hertz, running from DC to one-half of an actual sampling rate.
DFT Basis Functions The sine and cosine waves used in the DFT are commonly called the DFT basis functions.
In other words, the output of the DFT is a set of numbers that represent amplitudes.
The basis functions are a set of sine and cosine waves with unity amplitude.
If you assign each amplitude (the frequency domain) to the proper sine or cosine wave (the basis functions), the result is a set of scaled sine and cosine waves that can be added to form the time domain signal.
The DFT basis functions are generated from the equations: EQUATION 8- Equations for the DFT basis functions.
In these equations, ck[ i] and sk[ i] are the cosine and sine waves, each N points in length, running from i ' 0 to N& 1 .
The parameter, k, determines the frequency of the wave.
In an N point DFT, k takes on values between 0 and N/2 .
For example, Fig. 8-5 shows some of the 17 sine and 17 cosine waves used in an N ' 32 point DFT.
Since these sinusoids add to form the input signal, they must be the same length as the input signal.
In this case, each has 32 points running from i ' 0 to 31.
The parameter, k, sets the frequency of each sinusoid.
In particular, c1[ ] is the cosine wave that makes one complete cycle in N points, c5[ ] is the cosine wave that makes five complete cycles in N points, etc.
This is an important concept in understanding the basis functions; the frequency parameter, k, is equal to the number of complete cycles that occur over the N points of the signal.
DFT basis functions.
A 32 point DFT has 17 discrete cosine waves and 17 discrete sine waves for its basis functions.
Eight of these are shown in this figure.
These are discrete signals; the continuous lines are shown in these graphs only to help the reader's eye follow the waveforms.
Let's look at several of these basis functions in detail.
Figure (a) shows the cosine wave c0[ ] .
This is a cosine wave of zero frequency, which is a constant value of one.
This means that Re X[0] holds the average value of all the points in the time domain signal.
In electronics, it would be said that Re X[0] holds the DC offset.
The sine wave of zero frequency, s0[ ], is shown in (b), a signal composed of all zeros.
Since this can not affect the time domain signal being synthesized, the value of Im X [0] is irrelevant, and always set to zero.
More about this shortly.
Figures (c) & (d) show c2[ ] & s2[ ], the sinusoids that complete two cycles in the N points.
These correspond to Re X [2] & Im X [2], respectively.
Likewise, (e) & (f) show c10[ ] & s10[ ], the sinusoids that complete ten cycles in the N points.
These sinusoids correspond to the amplitudes held in the arrays Re X[10] & Im X [10] .
The problem is, the samples in (e) and (f) no longer look like sine and cosine waves.
If the continuous curves were not present in these graphs, you would have a difficult time even detecting the pattern of the waveforms.
This may make you a little uneasy, but don't worry about it.
From a mathematical point of view, these samples do form discrete sinusoids, even if your eye cannot follow the pattern.
The highest frequencies in the basis functions are shown in (g) and (h).
These are cN/2[ ] & sN/2[ ], or in this example, c16[ ] & s16[ ] .
The discrete cosine wave alternates in value between 1 and -1, which can be interpreted as sampling a continuous sinusoid at the peaks.
In contrast, the discrete sine wave contains all zeros, resulting from sampling at the zero crossings.
This makes the value of Im X [N/2] the same as Im X [0], always equal to zero, and not affecting the synthesis of the time domain signal.
Here's a puzzle: If there are N samples entering the DFT, and N%2 samples exiting, where did the extra information come from?
The answer: two of the output samples contain no information, allowing the other N samples to be fully independent.
As you might have guessed, the points that carry no information are Im X [0] and Im X [N/2], the samples that always have a value of zero.
Synthesis, Calculating the Inverse DFT Pulling together everything said so far, we can write the synthesis equation: The synthesis equation.
In this relation, x [i ] is the signal being synthesized, with the index, i, running from 0 to N& 1 .
Re X [k ] and Im X [k ] hold the amplitudes of the cosine and sine waves, respectively, with k running from 0 to N/2 .
Equation 8-3 provides the normalization to change this equation into the inverse DFT.
In words, any N point signal, x[i], can be created by adding N/2 % 1 cosine waves and N/2 % 1 sine waves.
The amplitudes of the cosine and sine waves are held in the arrays Im X [k] and Re X [k], respectively.
The synthesis equation multiplies these amplitudes by the basis functions to create a set of scaled sine and cosine waves.
Adding the scaled sine and cosine waves produces the time domain signal, x[i] .
In Eq. 8-2, the arrays are called Im X [k] and Re X [k], rather than Im X [k] and Re X [k] .
This is because the amplitudes needed for synthesis (called in this discussion: Im X [k] and Re X [k] ), are slightly different from the frequency domain of a signal (denoted by: Im X [k] and Re X [k] ).
This is the scaling factor issue we referred to earlier.
Although the conversion is only a simple normalization, it is a common bug in computer programs.
Look out for it!
In equation form, the conversion between the two is given by: Conversion between the sinusoidal amplitudes and the frequency domain values.
In these equations, Re X [k ] and Im X [k ] hold the amplitudes of the cosine and sine waves needed for synthesis, while Re X [k ] and Im X [k ] hold the real and imaginary parts of the frequency domain.
As usual, N is the number of points in the time domain signal, and k is an index that runs from 0 to N/2.
Suppose you are given a frequency domain representation, and asked to synthesize the corresponding time domain signal.
To start, you must find the amplitudes of the sine and cosine waves.
In other words, given Im X [k] and Re X [k], you must find Im X [k] and Re X [k] .
Equation 8-3 shows this in a mathematical form.
To do this in a computer program, three actions must be taken.
First, divide all the values in the frequency domain by N/2 .
Second, change the sign of all the imaginary values.
Third, divide the first and last samples in the real part, Re X[0] and Re X[N/2 ], by two.
This provides the amplitudes needed for the synthesis described by Eq. 8-2.
Taken together, Eqs.
The entire Inverse DFT is shown in the computer program listed in Table 8-1.
There are two ways that the synthesis (Eq.
In the first method, each of the scaled sinusoids are generated one at a time and added to an accumulation array, which ends up becoming the time domain signal.
In the second method, each sample in the time domain signal is calculated one at a time, as the sum of all the Example of the Inverse DFT. Figure (a) shows an example time domain signal, an impulse at sample zero with an amplitude of 32. Figure (b) shows the real part of the frequency domain of this signal, a constant value of 32.
The imaginary part of the frequency domain (not shown) is composed of all zeros.
Figure(c) shows the amplitudes of the cosine waves needed to reconstruct (a) using Eq.
The values in (c) are found from (b) by using Eq.
Both methods produce the same result.
The difference between these two programs is very minor; the inner and outer loops are swapped during the synthesis.
Figure 8-6 illustrates the operation of the Inverse DFT, and the slight differences between the frequency domain and the amplitudes needed for synthesis.
Figure 8-6a is an example signal we wish to synthesize, an impulse at sample zero with an amplitude of 32. Figure 8-6b shows the frequency domain representation of this signal.
The real part of the frequency domain is a constant value of 32.
The imaginary part (not shown) is composed of all zeros.
As discussed in the next chapter, this is an important DFT pair: an impulse in the time domain corresponds to a constant value in the frequency domain.
For now, the important point is that (b) is the DFT of (a), and (a) is the Inverse DFT of (b).
Equation 8-3 is used to convert the frequency domain signal, (b), into the amplitudes of the cosine waves, (c).
As shown, all of the cosine waves have an amplitude of two, except for samples 0 and 16, which have a value of one.
The amplitudes of the sine waves are not shown in this example because they have a value of zero, and therefore provide no contribution.
The synthesis equation, Eq. 8-2, is then used to convert the amplitudes of the cosine waves, (b), into the time domain signal, (a).
This describes how the frequency domain is different from the sinusoidal amplitudes, but it doesn't explain why it is different.
The difference occurs because the frequency domain is defined as a spectral density.
Figure 8- shows how this works.
The example in this figure is the real part of the frequency domain of a 32 point signal.
As you should expect, the samples run from 0 to 16, representing 17 frequencies equally spaced between 0 and 1/ of the sampling rate.
Spectral density describes how much signal (amplitude) is present per unit of bandwidth.
To convert the sinusoidal amplitudes into a spectral density, divide each amplitude by the bandwidth represented by each amplitude.
This brings up the next issue: how do we determine the bandwidth of each of the discrete frequencies in the frequency domain?
As shown in the figure, the bandwidth can be defined by drawing dividing lines between the samples.
For instance, sample number 5 occurs in the band between 4.5 and 5.5; sample number 6 occurs in the band between 5. and 6.5, etc. Expressed as a fraction of the total bandwidth (i.e., N/2 ), the bandwidth of each sample is 2/N .
An exception to this is the samples on each end, which have one-half of this bandwidth, 1/N .
This accounts for the 2/N scaling factor between the sinusoidal amplitudes and frequency domain, as well as the additional factor of two needed for the first and last samples.
Why the negation of the imaginary part?
This is done solely to make the real DFT consistent with its big brother, the complex DFT.
In Chapter 29 we will show that it is necessary to make the mathematics of the complex DFT work.
When dealing only with the real DFT, many authors do not include this negation.
For that matter, many authors do not even include The bandwidth of frequency domain samples.
Each sample in the frequency domain can be thought of as being contained in a frequency band of width 2/N, expressed as a fraction of the total bandwidth.
An exception to this is the first and last samples, which have a bandwidth only one-half this wide, 1/N. the 2/N scaling factor.
Be prepared to find both of these missing in some discussions.
They are included here for a tremendously important reason: The most efficient way to calculate the DFT is through the Fast Fourier Transform (FFT) algorithm, presented in Chapter 12.
The FFT generates a frequency domain defined according to Eq. 8-2 and 8-3.
If you start messing with these normalization factors, your programs containing the FFT are not going to work as expected.
Analysis, Calculating the DFT The DFT can be calculated in three completely different ways.
First, the problem can be approached as a set of simultaneous equations.
This method is useful for understanding the DFT, but it is too inefficient to be of practical use.
The second method brings in an idea from the last chapter: correlation.
This is based on detecting a known waveform in another signal.
The third method, called the Fast Fourier Transform (FFT), is an ingenious algorithm that decomposes a DFT with N points, into N DFTs each with a single point.
The FFT is typically hundreds of times faster than the other methods.
The first two methods are discussed here, while the FFT is the topic of Chapter 12.
It is important to remember that all three of these methods produce an identical output.
Which should you use?
In actual practice, correlation is the preferred technique if the DFT has less than about 32 points, otherwise the FFT is used.
DFT by Simultaneous Equations Think about the DFT calculation in the following way.
You are given N values from the time domain, and asked to calculate the N values of the frequency domain (ignoring the two frequency domain values that you know must be zero).
Basic algebra provides the answer: to solve for N unknowns, you must be able to write N linearly independent equations.
To do this, take the first sample from each sinusoid and add them together.
The sum must be equal to the first sample in the time domain signal, thus providing the first equation.
Likewise, an equation can be written for each of the remaining points in the time domain signal, resulting in the required N equations.
The solution can then be found by using established methods for solving simultaneous equations, such as Gauss Elimination.
Unfortunately, this method requires a tremendous number of calculations, and is virtually never used in DSP.
However, it is important for another reason, it shows why it is possible to decompose a signal into sinusoids, how many sinusoids are needed, and that the basis functions must be linearly independent (more about this shortly).
DFT by Correlation Let's move on to a better way, the standard way of calculating the DFT.
An example will show how this method works.
Suppose we are trying to calculate the DFT of a 64 point signal.
This means we need to calculate the 33 points in the real part, and the 33 points in the imaginary part of the frequency domain.
In this example we will only show how to calculate a single sample, Im X [3], i.e., the amplitude of the sine wave that makes three complete cycles between point 0 and point 63.
All of the other frequency domain values are calculated in a similar manner.
Figure 8-8 illustrates using correlation to calculate Im X [3] .
Figures (a) and (b) show two example time domain signals, called: x1[ ] and x2[ ], respectively.
The first signal, x1[ ], is composed of nothing but a sine wave that makes three cycles between points 0 and 63.
In contrast, x2[ ] is composed of several sine and cosine waves, none of which make three cycles between points 0 and 63.
These two signals illustrate what the algorithm for calculating Im X [3] must do.
When fed x1[ ], the algorithm must produce a value of 32, the amplitude of the sine wave present in the signal (modified by the scaling factors of Eq. 8-3).
In comparison, when the algorithm is fed the other signal, x2[ ], a value of zero must be produced, indicating that this particular sine wave is not present in this signal.
The concept of correlation was introduced in Chapter 7. As you recall, to detect a known waveform contained in another signal, multiply the two signals and add all the points in the resulting signal.
The single number that results from this procedure is a measure of how similar the two signals are. Figure 8-8 illustrates this approach.
Figures (c) and (d) both display the signal we are looking for, a sine wave that makes 3 cycles between samples 0 and 63. Figure (e) shows the result of multiplying (a) and (c).
Likewise, (f) shows the result of multiplying (b) and (d).
The sum of all the points in (e) is 32, while the sum of all the points in (f) is zero, showing we have found the desired algorithm.
The other samples in the frequency domain are calculated in the same way.
This procedure is formalized in the analysis equation, the mathematical way to calculate the frequency domain from the time domain: In words, each sample in the frequency domain is found by multiplying the time domain signal by the sine or cosine wave being looked for, and adding the resulting points.
If someone asks you what you are doing, say with confidence: "I am correlating the input signal with each basis function."
Table 8-2 shows a computer program for calculating the DFT in this way.
The analysis equation does not require special handling of the first and last points, as did the synthesis equation.
There is, however, a negative sign in the imaginary part in Eq. 8-4.
Just as before, this negative sign makes the real DFT consistent with the complex DFT, and is not always included.
Two example signals, (a) and (b), are analyzed for containing the specific basis function shown in (c) and (d).
Figures (e) and (f) show the result of multiplying each example signal by the basis function.
Figure (e) has an average of 0.5, indicating that x1 [ ] contains the basis function with an amplitude of 1.0.
Conversely, (f) has a zero average, indicating that x2 [ ] does not contain the basis function.
In order for this correlation algorithm to work, the basis functions must have an interesting property: each of them must be completely uncorrelated with all of the others.
This means that if you multiply any two of the basis functions, the sum of the resulting points will be equal to zero.
Basis functions that have this property are called orthognal.
Many other orthognal basis functions exist, including: square waves, triangle waves, impulses, etc. Signals can be decomposed into these other orthognal basis functions using correlation, just as done here with sinusoids.
This is not to suggest that this is useful, only that it is possible.
As previously shown in Table 8-1, the Inverse DFT has two ways to be implemented in a computer program.
This difference involves swapping the inner and outer loops during the synthesis.
While this does not change the output of the program, it makes a difference in how you view what is being done.
The DFT program in Table 8-2 can also be changed in this fashion, by swapping the inner and outer loops in lines 310 to 380.
Just as before, the output of the program is the same, but the way you think about the calculation is different.
As the program in Table 8-2 is written, it describes how an individual sample in the frequency domain is affected by all of the samples in the time domain.
That is, the program calculates each of the values in the frequency domain in succession, not as a group.
When the inner and outer loops are exchanged, the program loops through each sample in the time domain, calculating the contribution of that point to the frequency domain.
The overall frequency domain is found by adding the contributions from the individual time domain points.
This brings up our next question: what kind of contribution does an individual sample in the time domain provide to the frequency domain?
The answer is contained in an interesting aspect of Fourier analysis called duality.
Duality The synthesis and analysis equations (Eqs.
To move from one domain to the other, the known values are multiplied by the basis functions, and the resulting products added.
The fact that the DFT and the Inverse DFT use this same mathematical approach is really quite remarkable, considering the totally different way we arrived at the two procedures.
In fact, the only significant difference between the two equations is a result of the time domain being one signal of N points, while the frequency domain is two signals of N/2 % 1 points.
As discussed in later chapters, the complex DFT expresses both the time and the frequency domains as complex signals of N points each.
This makes the two domains completely symmetrical, and the equations for moving between them virtually identical.
This symmetry between the time and frequency domains is called duality, and gives rise to many interesting properties.
For example, a single point in the frequency domain corresponds to a sinusoid in the time domain.
By duality, the inverse is also true, a single point in the time domain corresponds to a sinusoid in the frequency domain.
As another example, convolution in the time domain corresponds to multiplication in the frequency domain.
By duality, the reverse is also true: convolution in the frequency domain corresponds to multiplication in the time domain.
These and other duality relationships are discussed in more detail in Chapters and 11.
Polar Notation As it has been described so far, the frequency domain is a group of amplitudes of cosine and sine waves (with slight scaling modifications).
This is called rectangular notation.
Alternatively, the frequency domain can be expressed in polar form.
In this notation, Re X[ ] & Im X [ ] are replaced with two other arrays, called the Magnitude of X [ ], written in equations as: Mag X [ ], and the Phase of X [ ], written as: Phase X [ ] .
The magnitude and phase are a pair-for-pair replacement for the real and imaginary parts.
For example, Mag X [0] and Phase X [0] are calculated using only Re X[0] and Im X [0] .
Likewise, Mag X [14] and Phase X [14] are calculated using only Re X[14] and Im X [14], and so forth.
To understand the conversion, consider what happens when you add a cosine wave and a sine wave of the same frequency.
The result is a cosine wave of the same frequency, but with a new amplitude and a new phase shift.
In equation form, the two representations are related: EQUATION 8- The addition of a cosine and sine wave results in a cosine wave with a different amplitude and phase shift.
The information contained in A & B is transferred to two other variables, M and 2. A cos (x) % B sin(x) ' M cos ( x % 2) The important point is that no information is lost in this process; given one representation you can calculate the other.
In other words, the information contained in the amplitudes A and B, is also contained in the variables M and 2.
Although this equation involves sine and cosine waves, it follows the same conversion equations as do simple vectors.
Figure 8-9 shows the analogous vector representation of how the two variables, A and B, can be viewed in a rectangular coordinate system, while M and 2 are parameters in polar coordinates.
Rectangular-to-polar conversion.
The addition of a cosine wave and a sine wave (of the same frequency) follows the same mathematics as the addition of simple vectors.
In polar notation, Mag X [ ] holds the amplitude of the cosine wave (M in Eq. 8-5 and Fig. 8-9), while Phase X [ ] holds the phase angle of the cosine wave ( 2 in Eq. 8-5 and Fig. 8-9).
The following equations convert the frequency domain from rectangular to polar notation, and vice versa: EQUATION 8- Rectangular-to-polar conversion.
The rectangular representation of the frequency domain, Re X[k] and Im X [k], is changed into the polar form, Mag X [k] and Phase X [k] .
Rectangular and polar notation allow you to think of the DFT in two different ways.
With rectangular notation, the DFT decomposes an N point signal into N/2 % cosine waves and N/2 % 1 sine waves, each with a specified amplitude.
In polar notation, the DFT decomposes an N point signal into N/2 % 1 cosine waves, each with a specified amplitude (called the magnitude) and phase shift.
Why does polar notation use cosine waves instead of sine waves?
Sine waves cannot represent the DC component of a signal, since a sine wave of zero frequency is composed of all zeros (see Figs. 8-5 a&b).
Even though the polar and rectangular representations contain exactly the same information, there are many instances where one is easier to use that the other.
For example, Fig. 8-10 shows a frequency domain signal in both rectangular and polar form.
Warning: Don't try to understand the shape of the real and imaginary parts; your head will explode!
In comparison, the polar curves are straightforward: only frequencies below about 0.25 are present, and the phase shift is approximately proportional to the frequency.
This is the frequency response of a low-pass filter.
Example of rectangular and polar frequency domains.
This example shows a frequency domain expressed in both rectangular and polar notation.
As in this case, polar notation usually provides human observers with a better understanding of the characteristics of the signal.
In comparison, the rectangular form is almost always used when math computations are required.
Pay special notice to the fact that the first and last samples in the phase must be zero, just as they are in the imaginary part.
When should you use rectangular notation and when should you use polar?
Rectangular notation is usually the best choice for calculations, such as in equations and computer programs.
In comparison, graphs are almost always in polar form.
As shown by the previous example, it is nearly impossible for humans to understand the characteristics of a frequency domain signal by looking at the real and imaginary parts.
In a typical program, the frequency domain signals are kept in rectangular notation until an observer needs to look at them, at which time a rectangular-to-polar conversion is done.
Why is it easier to understand the frequency domain in polar notation?
This question goes to the heart of why decomposing a signal into sinusoids is useful.
Recall the property of sinusoidal fidelity from Chapter 5: if a sinusoid enters a linear system, the output will also be a sinusoid, and at exactly the same frequency as the input.
Only the amplitude and phase can change.
Polar notation directly represents signals in terms of the amplitude and phase of the component cosine waves.
In turn, systems can be represented by how they modify the amplitude and phase of each of these cosine waves.
Now consider what happens if rectangular notation is used with this scenario.
A mixture of cosine and sine waves enter the linear system, resulting in a mixture of cosine and sine waves leaving the system.
The problem is, a cosine wave on the input may result in both cosine and sine waves on the output.
Likewise, a sine wave on the input can result in both cosine and sine waves on the output.
While these cross-terms can be straightened out, the overall method doesn't match with why we wanted to use sinusoids in the first place.
Polar Nuisances There are many nuisances associated with using polar notation.
None of these are overwhelming, just really annoying!
Table 8-3 shows a computer program for converting between rectangular and polar notation, and provides solutions for some of these pests.
Nuisance 1: Radians vs. Degrees It is possible to express the phase in either degrees or radians.
When expressed in degrees, the values in the phase signal are between -180 and 180.
Using radians, each of the values will be between -B and B, that is, between -3.141592 to 3.141592.
Most computer languages require the use radians for their trigonometric functions, such as cosine, sine, arctangent, etc.
It can be irritating to work with these long decimal numbers, and difficult to interpret the data you receive.
For example, if you want to introduce a 90 degree phase shift into a signal, you need to add 1.570796 to the phase.
While it isn't going to kill you to type this into your program, it does become tiresome.
The best way to handle this problem is to define the constant, PI = 3.141592, at the beginning of your program.
A 90 degree phase shift can then be written as PI /2 .
Degrees and radians are both widely used in DSP and you need to become comfortable with both.
Nuisance 2: Divide by zero error When converting from rectangular to polar notation, it is very common to find frequencies where the real part is zero and the imaginary part is some nonzero value.
This simply means that the phase is exactly 90 or - degrees.
Try to tell your computer this!
When your program tries to calculate the phase from: Phase X [k] ' arctan( Im X [k] / Re X [k]), a divide by zero error occurs.
Even if the program execution doesn't halt, the phase you obtain for this frequency won't be correct.
To avoid this problem, the real part must be tested for being zero before the division.
If it is zero, the imaginary part must be tested for being positive or negative, to determine whether to set the phase to B /2 or - B /2, respectively.
Lastly, the division needs to be bypassed.
Nothing difficult in all these steps, just the potential for aggravation.
An alternative way to handle this problem is shown in line 250 of Table 8-3.
If the real part is zero, change it to a negligibly small number to keep the math processor happy during the division.
Nuisance 3: Incorrect arctan Consider a frequency domain sample where Re X[k] ' 1 and Im X [k] ' 1 .
Equation 8-6 provides the corresponding polar values of Mag X[k] ' 1.414 and Phase X [k] ' 45E .
Now consider another sample where Re X[k] ' & 1 and Im X [k] ' & 1 .
Again, Eq. 8-6 provides the values of Mag X [k] ' 1.414 and Phase X [k] ' 45E .
The problem is, the phase is wrong!
It should be & 135E .
This error occurs whenever the real part is negative.
This problem can be corrected by testing the real and imaginary parts after the phase has been calculated.
If both the real and imaginary parts are negative, subtract 180E (or B radians) from the calculated phase.
If the real part is negative and the imaginary part is positive, add 180E (or B radians).
Lines 340 and 350 of the program in Table 8-3 show how this is done.
If you fail to catch this problem, the calculated value of the phase will only run between -B/2 and B/2, rather than between -B and B. Drill this into your mind.
If you see the phase only extending to ±1.5708, you have forgotten to correct the ambiguity in the arctangent calculation.
Nuisance 4: Phase of very small magnitudes Imagine the following scenario.
You are grinding away at some DSP task, and suddenly notice that part of the phase doesn't look right.
It might be noisy, jumping all over, or just plain wrong.
After spending the next hour looking through hundreds of lines of computer code, you find the answer.
The corresponding values in the magnitude are so small that they are buried in round-off noise.
If the magnitude is negligibly small, the phase doesn't have any meaning, and can assume unusual values.
An example of this is shown in Fig. 8-11.
It is usually obvious when an amplitude signal is lost in noise; the values are so small that you are forced to suspect that the values are meaningless.
The phase is different.
When a polar signal is contaminated with noise, the values in the phase are random numbers between -B and B. Unfortunately, this often looks like a real signal, rather than the nonsense it really is.
Nuisance 5: 2 B ambiguity of the phase Look again at Fig. 8-10d, and notice the several discontinuities in the data.
Every time a point looks as if it is going to dip below -3.14592, it snaps back to 3.141592.
This is a result of the periodic nature of sinusoids.
For The phase of small magnitude signals.
At frequencies where the magnitude drops to a very low value, round-off noise can cause wild excursions of the phase.
Don't make the mistake of thinking this is a meaningful signal.
Example of phase unwrapping.
The top curve shows a typical phase signal obtained from a rectangular-to-polar conversion routine.
Each value in the signal must be between -B and B (i.e., -3.14159 and 3.14159).
As shown in the lower curve, the phase can be unwrapped by adding or subtracting integer multiplies of 2B from each sample, where the integer is chosen to minimize the discontinuities between points.
Frequency example, a phase shift of θ is exactly the same as a phase shift of θ + 2π, θ + 4π, θ + 6π, etc. Any sinusoid is unchanged when you add an integer multiple of 2B to the phase.
The apparent discontinuities in the signal are a result of the computer algorithm picking its favorite choice from an infinite number of equivalent possibilities.
The smallest possible value is always chosen, keeping the phase between -B and B. It is often easier to understand the phase if it does not have these discontinuities, even if it means that the phase extends above B, or below -B.
This is called unwrapping the phase, and an example is shown in Fig. 8-12.
As shown by the program in Table 8-4, a multiple of 2B is added or subtracted from each value of the phase.
The exact value is determined by an algorithm that minimizes the difference between adjacent samples.
Nuisance 6: The magnitude is always positive (B ambiguity of the phase) Figure 8-13 shows a frequency domain signal in rectangular and polar form.
The real part is smooth and quite easy to understand, while the imaginary part is entirely zero.
In comparison, the polar signals contain abrupt Example signals in rectangular and polar form.
Since the magnitude must always be positive (by definition), the magnitude and phase may contain abrupt discontinuities and sharp corners. Figure (d) also shows another nuisance: random noise can cause the phase to rapidly oscillate between B or -B .
This is because the magnitude must always be positive, by definition.
Whenever the real part dips below zero, the magnitude remains positive by changing the phase by B (or -B, which is the same thing).
While this is not a problem for the mathematics, the irregular curves can be difficult to interpret.
One solution is to allow the magnitude to have negative values.
In the example of Fig. 8-13, this would make the magnitude appear the same as the real part, while the phase would be entirely zero.
There is nothing wrong with this if it helps your understanding.
Just be careful not to call a signal with negative values the "magnitude" since this violates its formal definition.
In this book we use the weasel words: unwrapped magnitude to indicate a "magnitude" that is allowed to have negative values.
Nuisance 7: Spikes between B and - B Since B and - B represent the same phase shift, round-off noise can cause adjacent points in the phase to rapidly switch between the two values.
As shown in Fig. 8-13d, this can produce sharp breaks and spikes in an otherwise smooth curve.
Don't be fooled, the phase isn't really this discontinuous.
The Discrete Fourier Transform (DFT) is one of the most important tools in Digital Signal Processing.
This chapter discusses three common ways it is used.
First, the DFT can calculate a signal's frequency spectrum.
This is a direct examination of information encoded in the frequency, phase, and amplitude of the component sinusoids.
For example, human speech and hearing use signals with this type of encoding.
Second, the DFT can find a system's frequency response from the system's impulse response, and vice versa.
This allows systems to be analyzed in the frequency domain, just as convolution allows systems to be analyzed in the time domain.
Third, the DFT can be used as an intermediate step in more elaborate signal processing techniques.
The classic example of this is FFT convolution, an algorithm for convolving signals that is hundreds of times faster than conventional methods.
Spectral Analysis of Signals It is very common for information to be encoded in the sinusoids that form a signal.
This is true of naturally occurring signals, as well as those that have been created by humans.
Many things oscillate in our universe.
For example, speech is a result of vibration of the human vocal cords; stars and planets change their brightness as they rotate on their axes and revolve around each other; ship's propellers generate periodic displacement of the water, and so on.
The shape of the time domain waveform is not important in these signals; the key information is in the frequency, phase and amplitude of the component sinusoids.
The DFT is used to extract this information.
An example will show how this works.
Suppose we want to investigate the sounds that travel through the ocean.
To begin, a microphone is placed in the water and the resulting electronic signal amplified to a reasonable level, say a few volts.
An analog low-pass filter is then used to remove all frequencies above 80 hertz, so that the signal can be digitized at 160 samples per second.
After acquiring and storing several thousand samples, what next?
The first thing is to simply look at the data.
Figure 9-1a shows 256 samples from our imaginary experiment.
All that can be seen is a noisy waveform that conveys little information to the human eye.
For reasons explained shortly, the next step is to multiply this signal by a smooth curve called a Hamming window, shown in (b).
This results in a 256 point signal where the samples near the ends have been reduced in amplitude, as shown in (c).
Taking the DFT, and converting to polar notation, results in the 129 point frequency spectrum in (d).
Unfortunately, this also looks like a noisy mess.
This is because there is not enough information in the original 256 points to obtain a well behaved curve.
Using a longer DFT does nothing to help this problem.
For example, if a 2048 point DFT is used, the frequency spectrum becomes 1025 samples long.
Even though the original 2048 points contain more information, the greater number of samples in the spectrum dilutes the information by the same factor.
Longer DFTs provide better frequency resolution, but the same noise level.
The answer is to use more of the original signal in a way that doesn't increase the number of points in the frequency spectrum.
This can be done by breaking the input signal into many 256 point segments.
Each of these segments is multiplied by the Hamming window, run through a 256 point DFT, and converted to polar notation.
The resulting frequency spectra are then averaged to form a single 129 point frequency spectrum.
Figure (e) shows an example of averaging 100 of the frequency spectra typified by (d).
The improvement is obvious; the noise has been reduced to a level that allows interesting features of the signal to be observed.
Only the magnitude of the frequency domain is averaged in this manner; the phase is usually discarded because it doesn't contain useful information.
The random noise reduces in proportion to the square-root of the number of segments.
While 100 segments is typical, some applications might average millions of segments to bring out weak features.
There is also a second method for reducing spectral noise.
Start by taking a very long DFT, say 16,384 points.
The resulting frequency spectrum is high resolution (8193 samples), but very noisy.
A low-pass digital filter is then used to smooth the spectrum, reducing the noise at the expense of the resolution.
For example, the simplest digital filter might average 64 adjacent samples in the original spectrum to produce each sample in the filtered spectrum.
Going through the calculations, this provides about the same noise and resolution as the first method, where the 16,384 points would be broken into 64 segments of 256 points each.
Which method should you use?
The first method is easier, because the digital filter isn't needed.
The second method has the potential of better performance, because the digital filter can be tailored to optimize the tradeoff between noise and resolution.
However, this improved performance is seldom worth the trouble.
This is because both noise and resolution can be improved by using more data from the input signal.
For example, An example of spectral analysis.
Figure (a) shows 256 samples taken from a (simulated) undersea microphone at a rate of 160 samples per second.
This signal is multiplied by the Hamming window shown in (b), resulting in the windowed signal in (c).
The frequency spectrum of the windowed signal is found using the DFT, and is displayed in (d) (magnitude only).
Averaging 100 of these spectra reduces the random noise, resulting in the averaged frequency spectrum shown in (e).
Frequency imagine breaking the acquired data into 10,000 segments of 16,384 samples each.
This resulting frequency spectrum is high resolution (8193 points) and low noise (10,000 averages).
Problem solved!
For this reason, we will only look at the averaged segment method in this discussion.
Figure 9-2 shows an example spectrum from our undersea microphone, illustrating the features that commonly appear in the frequency spectra of acquired signals.
Ignore the sharp peaks for a moment.
Between 10 and hertz, the signal consists of a relatively flat region.
This is called white noise because it contains an equal amount of all frequencies, the same as white light.
It results from the noise on the time domain waveform being uncorrelated from sample-to-sample.
That is, knowing the noise value present on any one sample provides no information on the noise value present on any other sample.
For example, the random motion of electrons in electronic circuits produces white noise.
As a more familiar example, the sound of the water spray hitting the shower floor is white noise.
The white noise shown in Fig. 9-2 could be originating from any of several sources, including the analog electronics, or the ocean itself.
Above 70 hertz, the white noise rapidly decreases in amplitude.
This is a result of the roll-off of the antialias filter.
An ideal filter would pass all frequencies below 80 hertz, and block all frequencies above.
In practice, a perfectly sharp cutoff isn't possible, and you should expect to see this gradual drop.
If you don't, suspect that an aliasing problem is present.
Below about 10 hertz, the noise rapidly increases due to a curiosity called 1/f noise (one-over-f noise).
It has been measured in very diverse systems, such as traffic density on freeways and electronic noise in transistors.
It probably could be measured in all systems, if you look low enough in frequency.
In spite of its wide occurrence, a general theory and understanding of 1/f noise has eluded researchers.
The cause of this noise can be identified in some specific systems; however, this doesn't answer the question of why 1/f noise is everywhere.
For common analog electronics and most physical systems, the transition between white noise and 1/f noise occurs between about 1 and 100 hertz.
Now we come to the sharp peaks in Fig. 9-2.
The easiest to explain is at hertz, a result of electromagnetic interference from commercial electrical power.
Also expect to see smaller peaks at multiples of this frequency (120, 180, 240 hertz, etc.) since the power line waveform is not a perfect sinusoid.
It is also common to find interfering peaks between 25-40 kHz, a favorite for designers of switching power supplies.
Nearby radio and television stations produce interfering peaks in the megahertz range.
Low frequency peaks can be caused by components in the system vibrating when shaken.
This is called microphonics, and typically creates peaks at 10 to 100 hertz.
Now we come to the actual signals.
There is a strong peak at 13 hertz, with weaker peaks at 26 and 39 hertz.
As discussed in the next chapter, this is the frequency spectrum of a nonsinusoidal periodic waveform.
The peak at hertz is called the fundamental frequency, while the peaks at 26 and 1/f noise Example frequency spectrum.
Three types of features appear in the spectra of acquired signals: (1) random noise, such as white noise and 1/f noise, (2) interfering signals from power lines, switching power supplies, radio and TV stations, microphonics, etc., and (3) real signals, usually appearing as a fundamental plus harmonics.
This example spectrum (magnitude only) shows several of these features.
You would also expect to find peaks at other multiples of 13 hertz, such as 52, 65, 78 hertz, etc.
You don't see these in Fig. 9-2 because they are buried in the white noise.
This 13 hertz signal might be generated, for example, by a submarines's three bladed propeller turning at 4.33 revolutions per second.
This is the basis of passive sonar, identifying undersea sounds by their frequency and harmonic content.
Suppose there are peaks very close together, such as shown in Fig. 9-3.
There are two factors that limit the frequency resolution that can be obtained, that is, how close the peaks can be without merging into a single entity.
The first factor is the length of the DFT.
The frequency spectrum produced by an N point DFT consists of N/2 %1 samples equally spaced between zero and onehalf of the sampling frequency.
To separate two closely spaced frequencies, the sample spacing must be smaller than the distance between the two peaks.
For example, a 512 point DFT is sufficient to separate the peaks in Fig. 9-3, while a 128 point DFT is not.
The second factor limiting resolution is more subtle.
Imagine a signal created by adding two sine waves with only a slight difference in their frequencies.
Over a short segment of this signal, say a few periods, the waveform will look like a single sine wave.
The closer the frequencies, the longer the segment must be to conclude that more than one frequency is present.
In other words, the length of the signal limits the frequency resolution.
This is distinct from the first factor, because the length of the input signal does not have to be the same as the length of the DFT.
For example, a 256 point signal could be padded with zeros to make it points long.
Taking a 2048 point DFT produces a frequency spectrum with 1025 samples.
The added zeros don't change the shape of the spectrum, they only provide more samples in the frequency domain.
In spite of this very close sampling, the ability to separate closely spaced peaks would be only slightly better than using a 256 point DFT.
When the DFT is the same length as the input signal, the resolution is limited about equally by these two factors.
We will come back to this issue shortly.
Next question: What happens if the input signal contains a sinusoid with a frequency between two of the basis functions?
Figure 9-4a shows the answer.
This is the frequency spectrum of a signal composed of two sine waves, one having a frequency matching a basis function, and the other with a frequency between two of the basis functions.
As you should expect, the first sine wave is represented as a single point.
The other peak is more difficult to understand.
Since it cannot be represented by a single sample, it becomes a peak with tails that extend a significant distance away.
The solution?
Multiply the signal by a Hamming window before taking the DFT, as was previously discussed.
Figure (b) shows that the spectrum is changed in three ways by using the window.
First, the two peaks are made to look more alike.
This is good.
Second, the tails are greatly reduced.
Example of using a window in spectral analysis.
Figure (a) shows the frequency spectrum (magnitude only) of a signal consisting of two sine waves.
One sine wave has a frequency exactly equal to a basis function, allowing it to be represented by a single sample.
The other sine wave has a frequency between two of the basis functions, resulting in tails on the peak.
Figure (b) shows the frequency spectrum of the same signal, but with a Hamming window applied before taking the DFT.
The window makes the peaks look the same and reduces the tails, but broadens the peaks.
This is also good.
Third, the window reduces the resolution in the spectrum by making the peaks wider.
This is bad.
In DSP jargon, windows provide a tradeoff between resolution (the width of the peak) and spectral leakage (the amplitude of the tails).
To explore the theoretical aspects of this in more detail, imagine an infinitely long discrete sine wave at a frequency of 0.1 the sampling rate.
The frequency spectrum of this signal is an infinitesimally narrow peak, with all other frequencies being zero.
Of course, neither this signal nor its frequency spectrum can be brought into a digital computer, because of their infinite and infinitesimal nature.
To get around this, we change the signal in two ways, both of which distort the true frequency spectrum.
First, we truncate the information in the signal, by multiplying it by a window.
For example, a 256 point rectangular window would allow 256 points to retain their correct value, while all the other samples in the infinitely long signal would be set to a value of zero.
Likewise, the Hamming window would shape the retained samples, besides setting all points outside the window to zero.
The signal is still infinitely long, but only a finite number of the samples have a nonzero value.
How does this windowing affect the frequency domain?
As discussed in Chapter 10, when two time domain signals are multiplied, the corresponding frequency domains are convolved.
Since the original spectrum is an infinitesimally narrow peak (i.e., a delta function), the spectrum of the windowed signal is the spectrum of the window shifted to the location of the peak.
Figure 9-5 shows how the spectral peak would appear using four different window options (If you need a refresher on dB, look ahead to Chapter 14). Figure 9-5a results from a rectangular window.
Figures (b) and (c) result from using two popular windows, the Hamming and the Blackman (as previously mentioned, see Eqs. 16-1 and 16-2, and Fig. 16-2a for information on these windows).
As shown in Fig. 9-5, all these windows have degraded the original spectrum by broadening the peak and adding tails composed of numerous side lobes.
This is an unavoidable result of using only a portion of the original time domain signal.
Here we can see the tradeoff between the three windows.
The Blackman has the widest main lobe (bad), but the lowest amplitude tails (good).
The rectangular window has the narrowest main lobe (good) but the largest tails (bad).
The Hamming window sits between these two.
Notice in Fig. 9-5 that the frequency spectra are continuous curves, not discrete samples.
After windowing, the time domain signal is still infinitely long, even though most of the samples are zero.
This means that the frequency spectrum consists of 4 /2 %1 samples between 0 and 0.5, the same as a continuous line.
This brings in the second way we need to modify the time domain signal to allow it to be represented in a computer: select N points from the signal.
These N points must contain all the nonzero points identified by the window, but may also include any number of the zeros.
This has the effect Detailed view of a spectral peak using various windows.
Each peak in the frequency spectrum is a central lobe surrounded by tails formed from side lobes.
By changing the window shape, the amplitude of the side lobes can be reduced at the expense of making the main lobe wider.
The rectangular window, (a), has the narrowest main lobe but the largest amplitude side lobes.
The Hamming window, (b), and the Blackman window, (c), have lower amplitude side lobes at the expense of a wider main lobe.
The flat-top window, (d), is used when the amplitude of a peak must be accurately measured.
These curves are for 255 point windows; longer windows produce proportionately narrower peaks. of sampling the frequency spectrum's continuous curve.
For example, if N is chosen to be 1024, the spectrum's continuous curve will be sampled 513 times between 0 and 0.5.
If N is chosen to be much larger than the window length, the samples in the frequency domain will be close enough that the peaks and valleys of the continuous curve will be preserved in the new spectrum.
If N is made the same as the window length, the fewer number of samples in the spectrum results in the regular pattern of peaks and valleys turning into irregular tails, depending on where the samples happen to fall.
This explains why the two peaks in Fig. 94a do not look alike.
Each peak in Fig 9-4a is a sampling of the underlying curve in Fig. 9-5a.
The presence or absence of the tails depends on where the samples are taken in relation to the peaks and valleys.
If the sine wave exactly matches a basis function, the samples occur exactly at the valleys, eliminating the tails.
If the sine wave is between two basis functions, the samples occur somewhere along the peaks and valleys, resulting in various patterns of tails.
This leads us to the flat-top window, shown in Fig. 9-5d.
In some applications the amplitude of a spectral peak must be measured very accurately.
Since the DFT’s frequency spectrum is formed from samples, there is nothing to guarantee that a sample will occur exactly at the top of a peak.
More than likely, the nearest sample will be slightly off-center, giving a value lower than the true amplitude.
The solution is to use a window that produces a spectral peak with a flat top, insuring that one or more of the samples will always have the correct peak value.
As shown in Fig. 9-5d, the penalty for this is a very broad main lobe, resulting in poor frequency resolution.
As it turns out, the shape we want for a flat-top window is exactly the same shape as the filter kernel of a low-pass filter.
We will discuss the theoretical reasons for this in later chapters; for now, here is a cookbook description of how the technique is used.
Chapter 16 discusses a low-pass filter called the windowed-sinc.
Equation 16-4 describes how to generate the filter kernel (which we want to use as a window), and Fig. 16-4a illustrates the typical shape of the curve.
To use this equation, you will need to know the value of two parameters: M and fc .
These are found from the relations: M ' N & 2, and fc ' s/N, where N is the length of the DFT being used, and s is the number of samples you want on the flat portion of the peak (usually between 3 and 5).
Table 16-1 shows a program for calculating the filter kernel (our window), including two subtle features: the normalization constant, K, and how to avoid a divide-by-zero error on the center sample.
When using this method, remember that a DC value of one in the time domain will produce a peak of amplitude one in the frequency domain.
However, a sinusoid of amplitude one in the time domain will only produce a spectral peak of amplitude one-half.
Frequency Response of Systems Systems are analyzed in the time domain by using convolution.
A similar analysis can be done in the frequency domain.
Using the Fourier transform, every input signal can be represented as a group of cosine waves, each with a specified amplitude and phase shift.
Likewise, the DFT can be used to represent every output signal in a similar form.
This means that any linear system can be completely described by how it changes the amplitude and phase of cosine waves passing through it.
This information is called the system's frequency response.
Since both the impulse response and the frequency response contain complete information about the system, there must be a oneto-one correspondence between the two.
Given one, you can calculate the other.
The relationship between the impulse response and the frequency response is one of the foundations of signal processing: A system's frequency response is the Fourier Transform of its impulse response.
Figure 9- illustrates these relationships.
Keeping with standard DSP notation, impulse responses use lower case variables, while the corresponding frequency responses are upper case.
Since h[ ] is the common symbol for the impulse response, H[ ] is used for the frequency response.
Systems are described in the time domain by convolution, that is: x[n] t h[n] ' y[n] .
In the frequency domain, the input spectrum is multiplied by the frequency response, resulting in the output spectrum.
As an equation: X [f ] × H[ f ] ' Y[ f ] .
That is, convolution in the time domain corresponds to multiplication in the frequency domain.
Figure 9-7 shows an example of using the DFT to convert a system's impulse response into its frequency response.
Figure (a) is the impulse response of the system.
Looking at this curve isn't going to give you the slightest idea what the system does.
Taking a 64 point DFT of this impulse response produces the frequency response of the system, shown in (b).
Now the function of this system becomes obvious, it passes frequencies between 0.2 and 0.3, and rejects all others.
It is a band-pass filter.
The phase of the frequency response could also be examined; however, it is more difficult to interpret and less interesting.
It will be discussed in upcoming chapters.
Figure (b) is very jagged due to the low number of samples defining the curve.
This situation can be improved by padding the impulse response with zeros before taking the DFT.
For example, adding zeros to make the impulse response 512 samples long, as shown in (c), results in the higher resolution frequency response shown in (d).
How much resolution can you obtain in the frequency response?
The answer is: infinitely high, if you are willing to pad the impulse response with an infinite number of zeros.
In other words, there is nothing limiting the frequency resolution except the length of the DFT.
This leads to a very important concept.
Even though the impulse response is a discrete signal, the corresponding frequency response is continuous.
An N point DFT of the impulse response provides N/2 % 1 samples of this continuous curve.
If you make the DFT longer, the resolution improves, and you obtain a better idea of Comparing system operation in the time and frequency domains.
In the time domain, an input signal is convolved with an impulse response, resulting in the output signal, that is, x[n] t h[n] ' y[n] .
In the frequency domain, an input spectrum is multiplied by a frequency response, resulting in the output spectrum, that is, X[f] × H[f] ' Y[f] .
The DFT and the Inverse DFT relate the signals in the two domain.
Finding the frequency response from the impulse response.
By using the DFT, a system's impulse response, (a), can be transformed into the system's frequency response, (b).
By padding the impulse response with zeros (c), higher resolution can be obtained in the frequency response, (d).
Only the magnitude of the frequency response is shown in this example; discussion of the phase is postponed until the next chapter.
Remember what the frequency response represents: amplitude and phase changes experienced by cosine waves as they pass through the system.
Since the input signal can contain any frequency between 0 and 0.5, the system's frequency response must be a continuous curve over this range.
This can be better understood by bringing in another member of the Fourier transform family, the Discrete Time Fourier Transform (DTFT).
Consider an N sample signal being run through an N point DFT, producing an N/2 % 1 sample frequency domain.
Remember from the last chapter that the DFT considers the time domain signal to be infinitely long and periodic.
That is, the N points are repeated over and over from negative to positive infinity.
Now consider what happens when we start to pad the time domain signal with an ever increasing number of zeros, to obtain a finer and finer sampling in the frequency domain.
Adding zeros makes the period of the time domain longer, while simultaneously making the frequency domain samples closer together.
Now we will take this to the extreme, by adding an infinite number of zeros to the time domain signal.
This produces a different situation in two respects.
First, the time domain signal now has an infinitely long period.
In other words, it has turned into an aperiodic signal.
Second, the frequency domain has achieved an infinitesimally small spacing between samples.
That is, it has become a continuous signal.
This is the DTFT, the procedure that changes a discrete aperiodic signal in the time domain into a frequency domain that is a continuous curve.
In mathematical terms, a system's frequency response is found by taking the DTFT of its impulse response.
Since this cannot be done in a computer, the DFT is used to calculate a sampling of the true frequency response.
This is the difference between what you do in a computer (the DFT) and what you do with mathematical equations (the DTFT).
Convolution via the Frequency Domain Suppose that you despise convolution.
What are you going to do if given an input signal and impulse response, and need to find the resulting output signal? Figure 9-8 provides an answer: transform the two signals into the frequency domain, multiply them, and then transform the result back into the time domain.
This replaces one convolution with two DFTs, a multiplication, and an Inverse DFT.
Even though the intermediate steps are very different, the output is identical to the standard convolution algorithm.
Does anyone hate convolution enough to go to this trouble?
The answer is yes.
Convolution is avoided for two reasons.
First, convolution is mathematically difficult to deal with.
For instance, suppose you are given a system's impulse response, and its output signal.
How do you calculate what the input signal is?
This is called deconvolution, and is virtually impossible to understand in the time domain.
However, deconvolution can be carried out in the frequency domain as a simple division, the inverse operation of multiplication.
The frequency domain becomes attractive whenever the complexity of the Fourier Transform is less than the complexity of the convolution.
This isn't a matter of which you like better; it is a matter of which you hate less.
The second reason for avoiding convolution is computation speed.
For example, suppose you design a digital filter with a kernel (impulse response) containing 512 samples.
Using a 200 MHz personal computer with floating point numbers, each sample in the output signal requires about one millisecond to calculate, using the standard convolution algorithm.
In other words, the throughput of the system is only about 1,000 samples per second.
This is times too slow for high-fidelity audio, and 10,000 times too slow for television quality video!
The standard convolution algorithm is slow because of the large number of multiplications and additions that must be calculated.
Unfortunately, simply bringing the problem into the frequency domain via the DFT doesn't help at all.
Just as many calculations are required to calculate the DFTs, as are required to directly calculate the convolution.
A breakthrough was made in the problem in the early 1960s when the Fast Fourier Transform (FFT) was developed.
Frequency domain convolution.
In the time domain, x[n] is convolved with h[n] resulting in y[n], as is shown in Figs.
This same procedure to be accomplished in the frequency domain.
The DFT is used to find the frequency spectrum of the input signal, (b) & (c), and the system's frequency response, (e) & (f).
Multiplying these two frequency domain signals results in the frequency spectrum of the output signal, (h) & (i).
The Inverse DFT is then used to find the output signal, (g).
The FFT is a clever algorithm for rapidly calculating the DFT.
Using the FFT, convolution by multiplication in the frequency domain can be hundreds of times faster than conventional convolution.
Problems that take hours of calculation time are reduced to only minutes.
This is why people get excited about the FFT, and processing signals in the frequency domain.
The FFT will be presented in Chapter 12, and the method of FFT convolution in Chapter 18.
For now, focus on how signals are convolved by frequency domain multiplication.
To start, we need to define how to multiply one frequency domain signal by another, i.e., what it means to write: X [f ] × H[ f ] ' Y[f ] .
In polar form, the magnitudes are multiplied: Mag Y[ f ] ' Mag X [f ] × MagH [f ], and the phases are added: Phase Y[ f ] ' PhaseX [f ] % Phase H[ f ] .
To understand this, imagine a cosine wave entering a system with some amplitude and phase.
Likewise, the output signal is also a cosine wave with some amplitude and phase.
The polar form of the frequency response directly describes how the two amplitudes are related and how the two phases are related.
When frequency domain multiplication is carried out in rectangular form there are cross terms between the real and imaginary parts.
For example, a sine wave entering the system can produce both cosine and sine waves in the output.
To multiply frequency domain signals in rectangular notation: Focus on understanding multiplication using polar notation, and the idea of cosine waves passing through the system.
Then simply accept that these more elaborate equations result when the same operations are carried out in rectangular form.
For instance, let's look at the division of one frequency domain signal by another.
In polar form, the division of frequency domain signals is achieved by the inverse operations we used for multiplication.
To calculate: H [f ] ' Y[f ] / X [f ], divide the magnitudes and subtract the phases, i.e., Mag H[ f ] ' Mag Y[f ] / MagX [f ], Phase H[ f ] ' PhaseY[f ] & Phase X[ f ] .
In rectangular form this becomes: Now back to frequency domain convolution.
You may have noticed that we cheated slightly in Fig. 9-8.
Remember, the convolution of an N point signal with an M point impulse response results in an N% M& 1 point output signal.
We cheated by making the last part of the input signal all zeros to allow this expansion to occur.
Specifically, (a) contains 453 nonzero samples, and (b) contains 60 nonzero samples.
This means the convolution of the two, shown in (c), can fit comfortably in the 512 points provided.
Circular convolution.
A 256 sample signal, (a), convolved with a 51 sample impulse response, (b), results in a 306 sample signal, (c).
If this convolution is performed in the frequency domain using 256 point DFTs, the 306 points in the correct convolution cannot fit into the 256 samples provided.
As shown in (d), samples through 305 of the output signal are pushed into the next period to the right, where they add to the beginning of the next period's signal.
Figure (e) is a single period of the resulting signal.
Now consider the more general case in Fig. 9-9.
The input signal, (a), is points long, while the impulse response, (b), contains 51 nonzero points.
This makes the convolution of the two signals 306 samples long, as shown in (c).
The problem is, if we use frequency domain multiplication to perform the convolution, there are only 256 samples allowed in the output signal.
In other words, 256 point DFTs are used to move (a) and (b) into the frequency domain.
After the multiplication, a 256 point Inverse DFT is used to find the output signal.
How do you squeeze 306 values of the correct signal into the 256 points provided by the frequency domain algorithm?
The answer is, you can't!
The 256 points end up being a distorted version of the correct signal.
This process is called circular convolution.
It is important because you want to avoid it.
To understand circular convolution, remember that an N point DFT views the time domain as being an infinitely long periodic signal, with N samples per period.
Figure (d) shows three periods of how the DFT views the output signal in this example.
Since N ' 256, each period consists of 256 points: 0-255, 256-511, and 512-767.
Frequency domain convolution tries to place the point correct output signal, shown in (c), into each of these 256 point periods.
This results in 49 of the samples being pushed into the neighboring period to the right, where they overlap with the samples that are legitimately there.
These overlapping sections add, resulting in each of the periods appearing as shown in (e), the circular convolution.
Once the nature of circular convolution is understood, it is quite easy to avoid.
Simply pad each of the signals being convolved with enough zerosto allow the output signal room to handle the N% M& 1 points in the correct convolution.
For example, the signals in (a) and (b) could be padded with zeros to make them 512 points long, allowing the use of 512 point DFTs.
After the frequency domain convolution, the output signal would consist of 306 nonzero samples, plus 206 samples with a value of zero.
Chapter 18 explains this procedure in detail.
Why is it called circular convolution?
Look back at Fig. 9-9d and examine the center period, samples 256 to 511.
Since all of the periods are the same, the portion of the signal that flows out of this period to the right, is the same that flows into this period from the left.
If you only consider a single period, such as in (e), it appears that the right side of the signal is somehow connected to the left side.
Imagine a snake biting its own tail; sample 255 is located next to sample 0, just as sample 100 is located next to sample 101.
When a portion of the signal exits to the right, it magically reappears on the left.
In other words, the N point time domain behaves as if it were circular.
In the last chapter we posed the question: does it really matter if the DFT's time domain is viewed as being N points, rather than an infinitely long periodic signal of period N? Circular convolution is an example where it does matter.
If the time domain signal is understood to be periodic, the distortion encountered in circular convolution can be simply explained as the signal expanding from one period to the next.
In comparison, a rather bizarre conclusion is reached if only N points of the time domain are considered.
That is, frequency domain convolution acts as if the time domain is somehow wrapping into a circular ring with sample 0 being positioned next to sample N-1.
Fourier Transform Properties The time and frequency domains are alternative ways of representing signals.
The Fourier transform is the mathematical relationship between these two representations.
If a signal is modified in one domain, it will also be changed in the other domain, although usually not in the same way.
For example, it was shown in the last chapter that convolving time domain signals results in their frequency spectra being multiplied.
Other mathematical operations, such as addition, scaling and shifting, also have a matching operation in the opposite domain.
These relationships are called properties of the Fourier Transform, how a mathematical change in one domain results in a mathematical change in the other domain.
Linearity of the Fourier Transform The Fourier Transform is linear, that is, it possesses the properties of homogeneity and additivity.
This is true for all four members of the Fourier transform family (Fourier transform, Fourier Series, DFT, and DTFT).
Figure 10-1 provides an example of how homogeneity is a property of the Fourier transform.
Figure (a) shows an arbitrary time domain signal, with the corresponding frequency spectrum shown in (b).
We will call these two signals: x[ ] and X[ ], respectively.
Homogeneity means that a change in amplitude in one domain produces an identical change in amplitude in the other domain.
This should make intuitive sense: when the amplitude of a time domain waveform is changed, the amplitude of the sine and cosine waves making up that waveform must also change by an equal amount.
In mathematical form, if x[ ] and X [ ] are a Fourier Transform pair, then k x[ ] and k X[ ] are also a Fourier Transform pair, for any constant k.
If the frequency domain is represented in rectangular notation, k X[ ] means that both the real part and the imaginary part are multiplied by k.
If the frequency domain is represented in polar notation, k X[ ] means that the magnitude is multiplied by k, while the phase remains unchanged.
Homogeneity of the Fourier transform.
If the amplitude is changed in one domain, it is changed by the same amount in the other domain.
In other words, scaling in one domain corresponds to scaling in the other domain.
Additivity of the Fourier transform means that addition in one domain corresponds to addition in the other domain.
An example of this is shown in Fig. 10-2.
In this illustration, (a) and (b) are signals in the time domain called x1[ ] and x2[ ], respectively.
Adding these signals produces a third time domain signal called x3[ ], shown in (c).
Each of these three signals has a frequency spectrum consisting of a real and an imaginary part, shown in (d) through (i).
Since the two time domain signals add to produce the third time domain signal, the two corresponding spectra add to produce the third spectrum.
Frequency spectra are added in rectangular notation by adding the real parts to the real parts and the imaginary parts to the imaginary parts.
If: x1[n] % x2[n] ' x3[n], then: Re X1[f ] % ReX2[ f ] ' ReX3[ f ] and Im X1[f ] % ImX2[ f ] ' Im X3[f ] .
Think of this in terms of cosine and sine waves.
All the cosine waves add (the real parts) and all the sine waves add (the imaginary parts) with no interaction between the two.
Frequency spectra in polar form cannot be directly added; they must be converted into rectangular notation, added, and then reconverted back to Additivity of the Fourier transform.
Adding two or more signals in one domain results in the corresponding signals being added in the other domain.
In this illustration, the time domain signals in (a) and (b) are added to produce the signal in (c).
This results in the corresponding real and imaginary parts of the frequency spectra being added.
This can also be understood in terms of how sinusoids behave.
Imagine adding two sinusoids having the same frequency, but with different amplitudes ( A1 and A2 ) and phases ( N1 and N2 ).
If the two phases happen to be same ( N1 ' N2 ), the amplitudes will add ( A1 %A2 ) when the sinusoids are added.
However, if the two phases happen to be exactly opposite ( N1 ' &N2 ), the amplitudes will subtract ( A1 &A2 ) when the sinusoids are added.
The point is, when sinusoids (or spectra) are in polar form, they cannot be added by simply adding the magnitudes and phases.
In spite of being linear, the Fourier transform is not shift invariant.
In other words, a shift in the time domain does not correspond to a shift in the frequency domain.
This is the topic of the next section.
Characteristics of the Phase In mathematical form: if x[n] : Mag X [f ] & Phase X [f ], then a shift in the time domain results in: x[n%s] : Mag X [f ] & Phase X [f ] % 2B sf, (where f is expressed as a fraction of the sampling rate, running between 0 and 0.5).
In words, a shift of s samples in the time domain leaves the magnitude unchanged, but adds a linear term to the phase, 2B sf .
Let's look at an example of how this works.
Figure 10-3 shows how the phase is affected when the time domain waveform is shifted to the left or right.
The magnitude has not been included in this illustration because it isn't interesting; it is not changed by the time domain shift.
In Figs.
The time domain waveform in Fig. 10-3 is symmetrical around a vertical axis, that is, the left and right sides are mirror images of each other.
As mentioned in Chapter 7, signals with this type of symmetry are called linear phase, because the phase of their frequency spectrum is a straight line.
Likewise, signals that don't have this left-right symmetry are called nonlinear phase, and have phases that are something other than a straight line.
Figures (e) through (h) show the phase of the signals in (a) through (d).
As described in Chapter 7, these phase signals are unwrapped, allowing them to appear without the discontinuities associated with keeping the value between B and -B.
When the time domain waveform is shifted to the right, the phase remains a straight line, but experiences a decrease in slope.
When the time domain is shifted to the left, there is an increase in the slope.
This is the main property you need to remember from this section; a shift in the time domain corresponds to changing the slope of the phase.
Figures (b) and (f) display a unique case where the phase is entirely zero.
This occurs when the time domain signal is symmetrical around sample zero.
At first glance, this symmetry may not be obvious in (b); it may appear that the signal is symmetrical around sample 256 (i.e., N/2) instead.
Remember that the DFT views the time domain as circular, with sample zero inherently connected to sample N-1.
Any signal that is symmetrical around sample zero will also be symmetrical around sample N/2, and vice versa.
When using members of the Fourier Transform family that do not view the time domain as periodic (such as the DTFT), the symmetry must be around sample zero to produces a zero phase.
Figures (d) and (h) shows something of a riddle.
First imagine that (d) was formed by shifting the waveform in (c) slightly more to the right.
This means that the phase in (h) would have a slightly more negative slope than in (g).
This phase is shown as line 1.
Next, imagine that (d) was formed by starting with (a) and shifting it to the left.
In this case, the phase should have a slightly more positive slope than (e), as is illustrated by line 2. Lastly, notice that (d) is symmetrical around sample N/2, and should therefore have a zero phase, as illustrated by line 3. Which of these three phases is correct?
They all are, depending on how the B and 2B phase ambiguities (discussed in Chapter 8) are arranged.
For instance, every sample in line 2 differs from the corresponding sample in line 1 by an integer multiple of 2B, making them equal.
To relate line 3 to lines 1 and 2, the B ambiguities must also be taken into account.
To understand why the phase behaves as it does, imagine shifting a waveform by one sample to the right.
This means that all of the sinusoids that compose the waveform must also be shifted by one sample to the right.
Figure 10- shows two sinusoids that might be a part of the waveform.
In (a), the sine wave has a very low frequency, and a one sample shift is only a small fraction of a full cycle.
In (b), the sinusoid has a frequency of one-half of the sampling rate, the highest frequency that can exist in sampled data.
A one sample shift at this frequency is equal to an entire 1/2 cycle, or B radians.
That is, when a shift is expressed in terms of a phase change, it becomes proportional to the frequency of the sinusoid being shifted.
For example, consider a waveform that is symmetrical around sample zero, and therefore has a zero phase.
Figure 10-5a shows how the phase of this signal changes when it is shifted left or right.
At the highest frequency, one-half of the sampling rate, the phase increases by B for each one sample shift to the left, and decreases by B for each one sample shift to the right.
At zero frequency there is no phase shift, and all of the frequencies between follow in a straight line.
Phases resulting from time domain shifting.
For each sample that a time domain signal is shifted in the positive direction (i.e., to the right), the phase at frequency 0.5 will decrease by B radians.
For each sample shifted in the negative direction (i.e., to the left), the phase at frequency 0.5 will increase by B radians. Figure (a) shows this for a linear phase (a straight line), while (b) is an example using a nonlinear phase.
All of the examples we have used so far are linear phase.
Figure 10-5b shows that nonlinear phase signals react to shifting in the same way.
In this example the nonlinear phase is a straight line with two rectangular pulses.
When the time domain is shifted, these nonlinear features are simply superimposed on the changing slope.
What happens in the real and imaginary parts when the time domain waveform is shifted?
Recall that frequency domain signals in rectangular notation are nearly impossible for humans to understand.
The real and imaginary parts typically look like random oscillations with no apparent pattern.
When the time domain signal is shifted, the wiggly patterns of the real and imaginary parts become even more oscillatory and difficult to interpret.
Don't waste your time trying to understand these signals, or how they are changed by time domain shifting.
Figure 10-6 is an interesting demonstration of what information is contained in the phase, and what information is contained in the magnitude.
The waveform in (a) has two very distinct features: a rising edge at sample number 55, and a falling edge at sample number 110.
Edges are very important when information is encoded in the shape of a waveform.
An edge indicates when something happens, dividing whatever is on the left from whatever is on the right.
It is time domain encoded information in its purest form.
To begin the demonstration, the DFT is taken of the signal in (a), and the frequency spectrum converted into polar notation.
To find the signal in (b), the phase is replaced with random numbers between -B and B, and the inverse DFT used to reconstruct the time domain waveform.
In other words, (b) is based only on the information contained in the magnitude.
In a similar manner, (c) is found by replacing the magnitude with small random numbers before using the inverse DFT.
This makes the reconstruction of (c) based solely on the information contained in the phase.
Information contained in the phase.
Figure (a) shows a pulse-like waveform.
The signal in (b) is created by taking the DFT of (a), replacing the phase with random numbers, and taking the Inverse DFT.
The signal in (c) is found by taking the DFT of (a), replacing the magnitude with random numbers, and taking the Inverse DFT.
The location of the edges is retained in (c), but not in (b).
This shows that the phase contains information on the location of events in the time domain signal.
The result?
The locations of the edges are clearly present in (c), but totally absent in (b).
This is because an edge is formed when many sinusoids rise at the same location, possible only when their phases are coordinated.
In short, much of the information about the shape of the time domain waveform is contained in the phase, rather than the magnitude.
This can be contrasted with signals that have their information encoded in the frequency domain, such as audio signals.
The magnitude is most important for these signals, with the phase playing only a minor role.
In later chapters we will see that this type of understanding provides strategies for designing filters and other methods of processing signals.
Understanding how information is represented in signals is always the first step in successful DSP.
Why does left-right symmetry correspond to a zero (or linear) phase? Figure 10-7 provides the answer.
Such a signal can be decomposed into a left half and a right half, as shown in (a), (b) and (c).
The sample at the center of symmetry (zero in this case) is divided equally between the left and right halves, allowing the two sides to be perfect mirror images of each other.
The magnitudes of these two halves will be identical, as shown in (e) and (f), while the phases will be opposite in sign, as in (h) and (i).
Two important concepts fall out of this.
First, every signal that is symmetrical between the left and right will have a linear phase because the nonlinear phase of the left half exactly cancels the nonlinear phase of the right half.
Phase characteristics of left-right symmetry.
A signal with left-right symmetry, shown in (a), can be decomposed into a right half, (b), and a left half, (c).
The magnitudes of the two halves are identical, (e) and (f), while the phases are the negative of each other, (h) and (i).
Second, imagine flipping (b) such that it becomes (c).
This left-right flip in the time domain does nothing to the magnitude, but changes the sign of every point in the phase.
Likewise, changing the sign of the phase flips the time domain signal left-for-right.
If the signals are continuous, the flip is around zero.
If the signals are discrete, the flip is around sample zero and sample N/2, simultaneously.
Changing the sign of the phase is a common enough operation that it is given its own name and symbol.
The name is complex conjugation, and it is represented by placing a star to the upper-right of the variable.
For example, if X [f ] consists of Mag X [f ] and Phase X [f ], then X t[f ] is called the complex conjugate and is composed of Mag X [f ] and & Phase X [f ] .
In rectangular notation, the complex conjugate is found by leaving the real part alone, and changing the sign of the imaginary part.
In mathematical terms, if X [f ] is composed of Re X[ f ] and Im X[ f ], then X t[f ] is made up of Re X[ f ] and & Im X[ f ] .
Here are several examples of how the complex conjugate is used in DSP.
If x[ n ] has a Fourier transform of X [f ], then x [& n ] has a Fourier transform of X t[f ] .
In words, flipping the time domain left-for-right corresponds to changing the sign of the phase.
As another example, recall from Chapter 7 that correlation can be performed as a convolution.
This is done by flipping one of the signals left-for-right.
In mathematical form, a[ n ] t b[n ] is convolution, while a[ n ] t b[& n ] is correlation.
In the frequency domain these operations correspond to A [f ] × B[ f ] and A [f ] × B t[f ], respectively.
As the last example, consider an arbitrary signal, x[ n ], and its frequency spectrum, X [f ] .
The frequency spectrum can be changed to zero phase by multiplying it by its complex conjugate, that is, X[ f ] × X t[ f ] .
In words, whatever phase X [f ] happens to have will be canceled by adding its opposite (remember, when frequency spectra are multiplied, their phases are added).
In the time domain, this means that x [n ] t x [& n ] (a signal convolved with a left-right flipped version of itself) will have left-right symmetry around sample zero, regardless of what x [n ] is.
To many engineers and mathematicians, this kind of manipulation is DSP.
If you want to be able to communicate with this group, get used to using their language.
Periodic Nature of the DFT Unlike the other three Fourier Transforms, the DFT views both the time domain and the frequency domain as periodic.
This can be confusing and inconvenient since most of the signals used in DSP are not periodic.
Nevertheless, if you want to use the DFT, you must conform with the DFT's view of the world.
Figure 10-8 shows two different interpretations of the time domain signal.
First, look at the upper signal, the time domain viewed as N points.
This represents how digital signals are typically acquired in scientific experiments and engineering applications.
For instance, these 128 samples might have been acquired by sampling some parameter at regular intervals of time.
Sample is distinct and separate from sample 127 because they were acquired at different times.
From the way this signal was formed, there is no reason to think that the samples on the left of the signal are even related to the samples on the right.
Unfortunately, the DFT doesn't see things this way.
As shown in the lower figure, the DFT views these 128 points to be a single period of an infinitely long periodic signal.
This means that the left side of the acquired signal is connected to the right side of a duplicate signal.
Likewise, the right side of the acquired signal is connected to the left side of an identical period.
This can also be thought of as the right side of the acquired signal wrapping around and connecting to its left side.
In this view, sample 127 occurs next to sample 0, just as sample 43 occurs next to sample 44.
This is referred to as being circular, and is identical to viewing the signal as being periodic.
The most serious consequence of time domain periodicity is time domain aliasing.
To illustrate this, suppose we take a time domain signal and pass it through the DFT to find its frequency spectrum.
We could immediately pass this frequency spectrum through an Inverse DFT to reconstruct the original time domain signal, but the entire procedure wouldn't be very interesting.
Instead, we will modify the frequency spectrum in some manner before using the Inverse DFT.
For instance, selected frequencies might be deleted, changed in amplitude or phase, shifted around, etc.
These are the kinds of things routinely done in DSP.
Unfortunately, these changes in the frequency domain can create a time domain signal that is too long to fit into a single period.
This forces the signal to spill over from one period into the adjacent periods.
When the time domain is viewed as circular, portions of the signal that overflow on the right suddenly seem to reappear on the left side of the signal, and vice versa.
That is, the overflowing portions of the signal alias themselves to a new location in the time domain.
If this new location happens to already contain an existing signal, the whole mess adds, resulting in a loss of information.
Circular convolution resulting from frequency domain multiplication (discussed in Chapter 9), is an excellent example of this type of aliasing.
Periodicity in the frequency domain behaves in much the same way, but is more complicated.
Figure 10-9 shows an example.
The upper figures show the magnitude and phase of the frequency spectrum, viewed as being composed of N /2 % 1 samples spread between 0 and 0.5 of the sampling rate.
This is the simplest way of viewing the frequency spectrum, but it doesn't explain many of the DFT's properties.
The lower two figures show how the DFT views this frequency spectrum as being periodic.
The key feature is that the frequency spectrum between 0 and 0.5 appears to have a mirror image of frequencies that run between 0 and -0.5.
This mirror image of negative frequencies is slightly different for the magnitude and the phase signals.
In the magnitude, the signal is flipped leftfor-right.
In the phase, the signal is flipped left-for-right, and changed in sign.
As you recall, these two types of symmetry are given names: the magnitude is said to be an even signal (it has even symmetry), while the phase is said to be an odd signal (it has odd symmetry).
If the frequency spectrum is converted into the real and imaginary parts, the real part will always be even, while the imaginary part will always be odd.
Taking these negative frequencies into account, the DFT views the frequency domain as periodic, with a period of 1.0 times the sampling rate, such as -0. to 0.5, or 0 to 1.0.
In terms of sample numbers, this makes the length of the frequency domain period equal to N, the same as in the time domain.
The periodicity of the frequency domain makes it susceptible to frequency domain aliasing, completely analogous to the previously described time domain aliasing.
Imagine a time domain signal that corresponds to some frequency spectrum.
If the time domain signal is modified, it is obvious that the frequency spectrum will also be changed.
If the modified frequency spectrum cannot fit in the space provided, it will push into the adjacent periods.
Just as before, this aliasing causes two problems: frequencies aren't where they should be, and overlapping frequencies from different periods add, destroying information.
Frequency domain aliasing is more difficult to understand than time domain aliasing, since the periodic pattern is more complicated in the frequency domain.
Consider a single frequency that is being forced to move from 0. to 0.49 in the frequency domain.
The corresponding negative frequency is therefore moving from -0.01 to -0.49.
When the positive frequency moves Periodicity of the DFT's frequency domain.
The frequency domain can be viewed as running from 0 to 0.5 of the sampling rate (upper two figures), or an infinity long periodic signal with every other 0 to 0.5 segment flipped left-for-right (lower two figures).
Since the frequency domain is periodic, these same events are occurring in the other periods, such as between 0.5 and 1.5.
A clone of the positive frequency is crossing frequency 1.5 from left to right, while a clone of the negative frequency is crossing 0.5 from right to left.
Now imagine what this looks like if you can only see the frequency band of 0 to 0.5.
It appears that a frequency leaving to the right, reappears on the right, but moving in the opposite direction.
Figure 10-10 illustrates how aliasing appears in the time and frequency domains when only a single period is viewed.
As shown in (a), if one end of a time domain signal is too long to fit inside a single period, the protruding end will be cut off and pasted onto the other side.
In comparison, (b) shows that when a frequency domain signal overflows the period, the protruding end is folded over.
Regardless of where the aliased segment ends up, it adds to whatever signal is already there, destroying information.
Let's take a closer look at these strange things called negative frequencies.
Are they just some bizarre artifact of the mathematics, or do they have a real world meaning? Figure 10-11 shows what they are about.
Figure (a) is a discrete signal composed of 32 samples.
Imagine that you are given the task of finding the frequency spectrum that corresponds to these points.
To make your job easier, you are told that these points represent a discrete cosine wave.
In other words, you must find the frequency and phase shift ( f and 2 ) such that x[n] ' cos(2Bn f / N % 2) matches the given samples.
It isn't long before you come up with the solution shown in (b), that is, f ' 3 and 2 ' & B/4 .
If you stopped your analysis at this point, you only get 1/3 credit for the problem.
This is because there are two other solutions that you have missed.
As shown in (c), the second solution is f ' & 3 and 2 ' B/4 .
Even if the idea of a negative frequency offends your sensibilities, it doesn't Examples of aliasing in the time and frequency domains, when only a single period is considered.
In the time domain, shown in (a), portions of the signal that exits to the right, reappear on the left.
In the frequency domain, (b), portions of the signal that exit to the right, reappear on the right as if they had been folded over.
The meaning of negative frequencies.
The problem is to find the frequency spectrum of the discrete signal shown in (a).
That is, we want to find the frequency and phase of the sinusoid that passed through all of the samples.
Figure (b) is a solution using a positive frequency, while (c) is a solution using a negative frequency.
Figure (d) represents a family of solutions to the problem.
Every positive frequency sinusoid can alternately be expressed as a negative frequency sinusoid.
This applies to continuous as well as discrete signals The third solution is not a single answer, but an infinite family of solutions.
As shown in (d), the sinusoid with f ' 35 and 2 ' & B/4 passes through all of the discrete points, and is therefore a correct solution.
The fact that it shows oscillation between the samples may be confusing, but it doesn't disqualify it from being an authentic answer.
Likewise, f ' ±29, f ' ±35, f ' ±61, and f ' ±67 are all solutions with multiple oscillations between the points.
Each of these three solutions corresponds to a different section of the frequency spectrum.
For discrete signals, the first solution corresponds to frequencies between 0 and 0.5 of the sampling rate.
The second solution results in frequencies between 0 and -0.5.
Lastly, the third solution makes up the infinite number of duplicated frequencies below -0.5 and above 0.5.
If the signal we are analyzing is continuous, the first solution results in frequencies from zero to positive infinity, while the second solution results in frequencies from zero to negative infinity.
The third group of solutions does not exist for continuous signals.
Many DSP techniques do not require the use of negative frequencies, or an understanding of the DFT's periodicity.
For example, two common ones were described in the last chapter, spectral analysis, and the frequency response of systems.
For these applications, it is completely sufficient to view the time domain as extending from sample 0 to N-1, and the frequency domain from zero to one-half of the sampling frequency.
These techniques can use a simpler view of the world because they never result in portions of one period moving into another period.
In these cases, looking at a single period is just as good as looking at the entire periodic signal.
However, certain procedures can only be analyzed by considering how signals overflow between periods.
Two examples of this have already been presented, circular convolution and analog-to-digital conversion.
In circular convolution, multiplication of the frequency spectra results in the time domain signals being convolved.
If the resulting time domain signal is too long to fit inside a single period, it overflows into the adjacent periods, resulting in time domain aliasing.
In contrast, analog-to-digital conversion is an example of frequency domain aliasing.
A nonlinear action is taken in the time domain, that is, changing a continuous signal into a discrete signal by sampling.
The problem is, the spectrum of the original analog signal may be too long to fit inside the discrete signal's spectrum.
When we force the situation, the ends of the spectrum protrude into adjacent periods.
Let's look at two more examples where the periodic nature of the DFT is important, compression & expansion of signals, and amplitude modulation.
Compression and Expansion, Multirate methods As shown in Fig. 10-12, a compression of the signal in one domain results in an expansion in the other, and vice versa.
For continuous signals, if X (f ) is the Fourier Transform of x( t), then 1/k × X( f/ k) is the Fourier Transform of x(k t), where k is the parameter controlling the expansion or contraction.
If an event happens faster (it is compressed in time), it must be composed of higher frequencies.
If an event happens slower (it is expanded in time), it must be composed of lower frequencies.
This pattern holds if taken to either of the two extremes.
That is, if the time domain signal is compressed so far that it becomes an impulse, the corresponding frequency spectrum is expanded so far that it becomes a constant value.
Likewise, if the time domain is expanded until it becomes a constant value, the frequency domain becomes an impulse.
Discrete signals behave in a similar fashion, but there are a few more details.
The first issue with discrete signals is aliasing.
Imagine that the Compression and expansion.
Compressing a signal in one domain results in the signal being expanded in the other domain, and vice versa.
Figures (c) and (d) show a discrete signal and its spectrum, respectively.
In (a) and (b), the time domain signal has been compressed, resulting in the frequency spectrum being expanded.
Figures (e) and (f) show the opposite process.
As shown in these figures, discrete signals are expanded or contracted by expanding or contracting the underlying continuous waveform.
This underlying waveform is then resampled to find the new discrete signal.
The frequency spectrum is expanded by an equal factor, and several of the humps in (b) are pushed to frequencies beyond 0.5.
The resulting aliasing breaks the simple expansion/contraction relationship.
This type of aliasing can also happen in the time domain.
Imagine that the frequency spectrum in (f) is compressed much harder, resulting in the time domain signal in (e) expanding into neighboring periods.
A second issue is to define exactly what it means to compress or expand a discrete signal.
As shown in Fig. 10-12a, a discrete signal is compressed by compressing the underlying continuous curve that the samples lie on, and then resampling the new continuous curve to find the new discrete signal.
Likewise, this same process for the expansion of discrete signals is shown in (e).
When a discrete signal is compressed, events in the signal (such as the width of the pulse) happen over a fewer number of samples.
Likewise, events in an expanded signal happen over a greater number of samples.
An equivalent way of looking at this procedure is to keep the underlying continuous waveform the same, but resample it at a different sampling rate.
For instance, look at Fig. 10-13a, a discrete Gaussian waveform composed of 50 samples.
In (b), the same underlying curve is represented by 400 samples.
The change between (a) and (b) can be viewed in two ways: (1) the sampling rate has been kept constant, but the underlying waveform has been expanded to be eight times wider, or (2) the underlying waveform has been kept constant, but the sampling rate has increased by a factor of eight.
Methods for changing the sampling rate in this way are called multirate techniques.
If more samples are added, it is called interpolation.
If fewer samples are used to represent the signal, it is called decimation.
Chapter 3 describes how multirate techniques are used in ADC and DAC.
Here is the problem: if we are given an arbitrary discrete signal, how do we know what the underlying continuous curve is?
It depends on if the signal's information is encoded in the time domain or in the frequency domain.
For time domain encoded signals, we want the underlying continuous waveform to be a smooth curve that passes through all the samples.
In the simplest case, we might draw straight lines between the points and then round the rough corners.
The next level of sophistication is to use a curve fitting algorithm, such as a spline function or polynomial fit.
There is not a single "correct" answer to this problem.
This approach is based on minimizing irregularities in the time domain waveform, and completely ignores the frequency domain.
When a signal has information encoded in the frequency domain, we ignore the time domain waveform and concentrate on the frequency spectrum.
As discussed in the last chapter, a finer sampling of a frequency spectrum (more samples between frequency 0 and 0.5) can be obtained by padding the time domain signal with zeros before taking the DFT.
Duality allows this to work in the opposite direction.
If we want a finer sampling in the time domain (interpolation), pad the frequency spectrum with zeros before taking the Inverse DFT.
Say we want to interpolate a 50 sample signal into a sample signal.
It's done like this: (1) Take the 50 samples and add zeros to make the signal 64 samples long.
Figures (a) and (c) each consist of 50 samples.
These are interpolated to 400 samples by padding the frequency domain with zeros, resulting in (b) and (d), respectively.
This will result in a 512 sample signal that is a high resolution version of the 64 sample signal.
The first 400 samples of this signal are an interpolated version of the original 50 samples.
The key feature of this technique is that the interpolated signal is composed of exactly the same frequencies as the original signal.
This may or may not provide a well-behaved fit in the time domain.
For example, Figs. 10-13 (a) and (b) show a 50 sample signal being interpolated into a 400 sample signal by this method.
The interpolation is a smooth fit between the original points, much as if a curve fitting routine had been used.
In comparison, (c) and (d) show another example where the time domain is a mess!
The oscillatory behavior shown in (d) arises at edges or other discontinuities in the signal.
This also includes any discontinuity between sample zero and N-1, since the time domain is viewed as being circular.
This overshoot at discontinuities is called the Gibbs effect, and is discussed in Chapter 11.
Another frequency domain interpolation technique is presented in Chapter 3, adding zeros between the time domain samples and low-pass filtering.
Multiplying Signals (Amplitude Modulation) An important Fourier transform property is that convolution in one domain corresponds to multiplication in the other domain.
One side of this was discussed in the last chapter: time domain signals can be convolved by multiplying their frequency spectra.
Amplitude modulation is an example of the reverse situation, multiplication in the time domain corresponds to convolution in the frequency domain.
In addition, amplitude modulation provides an excellent example of how the elusive negative frequencies enter into everyday science and engineering problems.
Audio signals are great for short distance communication; when you speak, someone across the room hears you.
On the other hand, radio frequencies are very good at propagating long distances.
For instance, if a 100 volt, 1 MHz sine wave is fed into an antenna, the resulting radio wave can be detected in the next room, the next country, and even on the next planet.
Modulation is the process of merging two signals to form a third signal with desirable characteristics of both.
This always involves nonlinear processes such as multiplication; you can't just add the two signals together.
In radio communication, modulation results in radio signals that can propagate long distances and carry along audio or other information.
Radio communication is an extremely well developed discipline, and many modulation schemes have been developed.
One of the simplest is called amplitude modulation.
Figure 10-14 shows an example of how amplitude modulation appears in both the time and frequency domains.
Continuous signals will be used in this example, since modulation is usually carried out in analog electronics.
However, the whole procedure could be carried out in discrete form if needed (the shape of the future!). Figure (a) shows an audio signal with a DC bias such that the signal always has a positive value.
Figure (b) shows that its frequency spectrum is composed of frequencies from 300 Hz to 3 kHz, the range needed for voice communication, plus a spike for the DC component.
All other frequencies have been removed by analog filtering.
Figures (c) and (d) show the carrier wave, a pure sinusoid of much higher frequency than the audio signal.
In the time domain, amplitude modulation consists of multiplying the audio signal by the carrier wave.
As shown in (e), this results in an oscillatory waveform that has an instantaneous amplitude proportional to the original audio signal.
In the jargon of the field, the envelope of the carrier wave is equal to the modulating signal.
This signal can be routed to an antenna, converted into a radio wave, and then detected by a receiving antenna.
This results in a signal identical to (e) being generated in the radio receiver's electronics.
A detector or demodulator circuit is then used to convert the waveform in (e) back into the waveform in (a).
Since the time domain signals are multiplied, the corresponding frequency spectra are convolved.
That is, (f) is found by convolving (b) & (d).
Since the spectrum of the carrier is a shifted delta function, the spectrum of the Amplitude modulation.
In the time domain, amplitude modulation is achieved by multiplying the audio signal, (a), by the carrier signal, (c), to produce the modulated signal, (e).
Since multiplication in the time domain corresponds to convolution in the frequency domain, the spectrum of the modulated signal is the spectrum of the audio signal shifted to the frequency of the carrier.
This results in a modulated spectrum composed of three components: a carrier wave, an upper sideband, and a lower sideband.
These correspond to the three parts of the original audio signal: the DC component, the positive frequencies between 0.3 and 3 kHz, and the negative frequencies between -0.3 and -3 kHz, respectively.
Even though the negative frequencies in the original audio signal are somewhat elusive and abstract, the resulting frequencies in the lower sideband are as real as you could want them to be.
The ghosts have taken human form!
Communication engineers live and die by this type of frequency domain analysis.
For example, consider the frequency spectrum for television transmission.
A standard TV signal has a frequency spectrum from DC to MHz.
By using these frequency shifting techniques, 82 of these 6 MHz wide channels are stacked end-to-end.
For instance, channel 3 is from 60 to MHz, channel 4 is from 66 to 72 MHz, channel 83 is from 884 to 890 MHz, etc.
The television receiver moves the desired channel back to the DC to MHz band for display on the screen.
This scheme is called frequency domain multiplexing.
The Discrete Time Fourier Transform The Discrete Time Fourier Transform (DTFT) is the member of the Fourier transform family that operates on aperiodic, discrete signals.
The best way to understand the DTFT is how it relates to the DFT.
To start, imagine that you acquire an N sample signal, and want to find its frequency spectrum.
By using the DFT, the signal can be decomposed into N/2 % sine and cosine waves, with frequencies equally spaced between zero and one-half of the sampling rate.
As discussed in the last chapter, padding the time domain signal with zeros makes the period of the time domain longer, as well as making the spacing between samples in the frequency domain narrower.
As N approaches infinity, the time domain becomes aperiodic, and the frequency domain becomes a continuous signal.
This is the DTFT, the Fourier transform that relates an aperiodic, discrete signal, with a periodic, continuous frequency spectrum.
The mathematics of the DTFT can be understood by starting with the synthesis and analysis equations for the DFT (Eqs.
In this relation, x[n] is the time domain signal with n running from 0 to N& 1 .
The frequency spectrum is held in: Re X (T) and Im X(T), with T between 0 and pi.
There are many subtle details in these relations.
First, the time domain signal, x[n], is still discrete, and therefore is represented by brackets.
In comparison, the frequency domain signals, Re X(T) & Im X (T), are continuous, and are thus written with parentheses.
Since the frequency domain is continuous, the synthesis equation must be written as an integral, rather than a summation.
As discussed in Chapter 8, frequency is represented in the DFT's frequency domain by one of three variables: k, an index that runs from 0 to N/2; f, the fraction of the sampling rate, running from 0 to 0.5; or T, the fraction of the sampling rate expressed as a natural frequency, running from 0 to B. The spectrum of the DTFT is continuous, so either f or T can be used.
The common choice is T, because it makes the equations shorter by eliminating the always present factor of 2B.
Remember, when T is used, the frequency spectrum extends from 0 to B, which corresponds to DC to one-half of the sampling rate.
To make things even more complicated, many authors use S (an upper case omega) to represent this frequency in the DTFT, rather than T (a lower case omega).
When calculating the inverse DFT, samples 0 and N/2 must be divided by two (Eq.
This is not necessary with the DTFT.
As you recall, this action in the DFT is related to the frequency spectrum being defined as a spectral density, i.e., amplitude per unit of bandwidth.
When the spectrum becomes continuous, the special treatment of the end points disappear.
However, there is still a normalization factor that must be included, the 2/N in the DFT (Eq.
Some authors place these terms in front of the synthesis equation, while others place them in front of the analysis equation.
Suppose you start with some time domain signal.
After taking the Fourier transform, and then the Inverse Fourier transform, you want to end up with what you started.
That is, the 1/ B term (or the 2/N term) must be encountered somewhere along the way, either in the forward or in the inverse transform.
Some authors even split the term between the two transforms by placing 1/ B in front of both.
Since the DTFT involves infinite summations and integrals, it cannot be calculated with a digital computer.
Its main use is in theoretical problems as an alternative to the DFT.
For instance, suppose you want to find the frequency response of a system from its impulse response.
If the impulse response is known as an array of numbers, such as might be obtained from an experimental measurement or computer simulation, a DFT program is run on a computer.
This provides the frequency spectrum as another array of numbers, equally spaced between 0 and 0.5 of the sampling rate.
In other cases, the impulse response might be know as an equation, such as a sinc function (described in the next chapter) or an exponentially decaying sinusoid.
The DTFT is used here to mathematically calculate the frequency domain as another equation, specifying the entire continuous curve between 0 and 0.5.
While the DFT could also be used for this calculation, it would only provide an equation for samples of the frequency response, not the entire curve.
Parseval's Relation Since the time and frequency domains are equivalent representations of the same signal, they must have the same energy.
This is called Parseval's relation, and holds for all members of the Fourier transform family.
For the DFT, Parseval's relation is expressed: EQUATION 10- Parseval's relation.
In this equation, x[i] is a time domain signal with i running from to N-1, and X[k] is its modified frequency spectrum, with k running from 0 to N/2.
The modified frequency spectrum is found by taking the DFT of the signal, and dividing the first and last frequencies (sample 0 and N/2) by the square-root of two.
Likewise, the right side is the energy contained in the frequency domain, found by summing the energies of the N /2 % 1 sinusoids.
Remember from physics that energy is proportional to the amplitude squared.
For example, the energy in a spring is proportional to the displacement squared, and the energy stored in a capacitor is proportional to the voltage squared.
In Eq. 10-3, X [f ] is the frequency spectrum of x[n], with one slight modification: the first and last frequency components, X [0 ] & X [N/2 ], have been divided by 2 .
This modification, along with the 2/N factor on the right side of the equation, accounts for several subtle details of calculating and summing energies.
To understand these corrections, start by finding the frequency domain representation of the signal by using the DFT.
Next, convert the frequency domain into the amplitudes of the sinusoids needed to reconstruct the signal, as previously defined in Eq. 8-3.
This is done by dividing the first and last points (sample 0 and N/2) by 2, and then dividing all of the points by N/2.
While this provides the amplitudes of the sinusoids, they are expressed as a peak amplitude, not the root-mean-square (rms) amplitude needed for energy calculations.
In a sinusoid, the peak amplitude is converted to rms by dividing by 2 .
This correction must be made to all of the frequency domain values, except sample 0 and N/2.
This is because these two sinusoids are unique; one is a constant value, while the other alternates between two constant values.
For these two special cases, the peak amplitude is already equal to the rms value.
All of the values in the frequency domain are squared and then summed.
The last step is to divide the summed value by N, to account for each sample in the frequency domain being converted into a sinusoid that covers N values in the time domain.
Working through all of these details produces Eq. 10-3.
While Parseval's relation is interesting from the physics it describes (conservation of energy), it has few practical uses in DSP.
For every time domain waveform there is a corresponding frequency domain waveform, and vice versa.
For example, a rectangular pulse in the time domain coincides with a sinc function [i.e., sin(x)/x] in the frequency domain.
Duality provides that the reverse is also true; a rectangular pulse in the frequency domain matches a sinc function in the time domain.
Waveforms that correspond to each other in this manner are called Fourier transform pairs.
Several common pairs are presented in this chapter.
Delta Function Pairs For discrete signals, the delta function is a simple waveform, and has an equally simple Fourier transform pair.
Figure 11-1a shows a delta function in the time domain, with its frequency spectrum in (b) and (c).
The magnitude is a constant value, while the phase is entirely zero.
As discussed in the last chapter, this can be understood by using the expansion/compression property.
When the time domain is compressed until it becomes an impulse, the frequency domain is expanded until it becomes a constant value.
In (d) and (g), the time domain waveform is shifted four and eight samples to the right, respectively.
As expected from the properties in the last chapter, shifting the time domain waveform does not affect the magnitude, but adds a linear component to the phase.
The phase signals in this figure have not been unwrapped, and thus extend only from -B to B. Also notice that the horizontal axes in the frequency domain run from -0.5 to 0.5.
That is, they show the negative frequencies in the spectrum, as well as the positive ones.
The negative frequencies are redundant information, but they are often included in DSP graphs and you should become accustomed to seeing them.
Figure 11-2 presents the same information as Fig. 11-1, but with the frequency domain in rectangular form.
There are two lessons to be learned here.
First, compare the polar and rectangular representations of the Delta function pairs in polar form.
An impulse in the time domain corresponds to a constant magnitude and a linear phase in the frequency domain.
As is usually the case, the polar form is much easier to understand; the magnitude is nothing more than a constant, while the phase is a straight line.
In comparison, the real and imaginary parts are sinusoidal oscillations that are difficult to attach a meaning to.
The second interesting feature in Fig. 11-2 is the duality of the DFT.
In the conventional view, each sample in the DFT's frequency domain corresponds to a sinusoid in the time domain.
However, the reverse of this is also true, each sample in the time domain corresponds to sinusoids in the frequency domain.
Including the negative frequencies in these graphs allows the duality property to be more symmetrical.
For instance, Figs.
Each sample in the time domain results in a cosine wave in the real part, and a negative sine wave in the imaginary part of the frequency domain.
As you recall, an impulse at sample number four in the real part of the frequency spectrum results in four cycles of a cosine wave in the time domain.
Likewise, an impulse at sample number four in the imaginary part of the frequency spectrum results in four cycles of a negative sine wave being added to the time domain wave.
As mentioned in Chapter 8, this can be used as another way to calculate the DFT (besides correlating the time domain with sinusoids).
Each sample in the time domain results in a cosine wave being added to the real part of the frequency domain, and a negative sine wave being added to the imaginary part.
The amplitude of each sinusoid is given by the amplitude of the time domain sample.
The frequency of each sinusoid is provided by the sample number of the time domain point.
The algorithm involves: (1) stepping through each time domain sample, (2) calculating the sine and cosine waves that correspond to each sample, and (3) adding up all of the contributing sinusoids.
The resulting program is nearly identical to the correlation method (Table 8-2), except that the outer and inner loops are exchanged.
The Sinc Function Figure 11-4 illustrates a common transform pair: the rectangular pulse and the sinc function (pronounced “sink”).
The sinc function is defined as: sinc(a) ' sin(Ba)/ (Ba), however, it is common to see the vague statement: "the sinc function is of the general form: sin(x)/x ."
In other words, the sinc is a sine wave that decays in amplitude as 1/x.
In (a), the rectangular pulse is symmetrically centered on sample zero, making one-half of the pulse on the right of the graph and the other one-half on the left.
This appears to the DFT as a single pulse because of the time domain periodicity.
The DFT of this signal is shown in (b) and (c), with the unwrapped version in (d) and (e).
First look at the unwrapped spectrum, (d) and (e).
The unwrapped magnitude is an oscillation that decreases in amplitude with increasing frequency.
The phase is composed of all zeros, as you should expect for a time domain signal that is symmetrical around sample number zero.
We are using the term unwrapped magnitude to indicate that it can have both positive and negative values.
By definition, the magnitude must always be positive.
This is shown in (b) and (c) where the magnitude is made all positive by introducing a phase shift of B at all frequencies where the unwrapped magnitude is negative in (d).
In (f), the signal is shifted so that it appears as one contiguous pulse, but is no longer centered on sample number zero.
While this doesn't change the magnitude of the frequency domain, it does add a linear component to the phase, making it a jumbled mess.
What does the frequency spectrum look like as real and imaginary parts ?
Too confusing to even worry about.
An N point time domain signal that contains a unity amplitude rectangular pulse M points wide, has a DFT frequency spectrum given by: EQUATION 11- DFT spectrum of a rectangular pulse.
In this equation, N is the number of points in the time domain signal, all of which have a value of zero, except M adjacent points that have a value of one.
The frequency spectrum is contained in X[k], where k runs from 0 to N/2.
To avoid the division by zero, use X[0] ' M .
The sine function uses radians, not degrees.
This equation takes into account that the signal is aliased.
DFT of a rectangular pulse.
A rectangular pulse in one domain corresponds to a sinc function in the other domain.
Alternatively, the DTFT can be used to express the frequency spectrum as a fraction of the sampling rate, f: EQUATION 11- Equation 11-1 rewritten in terms of the sampling frequency.
The parameter, f, is the fraction of the sampling rate, running continiously from 0 to 0.5.
To avoid the division by zero, use Mag X(0) ' M .
In other words, Eq. 11-1 provides N/2 % 1 samples in the frequency spectrum, while Eq.
These equations only provide the magnitude.
The phase is determined solely by the left-right positioning of the time domain waveform, as discussed in the last chapter.
It is often important to understand what the frequency spectrum looks like when aliasing isn't present.
This is because discrete signals are often used to represent or model continuous signals, and continuous signals don't alias.
To remove the aliasing in Eqs.
Figure 11-4 shows the significance of this.
The quantity Bf can only run from 0 to 1.5708, since f can only run from 0 to 0.5.
Over this range there isn't much difference between sin (Bf ) and Bf .
At zero frequency they have the same value, and at a frequency of 0.5 there is only about a 36% difference.
Without aliasing, the curve in Fig. 11-3b would show a slightly lower amplitude near the right side of the graph, and no change near the left side.
When the frequency spectrum of the rectangular pulse is not aliased (because the time domain signal is continuous, or because you are ignoring the aliasing), it is of the general form: sin (x)/ x, i.e., a sinc function.
For continuous signals, the rectangular pulse and the sinc function are Fourier transform pairs.
For discrete signals this is only an approximation, with the error being due to aliasing.
The sinc function has an annoying problem at x ' 0, where sin (x)/x becomes zero divided by zero.
This is not a difficult mathematical problem; as x becomes very small, sin (x) approaches the value of x (see Fig. 11-4).
This turns the sinc function into x/x, which has a value of one.
In other words, as x becomes smaller and smaller, the value of sinc (x) approaches one, which includes sinc (0) ' 1 .
Now try to tell your computer this!
All it sees is a division by zero, causing it to complain and stop your program.
The important point to remember is that your program must include special handling at x ' when calculating the sinc function.
A key trait of the sinc function is the location of the zero crossings.
These occur at frequencies where an integer number of the sinusoid's cycles fit evenly into the rectangular pulse.
For example, if the rectangular pulse is 20 points wide, the first zero in the frequency domain is at the frequency that makes one complete cycle in 20 points.
The second zero is at the frequency that makes two complete cycles in 20 points, etc.
This can be understood by remembering how the DFT is calculated by correlation.
The amplitude of a frequency component is found by multiplying the time domain signal by a sinusoid and adding up the resulting samples.
If the time domain waveform is a rectangular pulse of unity amplitude, this is the same as adding the sinusoid's samples that are within the rectangular pulse.
If this summation occurs over an integral number of the sinusoid's cycles, the result will be zero.
The sinc function is widely used in DSP because it is the Fourier transform pair of a very simple waveform, the rectangular pulse.
For example, the sinc function is used in spectral analysis, as discussed in Chapter 9. Consider the analysis of an infinitely long discrete signal.
Since the DFT can only work with finite length signals, N samples are selected to represent the longer signal.
The key here is that "selecting N samples from a longer signal" is the same as multiplying the longer signal by a rectangular pulse.
The ones in the rectangular pulse retain the corresponding samples, while the zeros eliminate them.
How does this affect the frequency spectrum of the signal?
Multiplying the time domain by a rectangular pulse results in the frequency domain being convolved with a sinc function.
This reduces the frequency spectrum's resolution, as previously shown in Fig. 9-5a.
Other Transform Pairs Figure 11-5 (a) and (b) show the duality of the above: a rectangular pulse in the frequency domain corresponds to a sinc function (plus aliasing) in the time domain.
Including the effects of aliasing, the time domain signal is given by: EQUATION 11- Inverse DFT of the rectangular pulse.
In the frequency domain, the pulse has an amplitude of one, and runs from sample number 0 through sample number M-1.
The parameter N is the length of the DFT, and x[i] is the time domain signal with i running from 0 to N-1.
To avoid the division by zero, use x[0] ' (2M& 1)/ N .
To eliminate the effects of aliasing from this equation, imagine that the frequency domain is so finely sampled that it turns into a continuous curve.
This makes the time domain infinitely long with no periodicity.
The DTFT is the Fourier transform to use here, resulting in the time domain signal being given by the relation: EQUATION 11- Inverse DTFT of the rectangular pulse.
In the frequency domain, the pulse has an amplitude of one, and runs from zero frequency to the cutoff frequency, fc, a value between 0 and 0.5.
The time domain signal is held in x [i ] with i running from 0 to N-1.
To avoid the division by zero, use x[0] ' 2fc .
This equation is very important in DSP, because the rectangular pulse in the frequency domain is the perfect low-pass filter.
Therefore, the sinc function described by this equation is the filter kernel for the perfect low-pass filter.
This is the basis for a very useful class of digital filters called the windowedsinc filters, described in Chapter 15.
Figures (c) & (d) show that a triangular pulse in the time domain coincides with a sinc function squared (plus aliasing) in the frequency domain.
This transform pair isn't as important as the reason it is true.
A 2M & 1 point triangle in the time domain can be formed by convolving an M point rectangular pulse with itself.
Since convolution in the time domain results in multiplication in the frequency domain, convolving a waveform with itself will square the frequency spectrum.
Is there a waveform that is its own Fourier Transform?
The answer is yes, and there is only one: the Gaussian.
Figure (e) shows a Gaussian curve, and (f) shows the corresponding frequency spectrum, also a Gaussian curve.
This relationship is only true if you ignore aliasing.
The relationship between the standard deviation of the time domain and frequency domain is given by: 2BFf ' 1/Ft .
While only one side of a Gaussian is shown in (f), the negative frequencies in the spectrum complete the full curve, with the center of symmetry at zero frequency.
Figure (g) shows what can be called a Gaussian burst.
It is formed by multiplying a sine wave by a Gaussian.
For example, (g) is a sine wave multiplied by the same Gaussian shown in (e).
The corresponding frequency domain is a Gaussian centered somewhere other than zero frequency.
As before, this transform pair is not as important as the reason it is true.
Since the time domain signal is the multiplication of two signals, the frequency domain will be the convolution of the two frequency spectra.
The frequency spectrum of the sine wave is a delta function centered at the frequency of the sine wave.
The frequency spectrum of a Gaussian is a Gaussian centered at zero frequency.
Convolving the two produces a Gaussian centered at the frequency of the sine wave.
This should look familiar; it is identical to the procedure of amplitude modulation described in the last chapter.
Gibbs Effect Figure 11-6 shows a time domain signal being synthesized from sinusoids.
The signal being reconstructed is shown in the last graph, (h).
Since this signal is 1024 points long, there will be 513 individual frequencies needed for a complete reconstruction.
Figures (a) through (g) show what the reconstructed signal looks like if only some of these frequencies are used.
For example, (f) shows a reconstructed signal using frequencies 0 through 100.
This signal was created by taking the DFT of the signal in (h), setting frequencies 101 through 512 to a value of zero, and then using the Inverse DFT to find the resulting time domain signal.
As more frequencies are added to the reconstruction, the signal becomes closer to the final solution.
The interesting thing is how the final solution is approached at the edges in the signal.
There are three sharp edges in (h).
Two are the edges of the rectangular pulse.
The third is between sample numbers 1023 and 0, since the DFT views the time domain as periodic.
When only some of the frequencies are used in the reconstruction, each edge shows overshoot and ringing (decaying oscillations).
This overshoot and ringing is known as the Gibbs effect, after the mathematical physicist Josiah Gibbs, who explained the phenomenon in 1899.
Look closely at the overshoot in (e), (f), and (g).
As more sinusoids are added, the width of the overshoot decreases; however, the amplitude of the overshoot remains about the same, roughly 9 percent.
With discrete signals this is not a problem; the overshoot is eliminated when the last frequency is added.
However, the reconstruction of continuous signals cannot be explained so easily.
An infinite number of sinusoids must be added to synthesize a continuous signal.
The problem is, the amplitude of the overshoot does not decrease as the number of sinusoids approaches infinity, it stays about the same 9%.
Given this situation (and other arguments), it is reasonable to question if a summation of continuous sinusoids can reconstruct an edge.
Remember the squabble between Lagrange and Fourier?
The critical factor in resolving this puzzle is that the width of the overshoot becomes smaller as more sinusoids are included.
The overshoot is still present with an infinite number of sinusoids, but it has zero width.
Exactly at the discontinuity the value of the reconstructed signal converges to the midpoint of the step.
As shown by Gibbs, the summation converges to the signal in the sense that the error between the two has zero energy.
Problems related to the Gibbs effect are frequently encountered in DSP.
For example, a low-pass filter is a truncation of the higher frequencies, resulting in overshoot and ringing at the edges in the time domain.
Another common procedure is to truncate the ends of a time domain signal to prevent them from extending into neighboring periods.
By duality, this distorts the edges in the frequency domain.
These issues will resurface in future chapters on filter design.
If a signal is periodic with frequency f, the only frequencies composing the signal are integer multiples of f, i.e., f, 2f, 3f, 4f, etc.
These frequencies are called harmonics.
The first harmonic is f, the second harmonic is 2f, the third harmonic is 3f, and so forth.
The first harmonic (i.e., f) is also given a special name, the fundamental frequency.
Figure 11-7 shows an example.
Figure (a) is a pure sine wave, and (b) is its DFT, a single peak.
In (c), the sine wave has been distorted by poking in the tops of the peaks.
Figure (d) shows the result of this distortion in the frequency domain.
Because the distorted signal is periodic with the same frequency as the original sine wave, the frequency domain is composed of the original peak plus harmonics.
Harmonics can be of any amplitude; however, they usually become smaller as they increase in frequency.
As with any signal, sharp edges result in higher frequencies.
For example, consider a common TTL logic gate generating a 1 kHz square wave.
The edges rise in a few nanoseconds, resulting in harmonics being generated to nearly 100 MHz, the ten-thousandth harmonic!
Figure (e) demonstrates a subtlety of harmonic analysis.
If the signal is symmetrical around a horizontal axis, i.e., the top lobes are mirror images of the bottom lobes, all of the even harmonics will have a value of zero.
As shown in (f), the only frequencies contained in the signal are the fundamental, the third harmonic, the fifth harmonic, etc.
All continuous periodic signals can be represented as a summation of harmonics, just as described.
Discrete periodic signals have a problem that disrupts this simple relation.
As you might have guessed, the problem is aliasing.
Figure 11-8a shows a sine wave distorted in the same manner as before, by poking in the tops of the peaks.
This waveform looks much less regular and smooth than in the previous example because the sine wave is at a much higher frequency, resulting in fewer samples per cycle.
Figure (b) shows the frequency spectrum of this signal.
As you would expect, you can identify the fundamental and harmonics.
This example shows that harmonics can extend to frequencies greater than 0.5 of the sampling frequency, and will be aliased to frequencies somewhere between 0 and 0.5.
You don't notice them in (b) because their amplitudes are too low.
Figure (c) shows the frequency spectrum plotted on a logarithmic scale to reveal these low amplitude aliased peaks.
At first glance, this spectrum looks like random noise.
It isn't; this is a result of the many harmonics overlapping as they are aliased.
It is important to understand that this example involves distorting a signal after it has been digitally represented.
If this distortion occurred in an analog signal, you would remove the offending harmonics with an antialias filter before digitization.
Harmonic aliasing is only a problem when nonlinear operations are performed directly on a discrete signal.
Even then, the amplitude of these aliased harmonics is often low enough that they can be ignored.
The concept of harmonics is also useful for another reason: it explains why the DFT views the time and frequency domains as periodic.
In the frequency domain, an N point DFT consists of N/2+1 equally spaced frequencies.
You can view the frequencies between these samples as (1) having a value of zero, or (2) not existing.
Either way they don't contribute to the synthesis of the time domain signal.
In other words, a discrete frequency spectrum consists of harmonics, rather than a continuous range of frequencies.
This requires the time domain to be periodic with a frequency equal to the lowest sinusoid in the frequency domain, i.e., the fundamental frequency.
Neglecting the DC value, the lowest frequency represented in the frequency domain makes one complete cycle every N samples, resulting in the time domain being periodic with a period of N. In other words, if one domain is discrete, the other domain must be periodic, and vice versa.
This holds for all four members of the Fourier transform family.
Since the DFT views both domains as discrete, it must also view both domains as periodic.
The samples in each domain represent harmonics of the periodicity of the opposite domain.
Chirp Signals Chirp signals are an ingenious way of handling a practical problem in echo location systems, such as radar and sonar.
Figure 11-9 shows the frequency response of the chirp system.
The magnitude has a constant value of one, while the phase is a parabola: Frequency response of the chirp system.
The magnitude is a constant, while the phase is a parabola.
The parameter " introduces a linear slope in the phase, that is, it simply shifts the impulse response left or right as desired.
The parameter $ controls the curvature of the phase.
These two parameters must be chosen such that the phase at frequency 0.5 (i.e.
Remember, whenever the phase is directly manipulated, frequency 0 and 0.5 must both have a phase of zero (or a multiple of 2B, which is the same thing).
Figure 11-10 shows an impulse entering a chirp system, and the impulse response exiting the system.
The impulse response is an oscillatory burst that starts at a low frequency and changes to a high frequency as time progresses.
This is called a chirp signal for a very simple reason: it sounds like the chirp of a bird when played through a speaker.
The key feature of the chirp system is that it is completely reversible.
If you run the chirp signal through an antichirp system, the signal is again made into an impulse.
This requires the antichirp system to have a magnitude of one, and the opposite phase of the chirp system.
As discussed in the last The chirp system.
The impulse response of a chirp system is a chirp signal.
Interesting, but what is it good for?
Consider how a radar system operates.
A short burst of radio frequency energy is emitted from a directional antenna.
Aircraft and other objects reflect some of this energy back to a radio receiver located next to the transmitter.
Since radio waves travel at a constant rate, the elapsed time between the transmitted and received signals provides the distance to the target.
This brings up the first requirement for the pulse: it needs to be as short as possible.
For example, a 1 microsecond pulse provides a radio burst about 300 meters long.
This means that the distance information we obtain with the system will have a resolution of about this same length.
If we want better distance resolution, we need a shorter pulse.
The second requirement is obvious: if we want to detect objects farther away, you need more energy in your pulse.
Unfortunately, more energy and shorter pulse are conflicting requirements.
The electrical power needed to provide a pulse is equal to the energy of the pulse divided by the pulse length.
Requiring both more energy and a shorter pulse makes electrical power handling a limiting factor in the system.
The output stage of a radio transmitter can only handle so much power without destroying itself.
Chirp signals provide a way of breaking this limitation.
Before the impulse reaches the final stage of the radio transmitter, it is passed through a chirp system.
Instead of bouncing an impulse off the target aircraft, a chirp signal is used.
After the chirp echo is received, the signal is passed through an antichirp system, restoring the signal to an impulse.
This allows the portions of the system that measure distance to see short pulses, while the power handling circuits see long duration signals.
This type of waveshaping is a fundamental part of modern radar systems.
There are several ways to calculate the Discrete Fourier Transform (DFT), such as solving simultaneous linear equations or the correlation method described in Chapter 8.
The Fast Fourier Transform (FFT) is another method for calculating the DFT.
While it produces the same result as the other approaches, it is incredibly more efficient, often reducing the computation time by hundreds.
This is the same improvement as flying in a jet aircraft versus walking!
If the FFT were not available, many of the techniques described in this book would not be practical.
While the FFT only requires a few dozen lines of code, it is one of the most complicated algorithms in DSP.
But don't despair!
You can easily use published FFT routines without fully understanding the internal workings.
Real DFT Using the Complex DFT J.W. Cooley and J.W. Tukey are given credit for bringing the FFT to the world in their paper: "An algorithm for the machine calculation of complex Fourier Series," Mathematics Computation, Vol. 19, 1965, pp 297-301.
In retrospect, others had discovered the technique many years before.
For instance, the great German mathematician Karl Friedrich Gauss (1777-1855) had used the method more than a century earlier.
This early work was largely forgotten because it lacked the tool to make it practical: the digital computer.
Cooley and Tukey are honored because they discovered the FFT at the right time, the beginning of the computer revolution.
The FFT is based on the complex DFT, a more sophisticated version of the real DFT discussed in the last four chapters.
These transforms are named for the way each represents data, that is, using complex numbers or using real numbers.
The term complex does not mean that this representation is difficult or complicated, but that a specific type of mathematics is used.
Complex mathematics often is difficult and complicated, but that isn't where the name comes from.
Chapter 29 discusses the complex DFT and provides the background needed to understand the details of the FFT algorithm.
The Comparing the real and complex DFTs.
The real DFT takes an N point time domain signal and creates two N/2 % 1 point frequency domain signals.
The complex DFT takes two N point time domain signals and creates two N point frequency domain signals.
The crosshatched regions shows the values common to the two transforms.
Since the FFT is an algorithm for calculating the complex DFT, it is important to understand how to transfer real DFT data into and out of the complex DFT format.
Figure 12-1 compares how the real DFT and the complex DFT store data.
The real DFT transforms an N point time domain signal into two N /2 % 1 point frequency domain signals.
The time domain signal is called just that: the time domain signal.
The two signals in the frequency domain are called the real part and the imaginary part, holding the amplitudes of the cosine waves and sine waves, respectively.
This should be very familiar from past chapters.
In comparison, the complex DFT transforms two N point time domain signals into two N point frequency domain signals.
The two time domain signals are called the real part and the imaginary part, just as are the frequency domain signals.
In spite of their names, all of the values in these arrays are just ordinary numbers.
Recall that the operator, Im( ), returns a real number).
Suppose you have an N point signal, and need to calculate the real DFT by means of the Complex DFT (such as by using the FFT algorithm).
First, move the N point signal into the real part of the complex DFT's time domain, and then set all of the samples in the imaginary part to zero.
Calculation of the complex DFT results in a real and an imaginary signal in the frequency domain, each composed of N points.
Samples 0 through N/2 of these signals correspond to the real DFT's spectrum.
As discussed in Chapter 10, the DFT's frequency domain is periodic when the negative frequencies are included (see Fig. 10-9).
The choice of a single period is arbitrary; it can be chosen between -1.0 and 0, -0.5 and 0.5, 0 and 1.0, or any other one unit interval referenced to the sampling rate.
The complex DFT's frequency spectrum includes the negative frequencies in the to 1.0 arrangement.
In other words, one full period stretches from sample 0 to sample N& 1, corresponding with 0 to 1.0 times the sampling rate.
The positive frequencies sit between sample 0 and N/2, corresponding with 0 to 0.5.
The other samples, between N/2 % 1 and N& 1, contain the negative frequency values (which are usually ignored).
Calculating a real Inverse DFT using a complex Inverse DFT is slightly harder.
This is because you need to insure that the negative frequencies are loaded in the proper format.
Remember, points 0 through N/2 in the complex DFT are the same as in the real DFT, for both the real and the imaginary parts.
For the real part, point N/2 % 1 is the same as point N/2 & 1, point N/2 % 2 is the same as point N/2 & 2, etc.
This continues to point N& 1 being the same as point 1.
The same basic pattern is used for the imaginary part, except the sign is changed.
That is, point N/2 % 1 is the negative of point N/2 & 1, point N/2 % 2 is the negative of point N/2 & 2, etc. Notice that samples 0 and N/2 do not have a matching point in this duplication scheme.
Use Fig. 10-9 as a guide to understanding this symmetry.
In practice, you load the real DFT's frequency spectrum into samples 0 to N/2 of the complex DFT's arrays, and then use a subroutine to generate the negative frequencies between samples N/2 % 1 and N& 1 .
Table 12-1 shows such a program.
To check that the proper symmetry is present, after taking the inverse FFT, look at the imaginary part of the time domain.
It will contain all zeros if everything is correct (except for a few parts-permillion of noise, using single precision calculations).
How the FFT works The FFT is a complicated algorithm, and its details are usually left to those that specialize in such things.
This section describes the general operation of the FFT, but skirts a key issue: the use of complex numbers.
If you have a background in complex mathematics, you can read between the lines to understand the true nature of the algorithm.
Don't worry if the details elude you; few scientists and engineers that use the FFT could write the program from scratch.
In complex notation, the time and frequency domains each contain one signal made up of N complex points.
Each of these complex points is composed of two numbers, the real part and the imaginary part.
For example, when we talk about complex sample X[42], it refers to the combination of Re X[42] and Im X[42] .
In other words, each complex variable holds two numbers.
When two complex variables are multiplied, the four individual components must be combined to form the two components of the product (such as in Eq. 9-1).
The following discussion on "How the FFT works" uses this jargon of complex notation.
That is, the singular terms: signal, point, sample, and value, refer to the combination of the real part and the imaginary part.
The FFT operates by decomposing an N point time domain signal into N time domain signals each composed of a single point.
The second step is to calculate the N frequency spectra corresponding to these N time domain signals.
Lastly, the N spectra are synthesized into a single frequency spectrum.
The FFT decomposition.
An N point signal is decomposed into N signals each containing a single point.
Each stage uses an interlace decomposition, separating the even and odd numbered samples.
The FFT bit reversal sorting.
The FFT time domain decomposition can be implemented by sorting the samples according to bit reversed order.
The first stage breaks the 16 point signal into two signals each consisting of 8 points.
The second stage decomposes the data into four signals of 4 points.
This pattern continues until there are N signals composed of a single point.
An interlaced decomposition is used each time a signal is broken in two, that is, the signal is separated into its even and odd numbered samples.
The best way to understand this is by inspecting Fig. 12-2 until you grasp the pattern.
There are Log2 N stages required in this decomposition, i.e., a 16 point signal (24) requires 4 stages, a 512 point signal (27) requires stages, a 4096 point signal (212) requires 12 stages, etc. Remember this value, Log2 N ; it will be referenced many times in this chapter.
Now that you understand the structure of the decomposition, it can be greatly simplified.
The decomposition is nothing more than a reordering of the samples in the signal.
Figure 12-3 shows the rearrangement pattern required.
On the left, the sample numbers of the original signal are listed along with their binary equivalents.
On the right, the rearranged sample numbers are listed, also along with their binary equivalents.
The important idea is that the binary numbers are the reversals of each other.
For example, sample 3 (0011) is exchanged with sample number 12 (1100).
Likewise, sample number (1110) is swapped with sample number 7 (0111), and so forth.
The FFT time domain decomposition is usually carried out by a bit reversal sorting algorithm.
This involves rearranging the order of the N time domain samples by counting in binary with the bits flipped left-for-right (such as in the far right column in Fig. 12-3).
The next step in the FFT algorithm is to find the frequency spectra of the 1 point time domain signals.
Nothing could be easier; the frequency spectrum of a 1 point signal is equal to itself.
This means that nothing is required to do this step.
Although there is no work involved, don't forget that each of the 1 point signals is now a frequency spectrum, and not a time domain signal.
The last step in the FFT is to combine the N frequency spectra in the exact reverse order that the time domain decomposition took place.
This is where the algorithm gets messy.
Unfortunately, the bit reversal shortcut is not applicable, and we must go back one stage at a time.
In the first stage, frequency spectra (1 point each) are synthesized into 8 frequency spectra ( points each).
In the second stage, the 8 frequency spectra (2 points each) are synthesized into 4 frequency spectra (4 points each), and so on.
The last stage results in the output of the FFT, a 16 point frequency spectrum.
Figure 12-4 shows how two frequency spectra, each composed of 4 points, are combined into a single frequency spectrum of 8 points.
This synthesis must undo the interlaced decomposition done in the time domain.
In other words, the frequency domain operation must correspond to the time domain procedure of combining two 4 point signals by interlacing.
Consider two time domain signals, abcd and efgh.
An 8 point time domain signal can be formed by two steps: dilute each 4 point signal with zeros to make it an The FFT synthesis.
When a time domain signal is diluted with zeros, the frequency domain is duplicated.
If the time domain signal is also shifted by one sample during the dilution, the spectrum will additionally be multiplied by a sinusoid.
FFT synthesis flow diagram.
This shows the method of combining two 4 point frequency spectra into a single 8 point frequency spectrum.
The ×S operation means that the signal is multiplied by a sinusoid with an appropriately selected frequency.
Eight Point Frequency Spectrum 8 point signal, and then add the signals together.
That is, abcd becomes a0b0c0d0, and efgh becomes 0e0f0g0h.
Adding these two 8 point signals produces aebfcgdh.
As shown in Fig. 12-4, diluting the time domain with zeros corresponds to a duplication of the frequency spectrum.
Therefore, the frequency spectra are combined in the FFT by duplicating them, and then adding the duplicated spectra together.
In order to match up when added, the two time domain signals are diluted with zeros in a slightly different way.
In one signal, the odd points are zero, while in the other signal, the even points are zero.
In other words, one of the time domain signals (0e0f0g0h in Fig. 12-4) is shifted to the right by one sample.
This time domain shift corresponds to multiplying the spectrum by a sinusoid.
To see this, recall that a shift in the time domain is equivalent to convolving the signal with a shifted delta function.
This multiplies the signal's spectrum with the spectrum of the shifted delta function.
The spectrum of a shifted delta function is a sinusoid (see Fig 11-2).
Figure 12-5 shows a flow diagram for combining two 4 point spectra into a single 8 point spectrum.
To reduce the situation even more, notice that Fig. 125 is formed from the basic pattern in Fig 12-6 repeated over and over. 2 point input The FFT butterfly.
This is the basic calculation element in the FFT, taking two complex points and converting them into two other complex points.
This simple flow diagram is called a butterfly due to its winged appearance.
The butterfly is the basic computational element of the FFT, transforming two complex points into two other complex points.
Figure 12-7 shows the structure of the entire FFT.
The time domain decomposition is accomplished with a bit reversal sorting algorithm.
Transforming the decomposed data into the frequency domain involves nothing and therefore does not appear in the figure.
The frequency domain synthesis requires three loops.
The outer loop runs through the Log2 N stages (i.e., each level in Fig. 12-2, starting from the bottom and moving to the top).
The middle loop moves through each of the individual frequency spectra in the stage being worked on (i.e., each of the boxes on any one level in Fig. 12-2).
The innermost loop uses the butterfly to calculate the points in each frequency spectra (i.e., looping through the samples inside any one box in Fig. 12-2).
The overhead boxes in Fig. 12-7 determine the beginning and ending indexes for the loops, as well as calculating the sinusoids needed in the butterflies.
Now we come to the heart of this chapter, the actual FFT programs.
As discussed in Chapter 8, the real DFT can be calculated by correlating the time domain signal with sine and cosine waves (see Table 8-2).
Table 12-2 shows a program to calculate the complex DFT by the same method.
In an apples-to-apples comparison, this is the program that the FFT improves upon.
Tables 12-3 and 12-4 show two different FFT programs, one in FORTRAN and one in BASIC.
First we will look at the BASIC routine in Table 12-4.
This subroutine produces exactly the same output as the correlation technique in Table 12-2, except it does it much faster.
The block diagram in Fig. 12-7 can be used to identify the different sections of this program.
Data are passed to this FFT subroutine in the arrays: REX[ ] and IMX[ ], each running from sample 0 to N& 1 .
Upon return from the subroutine, REX[ ] and IMX[ ] are overwritten with the frequency domain data.
This is another way that the FFT is highly optimized; the same arrays are used for the input, intermediate storage, and output.
This efficient use of memory is important for designing fast hardware to calculate the FFT.
The term in-place computation is used to describe this memory usage.
While all FFT programs produce the same numerical result, there are subtle variations in programming that you need to look out for.
Several of these important differences are illustrated by the FORTRAN program listed in Table 12-3.
This program uses an algorithm called decimation in frequency, while the previously described algorithm is called decimation in time.
In a decimation in frequency algorithm, the bit reversal sorting is done after the three nested loops.
There are also FFT routines that completely eliminate the bit reversal sorting.
None of these variations significantly improve the performance of the FFT, and you shouldn't worry about which one you are using.
The important differences between FFT algorithms concern how data are passed to and from the subroutines.
In the BASIC program, data enter and leave the subroutine in the arrays REX[ ] and IMX[ ], with the samples running from index 0 to N& 1 .
In the FORTRAN program, data are passed in the complex array X( ), with the samples running from 1 to N. Since this is an array of complex variables, each sample in X( ) consists of two numbers, a real part and an imaginary part.
The length of the DFT must also be passed to these subroutines.
In the BASIC program, the variable N% is used for this purpose.
In comparison, the FORTRAN program uses the variable M, which is defined to equal Log2 N .
For instance, M will be 8 for a 256 point DFT, 12 for a 4096 point DFT, etc.
The point is, the programmer who writes an FFT subroutine has many options for interfacing with the host program.
Arrays that run from 1 to N, such as in the FORTRAN program, are especially aggravating.
Most of the DSP literature (including this book) explains algorithms assuming the arrays run from sample 0 to N& 1 .
For instance, if the arrays run from 1 to N, the symmetry in the frequency domain is around points 1 and N/2 % 1, rather than points 0 and N/2, Using the complex DFT to calculate the real DFT has another interesting advantage.
The complex DFT is more symmetrical between the time and frequency domains than the real DFT.
That is, the duality is stronger.
Among other things, this means that the Inverse DFT is nearly identical to the Forward DFT.
In fact, the easiest way to calculate an Inverse FFT is to calculate a Forward FFT, and then adjust the data.
Table 12-5 shows a subroutine for calculating the Inverse FFT in this manner.
Suppose you copy one of these FFT algorithms into your computer program and start it running.
How do you know if it is operating properly?
Two tricks are commonly used for debugging.
First, start with some arbitrary time domain signal, such as from a random number generator, and run it through the FFT.
Next, run the resultant frequency spectrum through the Inverse FFT and compare the result with the original signal.
They should be identical, except round-off noise (a few parts-per-million for single precision).
The second test of proper operation is that the signals have the correct symmetry.
When the imaginary part of the time domain signal is composed of all zeros (the normal case), the frequency domain of the complex DFT will be symmetrical around samples 0 and N/2, as previously described.
Likewise, when this correct symmetry is present in the frequency domain, the Inverse DFT will produce a time domain that has an imaginary part composes of all zeros (plus round-off noise).
These debugging techniques are essential for using the FFT; become familiar with them.
Speed and Precision Comparisons When the DFT is calculated by correlation (as in Table 12-2), the program uses two nested loops, each running through N points.
This means that the total number of operations is proportional to N times N.
The time to complete the program is thus given by: EQUATION 12- DFT execution time.
The time required to calculate a DFT by correlation is proportional to the length of the DFT squared.
ExecutionTime ' kDFT N where N is the number of points in the DFT and k D F T is a constant of proportionality.
If the sine and cosine values are calculated within the nested loops, k DFT is equal to about 25 microseconds on a Pentium at 100 MHz.
If you precalculate the sine and cosine values and store them in a look-up-table, k DFT drops to about 7 microseconds.
For example, a 1024 point DFT will require about 25 seconds, or nearly 25 milliseconds per point.
That's slow!
Using this same strategy we can derive the execution time for the FFT.
The time required for the bit reversal is negligible.
In each of the Log2 N stages there are N/2 butterfly computations.
This means the execution time for the program is approximated by: EQUATION 12- FFT execution time.
The time required to calculate a DFT using the FFT is proportional to N multiplied by the logarithm of N. ExecutionTime ' kFFT N log2N The value of k FFT is about 10 microseconds on a 100 MHz Pentium system.
A 1024 point FFT requires about 70 milliseconds to execute, or 70 microseconds per point.
This is more than 300 times faster than the DFT calculated by correlation!
Not only is N Log2 N less than N 2, it increases much more slowly as N becomes larger.
For example, a 32 point FFT is about ten times faster than the correlation method.
However, a 4096 point FFT is one-thousand times faster.
For small values of N (say, 32 to 128), the FFT is important.
For large values of N (1024 and above), the FFT is absolutely critical.
Figure 12-8 compares the execution times of the two algorithms in a graphical form.
Number points in DFT The FFT has another advantage besides raw speed.
The FFT is calculated more precisely because the fewer number of calculations results in less round-off error.
This can be demonstrated by taking the FFT of an arbitrary signal, and then running the frequency spectrum through an Inverse FFT.
This reconstructs the original time domain signal, except for the addition of roundoff noise from the calculations.
A single number characterizing this noise can be obtained by calculating the standard deviation of the difference between the two signals.
For comparison, this same procedure can be repeated using a DFT calculated by correlation, and a corresponding Inverse DFT.
How does the round-off noise of the FFT compare to the DFT by correlation?
See for yourself in Fig. 12-9.
Further Speed Increases There are several techniques for making the FFT even faster; however, the improvements are only about 20-40%.
In one of these methods, the time domain decomposition is stopped two stages early, when each signal is composed of only four points.
Instead of calculating the last two stages, highly optimized code is used to jump directly into the frequency domain, using the simplicity of four point sine and cosine waves.
Another popular algorithm eliminates the wasted calculations associated with the imaginary part of the time domain being zero, and the frequency spectrum being symmetrical.
In other words, the FFT is modified to calculate the real DFT, instead of the complex DFT.
These algorithms are called the real FFT and the real Inverse FFT (or similar names).
Expect them to be about 30% faster than the conventional FFT routines.
Tables 12-6 and 12-7 show programs for these algorithms.
There are two small disadvantages in using the real FFT.
First, the code is about twice as long.
While your computer doesn't care, you must take the time to convert someone else's program to run on your computer.
Second, debugging these programs is slightly harder because you cannot use symmetry as a check for proper operation.
These algorithms force the imaginary part of the time domain to be zero, and the frequency domain to have left-right symmetry.
For debugging, check that these programs produce the same output as the conventional FFT algorithms.
Figures 12-10 and 12-11 illustrate how the real FFT works.
In Fig. 12-10, (a) and (b) show a time domain signal that consists of a pulse in the real part, and all zeros in the imaginary part.
Figures (c) and (d) show the corresponding frequency spectrum.
As previously described, the frequency domain's real part has an even symmetry around sample 0 and sample N/2, while the imaginary part has an odd symmetry around these same points.
Real part symmetry of the DFT.
Now consider Fig. 12-11, where the pulse is in the imaginary part of the time domain, and the real part is all zeros.
The symmetry in the frequency domain is reversed; the real part is odd, while the imaginary part is even.
This situation will be discussed in Chapter 29.
For now, take it for granted that this is how the complex DFT behaves.
What if there is a signal in both parts of the time domain?
By additivity, the frequency domain will be the sum of the two frequency spectra.
Now the key element: a frequency spectrum composed of these two types of symmetry can be perfectly separated into the two component signals.
This is achieved by the even/odd decomposition discussed in Chapter 5.
In other words, two real DFT's can be calculated for the price of single FFT.
One of the signals is placed in the real part of the time domain, and the other signal is placed in the imaginary part.
After calculating the complex DFT (via the FFT, of course), the spectra are separated using the even/odd decomposition.
When two or more signals need to be passed through the FFT, this technique reduces the execution time by about 40%.
The improvement isn't a full factor of two because of the calculation time required for the even/odd decomposition.
This is a relatively simple technique with few pitfalls, nothing like writing an FFT routine from scratch.
Imaginary part symmetry of the DFT.
The next step is to modify the algorithm to calculate a single DFT faster.
It's ugly, but here is how it is done.
The input signal is broken in half by using an interlaced decomposition.
The N/2 even points are placed into the real part of the time domain signal, while the N/2 odd points go into the imaginary part.
An N/2 point FFT is then calculated, requiring about one-half the time as an N point FFT.
The resulting frequency domain is then separated by the even/odd decomposition, resulting in the frequency spectra of the two interlaced time domain signals.
These two frequency spectra are then combined into a single spectrum, just as in the last synthesis stage of the FFT.
To close this chapter, consider that the FFT is to Digital Signal Processing what the transistor is to electronics.
It is a foundation of the technology; everyone in the field knows its characteristics and how to use it.
However, only a small number of specialists really understand the details of the internal workings.
Continuous Signal Processing Continuous signal processing is a parallel field to DSP, and most of the techniques are nearly identical.
For example, both DSP and continuous signal processing are based on linearity, decomposition, convolution and Fourier analysis.
Since continuous signals cannot be directly represented in digital computers, don't expect to find computer programs in this chapter.
Continuous signal processing is based on mathematics; signals are represented as equations, and systems change one equation into another.
Just as the digital computer is the primary tool used in DSP, calculus is the primary tool used in continuous signal processing.
These techniques have been used for centuries, long before computers were developed.
The Delta Function Continuous signals can be decomposed into scaled and shifted delta functions, just as done with discrete signals.
The difference is that the continuous delta function is much more complicated and mathematically abstract than its discrete counterpart.
Instead of defining the continuous delta function by what it is, we will define it by the characteristics it has.
A thought experiment will show how this works.
Imagine an electronic circuit composed of linear components, such as resistors, capacitors and inductors.
Connected to the input is a signal generator that produces various shapes of short pulses.
The output of the circuit is connected to an oscilloscope, displaying the waveform produced by the circuit in response to each input pulse.
The question we want to answer is: how is the shape of the output pulse related to the characteristics of the input pulse?
To simplify the investigation, we will only use input pulses that are much shorter than the output.
For instance, if the system responds in milliseconds, we might use input pulses only a few microseconds in length.
After taking many measurement, we come to three conclusions: First, the shape of the input pulse does not affect the shape of the output signal.
This is illustrated in Fig. 13-1, where various shapes of short input pulses produce exactly the same shape of output pulse.
Second, the shape of the output waveform is totally determined by the characteristics of the system, i.e., the value and configuration of the resistors, capacitors and inductors.
Third, the amplitude of the output pulse is directly proportional to the area of the input pulse.
For example, the output will have the same amplitude for inputs of: 1 volt for 1 microsecond, 10 volts for 0.1 microseconds, 1,000 volts for 1 nanosecond, etc.
This relationship also allows for input pulses with negative areas.
For instance, imagine the combination of a volt pulse lasting 2 microseconds being quickly followed by a -1 volt pulse lasting 4 microseconds.
The total area of the input signal is zero, resulting in the output doing nothing.
Input signals that are brief enough to have these three properties are called impulses.
In other words, an impulse is any signal that is entirely zero except for a short blip of arbitrary shape.
For example, an impulse to a microwave transmitter may have to be in the picosecond range because the electronics responds in nanoseconds.
In comparison, a volcano that erupts for years may be a perfectly good impulse to geological changes that take millennia.
Mathematicians don't like to be limited by any particular system, and commonly use the term impulse to mean a signal that is short enough to be an impulse to any possible system.
That is, a signal that is infinitesimally narrow.
The continuous delta function is a normalized version of this type of impulse.
Specifically, the continuous delta function is mathematically defined by three idealized characteristics: (1) the signal must be infinitesimally brief, (2) the pulse must occur at time zero, and (3) the pulse must have an area of one.
Since the delta function is defined to be infinitesimally narrow and have a fixed area, the amplitude is implied to be infinite.
Don't let this bother you; it is completely unimportant.
Since the amplitude is part of the shape of the impulse, you will never encounter a problem where the amplitude makes any difference, infinite or not.
The delta function is a mathematical construct, not a real world signal.
Signals in the real world that act as delta functions will always have a finite duration and amplitude.
Just as in the discrete case, the continuous delta function is given the mathematical symbol: * ( ) .
Likewise, the output of a continuous system in response to a delta function is called the impulse response, and is often denoted by: h ( ) .
Notice that parentheses, ( ), are used to denote continuous signals, as compared to brackets, [ ], for discrete signals.
This notation is used in this book and elsewhere in DSP, but isn't universal.
Impulses are displayed in graphs as vertical arrows (see Fig. 13-1d), with the length of the arrow indicating the area of the impulse.
To better understand real world impulses, look into the night sky at a planet and a star, for instance, Mars and Sirius.
Both appear about the same brightness and size to the unaided eye.
The reason for this similarity is not The continuous delta function.
If the input to a linear system is brief compared to the resulting output, the shape of the output depends only on the characteristics of the system, and not the shape of the input.
Such short input signals are called impulses.
Figures a,b & c illustrate example input signals that are impulses for this particular system.
The term delta function is used to describe a normalized impulse, i.e., one that occurs at t ' 0 and has an area of one.
The mathematical symbols for the delta function are shown in (d), a vertical arrow and *(t) .
Mars is about 6000 kilometers in diameter and 60 million kilometers from earth.
In comparison, Sirius is about 300 times larger and over one-million times farther away.
These dimensions should make Mars appear more than three-thousand times larger than Sirius.
How is it possible that they look alike?
These objects look the same because they are small enough to be impulses to the human visual system.
The perceived shape is the impulse response of the eye, not the actual image of the star or planet.
This becomes obvious when the two objects are viewed through a small telescope; Mars appears as a dim disk, while Sirius still appears as a bright impulse.
This is also the reason that stars twinkle while planets do not.
The image of a star is small enough that it can be briefly blocked by particles or turbulence in the atmosphere, whereas the larger image of the planet is much less affected.
Convolution Just as with discrete signals, the convolution of continuous signals can be viewed from the input signal, or the output signal.
The input side viewpoint is the best conceptual description of how convolution operates.
In comparison, the output side viewpoint describes the mathematics that must be used.
These descriptions are virtually identical to those presented in Chapter 6 for discrete signals.
Figure 13-2 shows how convolution is viewed from the input side.
An input signal, x (t), is passed through a system characterized by an impulse response, h (t), to produce an output signal, y (t) .
This can be written in the familiar mathematical equation, y (t) ' x(t) th (t) .
The input signal is divided into narrow columns, each short enough to act as an impulse to the system.
In other words, the input signal is decomposed into an infinite number of scaled and shifted delta functions.
Each of these impulses produces a scaled and shifted version of the impulse response in the output signal.
The final output signal is then equal to the combined effect, i.e., the sum of all of the individual responses.
For this scheme to work, the width of the columns must be much shorter than the response of the system.
Of course, mathematicians take this to the extreme by making the input segments infinitesimally narrow, turning the situation into a calculus problem.
In this manner, the input viewpoint describes how a single point (or narrow region) in the input signal affects a larger portion of output signal.
In comparison, the output viewpoint examines how a single point in the output signal is determined by the various values from the input signal.
Just as with discrete signals, each instantaneous value in the output signal is affected by a section of the input signal, weighted by the impulse response flipped left-for-right.
In the discrete case, the signals are multiplied and summed.
In the continuous case, the signals are multiplied and integrated.
In equation form: This equation is called the convolution integral, and is the twin of the convolution sum (Eq.
Figure 13-3 shows how this equation can be understood.
The goal is to find an expression for calculating the value of the output signal at an arbitrary time, t.
The first step is to change the independent variable used to move through the input signal and the impulse response.
That is, we replace t with J (a lower case Convolution viewed from the input side.
The input signal, x(t), is divided into narrow segments, each acting as an impulse to the system.
The output signal, y (t), is the sum of the resulting scaled and shifted impulse responses.
This illustration shows how three points in the input signal contribute to the output signal.
Greek tau).
This makes x (t) and h (t) become x (J) and h (J), respectively.
This change of variable names is needed because t is already being used to represent the point in the output signal being calculated.
The next step is to flip the impulse response left-for-right, turning it into h (& J) .
Shifting the flipped impulse response to the location t, results in the expression becoming h(t& J) .
The input signal is then weighted by the flipped and shifted impulse response by multiplying the two, i.e., x (J) h (t& J) .
The value of the output signal is then found by integrating this weighted input signal from negative to positive infinity, as described by Eq. 13-1.
If you have trouble understanding how this works, go back and review the same concepts for discrete signals in Chapter 6. Figure 13-3 is just another way of describing the convolution machine in Fig. 6-8.
The only difference is that integrals are being used instead of summations.
Treat this as an extension of what you already know, not something new.
An example will illustrate how continuous convolution is used in real world problems and the mathematics required.
Figure 13-4 shows a simple continuous linear system: an electronic low-pass filter composed of a single resistor and a single capacitor.
As shown in the figure, an impulse entering this system produces an output that quickly jumps to some value, and then exponentially decays toward zero.
In other words, the impulse response of this simple electronic circuit is a one-sided exponential.
Mathematically, the Convolution viewed from the output side.
Each value in the output signal is influenced by many points from the input signal.
In this figure, the output signal at time t is being calculated.
The input signal, x (J), is weighted (multiplied) by the flipped and shifted impulse response, given by h ( t&J) .
Integrating the weighted input signal produces the value of the output point, y (t) Example of a continuous linear system.
This electronic circuit is a low-pass filter composed of a single resistor and capacitor.
The impulse response of this system is a one-sided exponential.
This is complicated by the fact that both signals are defined by regions rather than a single Calculating a convolution by segments.
Since many continuous signals are defined by regions, the convolution calculation must be performed region-by-region.
In this example, calculation of the output signal is broken into three sections: (a) no overlap, (b) partial overlap, and (c) total overlap, of the input signal and the shiftedflipped impulse response.
This is very common in continuous signal processing.
It is usually essential to draw a picture of how the two signals shift over each other for various values of t.
In this example, Fig. 13-6a shows that the two signals do not overlap at all for t < 0 .
This means that the product of the two signals is zero at all locations along the J axis, and the resulting output signal is: y(t ) ' for t < A second case is illustrated in (b), where t is between 0 and 1.
Here the two signals partially overlap, resulting in their product having nonzero values between J ' 0 and J ' t .
Since this is the only nonzero region, it is the only section where the integral needs to be evaluated.
This provides the output signal for 0 # t # 1, given by: The waveform in each of these three segments should agree with your knowledge of electronics: (1) The output signal must be zero until the input signal becomes nonzero.
That is, the first segment is given by y (t) ' 0 for t < 0 .
More intricate waveforms can be handled in the same way, although the mathematical complexity can rapidly become unmanageable.
When faced with a nasty continuous convolution problem, you need to spend significant time evaluating strategies for solving the problem.
If you start blindly evaluating integrals you are likely to end up with a mathematical mess.
A common strategy is to break one of the signals into simpler additive components that can be individually convolved.
Using the principles of linearity, the resulting waveforms can be added to find the answer to the original problem.
Figure 13-7 shows another strategy: modify one of the signals in some linear way, perform the convolution, and then undo the original modification.
In this example the modification is the derivative, and it is undone by taking the integral.
The derivative of a unit amplitude square pulse is two impulses, the first with an area of one, and the second with an area of negative one.
To understand this, think about the opposite process of taking the integral of the two impulses.
As you integrate past the first impulse, the integral rapidly increases from zero to one, i.e., a step function.
After passing the negative impulse, the integral of the signal rapidly returns from one back to zero, completing the square pulse.
Taking the derivative simplifies this problem because convolution is easy when one of the signals is composed of impulses.
Each of the two impulses in x ) (t) contributes a scaled and shifted version of the impulse response to A strategy for convolving signals.
Convolution problems can often be simplified by clever use of the rules governing linear systems.
In this example, the convolution of two signals is simplified by taking the derivative of one of them.
After performing the convolution, the derivative is undone by taking the integral.
That is, by inspection it is known that: y ) (t) ' h (t ) & h (t& 1) .
The output signal, y (t), can then be found by plugging in the exact equation for h (t), and integrating the expression.
A slight nuisance in this procedure is that the DC value of the input signal is lost when the derivative is taken.
This can result in an error in the DC value of the calculated output signal.
The mathematics reflects this as the arbitrary constant that can be added during the integration.
There is no systematic way of identifying this error, but it can usually be corrected by inspection of the problem.
For instance, there is no DC error in the example of Fig. 13-7.
This is known because the calculated output signal has the correct DC value when t becomes very large.
If an error is present in a particular problem, an appropriate DC term is manually added to the output signal to complete the calculation.
This method also works for signals that can be reduced to impulses by taking the derivative multiple times.
In the jargon of the field, these signals are called piecewise polynomials.
After the convolution, the initial operation of multiple derivatives is undone by taking multiple integrals.
The only catch is that the lost DC value must be found at each stage by finding the correct constant of integration.
Before starting a difficult continuous convolution problem, there is another approach that you should consider.
Ask yourself the question: Is a mathematical expression really needed for the output signal, or is a graph of the waveform sufficient?
If a graph is adequate, you may be better off to handle the problem with discrete techniques.
That is, approximate the continuous signals by samples that can be directly convolved by a computer program.
While not as mathematically pure, it can be much easier.
The Fourier Transform The Fourier Transform for continuous signals is divided into two categories, one for signals that are periodic, and one for signals that are aperiodic.
Periodic signals use a version of the Fourier Transform called the Fourier Series, and are discussed in the next section.
The Fourier Transform used with aperiodic signals is simply called the Fourier Transform.
This chapter describes these Fourier techniques using only real mathematics, just as the last several chapters have done for discrete signals.
The more powerful use of complex mathematics will be reserved for Chapter 31.
Figure 13-8 shows an example of a continuous aperiodic signal and its frequency spectrum.
The time domain signal extends from negative infinity to positive infinity, while each of the frequency domain signals extends from zero to positive infinity.
This frequency spectrum is shown in rectangular form (real and imaginary parts); however, the polar form (magnitude and phase) is also used with continuous signals.
Just as in the discrete case, the synthesis equation describes a recipe for constructing the time domain signal using the data in the frequency domain.
In mathematical form: x (t ) ' Re X (T) cos (T t ) & Im X (T) sin(T t ) dT EQUATION 13- The Fourier transform synthesis equation.
In this equation, x(t) is the time domain signal being synthesized, and Re X(T) & Im X(T) are the real and imaginary parts of the frequency spectrum, respectively.
In words, the time domain signal is formed by adding (with the use of an integral) an infinite number of scaled sine and cosine waves.
The real part of the frequency domain consists of the scaling factors for the cosine waves, while the imaginary part consists of the scaling factors for the sine waves.
Just as with discrete signals, the synthesis equation is usually written with negative sine waves.
Although the negative sign has no significance in this discussion, it is necessary to make the notation compatible with the complex mathematics described in Chapter 29.
The key point to remember is that some authors put this negative sign in the equation, while others do not.
Also notice that frequency is represented by the symbol, T, a lower case Example of the Fourier Transform.
The time domain signal, x(t ), extends from negative to positive infinity.
The frequency domain is composed of a real part, Re X(T), and an imaginary part, Im X(T), each extending from zero to positive infinity.
The frequency axis in this illustration is labeled in cycles per second (hertz).
To convert to natural frequency, multiply the numbers on the frequency axis by 2B .
Greek omega.
As you recall, this notation is called the natural frequency, and has the units of radians per second.
That is, T' 2Bf, where f is the frequency in cycles per second (hertz).
The natural frequency notation is favored by mathematicians and others doing signal processing by solving equations, because there are usually fewer symbols to write.
The analysis equations for continuous signals follow the same strategy as the discrete case: correlation with sine and cosine waves.
The equations are: EQUATION 13- The Fourier transform analysis equations.
In this equation, Re X(T) & Im X(T) are the real and imaginary parts of the frequency spectrum, respectively, and x(t) is the time domain signal being analyzed.
Frequency response of an RC low-pass filter.
These curves were derived by calculating the Fourier transform of the impulse response, and then converting to polar form.
The Fourier Series This brings us to the last member of the Fourier transform family: the Fourier series.
The time domain signal used in the Fourier series is periodic and continuous.
Figure 13-10 shows several examples of continuous waveforms that repeat themselves from negative to positive infinity.
Chapter 11 showed that periodic signals have a frequency spectrum consisting of harmonics.
For instance, if the time domain repeats at 1000 hertz (a period of 1 millisecond), the frequency spectrum will contain a first harmonic at 1000 hertz, a second harmonic at 2000 hertz, a third harmonic at 3000 hertz, and so forth.
The first harmonic, i.e., the frequency that the time domain repeats itself, is also called the fundamental frequency.
This means that the frequency spectrum can be viewed in two ways: (1) the frequency spectrum is continuous, but zero at all frequencies except the harmonics, or (2) the frequency spectrum is discrete, and only defined at the harmonic frequencies.
In other words, the frequencies between the harmonics can be thought of as having a value of zero, or simply not existing.
The important point is that they do not contribute to forming the time domain signal.
The Fourier series synthesis equation creates a continuous periodic signal with a fundamental frequency, f, by adding scaled cosine and sine waves with frequencies: f, 2f, 3f, 4f, etc.
The amplitudes of the cosine waves are held in the variables: a1, a2, a3, a4, etc., while the amplitudes of the sine waves are held in: b1, b2, b3, b4, and so on.
In other words, the "a" and "b" coefficients are the real and imaginary parts of the frequency spectrum, respectively.
In addition, the coefficient a0 is used to hold the DC value of the time domain waveform.
This can be viewed as the amplitude of a cosine wave with zero frequency (a constant value).
Sometimes a0 is grouped with the other "a" coefficients, but it is often handled separately because it requires special calculations.
There is no b0 coefficient since a sine wave of zero frequency has a constant value of zero, and would be quite useless.
The synthesis equation is written: x(t ) ' a0 % j an cos (2B f t n) & j bn sin(2B f t n) EQUATION 13- The Fourier series synthesis equation.
Any periodic signal, x(t), can be reconstructed from sine and cosine waves with frequencies that are multiples of the fundamental, f.
The a n and b n coefficients hold the amplitudes of the cosine and sine waves, respectively.
The corresponding analysis equations for the Fourier series are usually written in terms of the period of the waveform, denoted by T, rather than the fundamental frequency, f (where f ' 1/T ).
Since the time domain signal is periodic, the sine and cosine wave correlation only needs to be evaluated over a single period, i.e., & T /2 to T /2, 0 to T, -T to 0, etc. Selecting different limits makes the mathematics different, but the final answer is always the same.
The "b" coefficients are calculated in this same way; however, they all turn out to be zero.
In other words, this waveform can be constructed using only cosine waves, with no sine waves being needed.
The "a" and "b" coefficients will change if the time domain waveform is shifted left or right.
For instance, the "b" coefficients in this example will be zero only if one of the pulses is centered on t ' 0 .
Think about it this way.
If the waveform is even (i.e., symmetrical around t ' 0 ), it will be composed solely of even sinusoids, that is, cosine waves.
This makes all of the "b" coefficients equal to zero.
If the waveform if odd (i.e., symmetrical but opposite in sign around t ' 0 ), it will be composed of odd sinusoids, i.e., sine waves.
This results in the "a" coefficients being zero.
If the coefficients are converted to polar notation (say, M n and 2n coefficients), a shift in the time domain leaves the magnitude unchanged, but adds a linear component to the phase.
To complete this example, imagine a pulse train existing in an electronic circuit, with a frequency of 1 kHz, an amplitude of one volt, and a duty cycle of 0.2.
The table in Fig. 13-12 provides the amplitude of each harmonic contained in this waveform.
Figure 13-12 also shows the synthesis of the waveform using only the first fourteen of these harmonics.
Even with this number of harmonics, the reconstruction is not very good.
In mathematical jargon, the Fourier series converges very slowly.
This is just another way of saying that sharp edges in the time domain waveform results in very high frequencies in the spectrum.
Lastly, be sure and notice the overshoot at the sharp edges, i.e., the Gibbs effect discussed in Chapter 11.
An important application of the Fourier series is electronic frequency multiplication.
Suppose you want to construct a very stable sine wave oscillator at 150 MHz.
This might be needed, for example, in a radio transmitter operating at this frequency.
High stability calls for the circuit to be crystal controlled.
That is, the frequency of the oscillator is determined by a resonating quartz crystal that is a part of the circuit.
The problem is, quartz crystals only work to about 10 MHz.
The solution is to build a crystal controlled oscillator operating somewhere between 1 and 10 MHz, and then multiply the frequency to whatever you need.
This is accomplished by distorting the sine wave, such as by clipping the peaks with a diode, or running the waveform through a squaring circuit.
The harmonics in the distorted waveform are then isolated with band-pass filters.
This allows the frequency to be doubled, tripled, or multiplied by even higher integers numbers.
The most common technique is to use sequential stages of doublers and triplers to generate the required frequency multiplication, rather than just a single stage.
The Fourier series is important to this type of design because it describes the amplitude of the multiplied signal, depending on the type of distortion and harmonic selected.
Example of Fourier series synthesis.
The waveform being constructed is a pulse train at 1 kHz, an amplitude of one volt, and a duty cycle of 0.2 (as illustrated in Fig. 13-11).
This table shows the amplitude of the harmonics, while the graph shows the reconstructed waveform using only the first fourteen harmonics.
Digital filters are used for two general purposes: (1) separation of signals that have been combined, and (2) restoration of signals that have been distorted in some way.
Analog (electronic) filters can be used for these same tasks; however, digital filters can achieve far superior results.
The most popular digital filters are described and compared in the next seven chapters.
This introductory chapter describes the parameters you want to look for when learning about each of these filters.
Filter Basics Digital filters are a very important part of DSP.
In fact, their extraordinary performance is one of the key reasons that DSP has become so popular.
As mentioned in the introduction, filters have two uses: signal separation and signal restoration.
Signal separation is needed when a signal has been contaminated with interference, noise, or other signals.
For example, imagine a device for measuring the electrical activity of a baby's heart (EKG) while still in the womb.
The raw signal will likely be corrupted by the breathing and heartbeat of the mother.
A filter might be used to separate these signals so that they can be individually analyzed.
Signal restoration is used when a signal has been distorted in some way.
For example, an audio recording made with poor equipment may be filtered to better represent the sound as it actually occurred.
Another example is the deblurring of an image acquired with an improperly focused lens, or a shaky camera.
These problems can be attacked with either analog or digital filters.
Which is better?
Analog filters are cheap, fast, and have a large dynamic range in both amplitude and frequency.
Digital filters, in comparison, are vastly superior in the level of performance that can be achieved.
For example, a low-pass digital filter presented in Chapter 16 has a gain of 1 +/- 0.0002 from DC to 1000 hertz, and a gain of less than 0.0002 for frequencies above 1001 hertz.
The entire transition occurs within only 1 hertz.
Don't expect this from an op amp circuit!
Digital filters can achieve thousands of times better performance than analog filters.
This makes a dramatic difference in how filtering problems are approached.
With analog filters, the emphasis is on handling limitations of the electronics, such as the accuracy and stability of the resistors and capacitors.
In comparison, digital filters are so good that the performance of the filter is frequently ignored.
The emphasis shifts to the limitations of the signals, and the theoretical issues regarding their processing.
It is common in DSP to say that a filter's input and output signals are in the time domain.
This is because signals are usually created by sampling at regular intervals of time.
But this is not the only way sampling can take place.
The second most common way of sampling is at equal intervals in space.
For example, imagine taking simultaneous readings from an array of strain sensors mounted at one centimeter increments along the length of an aircraft wing.
Many other domains are possible; however, time and space are by far the most common.
When you see the term time domain in DSP, remember that it may actually refer to samples taken over time, or it may be a general reference to any domain that the samples are taken in.
As shown in Fig. 14-1, every linear filter has an impulse response, a step response and a frequency response.
Each of these responses contains complete information about the filter, but in a different form.
If one of the three is specified, the other two are fixed and can be directly calculated.
All three of these representations are important, because they describe how the filter will react under different circumstances.
The most straightforward way to implement a digital filter is by convolving the input signal with the digital filter's impulse response.
All possible linear filters can be made in this manner.
If it isn't, you probably don't have the background to understand this section on filter design.
Try reviewing the previous section on DSP fundamentals).
When the impulse response is used in this way, filter designers give it a special name: the filter kernel.
There is also another way to make digital filters, called recursion.
When a filter is implemented by convolution, each sample in the output is calculated by weighting the samples in the input, and adding them together.
Recursive filters are an extension of this, using previously calculated values from the output, besides points from the input.
Instead of using a filter kernel, recursive filters are defined by a set of recursion coefficients.
This method will be discussed in detail in Chapter 19.
For now, the important point is that all linear filters have an impulse response, even if you don't use it to implement the filter.
To find the impulse response of a recursive filter, simply feed in an impulse, and see what comes out.
The impulse responses of recursive filters are composed of sinusoids that exponentially decay in amplitude.
In principle, this makes their impulse responses infinitely long.
However, the amplitude eventually drops below the round-off noise of the system, and the remaining samples can be ignored.
Because Sample number Frequency FIGURE 14- Filter parameters.
Every linear filter has an impulse response, a step response, and a frequency response.
The step response, (b), can be found by discrete integration of the impulse response, (a).
The frequency response can be found from the impulse response by using the Fast Fourier Transform (FFT), and can be displayed either on a linear scale, (c), or in decibels, (d). of this characteristic, recursive filters are also called Infinite Impulse Response or IIR filters.
In comparison, filters carried out by convolution are called Finite Impulse Response or FIR filters.
As you know, the impulse response is the output of a system when the input is an impulse.
In this same manner, the step response is the output when the input is a step (also called an edge, and an edge response).
Since the step is the integral of the impulse, the step response is the integral of the impulse response.
This provides two ways to find the step response: (1) feed a step waveform into the filter and see what comes out, or (2) integrate the impulse response.
The frequency response can be found by taking the DFT (using the FFT algorithm) of the impulse response.
This will be reviewed later in this chapter.
The frequency response can be plotted on a linear vertical axis, such as in (c), or on a logarithmic scale (decibels), as shown in (d).
The linear scale is best at showing the passband ripple and roll-off, while the decibel scale is needed to show the stopband attenuation.
Don't remember decibels?
Here is a quick review.
A bel (in honor of Alexander Graham Bell) means that the power is changed by a factor of ten.
For example, an electronic circuit that has 3 bels of amplification produces an output signal with 10 ×10 ×10 ' 1000 times the power of the input.
A decibel (dB) is one-tenth of a bel.
Therefore, the decibel values of: -20dB, -10dB, 0dB, 10dB & 20dB, mean the power ratios: 0.01, 0.1, 1, 10, & 100, respectively.
In other words, every ten decibels mean that the power has changed by a factor of ten.
Here's the catch: you usually want to work with a signal's amplitude, not its power.
For example, imagine an amplifier with 20dB of gain.
By definition, this means that the power in the signal has increased by a factor of 100.
Since amplitude is proportional to the square-root of power, the amplitude of the output is 10 times the amplitude of the input.
While 20dB means a factor of 100 in power, it only means a factor of 10 in amplitude.
Every twenty decibels mean that the amplitude has changed by a factor of ten.
In equation form: EQUATION 14- Definition of decibels.
Decibels are a way of expressing a ratio between two signals.
Ratios of power (P1 & P2) use a different equation from ratios of amplitude (A1 & A2).
The natural log can be use by modifying the above equations: dB ' 4.342945 loge (P2 / P1) and dB ' 8.685890 loge (A2 / A1) .
Since decibels are a way of expressing the ratio between two signals, they are ideal for describing the gain of a system, i.e., the ratio between the output and the input signal.
However, engineers also use decibels to specify the amplitude (or power) of a single signal, by referencing it to some standard.
For example, the term: dBV means that the signal is being referenced to a 1 volt rms signal.
Likewise, dBm indicates a reference signal producing 1 mW into a 600 ohms load (about 0.78 volts rms).
If you understand nothing else about decibels, remember two things: First, -3dB means that the amplitude is reduced to 0.707 (and the power is therefore reduced to 0.5).
Second, memorize the following conversions between decibels and amplitude ratios: How Information is Represented in Signals The most important part of any DSP task is understanding how information is contained in the signals you are working with.
There are many ways that information can be contained in a signal.
This is especially true if the signal is manmade.
For instance, consider all of the modulation schemes that have been devised: AM, FM, single-sideband, pulse-code modulation, pulse-width modulation, etc.
The list goes on and on.
Fortunately, there are only two ways that are common for information to be represented in naturally occurring signals.
We will call these: information represented in the time domain, and information represented in the frequency domain.
Information represented in the time domain describes when something occurs and what the amplitude of the occurrence is.
For example, imagine an experiment to study the light output from the sun.
The light output is measured and recorded once each second.
Each sample in the signal indicates what is happening at that instant, and the level of the event.
If a solar flare occurs, the signal directly provides information on the time it occurred, the duration, the development over time, etc.
Each sample contains information that is interpretable without reference to any other sample.
Even if you have only one sample from this signal, you still know something about what you are measuring.
This is the simplest way for information to be contained in a signal.
In contrast, information represented in the frequency domain is more indirect.
Many things in our universe show periodic motion.
For example, a wine glass struck with a fingernail will vibrate, producing a ringing sound; the pendulum of a grandfather clock swings back and forth; stars and planets rotate on their axis and revolve around each other, and so forth.
By measuring the frequency, phase, and amplitude of this periodic motion, information can often be obtained about the system producing the motion.
Suppose we sample the sound produced by the ringing wine glass.
The fundamental frequency and harmonics of the periodic vibration relate to the mass and elasticity of the material.
A single sample, in itself, contains no information about the periodic motion, and therefore no information about the wine glass.
The information is contained in the relationship between many points in the signal.
This brings us to the importance of the step and frequency responses.
The step response describes how information represented in the time domain is being modified by the system.
In contrast, the frequency response shows how information represented in the frequency domain is being changed.
This distinction is absolutely critical in filter design because it is not possible to optimize a filter for both applications.
Good performance in the time domain results in poor performance in the frequency domain, and vice versa.
If you are designing a filter to remove noise from an EKG signal (information represented in the time domain), the step response is the important parameter, and the frequency response is of little concern.
If your task is to design a digital filter for a hearing aid (with the information in the frequency domain), the frequency response is all important, while the step response doesn't matter.
Now let's look at what makes a filter optimal for time domain or frequency domain applications.
Time Domain Parameters It may not be obvious why the step response is of such concern in time domain filters.
You may be wondering why the impulse response isn't the important parameter.
The answer lies in the way that the human mind understands and processes information.
Remember that the step, impulse and frequency responses all contain identical information, just in different arrangements.
The step response is useful in time domain analysis because it matches the way humans view the information contained in the signals.
For example, suppose you are given a signal of some unknown origin and asked to analyze it.
The first thing you will do is divide the signal into regions of similar characteristics.
You can't stop from doing this; your mind will do it automatically.
Some of the regions may be smooth; others may have large amplitude peaks; others may be noisy.
This segmentation is accomplished by identifying the points that separate the regions.
This is where the step function comes in.
The step function is the purest way of representing a division between two dissimilar regions.
It can mark when an event starts, or when an event ends.
It tells you that whatever is on the left is somehow different from whatever is on the right.
This is how the human mind views time domain information: a group of step functions dividing the information into regions of similar characteristics.
The step response, in turn, is important because it describes how the dividing lines are being modified by the filter.
The step response parameters that are important in filter design are shown in Fig. 14-2.
To distinguish events in a signal, the duration of the step response must be shorter than the spacing of the events.
This dictates that the step response should be as fast (the DSP jargon) as possible.
This is shown in Figs.
The most common way to specify the risetime (more jargon) is to quote the number of samples between the 10% and 90% amplitude levels.
Why isn't a very fast risetime always possible?
There are many reasons, noise reduction, inherent limitations of the data acquisition system, avoiding aliasing, etc. Sample number Sample number FIGURE 14- Parameters for evaluating time domain performance.
The step response is used to measure how well a filter performs in the time domain.
Three parameters are important: (1) transition speed (risetime), shown in (a) and (b), (2) overshoot, shown in (c) and (d), and (3) phase linearity (symmetry between the top and bottom halves of the step), shown in (e) and (f).
Figures (c) and (d) shows the next parameter that is important: overshoot in the step response.
Overshoot must generally be eliminated because it changes the amplitude of samples in the signal; this is a basic distortion of the information contained in the time domain.
This can be summed up in one question: Is the overshoot you observe in a signal coming from the thing you are trying to measure, or from the filter you have used?
Finally, it is often desired that the upper half of the step response be symmetrical with the lower half, as illustrated in (e) and (f).
This symmetry is needed to make the rising edges look the same as the falling edges.
This symmetry is called linear phase, because the frequency response has a phase that is a straight line (discussed in Chapter 19).
Make sure you understand these three parameters; they are the key to evaluating time domain filters.
Frequency Domain Parameters Figure 14-3 shows the four basic frequency responses.
The purpose of these filters is to allow some frequencies to pass unaltered, while completely blocking other frequencies.
The passband refers to those frequencies that are passed, while the stopband contains those frequencies that are blocked.
The transition band is between.
A fast roll-off means that the transition band is very narrow.
The division between the passband and transition band is called the cutoff frequency.
In analog filter design, the cutoff frequency is usually defined to be where the amplitude is reduced to 0.707 (i.e., -3dB).
Digital filters are less standardized, and it is common to see 99%, 90%, 70.7%, and 50% amplitude levels defined to be the cutoff frequency.
Figure 14-4 shows three parameters that measure how well a filter performs in the frequency domain.
To separate closely spaced frequencies, the filter must have a fast roll-off, as illustrated in (a) and (b).
For the passband frequencies to move through the filter unaltered, there must be no passband ripple, as shown in (c) and (d).
Lastly, to adequately block the stopband frequencies, it is necessary to have good stopband attenuation, displayed in (e) and (f).
Band-pass Amplitude transition band stopband Frequency Frequency d.
Band-reject Amplitude b.
High-pass Amplitude FIGURE 14- The four common frequency responses.
Frequency domain filters are generally used to pass certain frequencies (the passband), while blocking others (the stopband).
Four responses are the most common: low-pass, high-pass, band-pass, and band-reject.
Amplitude Amplitude (dB) Frequency Frequency FIGURE 14- Parameters for evaluating frequency domain performance.
The frequency responses shown are for low-pass filters.
Three parameters are important: (1) roll-off sharpness, shown in (a) and (b), (2) passband ripple, shown in (c) and (d), and (3) stopband attenuation, shown in (e) and (f).
Why is there nothing about the phase in these parameters?
First, the phase isn't important in most frequency domain applications.
For example, the phase of an audio signal is almost completely random, and contains little useful information.
Second, if the phase is important, it is very easy to make digital filters with a perfect phase response, i.e., all frequencies pass through the filter with a zero phase shift (also discussed in Chapter 19).
In comparison, analog filters are ghastly in this respect.
Previous chapters have described how the DFT converts a system's impulse response into its frequency response.
Here is a brief review.
The quickest way to calculate the DFT is by means of the FFT algorithm presented in Chapter 12. Starting with a filter kernel N samples long, the FFT calculates the frequency spectrum consisting of an N point real part and an N point imaginary part.
Only samples 0 to N/2 of the FFT's real and imaginary parts contain useful information; the remaining points are duplicates (negative frequencies) and can be ignored.
Since the real and imaginary parts are difficult for humans to understand, they are usually converted into polar notation as described in Chapter 8.
This provides the magnitude and phase signals, each running from sample 0 to sample N/2 (i.e., N/2 %1 samples in each signal).
For example, an impulse response of 256 points will result in a frequency response running from point 0 to 128.
Sample 0 represents DC, i.e., zero frequency.
Sample 128 represents one-half of the sampling rate.
Remember, no frequencies higher than one-half of the sampling rate can appear in sampled data.
The number of samples used to represent the impulse response can be arbitrarily large.
For instance, suppose you want to find the frequency response of a filter kernel that consists of 80 points.
Since the FFT only works with signals that are a power of two, you need to add 48 zeros to the signal to bring it to a length of 128 samples.
This padding with zeros does not change the impulse response.
To understand why this is so, think about what happens to these added zeros when the input signal is convolved with the system's impulse response.
The added zeros simply vanish in the convolution, and do not affect the outcome.
Taking this a step further, you could add many zeros to the impulse response to make it, say, 256, 512, or 1024 points long.
The important idea is that longer impulse responses result in a closer spacing of the data points in the frequency response.
That is, there are more samples spread between DC and one-half of the sampling rate.
Taking this to the extreme, if the impulse response is padded with an infinite number of zeros, the data points in the frequency response are infinitesimally close together, i.e., a continuous line.
In other words, the frequency response of a filter is really a continuous signal between DC and one-half of the sampling rate.
The output of the DFT is a sampling of this continuous line.
What length of impulse response should you use when calculating a filter's frequency response?
As a first thought, try N '1024, but don't be afraid to change it if needed (such as insufficient resolution or excessive computation time).
Keep in mind that the "good" and "bad" parameters discussed in this chapter are only generalizations.
Many signals don't fall neatly into categories.
For example, consider an EKG signal contaminated with 60 hertz interference.
The information is encoded in the time domain, but the interference is best dealt with in the frequency domain.
The best design for this application is Time Domain Frequency Domain a. Original filter kernel b.
Original frequency response Amplitude Amplitude Sample number Frequency c. Filter kernel with spectral inversion d.
Inverted frequency response Amplitude Amplitude Flipped top-for-bottom Sample number Frequency FIGURE 14- Example of spectral inversion.
The low-pass filter kernel in (a) has the frequency response shown in (b).
A high-pass filter kernel, (c), is formed by changing the sign of each sample in (a), and adding one to the sample at the center of symmetry.
This action in the time domain inverts the frequency spectrum (i.e., flips it top-forbottom), as shown by the high-pass frequency response in (d).
Remember the number one rule of education: A paragraph in a book doesn't give you a license to stop thinking.
High-Pass, Band-Pass and Band-Reject Filters High-pass, band-pass and band-reject filters are designed by starting with a low-pass filter, and then converting it into the desired response.
For this reason, most discussions on filter design only give examples of low-pass filters.
There are two methods for the low-pass to high-pass conversion: spectral inversion and spectral reversal.
Both are equally useful.
An example of spectral inversion is shown in 14-5.
Figure (a) shows a lowpass filter kernel called a windowed-sinc (the topic of Chapter 16).
This filter kernel is 51 points in length, although many of samples have a value so small that they appear to be zero in this graph.
The corresponding a. High-pass by adding parallel stages Low-pass Block diagram of spectral inversion.
In (a), the input signal, x[n], is applied to two systems in parallel, having impulse responses of h[n] and *[n] .
As shown in (b), the combined system has an impulse response of *[n] & h[n] .
This means that the frequency response of the combined system is the inversion of the frequency response of h[n] .
Two things must be done to change the low-pass filter kernel into a high-pass filter kernel.
First, change the sign of each sample in the filter kernel.
Second, add one to the sample at the center of symmetry.
This results in the high-pass filter kernel shown in (c), with the frequency response shown in (d).
Spectral inversion flips the frequency response top-for-bottom, changing the passbands into stopbands, and the stopbands into passbands.
In other words, it changes a filter from low-pass to high-pass, high-pass to low-pass, band-pass to band-reject, or band-reject to band-pass.
Figure 14-6 shows why this two step modification to the time domain results in an inverted frequency spectrum.
In (a), the input signal, x[n], is applied to two systems in parallel.
One of these systems is a low-pass filter, with an impulse response given by h[n] .
The other system does nothing to the signal, and therefore has an impulse response that is a delta function, *[n] .
The overall output, y[n], is equal to the output of the all-pass system minus the output of the low-pass system.
Since the low frequency components are subtracted from the original signal, only the high frequency components appear in the output.
Thus, a high-pass filter is formed.
This could be performed as a two step operation in a computer program: run the signal through a low-pass filter, and then subtract the filtered signal from the original.
However, the entire operation can be performed in a signal stage by combining the two filter kernels.
As described in Chapter Time Domain Frequency Domain Sample number Frequency FIGURE 14- Example of spectral reversal.
The low-pass filter kernel in (a) has the frequency response shown in (b).
A high-pass filter kernel, (c), is formed by changing the sign of every other sample in (a).
This action in the time domain results in the frequency domain being flipped left-for-right, resulting in the high-pass frequency response shown in (d).
As shown in (b), the filter kernel for the highpass filter is given by: *[n] & h[n] .
That is, change the sign of all the samples, and then add one to the sample at the center of symmetry.
For this technique to work, the low-frequency components exiting the low-pass filter must have the same phase as the low-frequency components exiting the all-pass system.
Otherwise a complete subtraction cannot take place.
This places two restrictions on the method: (1) the original filter kernel must have left-right symmetry (i.e., a zero or linear phase), and (2) the impulse must be added at the center of symmetry.
The second method for low-pass to high-pass conversion, spectral reversal, is illustrated in Fig. 14-7.
Just as before, the low-pass filter kernel in (a) corresponds to the frequency response in (b).
The high-pass filter kernel, (c), is formed by changing the sign of every other sample in (a).
As shown in (d), this flips the frequency domain left-for-right: 0 becomes 0.5 and 0. a. Band-pass by cascading stages FIGURE 14- Designing a band-pass filter.
As shown in (a), a band-pass filter can be formed by cascading a low-pass filter and a high-pass filter.
This can be reduced to a single stage, shown in (b).
The filter kernel of the single stage is equal to the convolution of the low-pass and highpass filter kernels.
Changing the sign of every other sample is equivalent to multiplying the filter kernel by a sinusoid with a frequency of 0.5.
As discussed in Chapter 10, this has the effect of shifting the frequency domain by 0.5.
Look at (b) and imagine the negative frequencies between -0.5 and 0 that are of mirror image of the frequencies between 0 and 0.5.
The frequencies that appear in (d) are the negative frequencies from (b) shifted by 0.5.
Lastly, Figs.
In short, adding the filter kernels produces a band-reject filter, while convolving the filter kernels produces a band-pass filter.
These are based on the way cascaded and parallel systems are be combined, as discussed in Chapter 7. Multiple combination of these techniques can also be used.
For instance, a band-pass filter can be designed by adding the two filter kernels to form a stop-pass filter, and then use spectral inversion or spectral reversal as previously described.
All these techniques work very well with few surprises.
Filter Classification Table 14-1 summarizes how digital filters are classified by their use and by their implementation.
The use of a digital filter can be broken into three categories: time domain, frequency domain and custom.
As previously described, time domain filters are used when the information is encoded in the shape of the signal's waveform.
Time domain filtering is used for such actions as: smoothing, DC removal, waveform shaping, etc.
In contrast, frequency domain filters are used when the information is contained in the a. Band-reject by adding parallel stages Low-pass amplitude, frequency, and phase of the component sinusoids.
The goal of these filters is to separate one band of frequencies from another.
Custom filters are used when a special action is required by the filter, something more elaborate than the four basic responses (high-pass, low-pass, band-pass and band-reject).
For instance, Chapter 17 describes how custom filters can be used for deconvolution, a way of counteracting an unwanted convolution.
Filter classification.
Filters can be divided by their use, and how they are implemented.
Digital filters can be implemented in two ways, by convolution (also called finite impulse response or FIR) and by recursion (also called infinite impulse response or IIR).
Filters carried out by convolution can have far better performance than filters using recursion, but execute much more slowly.
The next six chapters describe digital filters according to the classifications in Table 14-1.
First, we will look at filters carried out by convolution.
The moving average (Chapter 15) is used in the time domain, the windowed-sinc (Chapter 16) is used in the frequency domain, and FIR custom (Chapter 17) is used when something special is needed.
To finish the discussion of FIR filters, Chapter 18 presents a technique called FFT convolution.
This is an algorithm for increasing the speed of convolution, allowing FIR filters to execute faster.
Next, we look at recursive filters.
The single pole recursive filter (Chapter 19) is used in the time domain, while the Chebyshev (Chapter 20) is used in the frequency domain.
Recursive filters having a custom response are designed by iterative techniques.
For this reason, we will delay their discussion until Chapter 26, where they will be presented with another type of iterative procedure: the neural network.
As shown in Table 14-1, convolution and recursion are rival techniques; you must use one or the other for a particular application.
How do you choose?
Chapter 21 presents a head-to-head comparison of the two, in both the time and frequency domains.
The moving average is the most common filter in DSP, mainly because it is the easiest digital filter to understand and use.
In spite of its simplicity, the moving average filter is optimal for a common task: reducing random noise while retaining a sharp step response.
This makes it the premier filter for time domain encoded signals.
However, the moving average is the worst filter for frequency domain encoded signals, with little ability to separate one band of frequencies from another.
Relatives of the moving average filter include the Gaussian, Blackman, and multiplepass moving average.
These have slightly better performance in the frequency domain, at the expense of increased computation time.
Implementation by Convolution As the name implies, the moving average filter operates by averaging a number of points from the input signal to produce each point in the output signal.
In equation form, this is written: EQUATION 15- Equation of the moving average filter.
In this equation, x[ ] is the input signal, y[ ] is the output signal, and M is the number of points used in the moving average.
This equation only uses points on one side of the output sample being calculated.
This corresponds to changing the summation in Eq. 15-1 from: j ' 0 to M& 1, to: j ' & (M& 1) /2 to (M& 1) /2 .
For instance, in a 10 point moving average filter, the index, j, can run from 0 to 11 (one side averaging) or -5 to (symmetrical averaging).
Symmetrical averaging requires that M be an odd number.
Programming is slightly easier with the points on only one side; however, this produces a relative shift between the input and output signals.
You should recognize that the moving average filter is a convolution using a very simple filter kernel.
For example, a 5 point filter has the filter kernel: þ 0, 0, 1/5, 1/5, 1/5, 1/5, 1/5, 0, 0 þ .
That is, the moving average filter is a convolution of the input signal with a rectangular pulse having an area of one.
Table 15-1 shows a program to implement the moving average filter.
Noise Reduction vs. Step Response Many scientists and engineers feel guilty about using the moving average filter.
Because it is so very simple, the moving average filter is often the first thing tried when faced with a problem.
Even if the problem is completely solved, there is still the feeling that something more should be done.
This situation is truly ironic.
Not only is the moving average filter very good for many applications, it is optimal for a common problem, reducing random white noise while keeping the sharpest step response.
In (a), a rectangular pulse is buried in random noise.
In (b) and (c), this signal is filtered with 11 and point moving average filters, respectively.
As the number of points in the filter increases, the noise becomes lower; however, the edges becoming less sharp.
The moving average filter is the optimal solution for this problem, providing the lowest noise possible for a given edge sharpness.
The signal in (a) is a pulse buried in random noise.
In (b) and (c), the smoothing action of the moving average filter decreases the amplitude of the random noise (good), but also reduces the sharpness of the edges (bad).
Of all the possible linear filters that could be used, the moving average produces the lowest noise for a given edge sharpness.
The amount of noise reduction is equal to the square-root of the number of points in the average.
For example, a 100 point moving average filter reduces the noise by a factor of 10.
To understand why the moving average if the best solution, imagine we want to design a filter with a fixed edge sharpness.
For example, let's assume we fix the edge sharpness by specifying that there are eleven points in the rise of the step response.
This requires that the filter kernel have eleven points.
The optimization question is: how do we choose the eleven values in the filter kernel to minimize the noise on the output signal?
Since the noise we are trying to reduce is random, none of the input points is special; each is just as noisy as its neighbor.
Therefore, it is useless to give preferential treatment to any one of the input points by assigning it a larger coefficient in the filter kernel.
The lowest noise is obtained when all the input samples are treated equally, i.e., the moving average filter.
The point is, no filter is better than the simple moving average).
Frequency Response Figure 15-2 shows the frequency response of the moving average filter.
It is mathematically described by the Fourier transform of the rectangular pulse, as discussed in Chapter 11: EQUATION 15- Frequency response of an M point moving average filter.
The frequency, f, runs between 0 and 0.5.
For f ' 0, use: H [ f ] ' H [f ] ' sin(B f M ) M sin(B f ) The roll-off is very slow and the stopband attenuation is ghastly.
Clearly, the moving average filter cannot separate one band of frequencies from another.
Remember, good performance in the time domain results in poor performance in the frequency domain, and vice versa.
In short, the moving average is an exceptionally good smoothing filter (the action in the time domain), but an exceptionally bad low-pass filter (the action in the frequency domain).
The moving average is a very poor low-pass filter, due to its slow roll-off and poor stopband attenuation.
These curves are generated by Eq. 15-2.
Unfortunately, there are some applications where both domains are simultaneously important.
For instance, television signals fall into this nasty category.
Video information is encoded in the time domain, that is, the shape of the waveform corresponds to the patterns of brightness in the image.
However, during transmission the video signal is treated according to its frequency composition, such as its total bandwidth, how the carrier waves for sound & color are added, elimination & restoration of the DC component, etc.
As another example, electromagnetic interference is best understood in the frequency domain, even if Characteristics of multiple-pass moving average filters.
Figure (a) shows the filter kernels resulting from passing a seven point moving average filter over the data once, twice and four times.
Figure (b) shows the corresponding step responses, while (c) and (d) show the corresponding frequency responses.
For instance, the temperature monitor in a scientific experiment might be contaminated with hertz from the power lines, 30 kHz from a switching power supply, or kHz from a local AM radio station.
Relatives of the moving average filter have better frequency domain performance, and can be useful in these mixed domain applications.
Multiple-pass moving average filters involve passing the input signal through a moving average filter two or more times.
Figure 15-3a shows the overall filter kernel resulting from one, two and four passes.
Two passes are equivalent to using a triangular filter kernel (a rectangular filter kernel convolved with itself).
After four or more passes, the equivalent filter kernel looks like a Gaussian (recall the Central Limit Theorem).
As shown in (b), multiple passes produce an "s" shaped step response, as compared to the straight line of the single pass.
The frequency responses in (c) and (d) are given by Eq. 15-2 multiplied by itself for each pass.
That is, each time domain convolution results in a multiplication of the frequency spectra.
Figure 15-4 shows the frequency response of two other relatives of the moving average filter.
When a pure Gaussian is used as a filter kernel, the frequency response is also a Gaussian, as discussed in Chapter 11.
The Gaussian is important because it is the impulse response of many natural and manmade systems.
For example, a brief pulse of light entering a long fiber optic transmission line will exit as a Gaussian pulse, due to the different paths taken by the photons within the fiber.
The Gaussian filter kernel is also used extensively in image processing because it has unique properties that allow fast two-dimensional convolutions (see Chapter 24).
The second frequency response in Fig. 15-4 corresponds to using a Blackman window as a filter kernel.
The exact shape of the Blackman window is given in Chapter 16 (Eq.
How are these relatives of the moving average filter better than the moving average filter itself?
Three ways: First, and most important, these filters have better stopband attenuation than the moving average filter.
Second, the filter kernels taper to a smaller amplitude near the ends.
Recall that each point in the output signal is a weighted sum of a group of samples from the input.
If the filter kernel tapers, samples in the input signal that are farther away are given less weight than those close by.
Third, the step responses are smooth curves, rather than the abrupt straight line of the moving average.
These last two are usually of limited benefit, although you might find applications where they are genuine advantages.
The moving average filter and its relatives are all about the same at reducing random noise while maintaining a sharp step response.
The ambiguity lies in how the risetime of the step response is measured.
If the risetime is measured from 0% to 100% of the step, the moving average filter is the best you can do, as previously shown.
In comparison, measuring the risetime from 10% to 90% makes the Blackman window better than the moving average filter.
The point is, this is just theoretical squabbling; consider these filters equal in this parameter.
The biggest difference in these filters is execution speed.
Using a recursive algorithm (described next), the moving average filter will run like lightning in your computer.
In fact, it is the fastest digital filter available.
Multiple passes of the moving average will be correspondingly slower, but still very quick.
In comparison, the Gaussian and Blackman filters are excruciatingly slow, because they must use convolution.
Think a factor of ten times the number of points in the filter kernel (based on multiplication being about 10 times slower than addition).
For example, expect a 100 point Gaussian to be 1000 times slower than a moving average using recursion.
Recursive Implementation A tremendous advantage of the moving average filter is that it can be implemented with an algorithm that is very fast.
To understand this Amplitude (dB) FIGURE 15- Frequency response of the Blackman window and Gaussian filter kernels.
Both these filters provide better stopband attenuation than the moving average filter.
This has no advantage in removing random noise from time domain encoded signals, but it can be useful in mixed domain problems.
The disadvantage of these filters is that they must use convolution, a terribly slow algorithm.
Gaussian Blackman Frequency algorithm, imagine passing an input signal, x [ ], through a seven point moving average filter to form an output signal, y [ ] .
Now look at how two adjacent output points, y [50] and y [51], are calculated: These are nearly the same calculation; points x [48] through x [53] must be added for y [50], and again for y [51] .
If y [50] has already been calculated, the most efficient way to calculate y [51] is: Once y [51] has been found using y [50], then y [52] can be calculated from sample y [51], and so on.
After the first point is calculated in y [ ], all of the other points can be found with only a single addition and subtraction per point.
This can be expressed in the equation: EQUATION 15- Recursive implementation of the moving average filter.
In this equation, x[ ] is the input signal, y[ ] is the output signal, M is the number of points in the moving average (an odd number).
Before this equation can be used, the first point in the signal must be calculated using a standard summation.
Notice that this equation use two sources of data to calculate each point in the output: points from the input and previously calculated points from the output.
This is called a recursive equation, meaning that the result of one calculation is used in future calculations.
Chapter 19 discusses a variety of recursive filters in more detail.
Be aware that the moving average recursive filter is very different from typical recursive filters.
In particular, most recursive filters have an infinitely long impulse response (IIR), composed of sinusoids and exponentials.
The impulse response of the moving average is a rectangular pulse (finite impulse response, or FIR).
This algorithm is faster than other digital filters for several reasons.
First, there are only two computations per point, regardless of the length of the filter kernel.
Second, addition and subtraction are the only math operations needed, while most digital filters require time-consuming multiplication.
Third, the indexing scheme is very simple.
Each index in Eq. 15-3 is found by adding or subtracting integer constants that can be calculated before the filtering starts (i.e., p and q).
Fourth, the entire algorithm can be carried out with integer representation.
Depending on the hardware used, integers can be more than an order of magnitude faster than floating point.
Surprisingly, integer representation works better than floating point with this algorithm, in addition to being faster.
The round-off error from floating point arithmetic can produce unexpected results if you are not careful.
For example, imagine a 10,000 sample signal being filtered with this method.
The last sample in the filtered signal contains the accumulated error of 10,000 additions and 10,000 subtractions.
This appears in the output signal as a drifting offset.
Integers don't have this problem because there is no round-off error in the arithmetic.
If you must use floating point with this algorithm, the program in Table 15-2 shows how to use a double precision accumulator to eliminate this drift.
Windowed-sinc filters are used to separate one band of frequencies from another.
They are very stable, produce few surprises, and can be pushed to incredible performance levels.
These exceptional frequency domain characteristics are obtained at the expense of poor performance in the time domain, including excessive ripple and overshoot in the step response.
When carried out by standard convolution, windowed-sinc filters are easy to program, but slow to execute.
Chapter 18 shows how the FFT can be used to dramatically improve the computational speed of these filters.
Strategy of the Windowed-Sinc Figure 16-1 illustrates the idea behind the windowed-sinc filter.
In (a), the frequency response of the ideal low-pass filter is shown.
All frequencies below the cutoff frequency, fC, are passed with unity amplitude, while all higher frequencies are blocked.
The passband is perfectly flat, the attenuation in the stopband is infinite, and the transition between the two is infinitesimally small.
Taking the Inverse Fourier Transform of this ideal frequency response produces the ideal filter kernel (impulse response) shown in (b).
As previously discussed (see Chapter 11, Eq. 11-4), this curve is of the general form: sin(x)/x, called the sinc function, given by: h [i ] ' sin(2B fC i ) Convolving an input signal with this filter kernel provides a perfect low-pass filter.
The problem is, the sinc function continues to both negative and positive infinity without dropping to zero amplitude.
While this infinite length is not a problem for mathematics, it is a show stopper for computers.
To get around this problem, we will make two modifications to the sinc function in (b), resulting in the waveform shown in (c).
First, it is truncated to M % 1 points, symmetrically chosen around the main lobe, where M is an even number.
All samples outside these M % 1 points are set to zero, or simply ignored.
Second, the entire sequence is shifted to the right so that it runs from 0 to M. This allows the filter kernel to be represented using only positive indexes.
While many programming languages allow negative indexes, they are a nuisance to use.
The sole effect of this M /2 shift in the filter kernel is to shift the output signal by the same amount.
Since the modified filter kernel is only an approximation to the ideal filter kernel, it will not have an ideal frequency response.
To find the frequency response that is obtained, the Fourier transform can be taken of the signal in (c), resulting in the curve in (d).
It's a mess!
There is excessive ripple in the passband and poor attenuation in the stopband (recall the Gibbs effect discussed in Chapter 11).
These problems result from the abrupt discontinuity at the ends of the truncated sinc function.
Increasing the length of the filter kernel does not reduce these problems; the discontinuity is significant no matter how long M is made.
Fortunately, there is a simple method of improving this situation.
Figure (e) shows a smoothly tapered curve called a Blackman window.
Multiplying the truncated-sinc, (c), by the Blackman window, (e), results in the windowedsinc filter kernel shown in (f).
The idea is to reduce the abruptness of the truncated ends and thereby improve the frequency response.
Figure (g) shows this improvement.
The passband is now flat, and the stopband attenuation is so good it cannot be seen in this graph.
Several different windows are available, most of them named after their original developers in the 1950s.
Only two are worth using, the Hamming window and the Blackman window These are given by: EQUATION 16- The Hamming window.
These windows run from i ' 0 to M, for a total of M % 1 points.
EQUATION 16- The Blackman window.
Figure 16-2a shows the shape of these two windows for M ' 50 (i.e., 51 total points in the curves).
Which of these two windows should you use?
It's a trade-off between parameters.
As shown in Fig. 16-2b, the Hamming window has about a 20% faster roll-off than the Blackman.
However, FIGURE 16-1 (facing page) Derivation of the windowed-sinc filter kernel.
The frequency response of the ideal low-pass filter is shown in (a), with the corresponding filter kernel in (b), a sinc function.
Since the sinc is infinitely long, it must be truncated to be used in a computer, as shown in (c).
However, this truncation results in undesirable changes in the frequency response, (d).
The solution is to multiply the truncated-sinc with a smooth window, (e), resulting in the windowed-sinc filter kernel, (f).
The frequency response of the windowed-sinc, (g), is smooth and well behaved.
These figures are not to scale.
Time Domain Frequency Domain Characteristics of the Blackman and Hamming windows.
The shapes of these two windows are shown in (a), and given by Eqs.
As shown in (b), the Hamming window results in about 20% faster roll-off than the Blackman window.
However, the Blackman window has better stopband attenuation (Blackman: 0.02%, Hamming: 0.2%), and a lower passband ripple (Blackman: 0.02% Hamming: 0.2%).
Hamming Blackman Frequency (c) shows that the Blackman has a better stopband attenuation.
To be exact, the stopband attenuation for the Blackman is -74dB ( - 0.02%), while the Hamming is only -53dB (-0.2%).
Although it cannot be seen in these graphs, the Blackman has a passband ripple of only about 0.02%, while the Hamming is typically 0.2%.
In general, the Blackman should be your first choice; a slow roll-off is easier to handle than poor stopband attenuation.
There are other windows you might hear about, although they fall short of the Blackman and Hamming.
The Bartlett window is a triangle, using straight lines for the taper.
The Hanning window, also called the raised cosine window, is given by: w[i] ' 0.5 & 0.5 cos(2Bi /M) .
These two windows have about the same roll-off speed as the Hamming, but worse stopband attenuation (Bartlett: -25dB or 5.6%, Hanning -44dB or 0.63%).
You might also hear of a rectangular window.
This is the same as no window, just a truncation of the tails (such as in Fig. 16-1c).
While the roll-off is -2.5 times faster than the Blackman, the stopband attenuation is only -21dB (8.9%).
Designing the Filter To design a windowed-sinc, two parameters must be selected: the cutoff frequency, fC, and the length of the filter kernel, M. The cutoff frequency Filter length vs. roll-off of the windowed-sinc filter.
As shown in (a), for M = 20, 40, and 200, the transition bandwidths are BW = 0.2, 0.1, and 0.02 of the sampling rate, respectively.
As shown in (b), the shape of the frequency response does not change with different cutoff frequencies.
In (b), M = 60. is expressed as a fraction of the sampling rate, and therefore must be between 0 and 0.5.
The value for M sets the roll-off according to the approximation: EQUATION 16- Filter length vs. roll-off.
The length of the filter kernel, M, determines the transition bandwidth of the filter, BW.
This is only an approximation since roll-off depends on the particular window being used.
M .
The transition bandwidth is also expressed as a fraction of the sampling frequency, and must between 0 and 0.5.
Figure 16-3a shows an example of how this approximation is used.
The three curves shown are generated from filter kernels with: M ' 20, 40, and 200 .
From Eq. 16-3, the transition bandwidths are: BW ' 0.2, 0.1, and 0.02, respectively.
Figure (b) shows that the shape of the frequency response does not depend on the cutoff frequency selected.
Since the time required for a convolution is proportional to the length of the signals, Eq. 16-3 expresses a trade-off between computation time (depends on the value of M) and filter sharpness (the value of BW).
For instance, the 20% slower roll-off of the Blackman window (as compared with the Hamming) can be compensated for by using a filter kernel 20% longer.
In other words, it could be said that the Blackman window is 20% slower to execute that an equivalent roll-off Hamming window.
This is important because the execution speed of windowed-sinc filters is already terribly slow.
As also shown in Fig. 16-3b, the cutoff frequency of the windowed-sinc filter is measured at the one-half amplitude point.
Why use 0.5 instead of the standard 0.707 (-3dB) used in analog electronics and other digital filters?
This is because the windowed-sinc's frequency response is symmetrical between the passband and the stopband.
For instance, the Hamming window results in a passband ripple of 0.2%, and an identical stopband attenuation (i.e., ripple in the stopband) of 0.2%.
Other filters do not show this symmetry, and therefore have no advantage in using the one-half amplitude point to mark the cutoff frequency.
As shown later in this chapter, this symmetry makes the windowedsinc ideal for spectral inversion.
After fC and M have been selected, the filter kernel is calculated from the relation: The windowed-sinc filter kernel.
The cutoff frequency, fC, is expressed as a fraction of the sampling rate, a value between 0 and 0.5.
The length of the filter kernel is determined by M, which must be an even integer.
The sample number i, is an integer that runs from 0 to M, resulting in M%1 total points in the filter kernel.
The constant, K, is chosen to provide unity gain at zero frequency.
To avoid a divide-by-zero error, for i ' M/2, use h[i ] ' 2B fC K .
Don't be intimidated by this equation!
Based on the previous discussion, you should be able to identify three components: the sinc function, the M/2 shift, and the Blackman window.
For the filter to have unity gain at DC, the constant K must be chosen such that the sum of all the samples is equal to one.
In practice, ignore K during the calculation of the filter kernel, and then normalize all of the samples as needed.
The program listed in Table 16-1 shows how this is done.
Also notice how the calculation is handled at the center of the sinc, i ' M/2, which involves a division by zero.
This equation may be long, but it is easy to use; simply type it into your computer program and forget it.
Let the computer handle the calculations.
If you find yourself trying to evaluate this equation by hand, you are doing something very very wrong.
Let's be specific about where the filter kernel described by Eq. 16-4 is located in your computer array.
As an example, M will be chosen to be 100.
Remember, M must be an even number.
The first point in the filter kernel is in array location 0, while the last point is in array location 100.
This means that the entire signal is 101 points long.
The center of symmetry is at point 50, i.e., M/2 .
The 50 points to the left of point 50 are symmetrical with the points to the right.
Point 0 is the same value as point 100, and point 49 is the same as point 51.
If you must have a specific number of samples in the filter kernel, such as to use the FFT, simply add zeros to one end or the other.
For example, with M ' 100, you could make samples 101 through 127 equal to zero, resulting in a filter kernel 128 points long.
Example filter kernels and the corresponding step responses.
The frequency of the sinusoidal oscillation is approximately equal to the cutoff frequency, fC, while M determines the kernel length.
Figure 16-4 shows examples of windowed-sinc filter kernels, and their corresponding step responses.
The samples at the beginning and end of the filter kernels are so small that they can't even be seen in the graphs.
Don't make the mistake of thinking they are unimportant!
These samples may be small in value; however, they collectively have a large effect on the performance of the filter.
This is also why floating point representation is typically used to implement windowed-sinc filters.
Integers usually don't have enough dynamic range to capture the large variation of values contained in the filter kernel.
How does the windowed-sinc filter perform in the time domain?
Terrible!
The step response has overshoot and ringing; this is not a filter for signals with information encoded in the time domain.
Examples of Windowed-Sinc Filters An electroencephalogram, or EEG, is a measurement of the electrical activity of the brain.
It can be detected as millivolt level signals appearing on electrodes attached to the surface of the head.
Each nerve cell in the brain generates small electrical pulses.
The EEG is the combined result of an enormous number of these electrical pulses being generated in a (hopefully) coordinated manner.
Although the relationship between thought and this electrical coordination is very poorly understood, different frequencies in the EEG can be identified with specific mental states.
If you close your eyes and relax, the predominant EEG pattern will be a slow oscillation between about 7 and 12 hertz.
This waveform is called the alpha rhythm, and is associated with contentment and a decreased level of attention.
Opening your eyes and looking around causes the EEG to change to the beta rhythm, occurring between about 17 and 20 hertz.
Other frequencies and waveforms are seen in children, different depths of sleep, and various brain disorders such as epilepsy.
In this example, we will assume that the EEG signal has been amplified by analog electronics, and then digitized at a sampling rate of 100 samples per second.
Acquiring data for 50 seconds produces a signal of 5,000 points.
Our goal is to separate the alpha from the beta rhythms.
To do this, we will design a digital low-pass filter with a cutoff frequency of 14 hertz, or 0. Example of a windowed-sinc band-pass filter.
This filter was designed for a sampling rate of 10 kHz.
When referenced to the analog signal, the center frequency of the passband is at 2 kHz, the passband is 80 hertz, and the transition bands are 50 hertz.
The windowed-sinc uses 801 points in the filter kernel to achieve this roll-off, and a Blackman window for good stopband attenuation.
Figure (a) shows the resulting frequency response on a linear scale, while (b) shows it in decibels.
The frequency axis in (a) is expressed as a fraction of the sampling frequency, while (b) is expressed in terms of the analog signal before digitization.
The transition bandwidth will be set at 4 hertz, or 0.04 of the sampling rate.
From Eq. 16-3, the filter kernel needs to be about 101 points long, and we will arbitrarily choose to use a Hamming window.
The program in Table 16-1 shows how the filter is carried out.
The frequency response of the filter, obtained by taking the Fourier Transform of the filter kernel, is shown in Fig. 16-5.
In a second example, we will design a band-pass filter to isolate a signaling tone in an audio signal, such as when a button on a telephone is pressed.
We will assume that the signal has been digitized at 10 kHz, and the goal is to isolate an 80 hertz band of frequencies centered on 2 kHz.
In terms of the sampling rate, we want to block all frequencies below 0.196 and above 0. (corresponding to 1960 hertz and 2040 hertz, respectively).
To achieve a transition bandwidth of 50 hertz (0.005 of the sampling rate), we will make the filter kernel 801 points long, and use a Blackman window.
Table 16-2 contains a program for calculating the filter kernel, while Fig. 16-6 shows the frequency response.
The design involves several steps.
First, two low-pass filters are designed, one with a cutoff at 0.196, and the other with a cutoff at 0.204.
This second filter is then spectrally inverted, making it a high-pass filter (see Chapter 14, Fig. 14-6).
Next, the two filter kernels are added, resulting in a band-reject filter (see Fig. 14-8).
Finally, another spectral inversion makes this into the desired band-pass filter.
Pushing it to the Limit The windowed-sinc filter can be pushed to incredible performance levels without nasty surprises.
For instance, suppose you need to isolate a 1 millivolt signal riding on a 120 volt power line.
The low-pass filter will need a stopband attenuation of at least -120dB (one part in one-million for those that refuse to learn decibels).
As previously shown, the Blackman window only provides -74dB (one part in five-thousand).
Fortunately, greater stopband attenuation is easy to obtain.
The input signal can be filtered using a conventional windowed-sinc filter kernel, providing an intermediate signal.
The intermediate signal can then be passed through the filter a second time, further increasing the stopband attenuation to -148dB (1 part in 30 million, wow!).
It is also possible to combine the two stages into a single filter.
The kernel of the combined filter is equal to the convolution of the filter kernels of the two stages.
This also means that convolving any filter kernel with itself results in a filter kernel with a much improved stopband attenuation.
The price you pay is a longer filter kernel and a slower roll-off.
Figure 16-7a shows the frequency response of a 201 point lowpass filter, formed by convolving a 101 point Blackman windowed-sinc with itself.
Amazing performance!
Single precision The incredible performance of the windowed-sinc filter.
Figure (a) shows the frequency response of a windowed-sinc filter with increased stopband attenuation.
This is achieved by convolving a windowed-sinc filter kernel with itself.
Figure (b) shows the very rapid roll-off a 32,001 point windowed-sinc filter.
Figure 16-7b shows another example of the windowed-sinc's incredible performance: a low-pass filter with 32,001 points in the kernel.
The frequency response appears as expected, with a roll-off of 0.000125 of the sampling rate.
How good is this filter?
Try building an analog electronic filter that passes signals from DC to 1000 hertz with less than a 0.02% variation, and blocks all frequencies above 1001 hertz with less than 0.02% residue.
Now that's a filter!
If you really want to be impressed, remember that both the filters in Fig. 16-7 use single precision.
Using double precision allows these performance levels to be extended by a million times.
The strongest limitation of the windowed-sinc filter is the execution time; it can be unacceptably long if there are many points in the filter kernel and standard convolution is used.
A high-speed algorithm for this filter (FFT convolution) is presented in Chapter 18. Recursive filters (Chapter 19) also provide good frequency separation and are a reasonable alternative to the windowed-sinc filter.
Is the windowed-sinc the optimal filter kernel for separating frequencies?
No, filter kernels resulting from more sophisticated techniques can be better.
But beware!
Before you jump into this very mathematical field, you should consider exactly what you hope to gain.
The windowed-sinc will provide any level of performance that you could possibly need.
What the advanced filter design methods may provide is a slightly shorter filter kernel for a given level of performance.
This, in turn, may mean a slightly faster execution speed.
Be warned that you may get little return for the effort expended.
Most filters have one of the four standard frequency responses: low-pass, high-pass, band-pass or band-reject.
This chapter presents a general method of designing digital filters with an arbitrary frequency response, tailored to the needs of your particular application.
DSP excels in this area, solving problems that are far above the capabilities of analog electronics.
Two important uses of custom filters are discussed in this chapter: deconvolution, a way of restoring signals that have undergone an unwanted convolution, and optimal filtering, the problem of separating signals with overlapping frequency spectra.
This is DSP at its best.
Arbitrary Frequency Response The approach used to derive the windowed-sinc filter in the last chapter can also be used to design filters with virtually any frequency response.
The only difference is how the desired response is moved from the frequency domain into the time domain.
In the windowed-sinc filter, the frequency response and the filter kernel are both represented by equations, and the conversion between them is made by evaluating the mathematics of the Fourier transform.
In the method presented here, both signals are represented by arrays of numbers, with a computer program (the FFT) being used to find one from the other.
Figure 17-1 shows an example of how this works.
The frequency response we want the filter to produce is shown in (a).
To say the least, it is very irregular and would be virtually impossible to obtain with analog electronics.
This ideal frequency response is defined by an array of numbers that have been selected, not some mathematical equation.
In this example, there are 513 samples spread between 0 and 0.5 of the sampling rate.
More points could be used to better represent the desired frequency response, while a smaller number may be needed to reduce the computation time during the filter design.
However, these concerns are usually small, and 513 is a good length for most applications.
Besides the desired magnitude array shown in (a), there must be a corresponding phase array of the same length.
In this example, the phase of the desired frequency response is entirely zero (this array is not shown in Fig. 17-1).
Just as with the magnitude array, the phase array can be loaded with any arbitrary curve you would like the filter to produce.
However, remember that the first and last samples (i.e., 0 and 512) of the phase array must have a value of zero (or a multiple of 2 B, which is the same thing).
The frequency response can also be specified in rectangular form by defining the array entries for the real and imaginary parts, instead of using the magnitude and phase.
The next step is to take the Inverse DFT to move the filter into the time domain.
The quickest way to do this is to convert the frequency domain to rectangular form, and then use the Inverse FFT.
This results in a sample signal running from 0 to 1023, as shown in (b).
This is the impulse response that corresponds to the frequency response we want; however, it is not suitable for use as a filter kernel (more about this shortly).
Just as in the last chapter, it needs to be shifted, truncated, and windowed.
In this example, we will design the filter kernel with M ' 40, i.e., 41 points running from sample 0 to sample 40.
Table 17-1 shows a computer program that converts the signal in (b) into the filter kernel shown in (c).
As with the windowed-sinc filter, the points near the ends of the filter kernel are so small that they appear to be zero when plotted.
Don't make the mistake of thinking they can be deleted!
Example of FIR filter design.
Figure (a) shows the desired frequency response, with 513 samples running between 0 to 0.5 of the sampling rate.
Taking the Inverse DFT results in (b), an aliased impulse response composed of 1024 samples.
To form the filter kernel, (c), the aliased impulse response is truncated to M% samples, shifted to the right by M/2 samples, and multiplied by a Hamming or Blackman window.
In this example, M is 40.
The program in Table 17-1 shows how this is done.
The filter kernel is tested by padding it with zeros and taking the DFT, providing the actual frequency response of the filter, (d).
The last step is to test the filter kernel.
This is done by taking the DFT (using the FFT) to find the actual frequency response, as shown in (d).
To obtain better resolution in the frequency domain, pad the filter kernel with zeros before the FFT.
For instance, using 1024 total samples (41 in the filter kernel, plus 983 zeros), results in 513 samples between 0 and 0.5.
As shown in Fig. 17-2, the length of the filter kernel determines how well the actual frequency response matches the desired frequency response.
The exceptional performance of FIR digital filters is apparent; virtually any frequency response can be obtained if a long enough filter kernel is used.
This is the entire design method; however, there is a subtle theoretical issue that needs to be clarified.
Why isn't it possible to directly use the impulse response shown in 17-1b as the filter kernel?
After all, if (a) is the Fourier transform of (b), wouldn't convolving an input signal with (b) produce the exact frequency response we want?
The answer is no, and here's why.
When designing a custom filter, the desired frequency response is defined by the values in an array.
Now consider this: what does the frequency response do between the specified points?
For simplicity, two cases can be imagined, one "good" and one "bad."
In the "good" case, the frequency response is a smooth curve between the defined samples.
In the "bad" case, there are wild fluctuations between.
As luck would have it, the impulse response in (b) corresponds to the "bad" frequency response.
This can be shown by padding it with a large number of zeros, and then taking the DFT.
The frequency response obtained by this method will show the erratic behavior between the originally defined samples, and look just awful.
To understand this, imagine that we force the frequency response to be what we want by defining it at an infinite number of points between 0 and 0.5.
That is, we create a continuous curve.
The inverse DTFT is then used to find the impulse response, which will be infinite in length.
In other words, the "good" frequency response corresponds to something that cannot be represented in a computer, an infinitely long impulse response.
When we represent the frequency spectrum with N/2 % 1 samples, only N points are provided in the time domain, making it unable to correctly contain the signal.
The result is that the infinitely long impulse response wraps up (aliases) into the N points.
When this aliasing occurs, the frequency response changes from "good" to "bad."
Fortunately, windowing the N point impulse response greatly reduces this aliasing, providing a smooth curve between the frequency domain samples.
Designing a digital filter to produce a given frequency response is quite simple.
The hard part is finding what frequency response to use.
Let's look at some strategies used in DSP to design custom filters.
Deconvolution Unwanted convolution is an inherent problem in transferring analog information.
For instance, all of the following can be modeled as a convolution: image blurring in a shaky camera, echoes in long distance telephone calls, the finite bandwidth of analog sensors and electronics, etc. Deconvolution is the process of filtering a signal to compensate for an undesired convolution.
The goal of deconvolution is to recreate the signal as it existed before the convolution took place.
This usually requires the characteristics of the convolution (i.e., the impulse or frequency response) to be known.
This can be distinguished from blind deconvolution, where the characteristics of the parasitic convolution are not known.
Blind deconvolution is a much more difficult problem that has no general solution, and the approach must be tailored to the particular application.
Deconvolution is nearly impossible to understand in the time domain, but quite straightforward in the frequency domain.
Each sinusoid that composes the original signal can be changed in amplitude and/or phase as it passes through the undesired convolution.
To extract the original signal, the deconvolution filter must undo these amplitude and phase changes.
For Amplitude Amplitude Frequency Frequency Amplitude Frequency Frequency FIGURE 17- Frequency response vs. filter kernel length.
These figures show the frequency responses obtained with various lengths of filter kernels.
The number of points in each filter kernel is equal to M% 1, running from 0 to M. As more points are used in the filter kernel, the resulting frequency response more closely matches the desired frequency response.
Figure 17-1a shows the desired frequency response for this example.
Amplitude Amplitude Frequency example, if the convolution changes a sinusoid's amplitude by 0.5 with a degree phase shift, the deconvolution filter must amplify the sinusoid by 2. with a -30 degree phase change.
The example we will use to illustrate deconvolution is a gamma ray detector.
As illustrated in Fig. 17-3, this device is composed of two parts, a scintillator and a light detector.
A scintillator is a special type of transparent material, such as sodium iodide or bismuth germanate.
These compounds change the energy in each gamma ray into a brief burst of visible light.
This light Example of an unavoidable convolution.
A gamma ray detector can be formed by mounting a scintillator on a light detector.
When a gamma ray strikes the scintillator, its energy is converted into a pulse of light.
This pulse of light is then converted into an electronic signal by the light detector.
The gamma ray is an impulse, while the output of the detector (i.e., the impulse response) resembles a one-sided exponential.
Each pulse produced by the detector resembles a one-sided exponential, with some rounding of the corners.
This shape is determined by the characteristics of the scintillator used.
When a gamma ray deposits its energy into the scintillator, nearby atoms are excited to a higher energy level.
These atoms randomly deexcite, each producing a single photon of visible light.
The net result is a light pulse whose amplitude decays over a few hundred nanoseconds (for sodium iodide).
Since the arrival of each gamma ray is an impulse, the output pulse from the detector (i.e., the one-sided exponential) is the impulse response of the system.
Figure 17-4a shows pulses generated by the detector in response to randomly arriving gamma rays.
The information we would like to extract from this output signal is the amplitude of each pulse, which is proportional to the energy of the gamma ray that generated it.
This is useful information because the energy can tell interesting things about where the gamma ray has been.
For example, it may provide medical information on a patient, tell the age of a distant galaxy, detect a bomb in airline luggage, etc. Everything would be fine if only an occasional gamma ray were detected, but this is usually not the case.
As shown in (a), two or more pulses may overlap, shifting the measured amplitude.
One answer to this problem is to deconvolve the detector's output signal, making the pulses narrower so that less pile-up occurs.
Ideally, we would like each pulse to resemble the original impulse.
As you may suspect, this isn't possible and we must settle for a pulse that is finite in length, but significantly shorter than the detected pulse.
This goal is illustrated in Fig. 17-4b.
Amplitude Amplitude Sample number Sample number FIGURE 17- Example of deconvolution. Figure (a) shows the output signal from a gamma ray detector in response to a series of randomly arriving gamma rays.
The deconvolution filter is designed to convert (a) into (b), by reducing the width of the pulses.
This minimizes the amplitude shift when pulses land on top of each other.
Even though the detector signal has its information encoded in the time domain, much of our analysis must be done in the frequency domain, where the problem is easier to understand. Figure 17-5a is the signal produced by the detector (something we know).
Figure (c) is the signal we wish to have (also something we know).
This desired pulse was arbitrarily selected to be the same shape as a Blackman window, with a length about one-third that of the original pulse.
Our goal is to find a filter kernel, (e), that when convolved with the signal in (a), produces the signal in (c).
In equation form: if a t e ' c, and given a and c, find e.
If these signals were combined by addition or multiplication instead of convolution, the solution would be easy: subtraction is used to "de-add" and division is used to "de-multiply."
Convolution is different; there is not a simple inverse operation that can be called "deconvolution."
Convolution is too messy to be undone by directly manipulating the time domain signals.
Fortunately, this problem is simpler in the frequency domain.
Remember, convolution in one domain corresponds with multiplication in the other domain.
Again referring to the signals in Fig. 17-5: if b ×f ' d, and given b and d, find f.
This is an easy problem to solve: the frequency response of the filter, (f), is the frequency spectrum of the desired pulse, (d), divided by the frequency spectrum of the detected pulse, (b).
Since the detected pulse is asymmetrical, it will have a nonzero phase.
This means that a complex division must be used (that is, a magnitude & phase divided by another magnitude & phase).
In case you have forgotten, Chapter 9 defines how to perform a complex division of one spectrum by another.
The required filter kernel, (e), is then found from the frequency response by the custom filter method (IDFT, shift, truncate, & multiply by a window).
There are limits to the improvement that deconvolution can provide.
In other words, if you get greedy, things will fall apart.
Getting greedy in this example means trying to make the desired pulse excessively narrow.
Let's look at what happens.
If the desired pulse is made narrower, its frequency spectrum must contain more high frequency components.
Since these high frequency components are at a very low amplitude in the detected pulse, the filter must have a very high gain at these frequencies.
For instance, (f) shows that some frequencies must be multiplied by a factor of three to achieve the desired pulse in (c).
If the desired pulse is made narrower, the gain of the deconvolution filter will be even greater at high frequencies.
The problem is, small errors are very unforgiving in this situation.
For instance, if some frequency is amplified by 30, when only 28 is required, the deconvolved signal will probably be a mess.
When the deconvolution is pushed to greater levels of performance, the characteristics of the unwanted convolution must be understood with greater accuracy and precision.
There are always unknowns in real world applications, caused by such villains as: electronic noise, temperature drift, variation between devices, etc.
These unknowns set a limit on how well deconvolution will work.
Even if the unwanted convolution is perfectly understood, there is still a factor that limits the performance of deconvolution: noise.
For instance, most unwanted convolutions take the form of a low-pass filter, reducing the amplitude of the high frequency components in the signal.
Deconvolution corrects this by amplifying these frequencies.
However, if the amplitude of these components falls below the inherent noise of the system, the information contained in these frequencies is lost.
No amount of signal processing can retrieve it.
It's gone forever.
Adios!
Goodbye!
Sayonara!
Trying to reclaim this data will only amplify the noise.
As an extreme case, the amplitude of some frequencies may be completely reduced to zero.
This not only obliterates the information, it will try to make the deconvolution filter have infinite gain at these frequencies.
The solution: design a less aggressive deconvolution filter and/or place limits on how much gain is allowed at any of the frequencies.
How far can you go?
How greedy is too greedy?
This depends totally on the problem you are attacking.
If the signal is well behaved and has low noise, a significant improvement can probably be made (think a factor of 5-10).
If the signal changes over time, isn't especially well understood, or is noisy, you won't do nearly as well (think a factor of 1-2).
Successful deconvolution involves a great deal of testing.
If it works at some level, try going farther; you will know when it falls apart.
No amount of theoretical work will allow you to bypass this iterative process.
Deconvolution can also be applied to frequency domain encoded signals.
A classic example is the restoration of old recordings of the famous opera singer, Enrico Caruso (1873-1921).
These recordings were made with very primitive equipment by modern standards.
The most significant problem is the resonances of the long tubular recording horn used to gather the sound.
Whenever the singer happens to hit one of these resonance frequencies, the loudness of the recording abruptly increases.
Digital deconvolution has improved the subjective quality of these recordings by Example of deconvolution in the time and frequency domains.
The impulse response of the example gamma ray detector is shown in (a), while the desired impulse response is shown in (c).
The frequency spectra of these two signals are shown in (b) and (d), respectively.
The filter that changes (a) into (c) has a frequency response, (f), equal to (d) divided by (b).
The filter kernel of this filter, (e), is then found from the frequency response using the custom filter design method (inverse DFT, truncation, windowing).
Only the magnitudes of the frequency domain signals are shown in this illustration; however, the phases are nonzero and must also be used.
We will only describe the general method; for a detailed description, see the original paper: T. Stockham, T. Cannon, and R. Ingebretsen, "Blind Deconvolution Through Digital Signal Processing", Proc.
Deconvolution of old phonograph recordings.
The frequency spectrum produced by the original singer is illustrated in (a).
Resonance peaks in the primitive equipment, (b), produce distortion in the recorded frequency spectrum, (c).
The frequency response of the deconvolution filter, (d), is designed to counteracts the undesired convolution, restoring the original spectrum, (e).
These graphs are for illustrative purposes only; they are not actual signals.
Figure 17-6 shows the general approach.
The frequency spectrum of the original audio signal is illustrated in (a). Figure (b) shows the frequency response of the recording equipment, a relatively smooth curve except for several sharp resonance peaks.
The spectrum of the recorded signal, shown in (c), is equal to the true spectrum, (a), multiplied by the uneven frequency response, (b).
The goal of the deconvolution is to counteract the undesired convolution.
In other words, the frequency response of the deconvolution filter, (d), must be the inverse of (b).
That is, each peak in (b) is cancelled by a corresponding dip in (d).
If this filter were perfectly designed, the resulting signal would have a spectrum, (e), identical to that of the original.
Here's the catch: the original recording equipment has long been discarded, and its frequency response, (b), is a mystery.
In other words, this is a blind deconvolution problem; given only (c), how can we determine (d)?
Blind deconvolution problems are usually attacked by making an estimate or assumption about the unknown parameters.
To deal with this example, the average spectrum of the original music is assumed to match the average spectrum of the same music performed by a present day singer using modern equipment.
The average spectrum is found by the techniques of Chapter 9: break the signal into a large number of segments, take the DFT of each segment, convert into polar form, and then average the magnitudes together.
In the simplest case, the unknown frequency response is taken as the average spectrum of the old recording, divided by the average spectrum of the modern recording.
Figure 17-7a illustrates a common filtering problem: trying to extract a waveform (in this example, an exponential pulse) buried in random noise.
As shown in (b), this problem is no easier in the frequency domain.
The signal has a spectrum composed mainly of low frequency components.
In comparison, the spectrum of the noise is white (the same amplitude at all frequencies).
Since the spectra of the signal and noise overlap, it is not clear how the two can best be separated.
In fact, the real question is how to define what "best" means.
We will look at three filters, each of which is "best" (optimal) in a different way. Figure 17-8 shows the filter kernel and frequency response for each of these filters.
Figure 17-9 shows the result of using these filters on the example waveform of Fig. 17-7a.
The moving average filter is the topic of Chapter 15.
As you recall, each output point produced by the moving average filter is the average of a certain number of points from the input signal.
This makes the filter kernel a rectangular pulse with an amplitude equal to the reciprocal of the number of points in the average.
The moving average filter is optimal in the sense that it provides the fastest step response for a given noise reduction.
Example of optimal filters.
In (a), three filter kernels are shown, each of which is optimal in some sense.
The corresponding frequency responses are shown in (b).
The moving average filter is designed to have a rectangular pulse for a filter kernel.
In comparison, the filter kernel of the matched filter looks like the signal being detected.
The Wiener filter is designed in the frequency domain, based on the relative amounts of signal and noise present at each frequency.
The idea behind the matched filter is correlation, and this flip is required to perform correlation using convolution.
The amplitude of each point in the output signal is a measure of how well the filter kernel matches the corresponding section of the input signal.
Recall that the output of a matched filter does not necessarily look like the signal being detected.
This doesn't really matter; if a matched filter is used, the shape of the target signal must already be known.
The matched filter is optimal in the sense that the top of the peak is farther above the noise than can be achieved with any other linear filter (see Fig. 17-9b).
The Wiener filter (named after the optimal estimation theory of Norbert Wiener) separates signals based on their frequency spectra.
As shown in Fig. 17-7b, at some frequencies there is mostly signal, while at others there is mostly noise.
It seems logical that the "mostly signal" frequencies should be passed through the filter, while the "mostly noise" frequencies should be blocked.
The Wiener filter takes this idea a step further; the gain of the filter at each frequency is determined by the relative amount of signal and noise at that frequency: EQUATION 17- The Wiener filter.
The frequency response, represented by H [ f ], is determined by the frequency spectra of the noise, N [ f ], and the signal, S [ f ] .
Only the magnitudes are important; all of the phases are zero.
This relation is used to convert the spectra in Fig. 17-7b into the Wiener filter's frequency response in Fig. 17-8b.
The Wiener filter is optimal in the sense that it maximizes the ratio of the signal power to the noise power Example of using three optimal filters.
These signals result from filtering the waveform in Fig. 17-7 with the filters in Fig. 17-8.
Each of these three filters is optimal in some sense.
In (a), the moving average filter results in the sharpest edge response for a given level of random noise reduction.
In (b), the matched filter produces a peak that is farther above the residue noise than provided by any other filter.
In (c), the Wiener filter optimizes the signal-to-noise ratio.
Sample number (over the length of the signal, not at each individual point).
An appropriate filter kernel is designed from the Wiener frequency response using the custom method.
While the ideas behind these optimal filters are mathematically elegant, they often fail in practicality.
This isn't to say they should never be used.
The point is, don't hear the word "optimal" and stop thinking.
Let's look at several reasons why you might not want to use them.
First, the difference between the signals in Fig. 17-9 is very unimpressive.
In fact, if you weren't told what parameters were being optimized, you probably couldn't tell by looking at the signals.
This is usually the case for problems involving overlapping frequency spectra.
The small amount of extra performance obtained from an optimal filter may not be worth the the increased program complexity, the extra design effort, or the longer execution time.
Second: The Wiener and matched filters are completely determined by the characteristics of the problem.
Other filters, such as the windowed-sinc and moving average, can be tailored to your liking.
Optimal filter advocates would claim that this diddling can only reduce the effectiveness of the filter.
This is very arguable.
Remember, each of these filters is optimal in one specific way (i.e., "in some sense").
This is seldom sufficient to claim that the entire problem has been optimized, especially if the resulting signals are interpreted by a human observer.
For instance, a biomedical engineer might use a Wiener filter to maximize the signal-to-noise ratio in an electro-cardiogram.
However, it is not obvious that this also optimizes a physician's ability to detect irregular heart activity by looking at the signal.
Third: The Wiener and matched filter must be carried out by convolution, making them extremely slow to execute.
Even with the speed improvements discussed in the next chapter (FFT convolution), the computation time can be excessively long.
In comparison, recursive filters (such as the moving average or others presented in Chapter 19) are much faster, and may provide an acceptable level of performance.
FFT Convolution This chapter presents two important DSP techniques, the overlap-add method, and FFT convolution.
The overlap-add method is used to break long signals into smaller segments for easier processing.
FFT convolution uses the overlap-add method together with the Fast Fourier Transform, allowing signals to be convolved by multiplying their frequency spectra.
For filter kernels longer than about 64 points, FFT convolution is faster than standard convolution, while producing exactly the same result.
The Overlap-Add Method There are many DSP applications where a long signal must be filtered in segments.
For instance, high fidelity digital audio requires a data rate of about 5 Mbytes/min, while digital video requires about 500 Mbytes/min.
With data rates this high, it is common for computers to have insufficient memory to simultaneously hold the entire signal to be processed.
There are also systems that process segment-by-segment because they operate in real time.
For example, telephone signals cannot be delayed by more than a few hundred milliseconds, limiting the amount of data that are available for processing at any one instant.
In still other applications, the processing may require that the signal be segmented.
An example is FFT convolution, the main topic of this chapter.
The overlap-add method is based on the fundamental technique in DSP: (1) decompose the signal into simple components, (2) process each of the components in some useful way, and (3) recombine the processed components into the final signal.
Figure 18-1 shows an example of how this is done for the overlap-add method.
Figure (a) is the signal to be filtered, while (b) shows the filter kernel to be used, a windowed-sinc low-pass filter.
Jumping to the bottom of the figure, (i) shows the filtered signal, a smoothed version of (a).
The key to this method is how the lengths of these signals are affected by the convolution.
When an N sample signal is convolved with an M sample filter kernel, the output signal is N% M& 1 samples long.
For instance, the input signal, (a), is 300 samples (running from 0 to 299), the filter kernel, (b), is samples (running from 0 to 100), and the output signal, (i), is 400 samples (running from 0 to 399).
In other words, when an N sample signal is filtered, it will be expanded by M& 1 points to the right.
In (a), zeros have been added to the signal between sample 300 and 399 to illustrate where this expansion will occur.
Don't be confused by the small values at the ends of the output signal, (i).
This is simply a result of the windowed-sinc filter kernel having small values near its ends.
All 400 samples in (i) are nonzero, even though some of them are too small to be seen in the graph.
Figures (c), (d) and (e) show the decomposition used in the overlap-add method.
The signal is broken into segments, with each segment having samples from the original signal.
In addition, 100 zeros are added to the right of each segment.
In the next step, each segment is individually filtered by convolving it with the filter kernel.
This produces the output segments shown in (f), (g), and (h).
Since each input segment is 100 samples long, and the filter kernel is 101 samples long, each output segment will be 200 samples long.
The important point to understand is that the 100 zeros were added to each input segment to allow for the expansion during the convolution.
Notice that the expansion results in the output segments overlapping each other.
These overlapping output segments are added to give the output signal, (i).
For instance, samples 200 to 299 in (i) are found by adding the corresponding samples in (g) and (h).
The overlap-add method produces exactly the same output signal as direct convolution.
The disadvantage is a much greater program complexity to keep track of the overlapping samples.
FFT Convolution FFT convolution uses the principle that multiplication in the frequency domain corresponds to convolution in the time domain.
The input signal is transformed into the frequency domain using the DFT, multiplied by the frequency response of the filter, and then transformed back into the time domain using the Inverse DFT.
This basic technique was known since the days of Fourier; however, no one really cared.
This is because the time required to calculate the DFT was longer than the time to directly calculate the convolution.
This changed in 1965 with the development of the Fast Fourier Transform (FFT).
By using the FFT algorithm to calculate the DFT, convolution via the frequency domain can be faster than directly convolving the time domain signals.
The final result is the same; only the number of calculations has been changed by a more efficient algorithm.
For this reason, FFT convolution is also called high-speed convolution.
The overlap-add method.
The goal is to convolve the input signal, (a), with the filter kernel, (b).
This is done by breaking the input signal into a number of segments, such as (c), (d) and (e), each padded with enough zeros to allow for the expansion during the convolution.
Convolving each of the input segments with the filter kernel produces the output segments, (f), (g), and (h).
The output signal, (i), is then found by adding the overlapping output segments.
FFT convolution uses the overlap-add method shown in Fig. 18-1; only the way that the input segments are converted into the output segments is changed.
Figure 18-2 shows an example of how an input segment is converted into an output segment by FFT convolution.
To start, the frequency response of the filter is found by taking the DFT of the filter kernel, using the FFT.
For instance, (a) shows an example filter kernel, a windowed-sinc band-pass filter.
The FFT converts this into the real and imaginary parts of the frequency response, shown in (b) & (c).
These frequency domain signals may not look like a band-pass filter because they are in rectangular form.
Remember, polar form is usually best for humans to understand the frequency domain, while rectangular form is normally best for mathematical calculations.
These real and imaginary parts are stored in the computer for use when each segment is being calculated.
Figure (d) shows the input segment to being processed.
The FFT is used to find its frequency spectrum, shown in (e) & (f).
The frequency spectrum of the output segment, (h) & (i) is then found by multiplying the filter's frequency response, (b) & (c), by the spectrum of the input segment, (e) & (f).
Since these spectra consist of real and imaginary parts, they are multiplied according to Eq. 9-1 in Chapter 9.
The Inverse FFT is then used to find the output segment, (g), from its frequency spectrum, (h) & (i).
It is important to recognize that this output segment is exactly the same as would be obtained by the direct convolution of the input segment, (d), and the filter kernel, (a).
The FFTs must be long enough that circular convolution does not take place (also described in Chapter 9).
This means that the FFT should be the same length as the output segment, (g).
For instance, in the example of Fig. 18-2, the filter kernel contains 129 points and each segment contains 128 points, making output segment 256 points long.
This calls for 256 point FFTs to be used.
This means that the filter kernel, (a), must be padded with 127 zeros to bring it to a total length of 256 points.
Likewise, each of the input segments, (d), must be padded with 128 zeros.
As another example, imagine you need to convolve a very long signal with a filter kernel having 600 samples.
One alternative would be to use segments of 425 points, and 1024 point FFTs.
Another alternative would be to use segments of 1449 points, and 2048 point FFTs.
Table 18-1 shows an example program to carry out FFT convolution.
This program filters a 10 million point signal by convolving it with a 400 point filter kernel.
This is done by breaking the input signal into 16000 segments, with each segment having 625 points.
When each of these segments is convolved with the filter kernel, an output segment of 625 % 400 & 1 ' 1024 points is produced.
Thus, 1024 point FFTs are used.
After defining and initializing all the arrays (lines 130 to 230), the first step is to calculate and store the frequency response of the filter (lines 250 to 310).
Line 260 calls a mythical subroutine that loads the filter kernel into XX[0] through XX[399], and sets XX[400] through XX[1023] to a value of zero.
The subroutine in line 270 is the FFT, transforming the 1024 samples held in XX[ ] into the 513 samples held in REX[ ] & IMX[ ], the real and FFT convolution.
The filter kernel, (a), and the signal segment, (d), are converted into their respective spectra, (b) & (c) and (e) & (f), via the FFT.
These spectra are multiplied, resulting in the spectrum of the output segment, (h) & (i).
The Inverse FFT then finds the output segment, (g).
These values are transferred into the arrays REFR[ ] & IMFR[ ] (for: REal and IMaginary Frequency Response), to be used later in the program.
The FOR-NEXT loop between lines 340 and 580 controls how the segments are processed.
In line 360, a subroutine loads the next segment to be processed into XX[0] through XX[624], and sets XX[625] through XX[1023] to a value of zero.
In line 370, the FFT subroutine is used to find this segment's frequency spectrum, with the real part being placed in the 513 points of REX[ ], and the imaginary part being placed in the 513 points of IMX[ ].
Lines 390 to 430 show the multiplication of the segment's frequency spectrum, held in REX[ ] & IMX[ ], by the filter's frequency response, held in REFR[ ] and IMFR[ ].
The result of the multiplication is stored in REX[ ] & IMX[ ], overwriting the data previously there.
Since this is now the frequency spectrum of the output segment, the IFFT can be used to find the output segment.
This is done by the mythical IFFT subroutine in line 450, which transforms the points held in REX[ ] & IMX[ ] into the 1024 points held in XX[ ], the output segment.
Lines 470 to 550 handle the overlapping of the segments.
Each output segment is divided into two sections.
The first 625 points (0 to 624) need to be combined with the overlap from the previous output segment, and then written to the output signal.
The last 399 points (625 to 1023) need to be saved so that they can overlap with the next output segment.
To understand this, look back at Fig 18-1.
Samples 100 to 199 in (g) need to be combined with the overlap from the previous output segment, (f), and can then be moved to the output signal (i).
In comparison, samples 200 to 299 in (g) need to be saved so that they can be combined with the next output segment, (h).
Now back to the program.
The array OLAP[ ] is used to hold the 399 samples that overlap from one segment to the next.
In lines 470 to 490 the 399 values in this array (from the previous output segment) are added to the output segment currently being worked on, held in XX[ ].
The mythical subroutine in line 550 then outputs the 625 samples in XX[0] to XX[624] to the file holding the output signal.
The 399 samples of the current output segment that need to be held over to the next output segment are then stored in OLAP[ ] in lines to 530.
After all 0 to 15999 segments have been processed, the array, OLAP[ ], will contain the 399 samples from segment 15999 that should overlap segment 16000.
Since segment 16000 doesn't exist (or can be viewed as containing all zeros), the 399 samples are written to the output signal in line 600.
This makes the length of the output signal 16000 ×625 % 399 ' 10,000,399 points.
This matches the length of input signal, plus the length of the filter kernel, minus 1. Speed Improvements When is FFT convolution faster than standard convolution?
The answer depends on the length of the filter kernel, as shown in Fig. 18-3.
The time Execution times for FFT convolution.
FFT convolution is faster than the standard method when the filter kernel is longer than about 60 points.
These execution times are for a 100 MHz Pentium, using single precision floating point.
Execution Time (msec/point) Standard Impulse Response Length filter kernel.
The crossover occurs when the filter kernel has about 40 to samples (depending on the particular hardware used).
The important idea to remember: filter kernels shorter than about 60 points can be implemented faster with standard convolution, and the execution time is proportional to the kernel length.
Longer filter kernels can be implemented faster with FFT convolution.
With FFT convolution, the filter kernel can be made as long as you like, with very little penalty in execution time.
For instance, a 16,000 point filter kernel only requires about twice as long to execute as one with only 64 points.
The speed of the convolution also dictates the precision of the calculation (just as described for the FFT in Chapter 12).
This is because the round-off error in the output signal depends on the total number of calculations, which is directly proportional to the computation time.
If the output signal is calculated faster, it will also be calculated more precisely.
For instance, imagine convolving a signal with a 1000 point filter kernel, with single precision floating point.
Using standard convolution, the typical round-off noise can be expected to be about 1 part in 20,000 (from the guidelines in Chapter 4).
In comparison, FFT convolution can be expected to be an order of magnitude faster, and an order of magnitude more precise (i.e., 1 part in 200,000).
Keep FFT convolution tucked away for when you have a large amount of data to process and need an extremely long filter kernel.
Think in terms of a million sample signal and a thousand point filter kernel.
Anything less won't justify the extra programming effort.
Don't want to write your own FFT convolution routine?
Look in software libraries and packages for prewritten code.
Start with this book's web site (see the copyright page).
Recursive Filters Recursive filters are an efficient way of achieving a long impulse response, without having to perform a long convolution.
They execute very rapidly, but have less performance and flexibility than other digital filters.
Recursive filters are also called Infinite Impulse Response (IIR) filters, since their impulse responses are composed of decaying exponentials.
This distinguishes them from digital filters carried out by convolution, called Finite Impulse Response (FIR) filters.
This chapter is an introduction to how recursive filters operate, and how simple members of the family can be designed.
Chapters 20, 26 and 33 present more sophisticated design methods.
The Recursive Method To start the discussion of recursive filters, imagine that you need to extract information from some signal, x[ ] .
Your need is so great that you hire an old mathematics professor to process the data for you.
The professor's task is to filter x[ ] to produce y[ ], which hopefully contains the information you are interested in.
The professor begins his work of calculating each point in y[ ] according to some algorithm that is locked tightly in his over-developed brain.
Part way through the task, a most unfortunate event occurs.
The professor begins to babble about analytic singularities and fractional transforms, and other demons from a mathematician's nightmare.
It is clear that the professor has lost his mind.
You watch with anxiety as the professor, and your algorithm, are taken away by several men in white coats.
You frantically review the professor's notes to find the algorithm he was using.
You find that he had completed the calculation of points y[0] through y[27], and was about to start on point y[28] .
As shown in Fig. 19-1, we will let the variable, n, represent the point that is currently being calculated.
This means that y[n] is sample 28 in the output signal, y[n& 1] is sample 27, y[n& 2] is sample 26, etc.
Likewise, x[n] is point 28 in the input signal, x[n& 1] is point 27, etc.
To understand the algorithm being used, we ask ourselves: "What information was available to the professor to calculate y[n], the sample currently being worked on?"
The most obvious source of information is the input signal, that is, the values: x[n], x[n& 1], x[n& 2], þ.
The professor could have been multiplying each point in the input signal by a coefficient, and adding the products together: y [n ] ' a0 x [n ] % a1 x [n & 1] % a2 x [n & 2] % a3 x [n & 3] % You should recognize that this is nothing more than simple convolution, with the coefficients: a0, a1, a2, þ, forming the convolution kernel.
If this was all the professor was doing, there wouldn't be much need for this story, or this chapter.
However, there is another source of information that the professor had access to: the previously calculated values of the output signal, held in: The recursion equation.
In this equation, x[ ] is the input signal, y[ ] is the output signal, and the a's and b's are coefficients.
In words, each point in the output signal is found by multiplying the values from the input signal by the "a" coefficients, multiplying the previously calculated values from the output signal by the "b" coefficients, and adding the products together.
Notice that there isn't a value for b0, because this corresponds to the sample being calculated.
Equation 19-1 is called the recursion equation, and filters that use it are called recursive filters.
The "a" and "b" values that define the filter are called the recursion coefficients.
In actual practice, no more than about a dozen recursion coefficients can be used or the filter becomes unstable (i.e., the output continually increases or oscillates).
Table 19-1 shows an example recursive filter program.
Recursive filters are useful because they bypass a longer convolution.
For instance, consider what happens when a delta function is passed through a recursive filter.
The output is the filter's impulse response, and will typically be a sinusoidal oscillation that exponentially decays.
Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters.
In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved.
Recursive filter notation.
The output sample being calculated, y[n], is determined by the values from the input signal, x[n], x[n& 1], x[n& 2], þ, as well as the previously calculated values in the output signal, y[n& 1], y[n& 2], y[n& 3], þ.
These figures are shown for n ' 28 .
The relationship between the recursion coefficients and the filter's response is given by a mathematical technique called the z-transform, the topic of Chapter 33.
For example, the z-transform can be used for such tasks as: converting between the recursion coefficients and the frequency response, combining cascaded and parallel stages into a single filter, designing recursive systems that mimic analog filters, etc.
Unfortunately, the z-transform is very mathematical, and more complicated than most DSP users are willing to deal with.
This is the realm of those that specialize in DSP.
There are three ways to find the recursion coefficients without having to understand the z-transform.
First, this chapter provides design equations for several types of simple recursive filters.
Second, Chapter 20 provides a "cookbook" computer program for designing the more sophisticated Chebyshev low-pass and high-pass filters.
Third, Chapter 26 describes an iterative method for designing recursive filters with an arbitrary frequency response.
Single pole low-pass filter.
Digital recursive filters can mimic analog filters composed of resistors and capacitors.
As shown in this example, a single pole low-pass recursive filter smoothes the edge of a step input, just as an electronic RC filter.
Single Pole Recursive Filters Figure 19-2 shows an example of what is called a single pole low-pass filter.
This recursive filter uses just two coefficients, a0 ' 0.15 and b1 ' 0.85 .
For this example, the input signal is a step function.
As you should expect for a low-pass filter, the output is a smooth rise to the steady state level.
This figure also shows something that ties into your knowledge of electronics.
This lowpass recursive filter is completely analogous to an electronic low-pass filter composed of a single resistor and capacitor.
The beauty of the recursive method is in its ability to create a wide variety of responses by changing only a few parameters.
For example, Fig. 19-3 shows a filter with three coefficients: a0 ' 0.93, a1 ' & 0.93 and b1 ' 0.86 .
As shown by the similar step responses, this digital filter mimics an electronic RC high-pass filter.
These single pole recursive filters are definitely something you want to keep in your DSP toolbox.
You can use them to process digital signals just as you would use RC networks to process analog electronic signals.
This includes everything you would expect: DC removal, high-frequency noise suppression, wave shaping, smoothing, etc.
They are easy to program, fast Digital Filter Recursive Filter Single pole high-pass filter.
Proper coefficient selection can also make the recursive filter mimic an electronic RC high-pass filter.
These single pole recursive filters can be used in DSP just as you would use RC circuits in analog electronics.
The coefficients are found from these simple equations: EQUATION 19- Single pole low-pass filter.
The filter's response is controlled by the parameter, x, a value between zero and one.
Physically, x is the amount of decay between adjacent samples.
For instance, x is 0.86 in Fig. 19-3, meaning that the value of each sample in the output signal is 0.86 the value of the sample before it.
The higher the value of x, the slower the decay.
Notice that the Sample number Sample number FIGURE 19- Example of single pole recursive filters.
In (a), a high frequency burst rides on a slowly varying signal.
In (b), single pole low-pass and high-pass filters are used to separate the two components.
The low-pass filter uses x = 0.95, while the high-pass filter is for x = 0.86.
That is, any nonzero value on the input will make the output increase until an overflow occurs.
The value for x can be directly specified, or found from the desired time constant of the filter.
Just as R×C is the number of seconds it takes an RC circuit to decay to 36.8% of its final value, d is the number of samples it takes for a recursive filter to decay to this same level: EQUATION 19- Time constant of single pole filters.
This equation relates the amount of decay between samples, x, with the filter's time constant, d, the number of samples for the filter to decay to 36.8%.
There is also a fixed relationship between x and the -3dB cutoff frequency, fC, of the digital filter: EQUATION 19- Cutoff frequency of single pole filters.
The amount of decay between samples, x, is related to the cutoff frequency of the filter, fC, a value between 0 and 0.5.
In (a), the original signal is a smooth curve, except a burst of a high frequency sine wave.
Figure (b) shows the signal after passing through low-pass and high-pass filters.
The signals have been separated fairly well, but not perfectly, just as if simple RC circuits were used on an analog signal.
Frequency Frequency FIGURE 19- Single pole frequency responses.
Figures (a) and (b) show the frequency responses of highpass and low-pass single pole recursive filters, respectively.
Figure (c) shows the frequency response of a cascade of four low-pass filters.
The frequency response of recursive filters is not always what you expect, especially if the filter is pushed to extreme limits.
For example, the fC ' 0.25 curve in (c) is quite useless.
Many factors are to blame, including: aliasing, roundoff noise, and the nonlinear phase response.
Low-pass filter (4 stage) Amplitude Amplitude fc = 0. Frequency Figure 19-5 shows the frequency responses of various single pole recursive filters.
These curves are obtained by passing a delta function through the filter to find the filter's impulse response.
The FFT is then used to convert the impulse response into the frequency response.
In principle, the impulse response is infinitely long; however, it decays below the single precision roundoff noise after about 15 to 20 time constants.
For example, when the time constant of the filter is d ' 6.63 samples, the impulse response can be contained in about 128 samples.
The key feature in Fig. 19-5 is that single pole recursive filters have little ability to separate one band of frequencies from another.
In other words, they perform well in the time domain, and poorly in the frequency domain.
The frequency response can be improved slightly by cascading several stages.
This can be accomplished in two ways.
First, the signal can be passed through the filter several times.
Second, the z-transform can be used to find the recursion coefficients that combine the cascade into a single stage.
Both ways work and are commonly used.
Figure (c) shows the frequency response of a cascade of four low-pass filters.
Although the stopband attenuation is significantly improved, the roll-off is still terrible.
If you need better performance in the frequency domain, look at the Chebyshev filters of the next chapter.
The four stage low-pass filter is comparable to the Blackman and Gaussian filters (relatives of the moving average, Chapter 15), but with a much faster execution speed.
The design equations for a four stage low-pass filter are: EQUATION 19- Four stage low-pass filter.
These equations provide the "a" and "b" coefficients for a cascade of four single pole low-pass filters.
The relationship between x and the cutoff frequency of this filter is given by Eq. 19-5, with the 2B replaced by 14.445.
A common need in electronics and DSP is to isolate a narrow band of frequencies from a wider bandwidth signal.
For example, you may want to eliminate 60 hertz interference in an instrumentation system, or isolate the signaling tones in a telephone network.
Two types of frequency responses are available: the band-pass and the band-reject (also called a notch filter).
Figure 19-6 shows the frequency response of these filters, with the recursion coefficients provided by the following equations: EQUATION 19- Band-pass filter.
An example frequency response is shown in Fig. 19-6a.
To use these equations, first select the center frequency, f, and the bandwidth, BW.
Both of these are expressed as a fraction of the sampling rate, and therefore in the range of 0 to 0.5.
Next, calculate R, and then K, and then the recursion coefficients.
EQUATION 19- Band-reject filter.
This filter is commonly called a notch filter.
Example frequency responses are shown in Fig. 19-6b.
Figure (a) and (b) shows the frequency responses of various band-pass and band-reject filters.
The step response of the band-reject filter is shown in (c).
The band-reject (notch) filter is useful for removing 60 Hz and similar interference from time domain encoded waveforms.
Amplitude Amplitude BW=0.
BW=0.
Sample number Two parameters must be selected before using these equations: f, the center frequency, and BW, the bandwidth (measured at an amplitude of 0.707).
Both of these are expressed as a fraction of the sampling frequency, and therefore must be between 0 and 0.5.
From these two specified values, calculate the intermediate variables: R and K, and then the recursion coefficients.
As shown in (a), the band-pass filter has relatively large tails extending from the main peak.
This can be improved by cascading several stages.
Since the design equations are quite long, it is simpler to implement this cascade by filtering the signal several times, rather than trying to find the coefficients needed for a single filter.
Figure (b) shows examples of the band-reject filter.
The narrowest bandwidth that can be obtain with single precision is about 0.0003 of the sampling frequency.
When pushed beyond this limit, the attenuation of the notch will degrade.
Figure (c) shows the step response of the band-reject filter.
There is noticeable overshoot and ringing, but its amplitude is quite small.
This allows the filter to remove narrowband interference (60 Hz and the like) with only a minor distortion to the time domain waveform.
Phase Response There are three types of phase response that a filter can have: zero phase, linear phase, and nonlinear phase.
An example of each of these is shown in Figure 19-7.
As shown in (a), the zero phase filter is characterized by an impulse response that is symmetrical around sample zero.
The actual shape doesn't matter, only that the negative numbered samples are a mirror image of the positive numbered samples.
When the Fourier transform is taken of this symmetrical waveform, the phase will be entirely zero, as shown in (b).
The disadvantage of the zero phase filter is that it requires the use of negative indexes, which can be inconvenient to work with.
The linear phase filter is a way around this.
The impulse response in (d) is identical to that shown in (a), except it has been shifted to use only positive numbered samples.
The impulse response is still symmetrical between the left and right; however, the location of symmetry has been shifted from zero.
This shift results in the phase, (e), being a straight line, accounting for the name: linear phase.
The slope of this straight line is directly proportional to the amount of the shift.
Since the shift in the impulse response does nothing but produce an identical shift in the output signal, the linear phase filter is equivalent to the zero phase filter for most purposes.
Figure (g) shows an impulse response that is not symmetrical between the left and right.
Correspondingly, the phase, (h), is not a straight line.
In other words, it has a nonlinear phase.
Don't confuse the terms: nonlinear and linear phase with the concept of system linearity discussed in Chapter 5.
Although both use the word linear, they are not related.
Why does anyone care if the phase is linear or not?
Figures (c), (f), and (i) show the answer.
These are the pulse responses of each of the three filters.
The pulse response is nothing more than a positive going step response followed by a negative going step response.
The pulse response is used here because it displays what happens to both the rising and falling edges in a signal.
Here is the important part: zero and linear phase filters have left and right edges that look the same, while nonlinear phase filters have left and right edges that look different.
Many applications cannot tolerate the left and right edges looking different.
One example is the display of an oscilloscope, where this difference could be misinterpreted as a feature of the signal being measured.
Another example is in video processing.
Can you imagine turning on your TV to find the left ear of your favorite actor looking different from his right ear?
It is easy to make an FIR (finite impulse response) filter have a linear phase.
This is because the impulse response (filter kernel) is directly specified in the design process.
Making the filter kernel have left-right symmetry is all that is required.
This is not the case with IIR (recursive) filters, since the recursion coefficients are what is specified, not the impulse response.
The impulse response of a recursive filter is not symmetrical between the left and right, and therefore has a nonlinear phase.
Zero Phase Filter Frequency Sample number FIGURE 19- Zero, linear, and nonlinear phase filters.
A zero phase filter has an impulse response that has left-right symmetry around sample number zero, as in (a).
This results in a frequency response that has a phase composed entirely of zeros, as in (b).
Zero phase impulse responses are desirable because their step responses are symmetrical between the top and bottom, making the left and right edges of pulses look the same, as is shown in (c).
Linear phase filters have left-right symmetry, but not around sample zero, as illustrated in (d).
This results in a phase that is linear, that is, a straight line, as shown in (e).
The linear phase pulse response, shown in (f), has all the advantages of the zero phase pulse response.
In comparison, the impulse responses of nonlinear phase filters are not symmetrical between the left and right, as in (g), and the phases are not a straight line, as in (h).
The worst part is that the left and right edges of the pulse response are not the same, as shown in (i).
Analog electronic circuits have this same problem with the phase response.
Imagine a circuit composed of resistors and capacitors sitting on your desk.
If the input has always been zero, the output will also have always been zero.
When an impulse is applied to the input, the capacitors quickly charge to some value and then begin to exponentially decay through the resistors.
The impulse response (i.e., the output signal) is a combination of these various decaying exponentials.
The impulse response cannot be symmetrical, because the output was zero before the impulse, and the exponential decay never quite reaches a value of zero again.
Analog filter designers attack this problem with the Bessel filter, presented in Chapter 3. The Bessel filter is designed to have as linear phase as possible; however, it is far below the performance of digital filters.
The ability to provide an exact linear phase is a clear advantage of digital filters.
Fortunately, there is a simple way to modify recursive filters to obtain a zero phase.
Figure 19-8 shows an example of how this works.
The input signal to be filtered is shown in (a). Figure (b) shows the signal after it has been filtered by a single pole low-pass filter.
Since this is a nonlinear phase filter, the left and right edges do not look the same; they are inverted versions of each other.
As previously described, this recursive filter is implemented by starting at sample 0 and working toward sample 150, calculating each sample along the way.
Now, suppose that instead of moving from sample 0 toward sample 150, we start at sample 150 and move toward sample 0. In other words, each sample in the output signal is calculated from input and output samples to the right of the sample being worked on.
This means that the recursion equation, Eq. 19-1, is changed to: EQUATION 19- The reverse recursion equation.
This is the same as Eq.
This is analogous to passing an analog signal through an electronic RC circuit while running time backwards.
The magic happens when forward and reverse filtering are combined.
Figure (d) results from filtering the signal in the forward direction and then filtering again in the reverse direction.
Voila!
This produces a zero phase recursive filter.
In fact, any recursive filter can be converted to zero phase with this bidirectional filtering technique.
The only penalty for this improved performance is a factor of two in execution time and program complexity.
Filtered Amplitude Sample number Sample number c. Filtered FIGURE 19- Bidirectional recursive filtering.
A rectangular pulse input signal is shown in (a). Figure (b) shows the signal after being filtered with a single pole recursive low-pass filter, passing from left-to-right.
In (c), the signal has been processed in the same manner, except with the filter moving right-to-left. Figure (d) shows the signal after being filtered both left-to-right and then right-to-left.
Any recursive filter can be made zero phase by using this technique.
Amplitude Sample number d. Filtered Amplitude Amplitude Sample number How do you find the impulse and frequency responses of the overall filter?
The magnitude of the frequency response is the same for each direction, while the phases are opposite in sign.
When the two directions are combined, the magnitude becomes squared, while the phase cancels to zero.
In the time domain, this corresponds to convolving the original impulse response with a left-for-right flipped version of itself.
For instance, the impulse response of a single pole low-pass filter is a one-sided exponential.
The impulse response of the corresponding bidirectional filter is a one-sided exponential that decays to the right, convolved with a one-sided exponential that decays to the left.
Going through the mathematics, this turns out to be a double-sided exponential that decays both to the left and right, with the same decay constant as the original filter.
Some applications only have a portion of the signal in the computer at a particular time, such as systems that alternately input and output data on a continuing basis.
Bidirectional filtering can be used in these cases by combining it with the overlap-add method described in the last chapter.
When you come to the question of how long the impulse response is, don't say "infinite."
If you do, you will need to pad each signal segment with an infinite number of zeros.
Remember, the impulse response can be truncated when it has decayed below the round-off noise level, i.e., about 15 to 20 time constants.
Each segment will need to be padded with zeros on both the left and right to allow for the expansion during the bidirectional filtering.
Using Integers Single precision floating point is ideal to implement these simple recursive filters.
The use of integers is possible, but it is much more difficult.
There are two main problems.
First, the round-off error from the limited number of bits can degrade the response of the filter, or even make it unstable.
Second, the fractional values of the recursion coefficients must be handled with integer math.
One way to attack this problem is to express each coefficient as a fraction.
For example, 0.15 becomes 19/128.
Instead of multiplying by 0.15, you first multiply by 19 and then divide by 128.
Another way is to replace the multiplications with look-up tables.
For example, a 12 bit ADC produces samples with a value between 0 and 4095.
Instead of multiplying each sample by 0.15, you pass the samples through a look-up table that is 4096 entries long.
The value obtained from the look-up table is equal to 0.15 times the value entering the look-up table.
This method is very fast, but it does require extra memory; a separate look-up table is needed for each coefficient.
Before you try either of these integer methods, make sure the recursive algorithm for the moving average filter will not suit your needs.
It loves integers.
Chebyshev filters are used to separate one band of frequencies from another.
Although they cannot match the performance of the windowed-sinc filter, they are more than adequate for many applications.
The primary attribute of Chebyshev filters is their speed, typically more than an order of magnitude faster than the windowed-sinc.
This is because they are carried out by recursion rather than convolution.
The design of these filters is based on a mathematical technique called the z-transform, discussed in Chapter 33.
This chapter presents the information needed to use Chebyshev filters without wading through a mire of advanced mathematics.
The Chebyshev and Butterworth Responses The Chebyshev response is a mathematical strategy for achieving a faster rolloff by allowing ripple in the frequency response.
Analog and digital filters that use this approach are called Chebyshev filters.
For instance, analog Chebyshev filters were used in Chapter 3 for analog-to-digital and digital-toanalog conversion.
These filters are named from their use of the Chebyshev polynomials, developed by the Russian mathematician Pafnuti Chebyshev (1821-1894).
This name has been translated from Russian and appears in the literature with different spellings, such as: Chebychev, Tschebyscheff, Tchebysheff and Tchebichef. Figure 20-1 shows the frequency response of low-pass Chebyshev filters with passband ripples of: 0%, 0.5% and 20%.
As the ripple increases (bad), the roll-off becomes sharper (good).
The Chebyshev response is an optimal tradeoff between these two parameters.
When the ripple is set to 0%, the filter is called a maximally flat or Butterworth filter (after S. Butterworth, a British engineer who described this response in 1930).
A ripple of 0.5% is a often good choice for digital filters.
This matches the typical precision and accuracy of the analog electronics that the signal has passed through.
The Chebyshev filters discussed in this chapter are called type 1 filters, meaning that the ripple is only allowed in the passband.
In comparison, Ripple Amplitude FIGURE 20- The Chebyshev response.
Chebyshev filters achieve a faster roll-off by allowing ripple in the passband.
When the ripple is set to 0%, it is called a maximally flat or Butterworth filter.
Consider using a ripple of 0.5% in your designs; this passband unflatness is so small that it cannot be seen in this graph, but the roll-off is much faster than the Butterworth.
Type 2 filters are seldom used, and we won't discuss them.
There is, however, an important design called the elliptic filter, which has ripple in both the passband and the stopband.
Elliptic filters provide the fastest roll-off for a given number of poles, but are much harder to design.
We won't discuss the elliptic filter here, but be aware that it is frequently the first choice of professional filter designers, both in analog electronics and DSP.
If you need this level of performance, buy a software package for designing digital filters.
Designing the Filter You must select four parameters to design a Chebyshev filter: (1) a high-pass or low-pass response, (2) the cutoff frequency, (3) the percent ripple in the passband, and (4) the number of poles.
Just what is a pole?
Here are two answers.
If you don't like one, maybe the other will help: Answer 1- The Laplace transform and z-transform are mathematical ways of breaking an impulse response into sinusoids and decaying exponentials.
This is done by expressing the system's characteristics as one complex polynomial divided by another complex polynomial.
The roots of the numerator are called zeros, while the roots of the denominator are called poles.
Since poles and zeros can be complex numbers, it is common to say they have a "location" in the complex plane.
Elaborate systems have more poles and zeros than simple ones.
Recursive filters are designed by first selecting the location of the poles and zeros, and then finding the appropriate recursion coefficients (or analog components).
For example, Butterworth filters have poles that lie on a circle in the complex plane, while in a Chebyshev filter they lie on an ellipse.
This is the topic of Chapters 32 and 33.
Answer 2- Poles are containers filled with magic powder.
The more poles in a filter, the better the filter works.
High-pass frequency response Amplitude Amplitude 4 pole Frequency 6 pole Frequency b.
Low-pass frequency response (dB) d.
High-pass frequency response (dB) 4 pole Amplitude (dB) Amplitude (dB) 6 pole Frequency Frequency FIGURE 20- Chebyshev frequency responses.
Figures (a) and (b) show the frequency responses of low-pass Chebyshev filters with 0.5% ripple, while (c) and (d) show the corresponding high-pass filter responses.
Kidding aside, the point is that you can use these filters very effectively without knowing the nasty mathematics behind them.
Filter design is a specialty.
In actual practice, more engineers, scientists and programmers think in terms of answer 2, than answer 1. Figure 20-2 shows the frequency response of several Chebyshev filters with 0.5% ripple.
For the method used here, the number of poles must be even.
The cutoff frequency of each filter is measured where the amplitude crosses 0. (-3dB).
Filters with a cutoff frequency near 0 or 0.5 have a sharper roll-off than filters in the center of the frequency range.
For example, a two pole filter at fC ' 0.05 has about the same roll-off as a four pole filter at fC ' 0.25 .
This is fortunate; fewer poles can be used near 0 and 0.5 because of round-off noise.
More about this later.
There are two ways of finding the recursion coefficients without using the ztransform.
First, the cowards way: use a table.
Tables 20-1 and 20-2 provide the recursion coefficients for low-pass and high-pass filters with 0.5% passband ripple.
If you only need a quick and dirty design, copy the appropriate coefficients into your program, and you're done.
There are two problems with using tables to design digital filters.
First, tables have a limited choice of parameters.
For instance, Table 20-1 only provides 12 different cutoff frequencies, a maximum of 6 poles per filter, and no choice of passband ripple.
Without the ability to select parameters from a continuous range of values, the filter design cannot be optimized.
Second, the coefficients must be manually transferred from the table into the program.
This is very time consuming and will discourage you from trying alternative values.
Instead of using tabulated values, consider including a subroutine in your program that calculates the coefficients.
Such a program is shown in Table 204.
The good news is that the program is relatively simple in structure.
After the four filter parameters are entered, the program spits out the "a" and "b" coefficients in the arrays A[ ] and B[ ].
The bad news is that the program calls the subroutine in Table 20-5.
At first glance this subroutine is really ugly.
Don't despair; it isn't as bad as it seems!
There is one simple branch in line 1120.
Everything else in the subroutine is straightforward number crunching.
Six variables enter the routine, five variables leave the routine, and fifteen temporary variables (plus indexes) are used within.
Table 20-5 provides two sets of test data for debugging this subroutine.
Chapter 31 discusses the operation of this program in detail.
Step Response Overshoot Butterworth and Chebyshev filters have an overshoot of 5 to 30% in their step responses, becoming larger as the number of poles is increased.
Figure 20-3a shows the step response for two example Chebyshev filters.
Figure (b) shows something that is unique to digital filters and has no counterpart in analog electronics: the amount of overshoot in the step response depends to a small degree on the cutoff frequency of the filter.
The excessive overshoot and ringing in the step response results from the Chebyshev filter being optimized for the frequency domain at the expense of the time domain.
Overshoot a. Step response Percent overshoot 4 pole Amplitude 2 pole 6 pole 2 pole Sample number Frequency FIGURE 20- Chebyshev step response.
The overshoot in the Chebyshev filter's step response is 5% to 30%, depending on the number of poles, as shown in (a), and the cutoff frequency, as shown in (b). Figure (a) is for a cutoff frequency of 0.05, and may be scaled to other cutoff frequencies.
Stability The main limitation of digital filters carried out by convolution is execution time.
It is possible to achieve nearly any filter response, provided you are willing to wait for the result.
Recursive filters are just the opposite.
They run like lightning; however, they are limited in performance.
For example, consider a 6 pole, 0.5% ripple, low-pass filter with a 0.01 cutoff frequency.
The recursion coefficients for this filter can be obtained from Table 20-1: Look carefully at these coefficients.
The "b" coefficients have an absolute value of about ten.
Using single precision, the round-off noise on each of these numbers is about one ten-millionth of the value, i.e., 10&6 .
Now look at the "a" coefficients, with a value of about 10&9 .
Something is obviously wrong here.
The contribution from the input signal (via the "a" coefficients) will be times smaller than the noise from the previously calculated output signal (via the "b" coefficients).
This filter won't work!
In short, round-off noise limits the number of poles that can be used in a filter.
The actual number will depend slightly on the ripple and if it is a high or low-pass filter.
The approximate numbers for single precision are: TABLE 20- The maximum number of poles for single precision.
Cutoff frequency Maximum poles The filter's performance will start to degrade as this limit is approached; the step response will show more overshoot, the stopband attenuation will be poor, and the frequency response will have excessive ripple.
If the filter is pushed too far, or there is an error in the coefficients, the output will probably oscillate until an overflow occurs.
There are two ways of extending the maximum number of poles that can be used.
First, use double precision.
This requires using double precision in the coefficient calculation as well (including the value for pi ).
The second method is to implement the filter in stages.
For example, a six pole filter starts out as a cascade of three stages of two poles each.
The program in Table 20-4 combines these three stages into a single set of recursion coefficients for easier programming.
However, the filter is more stable if carried out as the original three separate stages.
This requires knowing the "a" and "b" coefficients for each of the stages.
These can Program to calculate the "a" and "b" coefficients for Chebyshev recursive filters.
In lines 270-300, four parameters are entered into the program.
The cutoff frequency, FC, is expressed as a fraction of the sampling frequency, and therefore must be in the range: 0 to 0.5.
The variable, LH, is set to a value of one for a high-pass filter, and zero for a low-pass filter.
The value entered for PR must be in the range of 0 to 29, corresponding to 0 to 29% ripple in the filter's frequency response.
The number of poles in the filter, entered in the variable NP, must be an even integer between 2 and 20.
At the completion of the program, the "a" and "b" coefficients are stored in the arrays A[ ] and B[ ] (a0 = A[0], a1 = A[1], etc.).
TABLE 20-5 is a subroutine called from line 340 of the main program.
Six variables are passed to this subroutine, and five variables are returned.
Table 20-6 (next page) contains two sets of data to help debug this subroutine.
The functions: COS and SIN, use radians, not degrees.
The function: LOG is the natural (base e) logarithm.
Declaring all floating point variables (including the value of B) to be double precision will allow more poles to be used.
Tables 20- and 20-2 were generated with this program and can be used to test for proper operation.
Chapter 33 describes the mathematical operation of this program.
Debugging data.
This table contains two sets of data for debugging the subroutine listed in Table 20-5.
The subroutine in Table 20-5 is called once for each stage in the cascade.
For example, it is called three times for a six pole filter.
At the completion of the subroutine, five variables are return to the main program: A0, A1, A2, B1, & B2.
These are the recursion coefficients for the two pole stage being worked on, and can be used to implement the filter in stages.
Decisions, decisions, decisions!
With all these filters to choose from, how do you know which to use?
This chapter is a head-to-head competition between filters; we'll select champions from each side and let them fight it out.
In the first match, digital filters are pitted against analog filters to see which technology is best.
In the second round, the windowed-sinc is matched against the Chebyshev to find the king of the frequency domain filters.
In the final battle, the moving average fights the single pole filter for the time domain championship.
Enough talk; let the competition begin!
Match #1: Analog vs. Digital Filters Most digital signals originate in analog electronics.
If the signal needs to be filtered, is it better to use an analog filter before digitization, or a digital filter after?
We will answer this question by letting two of the best contenders deliver their blows.
The goal will be to provide a low-pass filter at 1 kHz.
Fighting for the analog side is a six pole Chebyshev filter with 0.5 dB (6%) ripple.
As described in Chapter 3, this can be constructed with 3 op amps, 12 resistors, and capacitors.
In the digital corner, the windowed-sinc is warming up and ready to fight.
The analog signal is digitized at a 10 kHz sampling rate, making the cutoff frequency 0.1 on the digital frequency scale.
The length of the windowed-sinc will be chosen to be 129 points, providing the same 90% to 10% roll-off as the analog filter.
Fair is fair.
Figure 21-1 shows the frequency and step responses for these two filters.
Let's compare the two filters blow-by-blow.
As shown in (a) and (b), the analog filter has a 6% ripple in the passband, while the digital filter is perfectly flat (within 0.02%).
The analog designer might argue that the ripple can be selected in the design; however, this misses the point.
The flatness achievable with analog filters is limited by the accuracy of their resistors and capacitors.
Even if a Butterworth response is designed (i.e., 0% ripple), filters of this complexity will have a residue ripple of, perhaps, 1%.
On the other hand, the flatness of digital filters is primarily limited by round-off error, making them hundreds of times flatter than their analog counterparts.
Score one point for the digital filter.
Next, look at the frequency response on a log scale, as shown in (c) and (d).
Again, the digital filter is clearly the victor in both roll-off and stopband attenuation.
Even if the analog performance is improved by adding additional stages, it still can't compare to the digital filter.
For instance, imagine that you need to improve these two parameters by a factor of 100.
This can be done with simple modifications to the windowed-sinc, but is virtually impossible for the analog circuit.
Score two more for the digital filter.
The step response of the two filters is shown in (e) and (f).
The digital filter's step response is symmetrical between the lower and upper portions of the step, i.e., it has a linear phase.
The analog filter's step response is not symmetrical, i.e., it has a nonlinear phase.
One more point for the digital filter.
Lastly, the analog filter overshoots about 20% on one side of the step.
The digital filter overshoots about 10%, but on both sides of the step.
Since both are bad, no points are awarded.
In spite of this beating, there are still many applications where analog filters should, or must, be used.
This is not related to the actual performance of the filter (i.e., what goes in and what comes out), but to the general advantages that analog circuits have over digital techniques.
The first advantage is speed: digital is slow; analog is fast.
For example, a personal computer can only filter data at about 10,000 samples per second, using FFT convolution.
Even simple op amps can operate at 100 kHz to 1 MHz, 10 to 100 times as fast as the digital system!
The second inherent advantage of analog over digital is dynamic range.
This comes in two flavors.
Amplitude dynamic range is the ratio between the largest signal that can be passed through a system, and the inherent noise of the system.
For instance, a 12 bit ADC has a saturation level of 4095, and an rms quantization noise of 0.29 digital numbers, for a dynamic range of about 14000.
In comparison, a standard op amp has a saturation voltage of about 20 volts and an internal noise of about 2 microvolts, for a dynamic range of about ten million.
Just as before, a simple op amp devastates the digital system.
The other flavor is frequency dynamic range.
For example, it is easy to design an op amp circuit to simultaneously handle frequencies between 0. Hz and 100 kHz (seven decades).
When this is tried with a digital system, the computer becomes swamped with data.
For instance, sampling at kHz, it takes 20 million points to capture one complete cycle at 0.01 Hz.
You may have noticed that the frequency response of digital filters is almost always plotted on a linear frequency scale, while analog filters are usually displayed with a logarithmic frequency.
This is because digital filters need Analog Filter Digital Filter (6 pole 0.5dB Chebyshev) Sample number FIGURE 21- Comparison of analog and digital filters.
Digital filters have better performance in many areas, such as: passband ripple, (a) vs. (b), roll-off and stopband attenuation, (c) vs. (d), and step response symmetry, (e) vs. (f).
The digital filter in this example has a cutoff frequency of 0.1 of the 10 kHz sampling rate.
This provides a fair comparison to the 1 kHz cutoff frequency of the analog filter.
Match #2: Windowed-Sinc vs. Chebyshev Both the windowed-sinc and the Chebyshev filters are designed to separate one band of frequencies from another.
The windowed-sinc is an FIR filter implemented by convolution, while the Chebyshev is an IIR filter carried out by recursion.
Which is the best digital filter in the frequency domain?
We'll let them fight it out.
The recursive filter contender will be a 0.5% ripple, 6 pole Chebyshev low-pass filter.
A fair comparison is complicated by the fact that the Chebyshev's frequency response changes with the cutoff frequency.
We will use a cutoff frequency of 0.2, and select the windowed-sinc's filter kernel to be 51 points.
This makes both filters have the same 90% to 10% roll-off, as shown in Fig. 21-2(a).
Now the pushing and shoving begins.
The recursive filter has a 0.5% ripple in the passband, while the windowed-sinc is flat.
However, we could easily set the recursive filter ripple to 0% if needed.
No points.
Figure 21-2b shows that the windowed-sinc has a much better stopband attenuation than the Chebyshev.
One point for the windowed-sinc.
Figure 21-3 shows the step response of the two filters.
Both are bad, as you should expect for frequency domain filters.
The recursive filter has a nonlinear phase, but this can be corrected with bidirectional filtering.
Since both filters are so ugly in this parameter, we will call this a draw.
So far, there isn't much difference between these two filters; either will work when moderate performance is needed.
The heavy hitting comes over two critical issues: maximum performance and speed.
The windowed-sinc is a powerhouse, while the Chebyshev is quick and agile.
Suppose you have a really tough frequency separation problem, say, needing to isolate a Chebyshev recursive b.
Frequency response (dB) Amplitude (dB) Amplitude a. Frequency response windowed-sinc Chebyshev recursive windowed-sinc Frequency Frequency FIGURE 21- Windowed-sinc and Chebyshev frequency responses.
Frequency responses are shown for a 51 point windowed-sinc filter and a 6 pole, 0.5% ripple Chebyshev recursive filter.
The windowed-sinc has better stopband attenuation, but either will work in moderate performance applications.
The cutoff frequency of both filters is 0.2, measured at an amplitude of 0.5 for the windowed-sinc, and 0.707 for the recursive.
Chebyshev step response Amplitude Amplitude -0.
Sample number -0.
Sample number FIGURE 21- Windowed--sinc and Chebyshev step responses.
The step responses are shown for a 51 point windowed-sinc filter and a 6 pole, 0.5% ripple Chebyshev recursive filter.
Each of these filters has a cutoff frequency of 0.2.
The windowed-sinc has a slightly better step response because it has less overshoot and a zero phase.
Figure 21-4 shows how these two filters compare when you need maximum performance.
The recursive filter is a 6 pole Chebyshev with 0.5% ripple.
This is the maximum number of poles that can be used at a 0.05 cutoff frequency with single precision.
The windowed-sinc uses a 1001 point filter kernel, formed by convolving a 501 point windowed-sinc filter kernel with itself.
As shown in Chapter 16, this provides greater stopband attenuation.
How do these two filters compare when maximum performance is needed?
The windowed-sinc crushes the Chebyshev!
Even if the recursive filter were improved (more poles, multistage implementation, double precision, etc.), it is still no match for the FIR performance.
This is especially impressive when you consider that the windowed-sinc has only begun to fight.
There are strong limits on the maximum performance that recursive filters can provide.
The windowed-sinc, in contrast, can be pushed to incredible levels.
This is, of course, provided you are willing to wait for the result.
Which brings up the second critical issue: speed.
Amplitude (dB) FIGURE 21- Maximum performance of FIR and IIR filters.
The frequency response of the windowed-sinc can be virtually any shape needed, while the Chebyshev recursive filter is very limited.
This graph compares the frequency response of a six pole Chebyshev recursive filter with a point windowed-sinc filter.
Chebyshev (IIR) Windowed-sinc (FIR) Frequency standard convolution Relative execution time FIGURE 21- Comparing FIR and IIR execution speeds.
These curves shows the relative execution times for a windowed-sinc filter compared with an equivalent six pole Chebyshev recursive filter.
Curves are shown for implementing the FIR filter by both the standard and the FFT convolution algorithms.
The windowed-sinc execution time rises at low and high frequencies because the filter kernel must be made longer to keep up with the greater performance of the recursive filter at these frequencies.
In general, IIR filters are an order of magnitude faster than FIR filters of comparable performance.
Figure 21-5 shows how much longer the windowed-sinc takes to execute, compared to a six pole recursive filter.
Since the recursive filter has a faster roll-off at low and high frequencies, the length of the windowed-sinc kernel must be made longer to match the performance (i.e., to keep the comparison fair).
This accounts for the increased execution time for the windowed-sinc near frequencies 0 and 0.5.
The important point is that FIR filters can be expected to be about an order of magnitude slower than comparable IIR filters (go-cart: 15 mph, Ferrari: 150 mph).
Match #3: Moving Average vs. Single Pole Our third competition will be a battle of the time domain filters.
The first fighter will be a nine point moving average filter.
Its opponent for today's match will be a single pole recursive filter using the bidirectional technique.
To achieve a comparable frequency response, the single pole filter will use a sample-to-sample decay of x ' 0.70 .
The battle begins in Fig. 21-6 where the frequency response of each filter is shown.
Neither one is very impressive, but of course, frequency separation isn't what these filters are used for.
No points for either side.
Figure 21-7 shows the step responses of the filters.
In (a), the moving average step response is a straight line, the most rapid way of moving from one level to another.
In (b), the recursive filter's step response is smoother, which may be better for some applications.
One point for each side.
These filters are quite equally matched in terms of performance and often the choice between the two is made on personal preference.
However, there are Amplitude FIGURE 21- Moving average and single pole frequency responses.
Both of these filters have a poor frequency response, as you should expect for time domain filters.
Single pole recursive Moving average Frequency two cases where one filter has a slight edge over the other.
These are based on the trade-off between development time and execution time.
In the first instance, you want to reduce development time and are willing to accept a slower filter.
For example, you might have a one time need to filter a few thousand points.
Since the entire program runs in only a few seconds, it is pointless to spend time optimizing the algorithm.
Floating point will almost certainly be used.
The choice is to use the moving average filter carried out by convolution, or a single pole recursive filter.
The winner here is the recursive filter.
It will be slightly easier to program and modify, and will execute much faster.
The second case is just the opposite; your filter must operate as fast as possible and you are willing to spend the extra development time to get it.
For instance, this filter might be a part of a commercial product, with the potential to be run millions of times.
You will probably use integers for the highest possible speed.
Your choice of filters will be the moving average Step responses of the moving average and the bidirectional single pole filter.
The moving average step response occurs over a smaller number of samples, while the single pole filter's step response is smoother.
The winner is the moving average filter.
It will execute faster and not be susceptible to the development and execution problems of integer arithmetic.
Audio processing covers many diverse fields, all involved in presenting sound to human listeners.
Three areas are prominent: (1) high fidelity music reproduction, such as in audio compact discs, (2) voice telecommunications, another name for telephone networks, and (3) synthetic speech, where computers generate and recognize human voice patterns.
While these applications have different goals and problems, they are linked by a common umpire: the human ear.
Digital Signal Processing has produced revolutionary changes in these and other areas of audio processing.
Human Hearing The human ear is an exceedingly complex organ.
To make matters even more difficult, the information from two ears is combined in a perplexing neural network, the human brain.
Keep in mind that the following is only a brief overview; there are many subtle effects and poorly understood phenomena related to human hearing.
Figure 22-1 illustrates the major structures and processes that comprise the human ear.
The outer ear is composed of two parts, the visible flap of skin and cartilage attached to the side of the head, and the ear canal, a tube about 0.5 cm in diameter extending about 3 cm into the head.
These structures direct environmental sounds to the sensitive middle and inner ear organs located safely inside of the skull bones.
Stretched across the end of the ear canal is a thin sheet of tissue called the tympanic membrane or ear drum.
Sound waves striking the tympanic membrane cause it to vibrate.
The middle ear is a set of small bones that transfer this vibration to the cochlea (inner ear) where it is converted to neural impulses.
The cochlea is a liquid filled tube roughly 2 mm in diameter and 3 cm in length.
Although shown straight in Fig. 22-1, the cochlea is curled up and looks like a small snail shell.
In fact, cochlea is derived from the Greek word for snail.
When a sound wave tries to pass from air into liquid, only a small fraction of the sound is transmitted through the interface, while the remainder of the energy is reflected.
This is because air has a low mechanical impedance (low acoustic pressure and high particle velocity resulting from low density and high compressibility), while liquid has a high mechanical impedance.
In less technical terms, it requires more effort to wave your hand in water than it does to wave it in air.
This difference in mechanical impedance results in most of the sound being reflected at an air/liquid interface.
The middle ear is an impedance matching network that increases the fraction of sound energy entering the liquid of the inner ear.
For example, fish do not have an ear drum or middle ear, because they have no need to hear in air.
Most of the impedance conversion results from the difference in area between the ear drum (receiving sound from the air) and the oval window (transmitting sound into the liquid, see Fig. 22-1).
The ear drum has an area of about ( mm) 2, while the oval window has an area of roughly 4 ( mm) 2. Since pressure is equal to force divided by area, this difference in area increases the sound wave pressure by about 15 times.
Contained within the cochlea is the basilar membrane, the supporting structure for about 12,000 sensory cells forming the cochlear nerve.
The basilar membrane is stiffest near the oval window, and becomes more flexible toward the opposite end, allowing it to act as a frequency spectrum analyzer.
When exposed to a high frequency signal, the basilar membrane resonates where it is stiff, resulting in the excitation of nerve cells close to the oval window.
Likewise, low frequency sounds excite nerve cells at the far end of the basilar membrane.
This makes specific fibers in the cochlear nerve respond to specific frequencies.
This organization is called the place principle, and is preserved throughout the auditory pathway into the brain.
Another information encoding scheme is also used in human hearing, called the volley principle.
Nerve cells transmit information by generating brief electrical pulses called action potentials.
A nerve cell on the basilar membrane can encode audio information by producing an action potential in response to each cycle of the vibration.
For example, a 200 hertz sound wave can be represented by a neuron producing 200 action potentials per second.
However, this only works at frequencies below about 500 hertz, the maximum rate that neurons can produce action potentials.
The human ear overcomes this problem by allowing several nerve cells to take turns performing this single task.
For example, a 3000 hertz tone might be represented by ten nerve cells alternately firing at 300 times per second.
This extends the range of the volley principle to about 4 kHz, above which the place principle is exclusively used.
Table 22-1 shows the relationship between sound intensity and perceived loudness.
It is common to express sound intensity on a logarithmic scale, called decibel SPL (Sound Power Level).
On this scale, 0 dB SPL is a sound wave power of 10-16 watts/cm 2, about the weakest sound detectable by the human ear.
Normal speech is at about 60 dB SPL, while painful damage to the ear occurs at about 140 dB SPL.
The outer ear collects sound waves from the environment and channels them to the tympanic membrane (ear drum), a thin sheet of tissue that vibrates in synchronization with the air waveform.
The middle ear bones (hammer, anvil and stirrup) transmit these vibrations to the oval window, a flexible membrane in the fluid filled cochlea.
Contained within the cochlea is the basilar membrane, the supporting structure for about 12,000 nerve cells that form the cochlear nerve.
Due to the varying stiffness of the basilar membrane, each nerve cell only responses to a narrow range of audio frequencies, making the ear a frequency spectrum analyzer.
The difference between the loudest and faintest sounds that humans can hear is about 120 dB, a range of one-million in amplitude.
Listeners can detect a change in loudness when the signal is altered by about 1 dB (a 12% change in amplitude).
In other words, there are only about 120 levels of loudness that can be perceived from the faintest whisper to the loudest thunder.
The sensitivity of the ear is amazing; when listening to very weak sounds, the ear drum vibrates less than the diameter of a single molecule!
The perception of loudness relates roughly to the sound power to an exponent of 1/3.
For example, if you increase the sound power by a factor of ten, listeners will report that the loudness has increased by a factor of about two ( 101/3 . 2 ).
This is a major problem for eliminating undesirable environmental sounds, for instance, the beefed-up stereo in the next door apartment.
Suppose you diligently cover 99% of your wall with a perfect soundproof material, missing only 1% of the surface area due to doors, corners, vents, etc.
Even though the sound power has been reduced to only 1% of its former value, the perceived loudness has only dropped to about 0.011/3 .
The range of human hearing is generally considered to be 20 Hz to 20 kHz, but it is far more sensitive to sounds between 1 kHz and 4 kHz.
For example, listeners can detect sounds as low as 0 dB SPL at 3 kHz, but require 40 dB SPL at 100 hertz (an amplitude increase of 100).
Listeners can tell that two tones are different if their frequencies differ by more than about 0.3% at kHz.
This increases to 3% at 100 hertz.
For comparison, adjacent keys on a piano differ by about 6% in frequency.
Softer TABLE 22- Units of sound intensity.
Sound intensity is expressed as power per unit area (such as watts/cm 2), or more commonly on a logarithmic scale called decibels SPL.
As this table shows, human hearing is the most sensitive between 1 kHz and 4 kHz.
The primary advantage of having two ears is the ability to identify the direction of the sound.
Human listeners can detect the difference between two sound sources that are placed as little as three degrees apart, about the width of a person at 10 meters.
This directional information is obtained in two separate ways.
First, frequencies above about 1 kHz are strongly shadowed by the head.
In other words, the ear nearest the sound receives a stronger signal than the ear on the opposite side of the head.
The second clue to directionality is that the ear on the far side of the head hears the sound slightly later than the near ear, due to its greater distance from the source.
Based on a typical head size (about 22 cm) and the speed of sound (about 340 meters per second), an angular discrimination of three degrees requires a timing precision of about 30 microseconds.
Since this timing requires the volley principle, this clue to directionality is predominately used for sounds less than about 1 kHz.
Both these sources of directional information are greatly aided by the ability to turn the head and observe the change in the signals.
An interesting sensation occurs when a listener is presented with exactly the same sounds to both ears, such as listening to monaural sound through headphones.
The brain concludes that the sound is coming from the center of the listener's head!
While human hearing can determine the direction a sound is from, it does poorly in identifying the distance to the sound source.
This is because there are few clues available in a sound wave that can provide this information.
Human hearing weakly perceives that high frequency sounds are nearby, while low frequency sounds are distant.
This is because sound waves dissipate their higher frequencies as they propagate long distances.
Echo content is another weak clue to distance, providing a perception of the room size.
For example, sounds in a large auditorium will contain echoes at about 100 millisecond intervals, while 10 milliseconds is typical for a small office.
Some species have solved this ranging problem by using active sonar.
For example, bats and dolphins produce clicks and squeaks that reflect from nearby objects.
By measuring the interval between transmission and echo, these animals can locate objects with about 1 cm resolution.
Experiments have shown that some humans, particularly the blind, can also use active echo localization to a small extent.
Timbre The perception of a continuous sound, such as a note from a musical instrument, is often divided into three parts: loudness, pitch, and timbre (pronounced "timber").
Loudness is a measure of sound wave intensity, as previously described.
Pitch is the frequency of the fundamental component in the sound, that is, the frequency with which the waveform repeats itself.
While there are subtle effects in both these perceptions, they are a straightforward match with easily characterized physical quantities.
Timbre is more complicated, being determined by the harmonic content of the signal.
Figure 22-2 illustrates two waveforms, each formed by adding a 1 kHz sine wave with an amplitude of one, to a 3 kHz sine wave with an amplitude of one-half.
The difference between the two waveforms is that the one shown in (b) has the higher frequency inverted before the addition.
Put another way, the third harmonic (3 kHz) is phase shifted by 180 degrees compared to the first harmonic (1 kHz).
In spite of the very different time domain waveforms, these two signals sound identical.
This is because hearing is based on the amplitude of the frequencies, and is very insensitive to their phase.
The shape of the time domain waveform is only indirectly related to hearing, and usually not considered in audio systems.
The human ear is very insensitive to the relative phase of the component sinusoids.
For example, these two waveforms would sound identical, because the amplitudes of their components are the same, even though their relative phases are different.
The ear's insensitivity to phase can be understood by examining how sound propagates through the environment.
Suppose you are listening to a person speaking across a small room.
Much of the sound reaching your ears is reflected from the walls, ceiling and floor.
Since sound propagation depends on frequency (such as: attenuation, reflection, and resonance), different frequencies will reach your ear through different paths.
This means that the relative phase of each frequency will change as you move about the room.
Since the ear disregards these phase variations, you perceive the voice as unchanging as you move position.
From a physics standpoint, the phase of an audio signal becomes randomized as it propagates through a complex environment.
Put another way, the ear is insensitive to phase because it contains little useful information.
However, it cannot be said that the ear is completely deaf to the phase.
This is because a phase change can rearrange the time sequence of an audio signal.
An example is the chirp system (Chapter 11) that changes an impulse into a much longer duration signal.
Although they differ only in their phase, the ear can distinguish between the two sounds because of their difference in duration.
For the most part, this is just a curiosity, not something that happens in the normal listening environment.
Suppose that we ask a violinist to play a note, say, the A below middle C. When the waveform is displayed on an oscilloscope, it appear much as the sawtooth shown in Fig. 22-3a.
This is a result of the sticky rosin applied to the fibers of the violinist's bow.
As the bow is drawn across the string, the waveform is formed as the string sticks to the bow, is pulled back, and eventually breaks free.
This cycle repeats itself over and over resulting in the sawtooth waveform.
Figure 22-3b shows how this sound is perceived by the ear, a frequency of hertz, plus harmonics at 440, 660, 880 hertz, etc.
If this note were played on another instrument, the waveform would look different; however, the ear would still hear a frequency of 220 hertz plus the harmonics.
Since the two instruments produce the same fundamental frequency for this note, they sound similar, and are said to have identical pitch.
Since the relative amplitude of the harmonics is different, they will not sound identical, and will be said to have different timbre.
It is often said that timbre is determined by the shape of the waveform.
This is true, but slightly misleading.
The perception of timbre results from the ear detecting harmonics.
While harmonic content is determined by the shape of the waveform, the insensitivity of the ear to phase makes the relationship very onesided.
That is, a particular waveform will have only one timbre, while a particular timbre has an infinite number of possible waveforms.
The ear is very accustomed to hearing a fundamental plus harmonics.
If a listener is presented with the combination of a 1 kHz and 3 kHz sine wave, they will report that it sounds natural and pleasant.
If sine waves of 1 kHz and 3.1 kHz are used, it will sound objectionable.
Time domain waveform b.
Frequency spectrum Amplitude Amplitude fundamental harmonics Time (milliseconds) Frequency (hertz) FIGURE 22- Violin waveform.
A bowed violin produces a sawtooth waveform, as illustrated in (a).
The sound heard by the ear is shown in (b), the fundamental frequency plus harmonics.
This is the basis of the standard musical scale, as illustrated by the piano keyboard in Fig. 22-4.
Striking the farthest left key on the piano produces a fundamental frequency of 27.5 hertz, plus harmonics at 55, 110, 220, 440, hertz, etc. (there are also harmonics between these frequencies, but they aren't important for this discussion).
These harmonics correspond to the fundamental frequency produced by other keys on the keyboard.
Specifically, every seventh white key is a harmonic of the far left key.
That is, the eighth key from the left has a fundamental frequency of 55 hertz, the 15th key has a fundamental frequency of 110 hertz, etc. Being harmonics of each other, these keys sound similar when played, and are harmonious when played in unison.
For this reason, they are all called the note, A. In this same manner, the white key immediate right of each A is called a B, and they are all harmonics of each other.
This pattern repeats for the seven notes: A, B, C, D, E, F, and G.
The term octave means a factor of two in frequency.
On the piano, one octave comprises eight white keys, accounting for the name (octo is Latin for eight).
In other words, the piano’s frequency doubles after every seven white keys, and the entire keyboard spans a little over seven octaves.
The range of human hearing is generally quoted as 20 hertz to 20 kHz, BCDEF G The Piano keyboard.
The keyboard of the piano is a logarithmic frequency scale, with the fundamental frequency doubling after every seven white keys.
These white keys are the notes: A, B, C, D, E, F and G. corresponding to about ½ octave to the left, and two octaves to the right of the piano keyboard.
Since octaves are based on doubling the frequency every fixed number of keys, they are a logarithmic representation of frequency.
This is important because audio information is generally distributed in this same way.
For example, as much audio information is carried in the octave between 50 hertz and 100 hertz, as in the octave between 10 kHz and 20 kHz.
Even though the piano only covers about 20% of the frequencies that humans can hear (4 kHz out of 20 kHz), it can produce more than 70% of the audio information that humans can perceive (7 out of 10 octaves).
Likewise, the highest frequency a human can detect drops from about 20 kHz to 10 kHz over the course of an adult's lifetime.
However, this is only a loss of about 10% of the hearing ability (one octave out of ten).
As shown next, this logarithmic distribution of information directly affects the required sampling rate of audio signals.
Sound Quality vs. Data Rate When designing a digital audio system there are two questions that need to be asked: (1) how good does it need to sound? and (2) what data rate can be tolerated?
The answer to these questions usually results in one of three categories.
First, high fidelity music, where sound quality is of the greatest importance, and almost any data rate will be acceptable.
Second, telephone communication, requiring natural sounding speech and a low data rate to reduce the system cost.
Third, compressed speech, where reducing the data rate is very important and some unnaturalness in the sound quality can be tolerated.
This includes military communication, cellular telephones, and digitally stored speech for voice mail and multimedia.
Table 22-2 shows the tradeoff between sound quality and data rate for these three categories.
High fidelity music systems sample fast enough (44.1 kHz), and with enough precision (16 bits), that they can capture virtually all of the sounds that humans are capable of hearing.
This magnificent sound quality comes at the price of a high data rate, 44.1 kHz × 16 bits = 706k bits/sec.
This is pure brute force.
Whereas music requires a bandwidth of 20 kHz, natural sounding speech only requires about 3.2 kHz.
Even though the frequency range has been reduced to only 16% (3.2 kHz out of 20 kHz), the signal still contains 80% of the original sound information (8 out of 10 octaves).
Telecommunication systems typically operate with a sampling rate of about 8 kHz, allowing natural sounding speech, but greatly reduced music quality.
You are probably already familiar with this difference in sound quality: FM radio stations broadcast with a bandwidth of almost 20 kHz, while AM radio stations are limited to about 3.2 kHz.
Voices sound normal on the AM stations, but the music is weak and unsatisfying.
Voice-only systems also reduce the precision from 16 bits to 12 bits per sample, with little noticeable change in the sound quality.
This can be reduced to only 8 bits per sample if the quantization step size is made unequal.
This is a widespread procedure called companding, and will be High fidelity music 12 bit DSP speech compression technique.
Very low data rates, poor voice quality.
Comments TABLE 22- Audio data rate vs. sound quality.
The sound quality of a digitized audio signal depends on its data rate, the product of its sampling rate and number of bits per sample.
This can be broken into three categories, high fidelity music ( kbits/sec), telephone quality speech (64 kbits/sec), and compressed speech (4 kbits/sec).
An 8 kHz sampling rate, with an ADC precision of 8 bits per sample, results in a data rate of 64k bits/sec.
This is the brute force data rate for natural sounding speech.
Notice that speech requires less than 10% of the data rate of high fidelity music.
The data rate of 64k bits/sec represents the straightforward application of sampling and quantization theory to audio signals.
Techniques for lowering the data rate further are based on compressing the data stream by removing the inherent redundancies in speech signals.
Data compression is the topic of Chapter 27.
One of the most efficient ways of compressing an audio signal is Linear Predictive Coding (LPC), of which there are several variations and subgroups.
Depending on the speech quality required, LPC can reduce the data rate to as little as 2-6k bits/sec.
We will revisit LPC later in this chapter with speech synthesis.
High Fidelity Audio Audiophiles demand the utmost sound quality, and all other factors are treated as secondary.
If you had to describe the mindset in one word, it would be: overkill.
Rather than just matching the abilities of the human ear, these systems are designed to exceed the limits of hearing.
It's the only way to be sure that the reproduced music is pristine.
Digital audio was brought to the world by the compact laser disc, or CD.
This was a revolution in music; the sound quality of the CD system far exceeds older systems, such as records and tapes.
DSP has been at the forefront of this technology.
Figure 22-5 illustrates the surface of a compact laser disc, such as viewed through a high power microscope.
The main surface is shiny (reflective of light), with the digital information stored as a series of dark pits burned on the surface with a laser.
The information is arranged in a single track that spirals from the inside to the outside, the opposite of a phonograph record.
The rotation of the CD is changed from about 210 to 480 rpm as the information is read from the outside to the inside of the spiral, making the scanning velocity a constant 1.2 meters per second.
During playback, an optical sensor detects if the surface is reflective or nonreflective, generating the corresponding binary information.
As shown by the geometry in Fig. 22-5, the CD stores about 1 bit per ( µm ) 2, corresponding to 1 million bits per ( mm) 2, and 15 billion bits per disk.
This is about the same feature size used in integrated circuit manufacturing, and for a good reason.
One of the properties of light is that it cannot be focused to smaller than about one-half wavelength, or 0.3 µm.
Since both integrated circuits and laser disks are created by optical means, the fuzziness of light below 0.3 µm limits how small of features can be used.
Figure 22-6 shows a block diagram of a typical compact disc playback system.
The raw data rate is 4.3 million bits per second, corresponding to 1 bit each 0.28 µm of track length.
However, this is in conflict with the specified geometry of the CD; each pit must be no shorter than 0.8 µm, and no longer than 3.5 µm.
In other words, each binary one must be part of a group of 3 to 13 ones.
This has the advantage of reducing the error rate due to the optical pickup, but how do you force the binary data to comply with this strange bunching?
The answer is an encoding scheme called eight-to-fourteen modulation (EFM).
Instead of directly storing a byte of data on the disc, the 8 bits are passed through a look-up table that pops out 14 bits.
These 14 bits have the desired bunching characteristics, and are stored on the laser disc.
Upon playback, the binary values read from the disc are passed through the inverse of the EFM look-up table, resulting in each 14 bit group being turned back into the correct 8 bits.
Micron size pits are burned into the surface of the CD to represent ones and zeros.
This results in a data density of 1 bit per µm 2, or one million bits per mm2.
The pit depth is 0.16 µm.
Mbits/sec) ReedSolomon decoding 14 bit Bessel Filter Amplifier Speaker Sample rate converter (×4) 14 bit Bessel Filter Amplifier Speaker 16 bit samples 14 bit samples at 44.1 kHz (706 Kbits/sec) at 176.4 kHz Right channel FIGURE 22- Compact disc playback block diagram.
The digital information is retrieved from the disc with an optical sensor, corrected for EFM and Reed-Solomon encoding, and converted to stereo analog signals.
In addition to EFM, the data are encoded in a format called two-level ReedSolomon coding.
This involves combining the left and right stereo channels along with data for error detection and correction.
Digital errors detected during playback are either: corrected by using the redundant data in the encoding scheme, concealed by interpolating between adjacent samples, or muted by setting the sample value to zero.
These encoding schemes result in the data rate being tripled, i.e., 1.4 Mbits/sec for the stereo audio signals versus 4.3 Mbits/sec stored on the disc.
After decoding and error correction, the audio signals are represented as 16 bit samples at a 44.1 kHz sampling rate.
In the simplest system, these signals could be run through a 16 bit DAC, followed by a low-pass analog filter.
However, this would require high performance analog electronics to pass frequencies below 20 kHz, while rejecting all frequencies above 22.05 kHz, ½ of the sampling rate.
A more common method is to use a multirate technique, that is, convert the digital data to a higher sampling rate before the DAC.
A factor of four is commonly used, converting from 44.1 kHz to 176.4 kHz.
This is called interpolation, and can be explained as a two step process (although it may not actually be carried out this way).
First, three samples with a value of zero are placed between the original samples, producing the higher sampling rate.
In the frequency domain, this has the effect of duplicating the 0 to 22. kHz spectrum three times, at 22.05 to 44.1 kHz, 41 to 66.15 kHz, and 66. to 88.2 kHz.
In the second step, an efficient digital filter is used to remove the newly added frequencies.
The sample rate increase makes the sampling interval smaller, resulting in a smoother signal being generated by the DAC.
The signal still contains frequencies between 20 Hz and 20 kHz; however, the Nyquist frequency has been increased by a factor of four.
This means that the analog filter only needs to pass frequencies below 20 kHz, while blocking frequencies above 88.2 kHz.
This is usually done with a three pole Bessel filter.
Why use a Bessel filter if the ear is insensitive to phase?
Overkill, remember?
Since there are four times as many samples, the number of bits per sample can be reduced from 16 bits to 15 bits, without degrading the sound quality.
The sin(x)/x correction needed to compensate for the zeroth order hold of the DAC can be part of either the analog or digital filter.
Audio systems with more than one channel are said to be in stereo (from the Greek word for solid, or three-dimensional).
Multiple channels send sound to the listener from different directions, providing a more accurate reproduction of the original music.
Music played through a monaural (one channel) system often sounds artificial and bland.
In comparison, a good stereo reproduction makes the listener feel as if the musicians are only a few feet away.
Since the 1960s, high fidelity music has used two channels (left and right), while motion pictures have used four channels (left, right, center, and surround).
In early stereo recordings (say, the Beatles or the Mamas And The Papas), individual singers can often be heard in only one channel or the other.
This rapidly progressed into a more sophisticated mix-down, where the sound from many microphones in the recording studio is combined into the two channels.
Mix-down is an art, aimed at providing the listener with the perception of being there.
The four channel sound used in motion pictures is called Dolby Stereo, with the home version called Dolby Surround Pro Logic.
The four channels are encoded into the standard left and right channels, allowing regular two-channel stereo systems to reproduce the music.
A Dolby decoder is used during playback to recreate the four channels of sound.
The left and right channels, from speakers placed on each side of the movie or television screen, is similar to that of a regular two-channel stereo system.
The speaker for the center channel is usually placed directly above or below the screen.
Its purpose is to reproduce speech and other visually connected sounds, keeping them firmly centered on the screen, regardless of the seating position of the viewer/listener.
The surround speakers are placed to the left and right of the listener, and may involve as many as twenty speakers in a large auditorium.
The surround channel only contains midrange frequencies (say, 100 Hz to 7 kHz), and is delayed by 15 to milliseconds.
This delay makes the listener perceive that speech is coming from the screen, and not the sides.
That is, the listener hears the speech coming from the front, followed by a delayed version of the speech coming from the sides.
The listener's mind interprets the delayed signal as a reflection from the walls, and ignores it.
Companding The data rate is important in telecommunication because it is directly proportional to the cost of transmitting the signal.
Saving bits is the same as saving money.
Companding is a common technique for reducing the data rate of audio signals by making the quantization levels unequal.
As previously mentioned, the loudest sound that can be tolerated (120 dB SPL) is about onemillion times the amplitude of the weakest sound that can be detected (0 dB SPL).
However, the ear cannot distinguish between sounds that are closer than about 1 dB (12% in amplitude) apart.
In other words, there are only about different loudness levels that can be detected, spaced logarithmically over an amplitude range of one-million.
This is important for digitizing audio signals.
If the quantization levels are equally spaced, 12 bits must be used to obtain telephone quality speech.
However, only 8 bits are required if the quantization levels are made unequal, matching the characteristics of human hearing.
This is quite intuitive: if the signal is small, the levels need to be very close together; if the signal is large, a larger spacing can be used.
Companding can be carried out in three ways: (1) run the analog signal through a nonlinear circuit before reaching a linear 8 bit ADC, (2) use an 8 bit ADC that internally has unequally spaced steps, or (3) use a linear 12 bit ADC followed by a digital look-up table (12 bits in, 8 bits out).
Each of these three options requires the same nonlinearity, just in a different place: an analog circuit, an ADC, or a digital circuit.
Two nearly identical standards are used for companding curves: µ255 law (also called mu law), used in North America, and "A" law, used in Europe.
Both use a logarithmic nonlinearity, since this is what converts the spacing detectable by the human ear into a linear spacing.
In equation form, the curves used in µ255 law and "A" law are given by: EQUATION 22- Mu law companding.
This equation provides the nonlinearity for µ255 law companding.
The constant, µ, has a value of 255, accounting for the name of this standard.
The constant, A, has a value of 87.6.
Figure 22-7 graphs these equations for the input variable, x, being between - and +1, resulting in the output variable also assuming values between -1 and +1.
Equations 22-1 and 22-2 only handle positive input values; portions of the curves for negative input values are found from symmetry.
As shown in (a), the curves for µ255 law and "A" law are nearly identical.
The only significant difference is near the origin, shown in (b), where µ255 law is a smooth curve, and "A" law switches to a straight line.
Producing a stable nonlinearity is a difficult task for analog electronics.
One method is to use the logarithmic relationship between current and a. µ law and A law b.
Zoom of zero crossing Output Output Companding curves.
The µ255 law and "A" law companding curves are nearly identical, differing only near the origin.
Companding increases the amplitude when the signal is small, and decreases it when it is large.
Most companding circuits take another strategy: approximate the nonlinearity with a group of straight lines.
A typical scheme is to approximate the logarithmic curve with a group of 16 straight segments, called cords.
The first bit of the 8 bit output indicates if the input is positive or negative.
The next three bits identify which of the 8 positive or 8 negative cords is used.
The last four bits break each cord into 16 equally spaced increments.
As with most integrated circuits, companding chips have sophisticated and proprietary internal designs.
Rather than worrying about what goes on inside of the chip, pay the most attention to the pinout and the specification sheet.
Speech Synthesis and Recognition Computer generation and recognition of speech are formidable problems; many approaches have been tried, with only mild success.
This is an active area of DSP research, and will undoubtedly remain so for many years to come.
You will be very disappointed if you are expecting this section to describe how to build speech synthesis and recognition circuits.
Only a brief introduction to the typical approaches can be presented here.
Before starting, it should be pointed out that most commercial products that produce human sounding speech do not synthesize it, but merely play back a digitally recorded segment from a human speaker.
This approach has great sound quality, but it is limited to the prerecorded words and phrases.
Nearly all techniques for speech synthesis and recognition are based on the model of human speech production shown in Fig. 22-8.
Most human speech sounds can be classified as either voiced or fricative.
Voiced sounds occur when air is forced from the lungs, through the vocal cords, and out of the mouth and/or nose.
The vocal cords are two thin flaps of tissue stretched across Generator unvoiced Digital Filter voiced Pulse train Generator synthetic speech vocal tract response pitch FIGURE 22- Human speech model.
Over a short segment of time, about 2 to 40 milliseconds, speech can be modeled by three parameters: (1) the selection of either a periodic or a noise excitation, (2) the pitch of the periodic excitation, and (3) the coefficients of a recursive linear filter mimicking the vocal tract response.
In response to varying muscle tension, the vocal cords vibrate at frequencies between 50 and 1000 Hz, resulting in periodic puffs of air being injected into the throat.
Vowels are an example of voiced sounds.
In Fig. 22-8, voiced sounds are represented by the pulse train generator, with the pitch (i.e., the fundamental frequency of the waveform) being an adjustable parameter.
In comparison, fricative sounds originate as random noise, not from vibration of the vocal cords.
This occurs when the air flow is nearly blocked by the tongue, lips, and/or teeth, resulting in air turbulence near the constriction.
Fricative sounds include: s, f, sh, z, v, and th.
In the model of Fig. 22-8, fricatives are represented by a noise generator.
Both these sound sources are modified by the acoustic cavities formed from the tongue, lips, mouth, throat, and nasal passages.
Since sound propagation through these structures is a linear process, it can be represented as a linear filter with an appropriately chosen impulse response.
In most cases, a recursive filter is used in the model, with the recursion coefficients specifying the filter's characteristics.
Because the acoustic cavities have dimensions of several centimeters, the frequency response is primarily a series of resonances in the kilohertz range.
In the jargon of audio processing, these resonance peaks are called the formant frequencies.
By changing the relative position of the tongue and lips, the formant frequencies can be changed in both frequency and amplitude.
Figure 22-9 shows a common way to display speech signals, the voice spectrogram, or voiceprint.
The audio signal is broken into short segments, say 2 to 40 milliseconds, and the FFT used to find the frequency spectrum of each segment.
These spectra are placed side-by-side, and converted into a grayscale image (low amplitude becomes light, and high amplitude becomes dark).
This provides a graphical way of observing how the frequency content of speech changes with time.
The segment length is chosen as a tradeoff between frequency resolution (favored by longer segments) and time resolution (favored by shorter segments).
As demonstrated by the a in rain, voiced sounds have a periodic time domain waveform, shown in (a), and a frequency spectrum that is a series of regularly spaced harmonics, shown in (b).
In comparison, the s in storm, shows that fricatives have a noisy time domain signal, as in (c), and a noisy spectrum, displayed in (d).
These spectra also show the shaping by the formant frequencies for both sounds.
Also notice that the time-frequency display of the word rain looks similar both times it is spoken.
Over a short period, say 25 milliseconds, a speech signal can be approximated by specifying three parameters: (1) the selection of either a periodic or random noise excitation, (2) the frequency of the periodic wave (if used), and (3) the coefficients of the digital filter used to mimic the vocal tract response.
Continuous speech can then be synthesized by continually updating these three parameters about 40 times a second.
This approach was responsible for one the early commercial successes of DSP: the Speak & Spell, a widely marketed electronic learning aid for children.
The sound quality of this type of speech synthesis is poor, sounding very mechanical and not quite human.
However, it requires a very low data rate, typically only a few kbits/sec.
This is also the basis for the linear predictive coding (LPC) method of speech compression.
Digitally recorded human speech is broken into short segments, and each is characterized according to the three parameters of the model.
This typically requires about a dozen bytes per segment, or 2 to kbytes/sec.
The segment information is transmitted or stored as needed, and then reconstructed with the speech synthesizer.
Speech recognition algorithms take this a step further by trying to recognize patterns in the extracted parameters.
This typically involves comparing the segment information with templates of previously stored sounds, in an attempt to identify the spoken words.
The problem is, this method does not work very well.
It is useful for some applications, but is far below the capabilities of human listeners.
To understand why speech recognition is so difficult for computers, imagine someone unexpectedly speaking the following sentence: Larger run medical buy dogs fortunate almost when.
Of course, you will not understand the meaning of this sentence, because it has none.
More important, you will probably not even understand all of the individual words that were spoken.
This is basic to the way that humans The rain bow was seen after the rain storm Frequency (kHz) a.
Time domain: a in rain c.
Time domain: s in storm Amplitude Amplitude Time (seconds) Time (milliseconds) Time (milliseconds) b.
Frequency domain: a in rain d.
Frequency domain: s in storm Amplitude Amplitude (×10) Frequency (kHz) Frequency (kHz) FIGURE 22- Voice spectrogram.
The spectrogram of the phrase: "The rainbow was seen after the rain storm."
Figures (a) and (b) shows the time and frequency signals for the voiced a in rain.
Figures (c) and (d) show the time and frequency signals for the fricative s in storm.
Words are recognized by their sounds, but also by the context of the sentence, and the expectations of the listener.
For example, imagine hearing the two sentences: The child wore a spider ring on Halloween.
He was an American spy during the war.
Even if exactly the same sounds were produced to convey the underlined words, listeners hear the correct words for the context.
From your accumulated knowledge about the world, you know that children don't wear secret agents, and people don't become spooky jewelry during wartime.
This usually isn't a conscious act, but an inherent part of human hearing.
Most speech recognition algorithms rely only on the sound of the individual words, and not on their context.
They attempt to recognize words, but not to understand speech.
This places them at a tremendous disadvantage compared to human listeners.
Three annoyances are common in speech recognition systems: (1) The recognized speech must have distinct pauses between the words.
This eliminates the need for the algorithm to deal with phrases that sound alike, but are composed of different words (i.e., spider ring and spy during).
This is slow and awkward for people accustomed to speaking in an overlapping flow.
This means that the algorithm only has to search a limited set to find the best match.
As the vocabulary is made larger, the recognition time and error rate both increase.
This requires each person using the system to speak each word to be recognized, often needing to be repeated five to ten times.
This personalized database greatly increases the accuracy of the word recognition, but it is inconvenient and time consuming.
The prize for developing a successful speech recognition technology is enormous.
Speech is the quickest and most efficient way for humans to communicate.
Speech recognition has the potential of replacing writing, typing, keyboard entry, and the electronic control provided by switches and knobs.
It just needs to work a little better to become accepted by the commercial marketplace.
Progress in speech recognition will likely come from the areas of artificial intelligence and neural networks as much as through DSP itself.
Don't think of this as a technical difficulty; think of it as a technical opportunity.
Nonlinear Audio Processing Digital filtering can improve audio signals in many ways.
For instance, Wiener filtering can be used to separate frequencies that are mainly signal, from frequencies that are mainly noise (see Chapter 17).
Likewise, deconvolution can compensate for an undesired convolution, such as in the restoration of old recordings (also discussed in Chapter 17).
These types of linear techniques are the backbone of DSP.
Several nonlinear techniques are also useful for audio processing.
Two will be briefly described here.
The first nonlinear technique is used for reducing wideband noise in speech signals.
This type of noise includes: magnetic tape hiss, electronic noise in analog circuits, wind blowing by microphones, cheering crowds, etc. Linear filtering is of little use, because the frequencies in the noise completely overlap the frequencies in the voice signal, both covering the range from 200 hertz to 3.2 kHz.
How can two signals be separated when they overlap in both the time domain and the frequency domain?
Here's how it is done.
In a short segment of speech, the amplitude of the frequency components are greatly unequal.
As an example, Fig. 22-10a illustrates the frequency spectrum of a 16 millisecond segment of speech (i.e., 128 samples at an 8 kHz sampling rate).
Most of the signal is contained in a few large amplitude frequencies.
In contrast, (b) illustrates the spectrum when only random noise is present; it is very irregular, but more uniformly distributed at a low amplitude.
Now the key concept: if both signal and noise are present, the two can be partially separated by looking at the amplitude of each frequency.
If the amplitude is large, it is probably mostly signal, and should therefore be retained.
If the amplitude is small, it can be attributed to mostly noise, and should therefore be discarded, i.e., set to zero.
Mid-size frequency components are adjusted in some smooth manner between the two extremes.
Another way to view this technique is as a time varying Wiener filter.
As you recall, the frequency response of the Wiener filter passes frequencies that are mostly signal, and rejects frequencies that are mostly noise.
This a. Signal spectrum b.
Noise spectrum Amplitude Amplitude Frequency Frequency FIGURE 22- Spectra of speech and noise.
While the frequency spectra of speech and noise generally overlap, there is some separation if the signal segment is made short enough.
Figure (a) illustrates the spectrum of a 16 millisecond speech segment, showing that many frequencies carry little speech information, in this particular segment.
Figure (b) illustrates the spectrum of a random noise source; all the components have a small amplitude.
This nonlinear technique uses the same idea, except that the Wiener filter's frequency response is recalculated for each segment, based on the spectrum of that segment.
In other words, the filter's frequency response changes from segment-to-segment, as determined by the characteristics of the signal itself.
One of the difficulties in implementing this (and other) nonlinear techniques is that the overlap-add method for filtering long signals is not valid.
Since the frequency response changes, the time domain waveform of each segment will no longer align with the neighboring segments.
This can be overcome by remembering that audio information is encoded in frequency patterns that change over time, and not in the shape of the time domain waveform.
A typical approach is to divide the original time domain signal into overlapping segments.
After processing, a smooth window is applied to each of the overlapping segments before they are recombined.
This provides a smooth transition of the frequency spectrum from one segment to the next.
The second nonlinear technique is called homomorphic signal processing.
This term literally means: the same structure.
Addition is not the only way that noise and interference can be combined with a signal of interest; multiplication and convolution are also common means of mixing signals together.
If signals are combined in a nonlinear way (i.e., anything other than addition), they cannot be separated by linear filtering.
Homomorphic techniques attempt to separate signals combined in a nonlinear way by making the problem become linear.
That is, the problem is converted to the same structure as a linear system.
For example, consider an audio signal transmitted via an AM radio wave.
As atmospheric conditions change, the received amplitude of the signal increases and decreases, resulting in the loudness of the received audio signal slowly changing over time.
This can be modeled as the audio signal, represented by a[ ], being multiplied by a slowly varying signal, g[ ], that represents the changing gain.
This problem is usually handled in an electronic circuit called an automatic gain control (AGC), but it can also be corrected with nonlinear DSP.
As shown in Fig. 22-11, the input signal, a [ ] ×g [ ], is passed through the logarithm function.
From the identity, log (x y) ' log x % log y, this results in two signals that are combined by addition, i.e., log a [ ] % log g [ ] .
In other words, the logarithm is the homomorphic transform that turns the nonlinear problem of multiplication into the linear problem of addition.
Next, the added signals are separated by a conventional linear filter, that is, some frequencies are passed, while others are rejected.
For the AGC, the gain signal, g[ ], will be composed of very low frequencies, far below the 200 hertz to 3.2 kHz band of the voice signal.
The logarithm of these signals will have more complicated spectra, but the idea is the same: a high-pass filter is used to eliminate the varying gain component from the signal.
Logarithm Linear Filter log a [ ] % log g [ ] AntiLogarithm Homomorphic separation of multiplied signals.
Taking the logarithm of the input signal transforms components that are multiplied into components that are added.
These components can then be separated by linear filtering, and the effect of the logarithm undone.
In effect, log a [ ] % log g [ ] is converted into log a [ ] .
In the last step, the logarithm is undone by using the exponential function (the anti-logarithm, or e x ), producing the desired output signal, a [ ]. Figure 22-12 shows a homomorphic system for separating signals that have been convolved.
An application where this has proven useful is in removing echoes from audio signals.
That is, the audio signal is convolved with an impulse response consisting of a delta function plus a shifted and scaled delta function.
The homomorphic transform for convolution is composed of two stages, the Fourier transform, changing the convolution into a multiplication, followed by the logarithm, turning the multiplication into an addition.
As before, the signals are then separated by linear filtering, and the homomorphic transform undone.
An interesting twist in Fig. 22-12 is that the linear filtering is dealing with frequency domain signals in the same way that time domain signals are usually processed.
In other words, the time and frequency domains have been swapped from their normal use.
For example, if FFT convolution were used to carry out the linear filtering stage, the "spectra" being multiplied would be in the time domain.
This role reversal has given birth to a strange jargon.
For instance, cepstrum (a rearrangment of spectrum) is the Fourier transform of the logarithm of the Fourier transform.
Likewise, there are long-pass and shortpass filters, rather than low-pass and high-pass filters.
Some authors even use the terms Quefrency Alanysis and liftering.
Homomorphic Transform Homomorphic separation of convolved signals.
Components that have been convolved are converted into components that are added by taking the Fourier transform followed by the logarithm.
After linear filtering to separate the added components, the original steps are undone.
Keep in mind that these are simplified descriptions of sophisticated DSP algorithms; homomorphic processing is filled with subtle details.
For example, the logarithm must be able to handle both negative and positive values in the input signal, since this is a characteristic of audio signals.
This requires the use of the complex logarithm, a more advanced concept than the logarithm used in everyday science and engineering.
When the linear filtering is restricted to be a zero phase filter, the complex log is found by taking the simple logarithm of the absolute value of the signal.
After passing through the zero phase filter, the sign of the original signal is reapplied to the filtered signal.
Another problem is aliasing that occurs when the logarithm is taken.
For example, imagine digitizing a continuous sine wave.
In accordance with the sampling theorem, two or more samples per cycle is sufficient.
Now consider digitizing the logarithm of this continuous sine wave.
The sharp corners require many more samples per cycle to capture the waveform, i.e., to prevent aliasing.
The required sampling rate can easily be 100 times as great after the log, as before.
Further, it doesn't matter if the logarithm is applied to the continuous signal, or to its digital representation; the result is the same.
Aliasing will result unless the sampling rate is high enough to capture the sharp corners produced by the nonlinearity.
The result is that audio signals may need to be sampled at 100 kHz or more, instead of only the standard 8 kHz.
Even if these details are handled, there is no guarantee that the linearized signals can be separated by the linear filter.
This is because the spectra of the linearized signals can overlap, even if the spectra of the original signals do not.
For instance, imagine adding two sine waves, one at 1 kHz, and one at 2 kHz.
Since these signals do not overlap in the frequency domain, they can be completely separated by linear filtering.
Now imagine that these two sine waves are multiplied.
Using homomorphic processing, the log is taken of the combined signal, resulting in the log of one sine wave plus the log of the other sine wave.
The problem is, the logarithm of a sine wave contains many harmonics.
Since the harmonics from the two signals overlap, their complete separation is not possible.
In spite of these obstacles, homomorphic processing teaches an important lesson: signals should be processed in a manner consistent with how they are formed.
Put another way, the first step in any DSP task is to understand how information is represented in the signals being processed.
Image Formation & Display Images are a description of how a parameter varies over a surface.
For example, standard visual images result from light intensity variations across a two-dimensional plane.
However, light is not the only parameter used in scientific imaging.
For example, an image can be formed of the temperature of an integrated circuit, blood velocity in a patient's artery, x-ray emission from a distant galaxy, ground motion during an earthquake, etc.
These exotic images are usually converted into conventional pictures (i.e., light images), so that they can be evaluated by the human eye.
This first chapter on image processing describes how digital images are formed and presented to human observers.
Digital Image Structure Figure 23-1 illustrates the structure of a digital image.
This example image is of the planet Venus, acquired by microwave radar from an orbiting space probe.
Microwave imaging is necessary because the dense atmosphere blocks visible light, making standard photography impossible.
The image shown is represented by 40,000 samples arranged in a two-dimensional array of columns by 200 rows.
Just as with one-dimensional signals, these rows and columns can be numbered 0 through 199, or 1 through 200.
In imaging jargon, each sample is called a pixel, a contraction of the phrase: picture element.
Each pixel in this example is a single number between 0 and 255.
When the image was acquired, this number related to the amount of microwave energy being reflected from the corresponding location on the planet's surface.
To display this as a visual image, the value of each pixel is converted into a grayscale, where 0 is black, 255 is white, and the intermediate values are shades of gray.
Images have their information encoded in the spatial domain, the image equivalent of the time domain.
In other words, features in images are represented by edges, not sinusoids.
This means that the spacing and number of pixels are determined by how small of features need to be seen, rather than by the formal constraints of the sampling theorem.
Aliasing can occur in images, but it is generally thought of as a nuisance rather than a major problem.
For instance, pinstriped suits look terrible on television because the repetitive pattern is greater than the Nyquist frequency.
The aliased frequencies appear as light and dark bands that move across the clothing as the person changes position.
A "typical" digital image is composed of about 500 rows by 500 columns.
This is the image quality encountered in television, personal computer applications, and general scientific research.
Images with fewer pixels, say 250 by 250, are regarded as having unusually poor resolution.
This is frequently the case with new imaging modalities; as the technology matures, more pixels are added.
These low resolution images look noticeably unnatural, and the individual pixels can often be seen.
On the other end, images with more than 1000 by 1000 pixels are considered exceptionally good.
This is the quality of the best computer graphics, high-definition television, and 35 mm motion pictures.
There are also applications needing even higher resolution, requiring several thousand pixels per side: digitized x-ray images, space photographs, and glossy advertisements in magazines.
The strongest motivation for using lower resolution images is that there are fewer pixels to handle.
This is not trivial; one of the most difficult problems in image processing is managing massive amounts of data.
For example, one second of digital audio requires about eight kilobytes.
In comparison, one second of television requires about eight Megabytes.
Transmitting a 500 by 500 pixel image over a 33.6 kbps modem requires nearly a minute!
Jumping to an image size of 1000 by 1000 quadruples these problems.
It is common for 256 gray levels (quantization levels) to be used in image processing, corresponding to a single byte per pixel.
There are several reasons for this.
First, a single byte is convenient for data management, since this is how computers usually store data.
Second, the large number of pixels in an image compensate to a certain degree for a limited number of quantization steps.
For example, imagine a group of adjacent pixels alternating in value between digital numbers (DN) 145 and 146.
The human eye perceives the region as a brightness of 145.5.
In other words, images are very dithered.
Third, and most important, a brightness step size of 1/256 (0.39%) is smaller than the eye can perceive.
An image presented to a human observer will not be improved by using more than 256 levels.
However, some images need to be stored with more than 8 bits per pixel.
Remember, most of the images encountered in DSP represent nonvisual parameters.
The acquired image may be able to take advantage of more quantization levels to properly capture the subtle details of the signal.
The point of this is, don't expect to human eye to see all the information contained in these finely spaced levels.
We will consider ways around this problem during a later discussion of brightness and contrast.
The value of each pixel in the digital image represents a small region in the continuous image being digitized.
For example, imagine that the Venus Column Column Column FIGURE 23- Digital image structure.
This example image is the planet Venus, as viewed in reflected microwaves.
Digital images are represented by a two-dimensional array of numbers, each called a pixel.
In this image, the array is 200 rows by columns, with each pixel a number between 0 to 255.
When this image was acquired, the value of each pixel corresponded to the level of reflected microwave energy.
A grayscale image is formed by assigning each of the 0 to 255 values to varying shades of gray.
This defines a square sample spacing and sampling grid, with each pixel representing a 10 meter by 10 meter area.
Now, imagine what happens in a single microwave reflection measurement.
The space probe emits a highly focused burst of microwave energy, striking the surface in, for example, a circular area 15 meters in diameter.
Each pixel therefore contains information about this circular area, regardless of the size of the sampling grid.
This region of the continuous image that contributes to the pixel value is called the sampling aperture.
The size of the sampling aperture is often related to the inherent capabilities of the particular imaging system being used.
For example, microscopes are limited by the quality of the optics and the wavelength of light, electronic cameras are limited by random electron diffusion in the image sensor, and so on.
In most cases, the sampling grid is made approximately the same as the sampling aperture of the system.
Resolution in the final digital image will be limited primary by the larger of the two, the sampling grid or the sampling aperture.
We will return to this topic in Chapter 25 when discussing the spatial resolution of digital images.
Color is added to digital images by using three numbers for each pixel, representing the intensity of the three primary colors: red, green and blue.
Mixing these three colors generates all possible colors that the human eye can perceive.
A single byte is frequently used to store each of the color intensities, allowing the image to capture a total of 256×256×256 = 16. million different colors.
Color is very important when the goal is to present the viewer with a true picture of the world, such as in television and still photography.
However, this is usually not how images are used in science and engineering.
The purpose here is to analyze a two-dimensional signal by using the human visual system as a tool.
Black and white images are sufficient for this.
Cameras and Eyes The structure and operation of the eye is very similar to an electronic camera, and it is natural to discuss them together.
Both are based on two major components: a lens assembly, and an imaging sensor.
The lens assembly captures a portion of the light emanating from an object, and focus it onto the imaging sensor.
The imaging sensor then transforms the pattern of light into a video signal, either electronic or neural.
Figure 23-2 shows the operation of the lens.
In this example, the image of an ice skater is focused onto a screen.
The term focus means there is a oneto-one match of every point on the ice skater with a corresponding point on the screen.
For example, consider a 1 mm × 1 mm region on the tip of the toe.
In bright light, there are roughly 100 trillion photons of light striking this one square millimeter area each second.
Depending on the characteristics of the surface, between 1 and 99 percent of these incident light photons will be reflected in random directions.
Only a small portion of these reflected photons will pass through the lens.
For example, only about one-millionth of the reflected light will pass through a one centimeter diameter lens located 3 meters from the object.
A lens gathers light expanding from a point source, and force it to return to a point at another location.
This allows a lens to project an image onto a surface.
Refraction in the lens changes the direction of the individual photons, depending on the location and angle they strike the glass/air interface.
These direction changes cause light expanding from a single point to return to a single point on the projection screen.
All of the photons that reflect from the toe and pass through the lens are brought back together at the "toe" in the projected image.
In a similar way, a portion of the light coming from any point on the object will pass through the lens, and be focused to a corresponding point in the projected image.
Figures 23-3 and 23-4 illustrate the major structures in an electronic camera and the human eye, respectively.
Both are light tight enclosures with a lens mounted at one end and an image sensor at the other.
The camera is filled with air, while the eye is filled with a transparent liquid.
Each lens system has two adjustable parameters: focus and iris diameter.
If the lens is not properly focused, each point on the object will project to a circular region on the imaging sensor, causing the image to be blurry.
In the camera, focusing is achieved by physically moving the lens toward or away from the imaging sensor.
In comparison, the eye contains two lenses, a bulge on the front of the eyeball called the cornea, and an adjustable lens inside the eye.
The cornea does most of the light refraction, but is fixed in shape and location.
Adjustment to the focusing is accomplished by the inner lens, a flexible structure that can be deformed by the action of the ciliary muscles.
As these muscles contract, the lens flattens to bring the object into a sharp focus.
In both systems, the iris is used to control how much of the lens is exposed to light, and therefore the brightness of the image projected onto the imaging sensor.
The iris of the eye is formed from opaque muscle tissue that can be contracted to make the pupil (the light opening) larger.
The iris in a camera is a mechanical assembly that performs the same function.
The parameters in optical systems interact in many unexpected ways.
For example, consider how the amount of available light and the sensitivity of the light sensor affects the sharpness of the acquired image.
This is because the iris diameter and the exposure time are adjusted to transfer the proper amount of light from the scene being viewed to the image sensor.
If more than enough light is available, the diameter of the iris can be reduced, resulting in a greater depth-of-field (the range of distance from the camera where an object remains in focus).
A greater depth-of-field provides a sharper image when objects are at various distances.
In addition, an abundance of light allows the exposure time to be reduced, resulting in less blur from camera shaking and object motion.
Optical systems are full of these kinds of trade-offs.
An adjustable iris is necessary in both the camera and eye because the range of light intensities in the environment is much larger than can be directly handled by the light sensors.
For example, the difference in light intensities between sunlight and moonlight is about one-million.
Adding to this that reflectance can vary between 1% and 99%, results in a light intensity range of almost one-hundred million.
The dynamic range of an electronic camera is typically 300 to 1000, defined as the largest signal that can be measured, divided by the inherent noise of the device.
Put another way, the maximum signal produced is 1 volt, and the rms noise in the dark is about 1 millivolt.
Typical camera lenses have an iris that change the area of the light opening by a factor of about 300.
This results in a typical electronic camera having a dynamic range of a few hundred thousand.
Clearly, the same camera and lens assembly used in bright sunlight will be useless on a dark night.
In comparison, the eye operates over a dynamic range that nearly covers the large environmental variations.
Surprisingly, the iris is not the main way that this tremendous dynamic range is achieved.
From dark to light, the area of the pupil only changes by a factor of about 20.
The light detecting nerve cells gradually adjust their sensitivity to handle the remaining dynamic range.
For instance, it takes several minutes for your eyes to adjust to the low light after walking into a dark movie theater.
One way that DSP can improve images is by reducing the dynamic range an observer is required to view.
That is, we do not want very light and very dark areas in the same image.
A reflection image is formed from two image signals: the two-dimensional pattern of how the scene is illuminated, multiplied by the two-dimensional pattern of reflectance in the scene.
The pattern of reflectance has a dynamic range of less than 100, because all ordinary materials reflect between 1% and 99% of the incident light.
This is where most of the image information is contained, such as where objects are located in the scene and what their surface characteristics are.
In comparison, the illumination signal depends on the light sources around the objects, but not on the objects themselves.
The illumination signal can have a dynamic range of millions, although 10 to 100 is more typical within a single image.
The illumination signal carries little interesting information, iris focus FIGURE 23- Diagram of an electronic camera.
Focusing is achieved by moving the lens toward or away from the imaging sensor.
The amount of light reaching the sensor is controlled by the iris, a mechanical device that changes the effective diameter of the lens.
The most common imaging sensor in present day cameras is the CCD, a two-dimensional array of light sensitive elements.
The eye is a liquid filled sphere about 3 cm in diameter, enclosed by a tough outer case called the sclera.
Focusing is mainly provided by the cornea, a fixed lens on the front of the eye.
The focus is adjusted by contracting muscles attached to a flexible lens within the eye.
The amount of light entering the eye is controlled by the iris, formed from opaque muscle tissue covering a portion of the lens.
The rear hemisphere of the eye contains the retina, a layer of light sensitive nerve cells that converts the image to a neural signal in the optic nerve.
TO EAR ciliary muscle iris fovea lens cornea TO NOSE (top view) clear liquid retina optic nerve but can degrade the final image by increasing its dynamic range.
DSP can improve this situation by suppressing the illumination signal, allowing the reflectance signal to dominate the image.
The next chapter presents an approach for implementing this algorithm.
The light sensitive surface that covers the rear of the eye is called the retina.
As shown in Fig. 23-5, the retina can be divided into three main layers of specialized nerve cells: one for converting light into neural signals, one for image processing, and one for transferring information to the optic nerve leading to the brain.
In nearly all animals, these layers are seemingly backward.
That is, the light sensitive cells are in last layer, requiring light to pass through the other layers before being detected.
There are two types of cells that detect light: rods and cones, named for their physical appearance under the microscope.
The rods are specialized in operating with very little light, such as under the nighttime sky.
Vision appears very noisy in near darkness, that is, the image appears to be filled with a continually changing grainy pattern.
This results from the image signal being very weak, and is not a limitation of the eye.
There is so little light entering the eye, the random detection of individual photons can be seen.
This is called statistical noise, and is encountered in all low-light imaging, such as military night vision systems.
Chapter 25 will revisit this topic.
Since rods cannot detect color, low-light vision is in black and white.
The cone receptors are specialized in distinguishing color, but can only operate when a reasonable amount of light is present.
There are three types of cones in the eye: red sensitive, green sensitive, and blue sensitive.
This results from their containing different photopigments, chemicals that absorbs different wavelengths (colors) of light.
Figure 23-6 shows the wavelengths of light that trigger each of these three receptors.
This is called RGB encoding, and is how color information leaves the eye through the optic nerve.
The human perception of color is made more complicated by neural processing in the lower levels of the brain.
The RGB encoding is converted into another encoding scheme, where colors are classified as: red or green, blue or yellow, and light or dark.
RGB encoding is an important limitation of human vision; the wavelengths that exist in the environment are lumped into only three broad categories.
In comparison, specialized cameras can separate the optical spectrum into hundreds or thousands of individual colors.
For example, these might be used to classify cells as cancerous or healthy, understand the physics of a distant star, or see camouflaged soldiers hiding in a forest.
Why is the eye so limited in detecting color?
Apparently, all humans need for survival is to find a red apple, among the green leaves, silhouetted against the blue sky.
Rods and cones are roughly 3 µm wide, and are closely packed over the entire 3 cm by 3 cm surface of the retina.
This results in the retina being composed of an array of roughly 10,000 × 10,000 = 100 million receptors.
In comparison, the optic nerve only has about one-million nerve fibers that connect to these cells.
On the average, each optic nerve fiber is connected to roughly 100 light receptors through the connecting layer.
In addition to consolidating information, the connecting layer enhances the image by sharpening edges and suppressing the illumination component of the scene.
This biological image processing will be discussed in the next chapter.
Directly in the center of the retina is a small region called the fovea (Latin for pit), which is used for high resolution vision (see Fig. 23-4).
The fovea is different from the remainder of the retina in several respects.
First, the optic nerve and interconnecting layers are pushed to the side of the fovea, allowing the receptors to be more directly exposed to the incoming light.
This results in the fovea appearing as a small depression in the retina.
Second, only cones are located in the fovea, and they are more tightly packed that in the remainder of the retina.
This absence of rods in the fovea explains why night vision is often better when looking to the side of an object, rather than directly at it.
Third, each optic nerve fiber is influenced by only a few cones, proving good localization ability.
The fovea is surprisingly small.
At normal reading distance, the fovea only sees about a 1 mm diameter area, less than the size of a single letter!
The resolution is equivalent to about a 20×20 grid of pixels within this region.
The retina contains three principle layers: (1) the rod and cone light receptors, (2) an intermediate layer for data reduction and image processing, and (3) the optic nerve fibers that lead to the brain.
The structure of these layers is seemingly backward, requiring light to pass through the other layers before reaching the light receptors.
Human vision overcomes the small size of the fovea by jerky eye movements called saccades.
These abrupt motions allow the high resolution fovea to rapidly scan the field of vision for pertinent information.
In addition, saccades present the rods and cones with a continually changing pattern of light.
This is important because of the natural ability of the retina to adapt to changing levels of light intensity.
In fact, if the eye is forced to remain fixed on the same scene, detail and color begin to fade in a few seconds.
The three types of cones in the human eye respond to different sections of the optical spectrum, roughly corresponding to red, green, and blue.
Combinations of these three form all colors that humans can perceive.
The cones do not have enough sensitivity to be used in low-light environments, where the rods are used to detect the image.
This is why colors are difficult to perceive at night.
The CCD is an integrated circuit that replaced most vacuum tube cameras in the 1980s, just as transistors replaced vacuum tube amplifiers twenty years before.
The heart of the CCD is a thin wafer of blue cones red cones Wavelength (nm) silicon, typically about 1 cm square.
As shown by the cross-sectional view in Fig. 23-7, the backside is coated with a thin layer of metal connected to ground potential.
The topside is covered with a thin electrical insulator, and a repetitive pattern of electrodes.
The most common type of CCD is the three phase readout, where every third electrode is connected together.
The silicon used is called p-type, meaning it has an excess of positive charge carriers called holes.
For this discussion, a hole can be thought of as a positively charged particle that is free to move around in the silicon.
Holes are represented in this figure by the "+" symbol.
In (a), +10 volts is applied to one of the three phases, while the other two are held at 0 volts.
This causes the holes to move away from every third electrode, since positive charges are repelled by a positive voltage.
This forms a region under these electrodes called a well, a shortened version of the physics term: potential well.
Each well in the CCD is a very efficient light sensor.
As shown in (b), a single photon of light striking the silicon converts its energy into the formation of two charged particles, one electron, and one hole.
The hole moves away, leaving the electron stuck in the well, held by the positive voltage on the electrode.
Electrons in this illustration are represented by the "-" symbol.
During the integration period, the pattern of light striking the CCD is transferred into a pattern of charge within the CCD wells.
Dimmer light sources require longer integration periods.
For example, the integration period for standard television is 1/60th of a second, while astrophotography can accumulate light for many hours.
Readout of the electronic image is quite clever; the accumulated electrons in each well are pushed to the output amplifier.
As shown in (c), a positive voltage is placed on two of the phase lines.
This results in each well expanding to the right.
As shown in (d), the next step is to remove the voltage from the first phase, causing the original wells to collapse.
This leaves the accumulated electrons in one well to the right of where they started.
By repeating this pulsing sequence among the three phase lines, the accumulated electrons are pushed to the right until they reach a charge sensitive amplifier.
This is a fancy name for a capacitor followed by a unity gain buffer.
As the electrons are pushed from the last well, they flow onto the capacitor where they produce a voltage.
To achieve high sensitivity, the capacitors are made extremely small, usually less than 1 DF.
This capacitor and amplifier are an integral part of the CCD, and are made on the same piece of silicon.
The signal leaving the CCD is a sequence of voltage levels proportional to the amount of light that has fallen on sequential wells.
Figure 23-8 shows how the two-dimensional image is read from the CCD.
After the integration period, the charge accumulated in each well is moved up the column, one row at a time.
For example, all the wells in row 15 are first moved into row 14, then row 13, then row 12, etc.
Each time the rows are moved up, all the wells in row number 1 are transferred into the horizontal register.
This is a group of specialized CCD wells that rapidly move the charge in a horizontal direction to the charge sensitive amplifier.
Operation of the charge coupled device (CCD).
As shown in this cross-sectional view, a thin sheet of p-type silicon is covered with an insulating layer and an array of electrodes.
The electrodes are connected in groups of three, allowing three separate voltages to be applied: N1, N2, and N3.
When a positive voltage is applied to an electrode, the holes (i.e., the positive charge carriers indicated by the "+") are pushed away.
This results in an area depleted of holes, called a well.
Incoming light generates holes and electrons, resulting in an accumulation of electrons confined to each well (indicated by the "-").
By manipulating the three electrode voltages, the electrons in each well can be moved to the edge of the silicon where a charge sensitive amplifier converts the charge into a voltage.
Architecture of the CCD.
The imaging wells of the CCD are arranged in columns.
During readout, the charge from each well is moved up the column into a horizontal register.
The horizontal register is then readout into the charge sensitive preamplifier.
Notice that this architecture converts a two-dimensional array into a serial data stream in a particular sequence.
The first pixel to be read is at the top-left corner of the image.
The readout then proceeds from left-to-right on the first line, and then continues from left-to-right on subsequent lines.
This is called row major order, and is almost always followed when a two-dimensional array (image) is converted to sequential data.
Television Video Signals Although over 50 years old, the standard television signal is still one of the most common way to transmit an image.
Figure 23-9 shows how the television signal appears on an oscilloscope.
This is called composite video, meaning that there are vertical and horizontal synchronization (sync) pulses mixed with the actual picture information.
These pulses are used in the television receiver to synchronize the vertical and horizontal deflection circuits to match the video being displayed.
Each second of standard video contains 30 complete images, commonly called frames.
A video engineer would say that each frame contains 525 lines, the television jargon for what programmers call rows.
This number is a little deceptive because only to 486 of these lines contain video information; the remaining 39 to 45 lines are reserved for sync pulses to keep the television's circuits synchronized with the video signal.
Standard television uses an interlaced format to reduce flicker in the displayed image.
This means that all the odd lines of each frame are transmitted first, followed by the even lines.
The group of odd lines is called the odd field, and the group of even lines is called the even field.
Since Signal level (volts) Composite video.
The NTSC video signal consists of 30 complete frames (images) per second, with each frame containing 480 to 486 lines of video.
Each frame is broken into two fields, one containing the odd lines and the other containing the even lines.
Each field starts with a group of vertical sync pulses, followed by successive lines of video information separated by horizontal sync pulses.
Each field starts with a complex series of vertical sync pulses lasting 1.3 milliseconds.
This is followed by either the even or odd lines of video.
Each line lasts for 63.5 microseconds, including a 10.2 microsecond horizontal sync pulse, separating one line from the next.
Within each line, the analog voltage corresponds to the grayscale of the image, with brighter values being in the direction away from the sync pulses.
This places the sync pulses beyond the black range.
In video jargon, the sync pulses are said to be blacker than black.
The hardware used for analog-to-digital conversion of video signals is called a frame grabber.
This is usually in the form of an electronics card that plugs into a computer, and connects to a camera through a coaxial cable.
Upon command from software, the frame grabber waits for the beginning of the next frame, as indicated by the vertical sync pulses.
During the following two fields, each line of video is sampled many times, typically 512, 640 or samples per line, at 8 bits per sample.
These samples are stored in memory as one row of the digital image.
This way of acquiring a digital image results in an important difference between the vertical and horizontal directions.
Each row in the digital image corresponds to one line in the video signal, and therefore to one row of wells in the CCD.
Unfortunately, the columns are not so straightforward.
In the CCD, each row contains between about 400 and 800 wells (columns), depending on the particular device used.
When a row of wells is read from the CCD, the resulting line of video is filtered into a smooth analog signal, such as in Fig. 23-9.
In other words, the video signal does not depend on how many columns are present in the CCD.
The resolution in the horizontal direction is limited by how rapidly the analog signal is allowed to change.
This is usually set at 3.2 MHz for color television, resulting in a risetime of about 100 nanoseconds, i.e., about 1/500th of the 53.2 microsecond video line.
When the video signal is digitized in the frame grabber, it is converted back into columns.
However, these columns in the digitized image have no relation to the columns in the CCD.
The number of columns in the digital image depends solely on how many times the frame grabber samples each line of video.
For example, a CCD might have 800 wells per row, while the digitized image might only have 512 pixels (i.e., columns) per row.
The number of columns in the digitized image is also important for another reason.
The standard television image has an aspect ratio of 4 to 3, i.e., it is slightly wider than it is high.
Motion pictures have the wider aspect ratio of 25 to 9. CCDs used for scientific applications often have an aspect ratio of 1 to 1, i.e., a perfect square.
In any event, the aspect ratio of a CCD is fixed by the placement of the electrodes, and cannot be altered.
However, the aspect ratio of the digitized image depends on the number of samples per line.
This becomes a problem when the image is displayed, either on a video monitor or in a hardcopy.
If the aspect ratio isn't properly reproduced, the image looks squashed horizontally or vertically.
The 525 line video signal described here is called NTSC (National Television Systems Committee), a standard defined way back in 1954.
This is the system used in the United States and Japan.
In Europe there are two similar standards called P A L (Phase Alternation by Line) and S E C A M (Sequential Chrominance And Memory).
The basic concepts are the same, just the numbers are different.
Both PAL and SECAM operate with 25 interlaced frames per second, with 625 lines per frame.
Just as with NTSC, some of these lines occur during the vertical sync, resulting in about 576 lines that carry picture information.
Other more subtle differences relate to how color and sound are added to the signal.
The most straightforward way of transmitting color television would be to have three separate analog signals, one for each of the three colors the human eye can detect: red, green and blue.
Unfortunately, the historical development of television did not allow such a simple scheme.
The color television signal was developed to allow existing black and white television sets to remain in use without modification.
This was done by retaining the same signal for brightness information, but adding a separate signal for color information.
In video jargon, the brightness is called the luminance signal, while the color is the chrominance signal.
The chrominance signal is contained on a 3.58 MHz carrier wave added to the black and white video signal.
Sound is added in this same way, on a 4.5 MHz carrier wave.
The television receiver separates these three signals, processes them individually, and recombines them in the final display.
Other Image Acquisition and Display Not all images are acquired an entire frame at a time.
Another very common way is by line scanning.
This involves using a detector containing a onedimensional array of pixels, say, 2048 pixels long by 1 pixel wide.
As an object is moved past the detector, the image is acquired line-by-line.
Line scanning is used by fax machines and airport x-ray baggage scanners.
As a variation, the object can be kept stationary while the detector is moved.
This is very convenient when the detector is already mounted on a moving object, such as an aircraft taking images of the ground beneath it.
The advantage of line scanning is that speed is traded for detector simplicity.
For example, a fax machine may take several seconds to scan an entire page of text, but the resulting image contains thousands of rows and columns.
An even more simplified approach is to acquire the image point-by-point.
For example, the microwave image of Venus was acquired one pixel at a time.
Another example is the scanning probe microscope, capable of imaging individual atoms.
A small probe, often consisting of only a single atom at its tip, is brought exceedingly close to the sample being imaged.
Quantum mechanical effects can be detected between the probe and the sample, allowing the probe to be stopped an exact distance from the sample's surface.
The probe is then moved over the surface of the sample, keeping a constant distance, tracing out the peaks and valleys.
In the final image, each pixel's value represents the elevation of the corresponding location on the sample's surface.
Printed images are divided into two categories: grayscale and halftone.
Each pixel in a grayscale image is a shade of gray between black and white, such as in a photograph.
In comparison, each pixel in a halftone image is formed from many individual dots, with each dot being completely black or completely white.
Shades of gray are produced by alternating various numbers of these black and white dots.
For example, imagine a laser printer with a resolution of 600 dots-per-inch.
To reproduce 256 levels of brightness between black and white, each pixel would correspond to an array of 16 by 16 printable dots.
Black pixels are formed by making all of these 256 dots black.
Likewise, white pixels are formed making all of these 256 dots white.
Midgray has one-half of the dots white and one-half black.
Since the individual dots are too small to be seen when viewed at a normal distance, the eye is fooled into thinking a grayscale has been formed.
Halftone images are easier for printers to handle, including photocopy machines.
The disadvantage is that the image quality is often worse than grayscale pictures.
Brightness and Contrast Adjustments An image must have the proper brightness and contrast for easy viewing.
Brightness refers to the overall lightness or darkness of the image.
Contrast is the difference in brightness between objects or regions.
For example, a white rabbit running across a snowy field has poor contrast, while a black dog against the same white background has good contrast.
Figure 23- shows four possible ways that brightness and contrast can be misadjusted.
When the brightness is too high, as in (a), the whitest pixels are saturated, destroying the detail in these areas.
The reverse is shown in (b), where the brightness is set too low, saturating the blackest pixels.
Figure (c) shows a. Brightness too high b.
Brightness too low c.
Contrast too high d.
Contrast too low FIGURE 23- Brightness and contrast adjustments.
Increasing the brightness makes every pixel in the image becomes lighter.
In comparison, increasing the contrast makes the light areas become lighter, and the dark areas become darker.
These images show the effect of misadjusting the brightness and contrast.
Lastly, (d) has the contrast set too low; all of the pixels are a mid-shade of gray making the objects fade into each other.
Figures 23-11 and 23-12 illustrate brightness and contrast in more detail.
A test image is displayed in Fig. 23-12, using six different brightness and contrast levels.
Figure 23-11 shows the construction of the test image, an array of 80×32 pixels, with each pixel having a value between 0 and 255.
The backgound of the test image is filled with random noise, uniformly distributed between 0 and 255.
The three square boxes have pixel values of 75, 150 and 225, from left-to-right.
Each square contains two triangles with pixel values only slightly different from their surroundings.
In other random 0 to 32 pixels FIGURE 23- Brightness and contrast test image.
This is the structure of the digital image used in Fig. 23-12.
The three squares form dark, medium, and bright objects, each containing two low contrast triangles.
This figure indicates the digital numbers (DN) of the pixels in each region.
Figure 23-12 shows how adjustment of the contrast and brightness allows different features in the image to be visualized.
In (a), the brightness and contrast are set at the normal level, as indicated by the B and C slide bars at the left side of the image.
Now turn your attention to the graph shown with each image, called an output transform, an output look-up table, or a gamma curve.
This controls the hardware that displays the image.
The value of each pixel in the stored image, a number between 0 and 255, is passed through this look-up table to produces another number between 0 and 255.
This new digital number drives the video intensity circuit, with 0 through being transformed into black through white, respectively.
That is, the look-up table maps the stored numbers into the displayed brightness.
Figure (a) shows how the image appears when the output transform is set to do nothing, i.e., the digital output is identical to the digital input.
Each pixel in the noisy background is a random shade of gray, equally distributed between black and white.
The three boxes are displayed as dark, medium and light, clearly distinct from each other.
The problem is, the triangles inside each square cannot be easily seen; the contrast is too low for the eye to distinguished these regions from their surroundings.
Figures (b) & (c) shows the effect of changing the brightness.
Increasing the brightness shifts the output transform to the left, while decreasing the brightness shifts it to the right.
Increasing the brightness makes every pixel in the image appear lighter.
Conversely, decreasing the brightness makes every pixel in the image appear darker.
These changes can improve the viewability of excessively dark or light areas in the image, but will saturate the image if taken too far.
For example, all of the pixels in the far right square in (b) are displayed with full intensity, i.e., 255.
The opposite effect is shown in (c), where all of the pixels in the far left square are displayed as blackest black, or digital number 0. Since all the pixels in these regions have the same value, the triangles are completely wiped out.
Also notice that none of the triangles in (b) and (c) are easier to see than in (a).
Changing the brightness provides little (if any) help in distinguishing low contrast objects from their surroundings.
Figure (d) shows the display optimized to view pixel values around digital number 75.
This is done by turning up the contrast, resulting in the output transform increasing in slope.
For example, the stored pixel values of 71 and 75 become 100 and 116 in the display, making the contrast a factor of four greater.
Pixel values between 46 and 109 are displayed as the blackest black, to the whitest white.
The price for this increased contrast is that pixel values 0 to 45 are saturated at black, and pixel values 110 to 255 are saturated at white.
As shown in (d), the increased contrast allows the triangles in the left square to be seen, at the cost of saturating the middle and right squares. Figure (e) shows the effect of increasing the contrast even further, resulting in only 16 of the possible 256 stored levels being displayed as nonsaturated.
The brightness has also been decreased so that the 16 usable levels are centered on digital number 150.
The details in the center square are now very visible; however, almost everything else in the image is saturated.
For example, look at the noise around the border of the image.
There are very few pixels with an intermediate gray shade; almost every pixel is either pure black or pure white.
This technique of using high contrast to view only a few levels is sometimes called a grayscale stretch.
The contrast adjustment is a way of zooming in on a smaller range of pixel values.
The brightness control centers the zoomed section on the pixel values of interest.
Most digital imaging systems allow the brightness and contrast to be adjusted in just this manner, and often provide a graphical display of the output transform (as in Fig. 23-12).
In comparison, the brightness and contrast controls on television and video monitors are analog circuits, and may operate differently.
For example, the contrast control of a monitor may adjust the gain of the analog signal, while the brightness might add or subtract a DC offset.
The moral is, don't be surprised if these analog controls don't respond in the way you think they should.
Grayscale Transforms The last image, Fig. 23-12f, is different from the rest.
Rather than having a slope in the curve over one range of input values, it has a slope in the curve over two ranges.
This allows the display to simultaneously show the triangles in both the left and the right squares.
Of course, this results in saturation of the pixel values that are not near these digital numbers.
Notice that the slide bars for contrast and brightness are not shown in (f); this display is beyond what brightness and contrast adjustments can provide.
Taking this approach further results in a powerful technique for improving the appearance of images: the grayscale transform.
The idea is to increase the contrast at pixel values of interest, at the expense of the pixel values we don't care about.
This is done by defining the relative importance of each of the to 255 possible pixel values.
The more important the value, the greater its contrast is made in the displayed image.
An example will show a systematic way of implementing this procedure.
Output value Grayscale processing.
Image (a) was acquired with an infrared camera in total darkness.
Brightness in the image is related to the temperature, accounting for the appearance of the warm human body and the hot truck grill.
Image (b) was processed with the manual grayscale transform shown in Fig. 23-14c.
The image in Fig. 23-13a was acquired in total darkness by using a CCD camera that is sensitive in the far infrared.
The parameter being imaged is temperature: the hotter the object, the more infrared energy it emits and the brighter it appears in the image.
This accounts for the background being very black (cold), the body being gray (warm), and the truck grill being white (hot).
These systems are great for the military and police; you can see the other guy when he can't even see himself!
The image in (a) is difficult to view because of the uneven distribution of pixel values.
Most of the image is so dark that details cannot be seen in the scene.
On the other end, the grill is near white saturation.
The histogram of this image is displayed in Fig. 23-14a, showing that the background, human, and grill have reasonably separate values.
In this example, we will increase the contrast in the background and the grill, at the expense of everything else, including the human body.
Figure (b) represents this strategy.
We declare that the lowest pixel values, the background, will have a relative contrast of twelve.
Likewise, the highest pixel values, the grill, will have a relative contrast of six.
The body will have a relative contrast of one, with a staircase transition between the regions.
All these values are determined by trial and error.
The grayscale transform resulting from this strategy is shown in (c), labeled manual.
It is found by taking the running sum (i.e., the discrete integral) of the curve in (b), and then normalizing so that it has a value of 255 at the a. Histogram Desired contrast background b.
Desired contrast human body grill Value of pixel FIGURE 23- Developing a grayscale transform.
Figure (a) is the histogram of the raw image in Fig. 23-13a.
In (b), a curve is manually generated indicating the desired contrast at each pixel value.
The LUT for the output transform is then found by integration and normalization of (b), resulting in the curve labeled manual in (c).
In histogram equalization, the histogram of the raw image, shown in (a), is integrated and normalized to find the LUT, shown in (c).
Value of pixel from histogram Display value Number of pixels manual c.
Output transform Input value right side.
Why take the integral to find the required curve?
Think of it this way: The contrast at a particular pixel value is equal to the slope of the output transform.
That is, we want (b) to be the derivative (slope) of (c).
This means that (c) must be the integral of (b).
Passing the image in Fig. 23-13a through this manually determined grayscale transform produces the image in (b).
The background has been made lighter, the grill has been made darker, and both have better contrast.
These improvements are at the expense of the body's contrast, producing a less detailed image of the intruder (although it can't get much worse than in the original image).
Grayscale transforms can significantly improve the viewability of an image.
The problem is, they can require a great deal of trial and error.
Histogram equalization is a way to automate the procedure.
Notice that the histogram in (a) and the contrast weighting curve in (b) have the same general shape.
Histogram equalization blindly uses the histogram as the contrast weighing curve, eliminating the need for human judgement.
That is, the output transform is found by integration and normalization of the histogram, rather than a manually generated curve.
This results in the greatest contrast being given to those values that have the greatest number of pixels.
Histogram equalization is an interesting mathematical procedure because it maximizes the entropy of the image, a measure of how much information is transmitted by a fixed number of bits.
The fault with histogram equalization is that it mistakes the shear number of pixels at a certain value with the importance of the pixels at that value.
For example, the truck grill and human intruder are the most prominent features in Fig. 23-13.
In spite of this, histogram equalization would almost completely ignore these objects because they contain relatively few pixels.
Histogram equalization is quick and easy.
Just remember, if it doesn't work well, a manually generated curve will probably do much better.
Warping One of the problems in photographing a planet's surface is the distortion from the curvature of the spherical shape.
For example, suppose you use a telescope to photograph a square region near the center of a planet, as illustrated in Fig. 23-15a.
After a few hours, the planet will have rotated on its axis, appearing as in (b).
The previously photographed region appears highly distorted because it is curved near the horizon of the planet.
Each of the two images contain complete information about the region, just from a different perspective.
It is quite common to acquire a photograph such as (a), but really want the image to look like (b), or vice versa.
For example, a satellite mapping the surface of a planet may take thousands of images from straight above, as in (a).
To make a natural looking picture of the entire planet, such as the image of Venus in Fig. 23-1, each image must be distorted and placed in the proper position.
On the other hand, consider a weather satellite looking at a hurricane that is not directly below it.
There is no choice but to acquire the image obliquely, as in (b).
The image is then converted into how it would appear from above, as in (a).
These spatial transformations are called warping.
Space photography is the most common use for warping, but there are others.
For example, many vacuum tube imaging detectors have various amounts of spatial distortion.
This includes night vision cameras used by the military and x-ray detectors used in the medical field.
Digital warping (or dewarping if you prefer) can be used to correct the inherent distortion in these devices.
Special effects artists for motion pictures love to warp images.
For example, a technique called morphing gradually warps one object into another over a series of frames.
This can produces illusions such as a child turning into an adult, or a man turning into a werewolf.
Warping takes the original image (a two-dimensional array) and generates a warped image (another two-dimensional array).
This is done by looping through each pixel in the warped image and asking: What is the proper pixel value that should be placed here?
Given the particular row and column being calculated in the warped image, there is a corresponding row and column in the original image.
The pixel value from the original image is transferred to the warped image to carry out the algorithm.
In the jargon of image processing, the row and column that the pixel comes from in the a.
Normal View b.
Oblique View FIGURE 23- Image warping.
As shown in (a), a normal view of a small section of a planet appears relatively distortion free.
In comparison, an oblique view presents significant spatial distortion.
Warping is the technique of changing one of these images into the other.
Transferring each pixel from the original to the warped image is the easy part.
The hard part is calculating the comes-from address associated with each pixel in the warped image.
This is usually a pure math problem, and can become quite involved.
Simply stretching the image in the horizontal or vertical direction is easier, involving only a multiplication of the row and/or column number to find the comes-from address.
One of the techniques used in warping is subpixel interpolation.
For example, suppose you have developed a set of equations that turns a row and column address in the warped image into the comes-from address in the original image.
Consider what might happen when you try to find the value of the pixel at row 10 and column 20 in the warped image.
You pass the information: row = 10, column = 20, into your equations, and out pops: comes-from row = 20.2,
The point being, your calculations will likely use floating point, and therefore the comes-from addresses will not be integers.
The easiest method to use is the nearest neighbor algorithm, that is, simply round the addresses to the nearest integer.
This is simple, but can produce a very grainy appearance at the edges of objects where pixels may appear to be slightly misplaced.
Bilinear interpolation requires a little more effort, but provides significantly better images.
Figure 23-16 shows how it works.
You know the value of the four pixels around the fractional address, i.e., the value of the pixels at row & 21, and column 14 and 15.
In this example we will assume the pixels values are 91, 210, 162 and 95.
The problem is to interpolate between these four values.
This is done in two steps.
First, interpolate in the horizontal direction between column 14 and 15.
This produces two intermediate values, 150.5 on line 20, and 128.5 on line 21.
Second, interpolate between these intermediate values in the vertical direction.
This produces the bilinear interpolated pixel value of 139.5, which is then transferred to the warped image.
Why interpolate in the horizontal direction and then the vertical direction instead of the reverse?
It doesn't matter; the final answer is the same regardless of which order is used.
Subpixel interpolation.
Subpixel interpolation for image warping is usually accomplished with bilinear interpolation.
As shown in (a), two intermediate values are calculated by linear interpolation in the horizontal direction.
The final value is then found by using linear interpolation in the vertical direction between the intermediate values.
As shown by the three-dimensional illustration in (b), this procedure uniquely defines all values between the four known pixels at each of the corners.
Linear image processing is based on the same two techniques as conventional DSP: convolution and Fourier analysis.
Convolution is the more important of these two, since images have their information encoded in the spatial domain rather than the frequency domain.
Linear filtering can improve images in many ways: sharpening the edges of objects, reducing random noise, correcting for unequal illumination, deconvolution to correct for blur and motion, etc.
These procedures are carried out by convolving the original image with an appropriate filter kernel, producing the filtered image.
A serious problem with image convolution is the enormous number of calculations that need to be performed, often resulting in unacceptably long execution times.
This chapter presents strategies for designing filter kernels for various image processing tasks.
Two important techniques for reducing the execution time are also described: convolution by separability and FFT convolution.
Image convolution works in the same way as one-dimensional convolution.
For instance, images can be viewed as a summation of impulses, i.e., scaled and shifted delta functions.
Likewise, linear systems are characterized by how they respond to impulses; that is, by their impulse responses.
As you should expect, the output image from a system is equal to the input image convolved with the system's impulse response.
The two-dimensional delta function is an image composed of all zeros, except for a single pixel at: row = 0, column = 0, which has a value of one.
For now, assume that the row and column indexes can have both positive and negative values, such that the one is centered in a vast sea of zeros.
When the delta function is passed through a linear system, the single nonzero point will be changed into some other two-dimensional pattern.
Since the only thing that can happen to a point is that it spreads out, the impulse response is often called the point spread function (PSF) in image processing jargon.
The human eye provides an excellent example of these concepts.
As described in the last chapter, the first layer of the retina transforms an image represented as a pattern of light into an image represented as a pattern of nerve impulses.
The second layer of the retina processes this neural image and passes it to the third layer, the fibers forming the optic nerve.
Imagine that the image being projected onto the retina is a very small spot of light in the center of a dark background.
That is, an impulse is fed into the eye.
Assuming that the system is linear, the image processing taking place in the retina can be determined by inspecting the image appearing at the optic nerve.
In other words, we want to find the point spread function of the processing.
We will revisit the assumption about linearity of the eye later in this chapter.
Figure 24-1 outlines this experiment.
Figure (a) illustrates the impulse striking the retina while (b) shows the image appearing at the optic nerve.
The middle layer of the eye passes the bright spike, but produces a circular region of increased darkness.
The eye accomplishes this by a process known as lateral inhibition.
If a nerve cell in the middle layer is activated, it decreases the ability of its nearby neighbors to become active.
When a complete image is viewed by the eye, each point in the image contributes a scaled and shifted version of this impulse response to the image appearing at the optic nerve.
In other words, the visual image is convolved with this PSF to produce the neural image transmitted to the brain.
The obvious question is: how does convolving a viewed image with this PSF improve the ability of the eye to understand the world?
Image at third layer FIGURE 24- The PSF of the eye.
The middle layer of the retina changes an impulse, shown in (a), into an impulse surrounded by a dark area, shown in (b).
This point spread function enhances the edges of objects.
Image processing in the retina results in a slowly changing edge, as in (a), being sharpened, as in (b).
This makes it easier to separate objects in the image, but produces an optical illusion called Mach bands.
Near the edge, the overshoot makes the dark region look darker, and the light region look lighter.
This produces dark and light bands that run parallel to the edge.
Perceived brightness Humans and other animals use vision to identify nearby objects, such as enemies, food, and mates.
This is done by distinguishing one region in the image from another, based on differences in brightness and color.
In other words, the first step in recognizing an object is to identify its edges, the discontinuity that separates an object from its background.
The middle layer of the retina helps this task by sharpening the edges in the viewed image.
As an illustration of how this works, Fig. 24-2 shows an image that slowly changes from dark to light, producing a blurry and poorly defined edge.
Figure (a) shows the intensity profile of this image, the pattern of brightness entering the eye.
Figure (b) shows the brightness profile appearing on the optic nerve, the image transmitted to the brain.
The processing in the retina makes the edge between the light and dark areas appear more abrupt, reinforcing that the two regions are different.
The overshoot in the edge response creates an interesting optical illusion.
Next to the edge, the dark region appears to be unusually dark, and the light region appears to be unusually light.
The resulting light and dark strips are called Mach bands, after Ernst Mach (1838-1916), an Austrian physicist who first described them.
As with one-dimensional signals, image convolution can be viewed in two ways: from the input, and from the output.
From the input side, each pixel in the input image contributes a scaled and shifted version of the point spread function to the output image.
As viewed from the output side, each pixel in the output image is influenced by a group of pixels from the input signal.
For one-dimensional signals, this region of influence is the impulse response flipped left-for-right.
For image signals, it is the PSF flipped left-for-right and topfor-bottom.
Since most of the PSFs used in DSP are symmetrical around the vertical and horizonal axes, these flips do nothing and can be ignored.
Later in this chapter we will look at nonsymmetrical PSFs that must have the flips taken into account.
Figure 24-3 shows several common PSFs.
In (a), the pillbox has a circular top and straight sides.
For example, if the lens of a camera is not properly focused, each point in the image will be projected to a circular spot on the image sensor (look back at Fig. 23-2 and consider the effect of moving the projection screen toward or away from the lens).
In other words, the pillbox is the point spread function of an out-of-focus lens.
The Gaussian, shown in (b), is the PSF of imaging systems limited by random imperfections.
For instance, the image from a telescope is blurred by atmospheric turbulence, causing each point of light to become a Gaussian in the final image.
Image sensors, such as the CCD and retina, are often limited by the scattering of light and/or electrons.
The Central Limit Theorem dictates that a Gaussian blur results from these types of random processes.
The pillbox and Gaussian are used in image processing the same as the moving average filter is used with one-dimensional signals.
An image convolved with these PSFs will appear blurry and have less defined edges, but will be lower in random noise.
These are called smoothing filters, for their action in the time domain, or low-pass filters, for how they treat the frequency domain.
The square PSF, shown in (c), can also be used as a smoothing filter, but it is not circularly symmetric.
This results in the blurring being different in the diagonal directions compared to the vertical and horizontal.
This may or may not be important, depending on the use.
The opposite of a smoothing filter is an edge enhancement or high-pass filter.
The spectral inversion technique, discussed in Chapter 14, is used to change between the two.
As illustrated in (d), an edge enhancement filter kernel is formed by taking the negative of a smoothing filter, and adding a delta function in the center.
The image processing which occurs in the retina is an example of this type of filter.
Figure (e) shows the two-dimensional sinc function.
One-dimensional signal processing uses the windowed-sinc to separate frequency bands.
Since images do not have their information encoded in the frequency domain, the sinc function is seldom used as an imaging filter kernel, although it does find use in some theoretical problems.
The sinc function can be hard to use because its tails decrease very slowly in amplitude ( 1/x ), meaning it must be treated as infinitely wide.
In comparison, the Gaussian's tails decrease very rapidly ( e &x ) and can eventually be truncated with no ill effect.
Common point spread functions.
The pillbox, Gaussian, and square, shown in (a), (b), & (c), are common smoothing (low-pass) filters.
Edge enhancement (high-pass) filters are formed by subtracting a low-pass kernel from an impulse, as shown in (d).
The sinc function, (e), is used very little in image processing because images have their information encoded in the spatial domain, not the frequency domain.
This shift moves the output signal by an equal amount, which is usually of no concern.
In comparison, a shift between the input and output images is generally not acceptable.
Correspondingly, negative indexes are the norm for filter kernels in image processing.
A problem with image convolution is that a large number of calculations are involved.
For instance, when a 512 by 512 pixel image is convolved with a by 64 pixel PSF, more than a billion multiplications and additions are needed (i.e., 64 ×64 ×512 ×512 ).
The long execution times can make the techniques impractical.
Three approaches are used to speed things up.
The first strategy is to use a very small PSF, often only 3×3 pixels.
This is carried out by looping through each sample in the output image, using optimized code to multiply and accumulate the corresponding nine pixels from the input image.
A surprising amount of processing can be achieved with a mere 3×3 PSF, because it is large enough to affect the edges in an image.
The second strategy is used when a large PSF is needed, but its shape isn't critical.
This calls for a filter kernel that is separable, a property that allows the image convolution to be carried out as a series of one-dimensional operations.
This can improve the execution speed by hundreds of times.
The third strategy is FFT convolution, used when the filter kernel is large and has a specific shape.
Even with the speed improvements provided by the highly efficient FFT, the execution time will be hideous.
Let's take a closer look at the details of these three strategies, and examples of how they are used in image processing.
Figure (a) is an image acquired by an airport x-ray baggage scanner.
When this image is convolved with a 3× delta function (a one surrounded by 8 zeros), the image remains unchanged.
While this is not interesting by itself, it forms the baseline for the other filter kernels.
Figure (b) shows the image convolved with a 3×3 kernel consisting of a one, a negative one, and 7 zeros.
This is called the shift and subtract operation, because a shifted version of the image (corresponding to the -1) is subtracted from the original image (corresponding to the 1).
This processing produces the optical illusion that some objects are closer or farther away than the background, making a 3D or embossed effect.
The brain interprets images as if the lighting is from above, the normal way the world presents itself.
If the edges of an object are bright on the top and dark on the bottom, the object is perceived to be poking out from the background.
To see another interesting effect, turn the picture upside down, and the objects will be pushed into the background.
Figure (c) shows an edge detection PSF, and the resulting image.
Every edge in the original image is transformed into narrow dark and light bands that run parallel to the original edge.
Thresholding this image can isolate either the dark or light band, providing a simple algorithm for detecting the edges in an image.
The original image, (a), was acquired on an airport x-ray baggage scanner.
The shift and subtract operation, shown in (b), results in a pseudo three-dimensional effect.
The edge detection operator in (c) removes all contrast, leaving only the edge information.
The edge enhancement filter, (d), adds various ratios of images (a) and (c), determined by the parameter, k.
A value of k = 2 was used to create this image.
A common image processing technique is shown in (d): edge enhancement.
This is sometimes called a sharpening operation.
In (a), the objects have good contrast (an appropriate level of darkness and lightness) but very blurry edges.
In (c), the objects have absolutely no contrast, but very sharp edges.
The strategy is to multiply the image with good edges by a constant, k, and add it to the image with good contrast.
This is equivalent to convolving the original image with the 3×3 PSF shown in (d).
If k is set to 0, the PSF becomes a delta function, and the image is left unchanged.
As k is made larger, the image shows better edge definition.
For the image in (d), a value of k = 2 was used: two parts of image (c) to one part of image (a).
This operation mimics the eye's ability to sharpen edges, allowing objects to be more easily separated from the background.
Convolution with any of these PSFs can result in negative pixel values appearing in the final image.
Even if the program can handle negative values for pixels, the image display cannot.
The most common way around this is to add an offset to each of the calculated pixels, as is done in these images.
An alternative is to truncate out-of-range values.
Convolution by Separability This is a technique for fast convolution, as long as the PSF is separable.
A PSF is said to be separable if it can be broken into two one-dimensional signals: a vertical and a horizontal projection.
Figure 24-5 shows an example of a separable image, the square PSF.
Specifically, the value of each pixel in the image is equal to the corresponding point in the horizontal projection multiplied by the corresponding point in the vertical projection.
In mathematical form: EQUATION 24- Image separation.
An image is referred to as separable if it can be decomposed into horizontal and vertical projections.
Obviously, most images do not satisfy this requirement.
For example, the pillbox is not separable.
There are, however, an infinite number of separable images.
This can be understood by generating arbitrary horizontal and vertical projections, and finding the image that corresponds to them.
For example, Fig. 24-6 illustrates this with profiles that are double-sided exponentials.
The image that corresponds to these profiles is then found from Eq. 24-1.
When displayed, the image appears as a diamond shape that exponentially decays to zero as the distance from the origin increases.
In most image processing tasks, the ideal PSF is circularly symmetric, such as the pillbox.
Even though digitized images are usually stored and processed in the rectangular format of rows and columns, it is desired to modify the image the same in all directions.
This raises the question: is there a PSF that is circularly symmetric and separable?
The answer is, yes, horz[c] FIGURE 24- Separation of the rectangular PSF.
A PSF is said to be separable if it can be decomposed into horizontal and vertical profiles.
Separable PSFs are important because they can be rapidly convolved.
An infinite number of separable PSFs can be generated by defining arbitrary projections, and then calculating the two-dimensional function that corresponds to them.
In this example, the profiles are chosen to be double-sided exponentials, resulting in a diamond shaped PSF.
FIGURE 24- Separation of the Gaussian.
The Gaussian is the only PSF that is circularly symmetric and separable.
This makes it a common filter kernel in image processing.
As is shown in Fig. 24-7, a two-dimensional Gaussian image has projections that are also Gaussians.
The image and projection Gaussians have the same standard deviation.
To convolve an image with a separable filter kernel, convolve each row in the image with the horizontal projection, resulting in an intermediate image.
Next, convolve each column of this intermediate image with the vertical projection of the PSF.
The resulting image is identical to the direct convolution of the original image and the filter kernel.
If you like, convolve the columns first and then the rows; the result is the same.
The convolution of an N×N image with an M×M filter kernel requires a time proportional to N 2M 2 .
In other words, each pixel in the output image depends on all the pixels in the filter kernel.
In comparison, convolution by separability only requires a time proportional to N 2M .
For filter kernels that are hundreds of pixels wide, this technique will reduce the execution time by a factor of hundreds.
Things can get even better.
If you are willing to use a rectangular PSF (Fig. 24-5) or a double-sided exponential PSF (Fig. 24-6), the calculations are even more efficient.
This is because the one-dimensional convolutions are the moving average filter (Chapter 15) and the bidirectional single pole filter (Chapter 19), respectively.
Both of these one-dimensional filters can be rapidly carried out by recursion.
This results in an image convolution time proportional to only N 2, completely independent of the size of the PSF.
In other words, an image can be convolved with as large a PSF as needed, with only a few integer operations per pixel.
For example, the convolution of a 512×512 image requires only a few hundred milliseconds on a personal computer.
That's fast!
Don't like the shape of these two filter kernels?
Convolve the image with one of them several times to approximate a Gaussian PSF (guaranteed by the Central Limit Theorem, Chapter 7).
These are great algorithms, capable of snatching success from the jaws of failure.
They are well worth remembering.
Example of a Large PSF: Illumination Flattening A common application requiring a large PSF is the enhancement of images with unequal illumination.
Convolution by separability is an ideal algorithm to carry out this processing.
With only a few exceptions, the images seen by the eye are formed from reflected light.
This means that a viewed image is equal to the reflectance of the objects multiplied by the ambient illumination.
Figure 24-8 shows how this works.
Figure (a) represents the reflectance of a scene being viewed, in this case, a series of light and dark bands.
Figure (b) illustrates an example illumination signal, the pattern of light falling on (a).
As in the real world, the illumination slowly varies over the imaging area.
Figure (c) is the image seen by the eye, equal to the reflectance image, (a), multiplied by the illumination image, (b).
The regions of poor illumination are difficult to view in (c) for two reasons: they are too dark and their contrast is too low (the difference between the peaks and the valleys).
To understand how this relates to the problem of everyday vision, imagine you are looking at two identically dressed men.
One of them is standing in the bright sunlight, while the other is standing in the shade of a nearby tree.
The percent of the incident light reflected from both men is the same.
For instance, their faces might reflect 80% of the incident light, their gray shirts 40% and their dark pants 5%.
The problem is, the illumination of the two might be, say, ten times different.
This makes the image of the man in the shade ten times darker than the person in the sunlight, and the contrast (between the face, shirt, and pants) ten times less.
The goal of the image processing is to flatten the illumination component in the acquired image.
In other words, we want the final image to be representative of the objects' reflectance, not the lighting conditions.
In terms of Fig. 24-8, given (c), find (a).
This is a nonlinear filtering problem, since the component images were combined by multiplication, not addition.
While this separation cannot be performed perfectly, the improvement can be dramatic.
To start, we will convolve image (c) with a large PSF, one-fifth the size of the entire image.
The goal is to eliminate the sharp features in (c), resulting a. Reflectance c. Viewed image b.
Illumination Brightness Reflectance Illumination Column number Column number Column number FIGURE 24- Model of image formation.
A viewed image, (c), results from the multiplication of an illumination pattern, (b), by a reflectance pattern, (a).
The goal of the image processing is to modify (c) to make it look more like (a).
This is performed in Figs.
This is where convolution by separability is used.
The exact shape of the PSF is not important, only that it is much wider than the features in the reflectance image.
Figure (d) is the result, using a Gaussian filter kernel.
Since a smoothing filter provides an estimate of the illumination image, we will use an edge enhancement filter to find the reflectance image.
That is, image (c) will be convolved with a filter kernel consisting of a delta function minus a Gaussian.
To reduce execution time, this is done by subtracting the smoothed image in (d) from the original image in (c).
Figure (e) shows the result.
It doesn't work!
While the dark areas have been properly lightened, the contrast in these areas is still terrible.
Linear filtering performs poorly in this application because the reflectance and illumination signals were original combined by multiplication, not addition.
Linear filtering cannot correctly separate signals combined by a nonlinear operation.
To separate these signals, they must be unmultiplied.
In other words, the original image should be divided by the smoothed image, as is shown in (f).
This corrects the brightness and restores the contrast to the proper level.
This procedure of dividing the images is closely related to homomorphic processing, previously described in Chapter 22. Homomorphic processing is a way of handling signals combined through a nonlinear operation.
The strategy is to change the nonlinear problem into a linear one, through an appropriate mathematical operation.
When two signals are combined by d.
Smoothed e. (c) - (d) f. (c)÷(d) Brightness Brightness Brightness -0.
Column number Column number Column number FIGURE 24-8 (continued) Figure (d) is a smoothed version of (c), used as an approximation to the illumination signal.
Figure (e) shows an approximation to the reflectance image, created by subtracting the smoothed image from the viewed image.
A better approximation is shown in (f), obtained by the nonlinear process of dividing the two images.
With the identity: log(a×b) ' log(a) % log(b), the problem of separating multiplied signals is converted into the problem of separating added signals.
For example, after taking the logarithm of the image in (c), a linear high-pass filter can be used to isolate the logarithm of the reflectance image.
As before, the quickest way to carry out the high-pass filter is to subtract a smoothed version of the image.
The antilogarithm (exponent) is then used to undo the logarithm, resulting in the desired approximation to the reflectance image.
Which is better, dividing or going along the homomorphic path?
They are nearly the same, since taking the logarithm and subtracting is equal to dividing.
The only difference is the approximation used for the illumination image.
One method uses a smoothed version of the acquired image, while the other uses a smoothed version of the logarithm of the acquired image.
This technique of flattening the illumination signal is so useful it has been incorporated into the neural structure of the eye.
The processing in the middle layer of the retina was previously described as an edge enhancement or high-pass filter.
While this is true, it doesn't tell the whole story.
The first layer of the eye is nonlinear, approximately taking the logarithm of the incoming image.
This makes the eye a homomorphic processor.
Just as described above, the logarithm followed by a linear edge enhancement filter flattens the illumination component, allowing the eye to see under poor lighting conditions.
Another interesting use of homomorphic processing occurs in photography.
The density (darkness) of a negative is equal to the logarithm of the brightness in the final photograph.
This means that any manipulation of the negative during the development stage is a type of homomorphic processing.
Before leaving this example, there is a nuisance that needs to be mentioned.
As discussed in Chapter 6, when an N point signal is convolved with an M point filter kernel, the resulting signal is N%M&1 points long.
Likewise, when an M×M image is convolved with an N×N filter kernel, the result is an ( M%N&1) × ( M%N&1) image.
The problem is, it is often difficult to manage a changing image size.
For instance, the allocated memory must change, the video display must be adjusted, the array indexing may need altering, etc.
The common way around this is to ignore it; if we start with a 512×512 image, we want to end up with a 512×512 image.
The pixels that do not fit within the original boundaries are discarded.
While this keeps the image size the same, it doesn't solve the whole problem; these is still the boundary condition for convolution.
For example, imagine trying to calculate the pixel at the upper-right corner of (d).
This is done by centering the Gaussian PSF on the upper-right corner of (c).
Each pixel in (c) is then multiplied by the corresponding pixel in the overlaying PSF, and the products are added.
The problem is, three-quarters of the PSF lies outside the defined image.
The easiest approach is to assign the undefined pixels a value of zero.
This is how (d) was created, accounting for the dark band around the perimeter of the image.
That is, the brightness smoothly decreases to the pixel value of zero, exterior to the defined image.
Fortunately, this dark region around the boarder can be corrected (although it hasn't been in this example).
This is done by dividing each pixel in (d) by a correction factor.
The correction factor is the fraction of the PSF that was immersed in the input image when the pixel was calculated.
That is, to correct an individual pixel in (d), imagine that the PSF is centered on the corresponding pixel in (c).
For example, the upper-right pixel in (c) results from only 25% of the PSF overlapping the input image.
Therefore, correct this pixel in (d) by dividing it by a factor of 0.25.
This means that the pixels in the center of (d) will not be changed, but the dark pixels around the perimeter will be brightened.
To find the correction factors, imagine convolving the filter kernel with an image having all the pixel values equal to one.
The pixels in the resulting image are the correction factors needed to eliminate the edge effect.
Fourier Image Analysis Fourier analysis is used in image processing in much the same way as with one-dimensional signals.
However, images do not have their information encoded in the frequency domain, making the techniques much less useful.
For example, when the Fourier transform is taken of an audio signal, the confusing time domain waveform is converted into an easy to understand frequency spectrum.
In comparison, taking the Fourier transform of an image converts the straightforward information in the spatial domain into a scrambled form in the frequency domain.
In short, don't expect the Fourier transform to help you understand the information encoded in images.
Likewise, don't look to the frequency domain for filter design.
The basic feature in images is the edge, the line separating one object or region from another object or region.
Since an edge is composed of a wide range of frequency components, trying to modify an image by manipulating the frequency spectrum is generally not productive.
Image filters are normally designed in the spatial domain, where the information is encoded in its simplest form.
Think in terms of smoothing and edge enhancement operations (the spatial domain) rather than high-pass and low-pass filters (the frequency domain).
In spite of this, Fourier image analysis does have several useful properties.
For instance, convolution in the spatial domain corresponds to multiplication in the frequency domain.
This is important because multiplication is a simpler mathematical operation than convolution.
As with one-dimensional signals, this property enables FFT convolution and various deconvolution techniques.
Another useful property of the frequency domain is the Fourier Slice Theorem, the relationship between an image and its projections (the image viewed from its sides).
This is the basis of computed tomography, an x-ray imaging technique widely used medicine and industry.
The frequency spectrum of an image can be calculated in several ways, but the FFT method presented here is the only one that is practical.
The original image must be composed of N rows by N columns, where N is a power of two, i.e., 256, 512, 1024, etc.
If the size of the original image is not a power of two, pixels with a value of zero are added to make it the correct size.
We will call the two-dimensional array that holds the image the real array.
In addition, another array of the same size is needed, which we will call the imaginary array.
The recipe for calculating the Fourier transform of an image is quite simple: take the one-dimensional FFT of each of the rows, followed by the onedimensional FFT of each of the columns.
Specifically, start by taking the FFT of the N pixel values in row 0 of the real array.
The real part of the FFT's output is placed back into row 0 of the real array, while the imaginary part of the FFT's output is placed into row 0 of the imaginary array.
After repeating this procedure on rows 1 through N&1, both the real and imaginary arrays contain an intermediate image.
Next, the procedure is repeated on each of the columns of the intermediate data.
Take the N pixel values from column 0 of the real array, and the N pixel values from column 0 of the imaginary array, and calculate the FFT.
The real part of the FFT's output is placed back into column 0 of the real array, while the imaginary part of the FFT's output is placed back into column 0 of the imaginary array.
After this is repeated on columns 1 through N&1, both arrays have been overwritten with the image's frequency spectrum.
Since the vertical and horizontal directions are equivalent in an image, this algorithm can also be carried out by transforming the columns first and then transforming the rows.
Regardless of the order used, the result is the same.
From the way that the FFT keeps track of the data, the amplitudes of the low frequency components end up being at the corners of the two-dimensional spectrum, while the high frequencies are at the center.
The inverse Fourier transform of an image is calculated by taking the inverse FFT of each row, followed by the inverse FFT of each column (or vice versa).
Figure 24-9 shows an example Fourier transform of an image.
Figure (a) is the original image, a microscopic view of the input stage of a 741 op amp integrated circuit.
Figure (b) shows the real and imaginary parts of the frequency spectrum of this image.
Since the frequency domain can contain negative pixel values, the grayscale values of these images are offset such that negative values are dark, zero is gray, and positive values are light.
The lowfrequency components in an image are normally much larger in amplitude than the high-frequency components.
This accounts for the very bright and dark pixels at the four corners of (b).
Other than this, the spectra of typical images have no discernable order, appearing random.
Of course, images can be contrived to have any spectrum you desire.
As shown in (c), the polar form of an image spectrum is only slightly easier to understand.
The low-frequencies in the magnitude have large positive values (the white corners), while the high-frequencies have small positive values (the black center).
The phase looks the same at low and high-frequencies, appearing to run randomly between -B and B radians. Figure (d) shows an alternative way of displaying an image spectrum.
Since the spatial domain contains a discrete signal, the frequency domain is periodic.
In other words, the frequency domain arrays are duplicated an infinite number of times to the left, right, top and bottom.
For instance, imagine a tile wall, with each tile being the N×N magnitude shown in (c).
Figure (d) is also an N×N section of this tile wall, but it straddles four tiles; the center of the image being where the four tiles touch.
In other words, (c) is the same image as (d), except it has been shifted N/2 pixels horizontally (either left or right) and N/2 pixels vertically (either up or down) in the periodic frequency spectrum.
This brings the bright pixels at the four corners of (c) together in the center of (d). Figure 24-10 illustrates how the two-dimensional frequency domain is organized (with the low-frequencies placed at the corners).
Row N/2 and column N/2 break the frequency spectrum into four quadrants.
For the real part and the magnitude, the upper-right quadrant is a mirror image of the lower-left, while the upper-left is a mirror image of the lower-right.
This symmetry also holds for the imaginary part and the phase, except that the values of the mirrored pixels are opposite in sign.
In other words, every point in the frequency spectrum has a matching point placed symmetrically on the other side of the center of the image (row N/2 and column N/2 ).
One of the points is the positive frequency, while the other is the matching a. Image FIGURE 24- Frequency spectrum of an image.
The example image, shown in (a), is a microscopic photograph of the silicon surface of an integrated circuit.
The frequency spectrum can be displayed as the real and imaginary parts, shown in (b), or as the magnitude and phase, shown in (c).
Figures (b) & (c) are displayed with the low-frequencies at the corners and the high-frequencies at the center.
Since the frequency domain is periodic, the display can be rearranged to reverse these positions.
This is shown in (d), where the magnitude and phase are displayed with the low-frequencies located at the center and the high-frequencies at the corners.
Imaginary Magnitude Magnitude b.
Frequency spectrum displayed in rectangular form (as the real and imaginary parts).
Frequency spectrum displayed in polar form (as the magnitude and phase).
Frequency spectrum displayed in polar form, with the spectrum shifted to place zero frequency at the center.
In equation form, this symmetry is expressed as: EQUATION 24- Symmetry of the two-dimensional frequency domain.
These equations can be used in both formats, when the low-frequencies are displayed at the corners, or when shifting places them at the center.
In polar form, the magnitude has the same symmetry as the real part, while the phase has the same symmetry as the imaginary part.
These equations take into account that the frequency spectrum is periodic, repeating itself every N samples with indexes running from 0 to N&1 .
In other words, X [r,N ] should be interpreted as X [r,0 ], X [N,c ] as X [0,c ], and This symmetry makes four points in the spectrum match with themselves.
These points are located at: [0, 0], [0, N/2], [N/2, 0] and [N/2, N/2] .
Each pair of points in the frequency domain corresponds to a sinusoid in the spatial domain.
As shown in (a), the value at [0, 0] corresponds to the zero frequency sinusoid in the spatial domain, i.e., the DC component of the image.
There is only one point shown in this figure, because this is one of the points that is its own match.
As shown in (b), (c), and (d), other pairs of points correspond to two-dimensional sinusoids that look like waves on the ocean.
One-dimensional sinusoids have a frequency, phase, and amplitude.
Two dimensional sinusoids also have a direction.
The frequency and direction of each sinusoid is determined by the location of the pair of points in the frequency domain.
As shown, draw a line from each point to the zero frequency location at the outside corner of the quadrant that the point is in, i.e., [0, 0], [0,N/2], [N/2, 0], or [N/2,N/2] (as indicated by the circles in this figure).
The direction of this line determines the direction of the spatial sinusoid, while its length is proportional to the frequency of the wave.
This results in the low frequencies being positioned near the corners, and the high frequencies near the center.
When the spectrum is displayed with zero frequency at the center ( Fig. 24-9d), the line from each pair of points is drawn to the DC value at the center of the image, i.e., [ N/2, N/2 ].
This organization is simpler to understand and work with, since all the lines are drawn to the same point.
Another advantage of placing zero at the center is that it matches the frequency spectra of continuous images.
When the spatial domain is continuous, the frequency domain is aperiodic.
This places zero frequency at the center, with the frequency becoming higher in all directions out to infinity.
In general, the frequency spectra of discrete images are displayed with zero frequency at the center whenever people will view the data, in textbooks, journal articles, and algorithm documentation.
However, most calculations are carried out with the computer arrays storing data in the other format (low-frequencies at the corners).
This is because the FFT has this format.
Spatial Domain Frequency Domain column column row row a. column column FIGURE 24- Two-dimensional sinusoids.
Image sine and cosine waves have both a frequency and a direction.
Four examples are shown here.
These spectra are displayed with the lowfrequencies at the corners.
The circles in these spectra show the location of zero frequency.
Even with the FFT, the time required to calculate the Fourier transform is a tremendous bottleneck in image processing.
For example, the Fourier transform of a 512×512 image requires several minutes on a personal computer.
This is roughly 10,000 times slower than needed for real time image processing, 30 frames per second.
This long execution time results from the massive amount of information contained in images.
For comparison, there are about the same number of pixels in a typical image, as there are words in this book.
Image processing via the frequency domain will become more popular as computers become faster.
This is a twentyfirst century technology; watch it emerge!
FFT Convolution Even though the Fourier transform is slow, it is still the fastest way to convolve an image with a large filter kernel.
For example, convolving a 512×512 image with a 50×50 PSF is about 20 times faster using the FFT compared with conventional convolution.
Chapter 18 discusses how FFT convolution works for one-dimensional signals.
The two-dimensional version is a simple extension.
We will demonstrate FFT convolution with an example, an algorithm to locate a predetermined pattern in an image.
Suppose we build a system for inspecting one-dollar bills, such as might be used for printing quality control, counterfeiting detection, or payment verification in a vending machine.
As shown in Fig. 24-11, a 100×100 pixel image is acquired of the bill, centered on the portrait of George Washington.
The goal is to search this image for a known pattern, in this example, the 29×29 pixel image of the face.
The problem is this: given an acquired image and a known pattern, what is the most effective way to locate where (or if) the pattern appears in the image?
If you paid attention in Chapter 6, you know that the solution to this problem is correlation (a matched filter) and that it can be implemented by using convolution.
Before performing the actual convolution, there are two modifications that need to be made to turn the target image into a PSF.
These are illustrated in Fig. 24-12.
Figure (a) shows the target signal, the pattern we are trying to detect.
In (b), the image has been rotated by 180E, the same as being flipped left-forright and then flipped top-for-bottom.
As discussed in Chapter 7, when performing correlation by using convolution, the target signal needs to be reversed to counteract the reversal that occurs during convolution.
We will return to this issue shortly.
The second modification is a trick for improving the effectiveness of the algorithm.
Rather than trying to detect the face in the original image, it is more effective to detect the edges of the face in the edges of the original image.
This is because the edges are sharper than the original features, making the correlation have a sharper peak.
This step isn't required, but it makes the results significantly better.
In the simplest form, a 3×3 edge detection filter is applied to both the original image and the target signal Target detection example.
The problem is to search the 100×100 pixel image of George Washington, (a), for the target pattern, (b), the 29×29 pixel face.
The optimal solution is correlation, which can be carried out by convolution.
From the associative property of convolution, this is the same as applying the edge detection filter to the target signal twice, and leaving the original image alone.
In actual practice, applying the edge detection 3×3 kernel only once is generally sufficient.
This is how (b) is changed into (c) in Fig. 24-12.
This makes (c) the PSF to be used in the convolution Figure 24-13 illustrates the details of FFT convolution.
In this example, we will convolve image (a) with image (b) to produce image (c).
The fact that these images have been chosen and preprocessed to implement correlation is irrelevant; this is a flow diagram of convolution.
The first step is to pad both signals being convolved with enough zeros to make them a power of two in size, and big enough to hold the final image.
For instance, when images of 100×100 and 29×29 pixels are convolved, the resulting image will be 128×128 pixels.
Therefore, enough zeros must be added to (a) and (b) to make them each 128×128 pixels in size.
If this isn't done, circular a. Original b.
Rotated c. Edge detection FIGURE 24- Development of a correlation filter kernel.
The target signal is shown in (a).
In (b) it is rotated by 180E to undo the rotation inherent in convolution, allowing correlation to be performed.
Applying an edge detection filter results in (c), the filter kernel used for this example.
If you are having trouble understanding these concepts, go back and review Chapter 18, where the one-dimensional case is discussed in more detail.
The FFT algorithm is used to transform (a) and (b) into the frequency domain.
This results in four 128×128 arrays, the real and imaginary parts of the two images being convolved.
Multiplying the real and imaginary parts of (a) with the real and imaginary parts of (b), generates the real and imaginary parts of (c).
Taking the Inverse FFT completes the algorithm by producing the final convolved image.
The value of each pixel in a correlation image is a measure of how well the target image matches the searched image at that point.
In this particular example, the correlation image in (c) is composed of noise plus a single bright peak, indicating a good match to the target signal.
Simply locating the brightest pixel in this image would specify the detected coordinates of the face.
If we had not used the edge detection modification on the target signal, the peak would still be present, but much less distinct.
While correlation is a powerful tool in image processing, it suffers from a significant limitation: the target image must be exactly the same size and rotational orientation as the corresponding area in the searched image.
Noise and other variations in the amplitude of each pixel are relatively unimportant, but an exact spatial match is critical.
For example, this makes the method almost useless for finding enemy tanks in military reconnaissance photos, tumors in medical images, and handguns in airport baggage scans.
One approach is to correlate the image multiple times with a variety of shapes and rotations of the target image.
This works in principle, but the execution time will make you loose interest in a hurry.
A Closer Look at Image Convolution Let's use this last example to explore two-dimensional convolution in more detail.
Just as with one dimensional signals, image convolution can be viewed from either the input side or the output side.
As you recall from Chapter 6, the input viewpoint is the best description of how convolution works, while the output viewpoint is how most of the mathematics and algorithms are written.
You need to become comfortable with both these ways of looking at the operation.
Figure 24-14 shows the input side description of image convolution.
Every pixel in the input image results in a scaled and shifted PSF being added to the output image.
The output image is then calculated as the sum of all the contributing PSFs.
This illustration show the contribution to the output image from the point at location [r,c] in the input image.
The PSF is shifted such that pixel [0,0] in the PSF aligns with pixel [r,c] in the output image.
If the PSF is defined with only positive indexes, such as in this example, the shifted PSF will be entirely to the lower-right of [r,c].
Don't Spatial Domain Frequency Domain Flow diagram of FFT image convolution.
The images in (a) and (b) are transformed into the frequency domain by using the FFT.
These two frequency spectra are multiplied, and the Inverse FFT is used to move back into the spatial domain.
In this example, the original images have been chosen and preprocessed to implement correlation through the action of convolution.
Output image Input image column row row column FIGURE 24- Image convolution viewed from the input side.
Each pixel in the input image contributes a scaled and shifted PSF to the output image.
The output image is the sum of these contributions.
The face is inverted in this illustration because this is the PSF we are using.
In the input side view, there is no rotation of the PSF, it is simply shifted.
Image convolution viewed from the output is illustrated in Fig. 24-15.
Each pixel in the output image, such as shown by the sample at [r,c], receives a contribution from many pixels in the input image.
The PSF is rotated by 180E around pixel [0,0], and then shifted such that pixel [0,0] in the PSF is aligned with pixel [r,c] in the input image.
If the PSF only uses positive indexes, it will be to the upper-left of pixel [r,c] in the input image.
The value of the pixel at [r,c] in the output image is found by multiplying the pixels in the rotated PSF with the corresponding pixels in the input image, and summing the products.
This procedure is given by Eq. 24-3, and in the program of Table Notice that the PSF rotation resulting from the convolution has undone the rotation made in the design of the PSF.
This makes the face appear upright in Fig. 24-15, allowing it to be in the same orientation as the pattern being detected in the input image.
That is, we have successfully used convolution to implement correlation.
Compare Fig. 24-13c with Fig. 24-15 to see how the bright spot in the correlation image signifies that the target has been detected.
Image convolution viewed from the output side.
Each pixel in the output signal is equal to the sum of the pixels in the rotated PSF multiplied by the corresponding pixels in the input image.
FFT convolution provides the same output image as the conventional convolution program of Table 24-1.
Is the reduced execution time provided by FFT convolution really worth the additional program complexity?
Let's take a closer look.
Figure 24-16 shows an execution time comparison between conventional convolution using floating point (labeled FP), conventional convolution using integers (labeled INT), and FFT convolution using floating point (labeled FFT).
Data for two different image sizes are presented, 512×512 and 128×128.
First, notice that the execution time required for FFT convolution does not depend on the size of the kernel, resulting in flat lines in this graph.
On a MHz Pentium personal computer, a 128×128 image can be convolved Execution time for image convolution.
This graph shows the execution time on a 100 MHz Pentium processor for three image convolution methods: conventional convolution carried out with floating point math (FP), conventional convolution using integers (INT), and FFT convolution using floating point (FFT).
The two sets of curves are for input image sizes of 512×512 and 128×128 pixels.
Using FFT convolution, the time depends only on the image size, and not the size of the kernel.
In contrast, conventional convolution depends on both the image and the kernel size. in about 15 seconds using FFT convolution, while a 512×512 image requires more than 4 minutes.
Adding up the number of calculations shows that the execution time for FFT convolution is proportional to N 2 Log2(N), for an N×N image.
That is, a 512×512 image requires about 20 times as long as a 128×128 image.
Conventional convolution has an execution time proportional to N 2M 2 for an N×N image convolved with an M×M kernel.
This can be understood by examining the program in Table 24-1.
In other words, the execution time for conventional convolution depends very strongly on the size of the kernel used.
As shown in the graph, FFT convolution is faster than conventional convolution using floating point if the kernel is larger than about 10×10 pixels.
In most cases, integers can be used for conventional convolution, increasing the breakeven point to about 30×30 pixels.
These break-even points depend slightly on the size of the image being convolved, as shown in the graph.
The concept to remember is that FFT convolution is only useful for large filter kernels.
Special Imaging Techniques This chapter presents four specific aspects of image processing.
First, ways to characterize the spatial resolution are discussed.
This describes the minimum size an object must be to be seen in an image.
Second, the signal-to-noise ratio is examined, explaining how faint an object can be and still be detected.
Third, morphological techniques are introduced.
These are nonlinear operations used to manipulate binary images (where each pixel is either black or white).
Fourth, the remarkable technique of computed tomography is described.
This has revolutionized medical diagnosis by providing detailed images of the interior of the human body.
Spatial Resolution Suppose we want to compare two imaging systems, with the goal of determining which has the best spatial resolution.
In other words, we want to know which system can detect the smallest object.
To simplify things, we would like the answer to be a single number for each system.
This allows a direct comparison upon which to base design decisions.
Unfortunately, a single parameter is not always sufficient to characterize all the subtle aspects of imaging.
This is complicated by the fact that spatial resolution is limited by two distinct but interrelated effects: sample spacing and sampling aperture size.
This section contains two main topics: (1) how a single parameter can best be used to characterize spatial resolution, and (2) the relationship between sample spacing and sampling aperture size.
Figure 25-1a shows profiles from three circularly symmetric PSFs: the pillbox, the Gaussian, and the exponential.
These are representative of the PSFs commonly found in imaging systems.
As described in the last chapter, the pillbox can result from an improperly focused lens system.
Likewise, the Gaussian is formed when random errors are combined, such as viewing stars through a turbulent atmosphere.
An exponential PSF is generated when electrons or x-rays strike a phosphor layer and are converted into FWHM versus MTF. Figure (a) shows profiles of three PSFs commonly found in imaging systems: (P) pillbox, (G) Gaussian, and (E) exponential.
Each of these has a FWHM of one unit.
The corresponding MTFs are shown in (b).
Unfortunately, similar values of FWHM do not correspond to similar MTF curves.
This is used in radiation detectors, night vision light amplifiers, and CRT displays.
The exact shape of these three PSFs is not important for this discussion, only that they broadly represent the PSFs seen in real world applications.
The PSF contains complete information about the spatial resolution.
To express the spatial resolution by a single number, we can ignore the shape of the PSF and simply measure its width.
The most common way to specify this is by the Full-Width-at-Half-Maximum (FWHM) value.
For example, all the PSFs in (a) have an FWHM of 1 unit.
Unfortunately, this method has two significant drawbacks.
First, it does not match other measures of spatial resolution, including the subjective judgement of observers viewing the images.
Second, it is usually very difficult to directly measure the PSF.
Imagine feeding an impulse into an imaging system; that is, taking an image of a very small white dot on a black background.
By definition, the acquired image will be the PSF of the system.
The problem is, the measured PSF will only contain a few pixels, and its contrast will be low.
Unless you are very careful, random noise will swamp the measurement.
For instance, imagine that the impulse image is a 512×512 array of all zeros except for a single pixel having a value of 255.
Now compare this to a normal image where all of the 512×512 pixels have an average value of about 128.
In loose terms, the signal in the impulse image is about 100,000 times weaker than a normal image.
No wonder the signal-to-noise ratio will be bad; there's hardly any signal!
A basic theme throughout this book is that signals should be understood in the domain where the information is encoded.
For instance, audio signals should be dealt with in the frequency domain, while image signals should be handled in the spatial domain.
In spite of this, one way to measure image resolution is by looking at the frequency response.
This goes against the fundamental philosophy of this book; however, it is a common method and you need to become familiar with it.
Taking the two-dimensional Fourier transform of the PSF provides the twodimensional frequency response.
If the PSF is circularly symmetric, its frequency response will also be circularly symmetric.
In this case, complete information about the frequency response is contained in its profile.
That is, after calculating the frequency domain via the FFT method, columns 0 to N/ in row 0 are all that is needed.
In imaging jargon, this display of the frequency response is called the Modulation Transfer Function (MTF).
Figure 25-1b shows the MTFs for the three PSFs in (a).
In cases where the PSF is not circularly symmetric, the entire two-dimensional frequency response contains information.
However, it is usually sufficient to know the MTF curves in the vertical and horizontal directions (i.e., columns 0 to N/2 in row 0, and rows to N/2 in column 0).
Take note: this procedure of extracting a row or column from the two-dimensional frequency spectrum is not equivalent to taking the one-dimensional FFT of the profiles shown in (a).
We will come back to this issue shortly.
As shown in Fig. 25-1, similar values of FWHM do not correspond to similar MTF curves.
Figure 25-2 shows a line pair gauge, a device used to measure image resolution via the MTF.
Line pair gauges come in different forms depending on the particular application.
For example, the black and white pattern shown in this figure could be directly used to test video cameras.
For an x-ray imaging system, the ribs might be made from lead, with an x-ray transparent material between.
The key feature is that the black and white lines have a closer spacing toward one end.
When an image is taken of a line pair gauge, the lines at the closely spaced end will be blurred together, while at the other end they will be distinct.
Somewhere in the middle the lines will be just barely separable.
An observer looks at the image, identifies this location, and reads the corresponding resolution on the calibrated scale.
The line pair gauge is a tool used to measure the resolution of imaging systems.
A series of black and white ribs move together, creating a continuum of spatial frequencies.
The resolution of a system is taken as the frequency where the eye can no longer distinguish the individual ribs.
This example line pair gauge is shown several times larger than the calibrated scale indicates.
Pixel value Pixel number The way that the ribs blur together is important in understanding the limitations of this measurement.
Imagine acquiring an image of the line pair gauge in Fig. 25-2.
Figures (a) and (b) show examples of the profiles at low and high spatial frequencies.
At the low frequency, shown in (b), the curve is flat on the top and bottom, but the edges are blurred, At the higher spatial frequency, (a), the amplitude of the modulation has been reduced.
This is exactly what the MTF curve in Fig. 25-1b describes: higher spatial frequencies are reduced in amplitude.
The individual ribs will be distinguishable in the image as long as the amplitude is greater than about 3% to 10% of the original height.
This is related to the eye's ability to distinguish the low contrast difference between the peaks and valleys in the presence of image noise.
A strong advantage of the line pair gauge measurement is that it is simple and fast.
The strongest disadvantage is that it relies on the human eye, and therefore has a certain subjective component.
Even if the entire MTF curve is measured, the most common way to express the system resolution is to quote the frequency where the MTF is reduced to either 3%, 5% or 10%.
Unfortunately, you will not always be told which of these values is being used; product data sheets frequently use vague terms such as "limiting resolution."
Since manufacturers like their specifications to be as good as possible (regardless of what the device actually does), be safe and interpret these ambiguous terms to mean 3% on the MTF curve.
A subtle point to notice is that the MTF is defined in terms of sine waves, while the line pair gauge uses square waves.
That is, the ribs are uniformly dark regions separated by uniformly light regions.
This is done for manufacturing convenience; it is very difficult to make lines that have a sinusoidally varying darkness.
What are the consequences of using a square wave to measure the MTF?
At high spatial frequencies, all frequency components but the fundamental of the square wave have been removed.
This causes the modulation to appear sinusoidal, such as is shown in Fig. 25-2a.
At low frequencies, such as shown in Fig. 25-2b, the wave appears square.
The fundamental sine wave contained in a square wave has an amplitude of 4/B ' 1.27 times the amplitude of the square wave (see Table 13-10).
The result: the line pair gauge provides a slight overestimate of the true resolution of the system, by starting with an effective amplitude of more than pure black to pure white.
Interesting, but almost always ignored.
Since square waves and sine waves are used interchangeably to measure the MTF, a special terminology has arisen.
Instead of the word "cycle," those in imaging use the term line pair (a dark line next to a light line).
For example, a spatial frequency would be referred to as 25 line pairs per millimeter, instead of 25 cycles per millimeter.
The width of the PSF doesn't track well with human perception and is difficult to measure.
The MTF methods are in the wrong domain for understanding how resolution affects the encoded information.
Is there a more favorable alternative?
The answer is yes, the line spread function (LSF) and the edge response.
As shown in Fig. 25-3, the line spread a. Line Spread Function (LSF) b.
Edge Response Full Width at Half Maximum (FWHM) 10% to 90% Edge response FIGURE 25- Line spread function and edge response.
The line spread function (LSF) is the derivative of the edge response.
The width of the LSF is usually expressed as the Full-Width-at-Half-Maximum (FWHM).
The width of the edge response is usually quoted by the 10% to 90% distance.
Similarly, the edge response is how the system responds to a sharp straight discontinuity (an edge).
Since a line is the derivative (or first difference) of an edge, the LSF is the derivative (or first difference) of the edge response.
The single parameter measurement used here is the distance required for the edge response to rise from 10% to 90%.
There are many advantages to using the edge response for measuring resolution.
First, the measurement is in the same form as the image information is encoded.
In fact, the main reason for wanting to know the resolution of a system is to understand how the edges in an image are blurred.
The second advantage is that the edge response is simple to measure because edges are easy to generate in images.
If needed, the LSF can easily be found by taking the first difference of the edge response.
The third advantage is that all common edges responses have a similar shape, even though they may originate from drastically different PSFs.
This is shown in Fig. 25-4a, where the edge responses of the pillbox, Gaussian, and exponential PSFs are displayed.
Since the shapes are similar, the 10%-90% distance is an excellent single parameter measure of resolution.
The fourth advantage is that the MTF can be directly found by taking the one-dimensional FFT of the LSF (unlike the PSF to MTF calculation that must use a twodimensional Fourier transform).
Figure 25-4b shows the MTFs corresponding to the edge responses of (a).
In other words, the curves in (a) are converted into the curves in (b) by taking the first difference (to find the LSF), and then taking the FFT.
Edge response and MTF. Figure (a) shows the edge responses of three PSFs: (P) pillbox, (G) Gaussian, and (E) exponential.
Each edge response has a 10% to 90% rise distance of 1 unit.
Figure (b) shows the corresponding MTF curves, which are similar above the 10% level.
Limiting resolution is a vague term indicating the frequency where the MTF has an amplitude of 3% to 10%.
The fifth advantage is that similar edge responses have similar MTF curves, as shown in Figs.
This allows us to easily convert between the two measurements.
In particular, a system that has a 10%-90% edge response of x distance, has a limiting resolution (10% contrast) of about 1 line pair per x distance.
The units of the "distance" will depend on the type of system being dealt with.
For example, consider three different imaging systems that have 10%-90% edge responses of 0.05 mm, 0.2 milliradian and 3.3 pixels.
The 10% contrast level on the corresponding MTF curves will occur at about: lp/mm, 5 lp/milliradian and 0.33 lp/pixel, respectively.
Figure 25-5 illustrates the mathematical relationship between the PSF and the LSF. Figure (a) shows a pillbox PSF, a circular area of value 1, displayed as white, surrounded by a region of all zeros, displayed as gray.
A profile of the PSF (i.e., the pixel values along a line drawn across the center of the image) will be a rectangular pulse.
Figure (b) shows the corresponding LSF.
As shown, the LSF is mathematically equal to the integrated profile of the PSF.
This is found by sweeping across the image in some direction, as illustrated by the rays (arrows).
Each value in the integrated profile is the sum of the pixel values along the corresponding ray.
In this example where the rays are vertical, each point in the integrated profile is found by adding all the pixel values in each column.
This corresponds to the LSF of a line that is vertical in the image.
The LSF of a line that is horizontal in the image is found by summing all of the pixel values in each row.
For continuous images these concepts are the same, but the summations are replaced by integrals.
As shown in this example, the LSF can be directly calculated from the PSF.
However, the PSF cannot always be calculated from the LSF.
This is because the PSF contains information about the spatial resolution in all directions, while the LSF is limited to only one specific direction.
A system FIGURE 25- Relationship between the PSF and LSF.
A pillbox PSF is shown in (a).
Any row or column through the white center will be a rectangular pulse.
Figure (b) shows the corresponding LSF, equivalent to an integrated profile of the PSF.
That is, the LSF is found by sweeping across the image in some direction and adding (integrating) the pixel values along each ray.
In the direction shown, this is done by adding all the pixels in each column.
For example, imagine a system that has an oblong PSF.
This makes the spatial resolution different in the vertical and horizontal directions, resulting in the LSF being different in these directions.
Measuring the LSF at a single angle does not provide enough information to calculate the complete PSF except in the special instance where the PSF is circularly symmetric.
Multiple LSF measurements at various angles make it possible to calculate a non-circular PSF; however, the mathematics is quite involved and usually not worth the effort.
In fact, the problem of calculating the PSF from a number of LSF measurements is exactly the same problem faced in computed tomography, discussed later in this chapter.
As a practical matter, the LSF and the PSF are not dramatically different for most imaging systems, and it is very common to see one used as an approximation for the other.
This is even more justifiable considering that there are two common cases where they are identical: the rectangular PSF has a rectangular LSF (with the same widths), and the Gaussian PSF has a Gaussian LSF (with the same standard deviations).
These concepts can be summarized into two skills: how to evaluate a resolution specification presented to you, and how to measure a resolution specification of your own.
Suppose you come across an advertisement stating: "This system will resolve 40 line pairs per millimeter."
You should interpret this to mean: "A sinusoid of 40 lp/mm will have its amplitude reduced to 3%-10% of its true value, and will be just barely visible in the image."
You should also do the mental calculation that lp/mm @ 10% contrast is equal to a 10%-90% edge response of 1/( lp/mm) = 0.025 mm.
If the MTF specification is for a 3% contrast level, the edge response will be about 1.5 to 2 times wider.
When you measure the spatial resolution of an imaging system, the steps are carried out in reverse.
Place a sharp edge in the image, and measure the resulting edge response.
The 10%-90% distance of this curve is the best single parameter measurement of the system's resolution.
To keep your boss and the marketing people happy, take the first difference of the edge response to find the LSF, and then use the FFT to find the MTF.
Sample Spacing and Sampling Aperture Figure 25-6 shows two extreme examples of sampling, which we will call a perfect detector and a blurry detector.
Imagine (a) being the surface of an imaging detector, such as a CCD.
Light striking anywhere inside one of the square pixels will contribute only to that pixel value, and no others.
This is shown in the figure by the black sampling aperture exactly filling one of the square pixels.
This is an optimal situation for an image detector, because all of the light is detected, and there is no overlap or crosstalk between adjacent pixels.
In other words, the sampling aperture is exactly equal to the sample spacing.
The alternative example is portrayed in (e).
The sampling aperture is considerably larger than the sample spacing, and it follows a Gaussian distribution.
In other words, each pixel in the detector receives a contribution from light striking the detector in a region around the pixel.
This should sound familiar, because it is the output side viewpoint of convolution.
From the corresponding input side viewpoint, a narrow beam of light striking the detector would contribute to the value of several neighboring pixels, also according to the Gaussian distribution.
Now turn your attention to the edge responses of the two examples.
The markers in each graph indicate the actual pixel values you would find in an image, while the connecting lines show the underlying curve that is being sampled.
An important concept is that the shape of this underlying curve is determined only by the sampling aperture.
This means that the resolution in the final image can be limited in two ways.
First, the underlying curve may have poor resolution, resulting from the sampling aperture being too large.
Second, the sample spacing may be too large, resulting in small details being lost between the samples.
Two edge response curves are presented for each example, illustrating that the actual samples can fall anywhere along the underlying curve.
In other words, the edge being imaged may be sitting exactly upon a pixel, or be straddling two pixels.
Notice that the perfect detector has zero or one sample on the rising part of the edge.
Likewise, the blurry detector has three to four samples on the rising part of the edge.
What is limiting the resolution in these two systems?
The answer is provided by the sampling theorem.
As discussed in Chapter 3, sampling captures all frequency components below one-half of the sampling rate, while higher frequencies are lost due to aliasing.
Now look at the MTF curve in (h).
The sampling aperture of the blurry detector has removed all frequencies greater than one-half the sampling rate; therefore, nothing is lost during sampling.
This means that the resolution of this system is completely limited by the sampling aperture, and not the sample spacing.
Put another way, the sampling aperture has acted as an antialias filter, allowing lossless sampling to take place.
In comparison, the MTF curve in (d) shows that both processes are limiting the resolution of this system.
The high-frequency fall-off of the MTF curve represents information lost due to the sampling aperture.
Since the MTF curve has not dropped to zero before a frequency of 0.5, there is also information lost during sampling, a result of the finite sample spacing.
Which is limiting the resolution more?
It is difficult to answer this question with a number, since they degrade the image in different ways.
Suffice it to say that the resolution in the perfect detector (example 1) is mostly limited by the sample spacing.
While these concepts may seem difficult, they reduce to a very simple rule for practical usage.
Consider a system with some 10%-90% edge response distance, for example 1 mm.
If the sample spacing is greater than 1 mm (there is less than one sample along the edge), the system will be limited by the sample spacing.
If the sample spacing is less than 0.33 mm (there are more than 3 samples along the edge), the resolution will be limited by the sampling aperture.
When a system has 1-3 samples per edge, it will be limited by both factors.
Signal-to-Noise Ratio An object is visible in an image because it has a different brightness than its surroundings.
That is, the contrast of the object (i.e., the signal) must overcome the image noise.
This can be broken into two classes: limitations of the eye, and limitations of the data.
Figure 25-7 illustrates an experiment to measure the eye's ability to detect weak signals.
Depending on the observation conditions, the human eye can detect a minimum contrast of 0.5% to 5%.
In other words, humans can distinguish about 20 to 200 shades of gray between the blackest black and the whitest white.
The exact number depends on a variety of factors, such Contrast detection.
The human eye can detect a minimum contrast of about 0.5 to 5%, depending on the observation conditions.
Minimum detectable SNR.
An object is visible in an image only if its contrast is large enough to overcome the random image noise.
In this example, the three squares have SNRs of 2.0, 1.0 and 0.5 (where the SNR is defined as the contrast of the object divided by the standard deviation of the noise).
The grayscale transform of Chapter 23 can be used to boost the contrast of a selected range of pixel values, providing a valuable tool in overcoming the limitations of the human eye.
The contrast at one brightness level is increased, at the cost of reducing the contrast at another brightness level.
However, this only works when the contrast of the object is not lost in random image noise.
This is a more serious situation; the signal does not contain enough information to reveal the object, regardless of the performance of the eye.
Figure 25-8 shows an image with three squares having contrasts of 5%, 10%, and 20%.
The background contains normally distributed random noise with a standard deviation of about 10% contrast.
The SNR is defined as the contrast divided by the standard deviation of the noise, resulting in the three squares having SNRs of 0.5, 1.0 and 2.0.
In general, trouble begins when the SNR falls below about 1.0.
The exact value for the minimum detectable SNR depends on the size of the object; the larger the object, the easier it is to detect.
To understand this, imagine smoothing the image in Fig. 25-8 with a 3×3 square filter kernel.
This leaves the contrast the same, but reduces the noise by a factor of three (i.e., the square root of the number of pixels in the kernel).
Since the SNR is tripled, lower contrast objects can be seen.
To see fainter objects, the filter kernel can be made even larger.
For example, a 5×5 kernel improves the SNR by a factor of 25 ' 5 .
This strategy can be continued until the filter kernel is equal to the size of the object being detected.
This means the ability to detect an object is proportional to the square-root of its area.
If an object's diameter is doubled, it can be detected in twice as much noise.
Visual processing in the brain behaves in much the same way, smoothing the viewed image with various size filter kernels in an attempt to recognize low contrast objects.
The three profiles in Fig. 25-8 illustrate just how good humans are at detecting objects in noisy environments.
Even though the objects can hardly be identified in the profiles, they are obvious in the image.
To really appreciate the capabilities of the human visual system, try writing algorithms that operate in this low SNR environment.
You'll be humbled by what your brain can do, but your code can't!
Random image noise comes in two common forms.
The first type, shown in Fig. 25-9a, has a constant amplitude.
In other words, dark and light regions in the image are equally noisy.
In comparison, (b) illustrates noise that increases with the signal level, resulting in the bright areas being more noisy than the dark ones.
Both sources of noise are present in most images, but one or the other is usually dominant.
For example, it is common for the noise to decrease as the signal level is decreased, until a plateau of constant amplitude noise is reached.
A common source of constant amplitude noise is the video preamplifier.
All analog electronic circuits produce noise.
However, it does the most harm where the signal being amplified is at its smallest, right at the CCD or other imaging sensor.
Preamplifier noise originates from the random motion of electrons in the transistors.
This makes the noise level depend on how the electronics are designed, but not on the level of the signal being amplified.
For example, a typical CCD camera will have an SNR of about 300 to 1000 ( to 60 dB), defined as the full scale signal level divided by the standard deviation of the constant amplitude noise.
Noise that increases with the signal level results when the image has been represented by a small number of individual particles.
For example, this might be the x-rays passing through a patient, the light photons entering a camera, or the electrons in the well of a CCD.
The mathematics governing these variations are called counting statistics or Poisson statistics.
Suppose that the face of a CCD is uniformly illuminated such that an average of 10,000 electrons are generated in each well.
By sheer chance, some wells will have more electrons, while some will have less.
To be more exact, the number of electrons will be normally distributed with a mean of 10,000, with some standard deviation that describes how much variation there is from Pixel value a. Constant amplitude noise Column number Pixel value b.
Noise dependent on signal level Column number FIGURE 25- Image noise.
Random noise in images takes two general forms.
In (a), the amplitude of the noise remains constant as the signal level changes.
This is typical of electronic noise.
In (b), the amplitude of the noise increases as the square-root of the signal level.
This type of noise originates from the detection of a small number of particles, such as light photons, electrons, or x-rays.
A key feature of Poisson statistics is that the standard deviation is equal to the square-root of the number of individual particles.
That is, if there are N particles in each pixel, the mean is equal to N and the standard deviation is equal to N .
This makes the signal-to-noise ratio equal to N/ N, or simply, N .
In equation form: EQUATION 25- Poisson statistics.
In a Poisson distributed signal, the mean, µ, is the average number of individual particles, N. The standard deviation, F, is equal to the square-root of the average number of individual particles.
The signal-to-noise ratio (SNR) is the mean divided by the standard deviation.
In the CCD example, the standard deviation is 10,000 ' 100 .
Likewise the signal-to-noise ratio is also 10,000 ' 100 .
If the average number of electrons per well is increased to one million, both the standard deviation and the SNR increase to 1,000.
That is, the noise becomes larger as the signal becomes larger, as shown in Fig. 25-9b.
However, the signal is becoming larger faster than the noise, resulting in an overall improvement in the SNR.
Don't be confused into thinking that a lower signal will provide less noise and therefore better information.
Remember, your goal is not to reduce the noise, but to extract a signal from the noise.
This makes the SNR the key parameter.
Many imaging systems operate by converting one particle type to another.
For example, consider what happens in a medical x-ray imaging system.
Within an x-ray tube, electrons strike a metal target, producing x-rays.
After passing through the patient, the x-rays strike a vacuum tube detector known as an image intensifier.
Here the x-rays are subsequently converted into light photons, then electrons, and then back to light photons.
These light photons enter the camera where they are converted into electrons in the well of a CCD.
In each of these intermediate forms, the image is represented by a finite number of particles, resulting in added noise as dictated by Eq. 25-1.
The final SNR reflects the combined noise of all stages; however, one stage is usually dominant.
This is the stage with the worst SNR because it has the fewest particles.
This limiting stage is called the quantum sink.
In night vision systems, the quantum sink is the number of light photons that can be captured by the camera.
The darker the night, the noisier the final image.
Medical x-ray imaging is a similar example; the quantum sink is the number of x-rays striking the detector.
Higher radiation levels provide less noisy images at the expense of more radiation to the patient.
When is the noise from Poisson statistics the primary noise in an image?
It is dominant whenever the noise resulting from the quantum sink is greater than the other sources of noise in the system, such as from the electronics.
For example, consider a typical CCD camera with an SNR of 300.
That is, the noise from the CCD preamplifier is 1/300th of the full scale signal.
An equivalent noise would be produced if the quantum sink of the system contains 90,000 particles per pixel.
If the quantum sink has a smaller number of particles, Poisson noise will dominate the system.
If the quantum sink has a larger number of particles, the preamplifier noise will be predominant.
Accordingly, most CCD's are designed with a full well capacity of 100,000 to 1,000,000 electrons, minimizing the Poisson noise.
Morphological Image Processing The identification of objects within an image can be a very difficult task.
One way to simplify the problem is to change the grayscale image into a binary image, in which each pixel is restricted to a value of either 0 or 1.
The techniques used on these binary images go by such names as: blob analysis, connectivity analysis, and morphological image processing (from the Greek word morphe, meaning shape or form).
The foundation of morphological processing is in the mathematically rigorous field of set theory; however, this level of sophistication is seldom needed.
Most morphological algorithms are simple logic operations and very ad hoc.
In Morphological operations.
Four basic morphological operations are used in the processing of binary images: erosion, dilation, opening, and closing.
Figure (a) shows an example binary image.
Figures (b) to (e) show the result of applying these operations to the image in (a).
This is usually more of an art than a science.
A bag of tricks is used rather than standard algorithms and formal mathematical properties.
Here are some examples.
Figure 25-10a shows an example binary image.
This might represent an enemy tank in an infrared image, an asteroid in a space photograph, or a suspected tumor in a medical x-ray.
Each pixel in the background is displayed as white, while each pixel in the object is displayed as black.
Frequently, binary images are formed by thresholding a grayscale image; pixels with a value greater than a threshold are set to 1, while pixels with a value below the threshold are set to 0. It is common for the grayscale image to be processed with linear techniques before the thresholding.
For instance, illumination flattening (described in Chapter 24) can often improve the quality of the initial binary image.
Figures (b) and (c) show how the image is changed by the two most common morphological operations, erosion and dilation.
In erosion, every object pixel that is touching a background pixel is changed into a background pixel.
In dilation, every background pixel that is touching an object pixel is changed into an object pixel.
Erosion makes the objects smaller, and can break a single object into multiple objects.
Dilation makes the objects larger, and can merge multiple objects into one.
As shown in (d), opening is defined as an erosion followed by a dilation.
Figure (e) shows the opposite operation of closing, defined as a dilation followed by an erosion.
As illustrated by these examples, opening removes small islands and thin filaments of object pixels.
Likewise, closing removes islands and thin filaments of background pixels.
These techniques are useful for handling noisy images where some pixels have the wrong binary value.
For instance, it might be known that an object cannot contain a "hole", or that the object's border must be smooth.
Figure 25-11 shows an example of morphological processing.
Figure (a) is the binary image of a fingerprint.
Algorithms have been developed to analyze these patterns, allowing individual fingerprints to be matched with those in a database.
A common step in these algorithms is shown in (b), an operation called skeletonization.
This simplifies the image by removing redundant pixels; that is, changing appropriate pixels from black to white.
This results in each ridge being turned into a line only a single pixel wide.
Tables 25-1 and 25-2 show the skeletonization program.
Even though the fingerprint image is binary, it is held in an array where each pixel can run from 0 to 255.
A black pixel is denoted by 0, while a white pixel is denoted by 255.
As shown in Table 25-1, the algorithm is composed of 6 iterations that gradually erode the ridges into a thin line.
The number of iterations is chosen by trial and error.
An alternative would be to stop when an iteration makes no changes.
During an iteration, each pixel in the image is evaluated for being removable; the pixel meets a set of criteria for being changed from black to white.
Lines 200-240 loop through each pixel in the image, while the subroutine in Table 25-2 makes the evaluation.
If the pixel under consideration is not removable, the subroutine does nothing.
If the pixel is removable, the subroutine changes its value from 0 to 1.
This indicates that the pixel is still black, but will be changed to white at the end of the iteration.
After all the pixels have been evaluated, lines 260-300 change the value of the marked pixels from 1 to 255.
This two-stage process results in the thick ridges being eroded equally from all directions, rather than a pattern based on how the rows and columns are scanned.
Skeletonized fingerprint FIGURE 25- Binary skeletonization.
The binary image of a fingerprint, (a), contains ridges that are many pixels wide.
The skeletonized version, (b), contains ridges only a single pixel wide.
The decision to remove a pixel is based on four rules, as contained in the subroutine shown in Table 25-2.
All of these rules must be satisfied for a pixel to be changed from black to white.
The first three rules are rather simple, while the fourth is quite complicated.
As shown in Fig. 25-12a, a pixel at location [R,C] has eight neighbors.
The four neighbors in the horizontal and vertical directions (labeled 2,4,6,8) are frequently called the close neighbors.
The diagonal pixels (labeled 1,3,5,7) are correspondingly called the distant neighbors.
The four rules are as follows: Rule one: The pixel under consideration must presently be black.
If the pixel is already white, no action needs to be taken.
Rule two: At least one of the pixel's close neighbors must be white.
This insures that the erosion of the thick ridges takes place from the outside.
In other words, if a pixel is black, and it is completely surrounded by black pixels, it is to be left alone on this iteration.
Why use only the close neighbors, rather than all of the neighbors?
The answer is simple: running the algorithm both ways shows that it works better.
Remember, this is very common in morphological image processing; trial and error is used to find if one technique performs better than another.
Rule three: The pixel must have more than one black neighbor.
If it has only one, it must be the end of a line, and therefore shouldn't be removed.
Rule four: A pixel cannot be removed if it results in its neighbors being disconnected.
This is so each ridge is changed into a continuous line, not a group of interrupted segments.
As shown by the examples in Fig. 25-12, connected means that all of the black neighbors touch each other.
Likewise, unconnected means that the black neighbors form two or more groups.
The algorithm for determining if the neighbors are connected or unconnected is based on counting the black-to-white transitions between adjacent neighboring pixels, in a clockwise direction.
For example, if pixel 1 is black and pixel 2 is white, it is considered a black-to-white transition.
Likewise, if pixel 2 is black and both pixel 3 and 4 are white, this is also a black-to-white transition.
In total, there are eight locations where a black-to-white transition may occur.
To illustrate this definition further, the examples in (b) and (c) have an asterisk placed by each black-to-white transition.
The key to this algorithm is that there will be zero or one black-to-white transition if the neighbors are connected.
More than one such transition indicates that the neighbors are unconnected.
As additional examples of binary image processing, consider the types of algorithms that might be useful after the fingerprint is skeletonized.
A disadvantage of this particular skeletonization algorithm is that it leaves a considerable amount of fuzz, short offshoots that stick out from the sides of longer segments.
There are several different approaches for eliminating these artifacts.
For example, a program might loop through the image removing the pixel at the end of every line.
These pixels are identified by having only one black neighbor.
Do this several times and the fuzz is removed at the expense of making each of the correct lines shorter.
A better method would loop through the image identifying branch pixels (pixels that have more than two neighbors).
Starting with each branch pixel, count the number of pixels in each offshoot.
If the number of pixels in an offshoot is less than some value (say, 5), declare it to be fuzz, and change the pixels in the branch from black to white.
Another algorithm might change the data from a bitmap to a vector mapped format.
This involves creating a list of the ridges contained in the image and the pixels contained in each ridge.
In the vector mapped form, each ridge in the fingerprint has an individual identity, as opposed to an image composed of many unrelated pixels.
This can be accomplished by looping through the image looking for the endpoints of each line, the pixels that have only one black neighbor.
Starting from the endpoint, each line is traced from pixel to connecting pixel.
After the opposite end of the line is reached, all the traced pixels are declared to be a single object, and treated accordingly in future algorithms.
Computed Tomography A basic problem in imaging with x-rays (or other penetrating radiation) is that a two-dimensional image is obtained of a three-dimensional object.
This means that structures can overlap in the final image, even though they are completely separate in the object.
This is particularly troublesome in medical diagnosis where there are many anatomic structures that can interfere with what the physician is trying to see.
During the 1930's, this problem was attacked by moving the x-ray source and detector in a coordinated motion during image formation.
From the geometry of this motion, a single plane within the patient remains in focus, while structures outside this plane become blurred.
This is analogous to a camera being focused on an object at 5 feet, while objects at a distance of 1 and 50 feet are blurry.
These related techniques based on motion blurring are now collectively called classical tomography.
The word tomography means "a picture of a plane."
In spite of being well developed for more than 50 years, classical tomography is rarely used.
This is because it has a significant limitation: the interfering objects are not removed from the image, only blurred.
The resulting image quality is usually too poor to be of practical use.
The long sought solution was a system that could create an image representing a 2D slice through a 3D object with no interference from other structures in the 3D object.
This problem was solved in the early 1970s with the introduction of a technique called computed tomography (CT).
CT revolutionized the medical x-ray field with its unprecedented ability to visualize the anatomic structure of the body.
Figure 25-13 shows a typical medical CT image.
Computed tomography was originally introduced to the marketplace under the names Computed Axial Tomography and CAT scanner.
These terms are now frowned upon in the medical field, although you hear them used frequently by the general public.
Figure 25-14 illustrates a simple geometry for acquiring a CT slice through the center of the head.
A narrow pencil beam of x-rays is passed from the x-ray source to the x-ray detector.
This means that the measured value at the detector is related to the total amount of material placed anywhere along the beam's path.
Materials such as bone and teeth block more of the xrays, resulting in a lower signal compared to soft tissue and fat.
As shown in the illustration, the source and detector assemblies are translated to acquire a view (CT jargon) at this particular angle.
While this figure shows only a single view being acquired, a complete CT scan requires 300 to 1000 views taken at rotational increments of about 0.3E to 1.0E.
This is accomplished by mounting the x-ray source and detector on a rotating gantry that surrounds the patient.
A key feature of CT data acquisition is that x-rays pass only through the slice of the body being examined.
This is unlike classical tomography where x-rays are passing through structures that you try to suppress in the final image.
Computed tomography doesn't allow information from irrelevant locations to even enter the acquired data.
A simple CT system passes a narrow beam of x-rays through the body from source to detector.
The source and detector are then translated to obtain a complete view.
The remaining views are obtained by rotating the source and detector in about 1 E increments, and repeating the translation process.
For instance, the logarithm must be taken of each x-ray measurement.
This is because x-rays decrease in intensity exponentially as they pass through material.
Taking the logarithm provides a signal that is linearly related to the characteristics of the material being measured.
Other preprocessing steps are used to compensate for the use of polychromatic (more than one energy) x-rays, and multielement detectors (as opposed to the single element shown in Fig. 25-14).
While these are a key step in the overall technique, they are not related to the reconstruction algorithms and we won't discuss them further.
Figure 25-15 illustrates the relationship between the measured views and the corresponding image.
Each sample acquired in a CT system is equal to the sum of the image values along a ray pointing to that sample.
For example, view is found by adding all the pixels in each row.
Likewise, view 3 is found by adding all the pixels in each column.
The other views, such as view 2, sum the pixels along rays that are at an angle.
There are four main approaches to calculating the slice image given the set of its views.
These are called CT reconstruction algorithms.
The first method is totally impractical, but provides a better understanding of the problem.
It is based on solving many simultaneous linear equations.
One equation can be written for each measurement.
That is, a particular sample in a particular profile is the sum of a particular group of pixels in the image.
To calculate N unknown variables (i.e., the image pixel values), there must be N independent equations, and therefore N 2 measurements.
Most CT scanners acquire about 50% more samples than rigidly required by this analysis.
For example, to reconstruct a 512×512 image, a system might take 700 views with 600 samples in each view.
By making the problem overdetermined in this manner, the final image has reduced noise and artifacts.
The problem with this first method of CT reconstruction is computation time.
Solving several hundred thousand simultaneous linear equations is an daunting task.
The second method of CT reconstruction uses iterative techniques to calculate the final image in small steps.
There are several variations of this method: the Algebraic Reconstruction Technique (ART), Simultaneous Iterative Reconstruction Technique (SIRT), and Iterative Least Squares Technique (ILST).
The difference between these methods is how the successive corrections are made: ray-by-ray, pixel-by-pixel, or simultaneously correcting the entire data set, respectively.
As an example of these techniques, we will look at ART.
To start the ART algorithm, all the pixels in the image array are set to some arbitrary value.
An iterative procedure is then used to gradually change the image array to correspond to the profiles.
An iteration cycle consists of looping through each of the measured data points.
For each measured value, the following question is asked: how can the pixel values in the array be changed to make them consistent with this particular measurement?
In other words, the measured sample is compared with the view view FIGURE 25- CT views.
Computed tomography acquires a set of views and then reconstructs the corresponding image.
Each sample in a view is equal to the sum of the image values along the ray that points to that sample.
In this example, the image is a small pillbox surrounded by zeros.
While only three views are shown here, a typical CT scan uses hundreds of views at slightly different angles.
If the ray sum is lower than the measured sample, all the pixels along the ray are increased in value.
Likewise, if the ray sum is higher than the measured sample, all of the pixel values along the ray are decreased.
After the first complete iteration cycle, there will still be an error between the ray sums and the measured values.
This is because the changes made for any one measurement disrupts all the previous corrections made.
The idea is that the errors become smaller with repeated iterations until the image converges to the proper solution.
Iterative techniques are generally slow, but they are useful when better algorithms are not available.
In fact, ART was used in the first commercial medical CT scanner released in 1972, the EMI Mark I. We will revisit iterative techniques in the next chapter on neural networks.
Development of the third and forth methods have almost entirely replaced iterative techniques in commercial CT products.
The last two reconstruction algorithms are based on formal mathematical solutions to the problem.
These are elegant examples of DSP.
The third method is called filtered backprojection.
It is a modification of an older view view a.
Using 3 views b.
Using many views FIGURE 25- Backprojection.
Backprojection reconstructs an image by taking each view and smearing it along the path it was originally acquired.
The resulting image is a blurry version of the correct image.
Figure 25- shows that simple backprojection is a common sense approach, but very unsophisticated.
An individual sample is backprojected by setting all the image pixels along the ray pointing to the sample to the same value.
In less technical terms, a backprojection is formed by smearing each view back through the image in the direction it was originally acquired.
The final backprojected image is then taken as the sum of all the backprojected views.
While backprojection is conceptually simple, it does not correctly solve the problem.
As shown in (b), a backprojected image is very blurry.
A single point in the true image is reconstructed as a circular region that decreases in intensity away from the center.
In more formal terms, the point spread function of backprojection is circularly symmetric, and decreases as the reciprocal of its radius.
Filtered backprojection is a technique to correct the blurring encountered in simple backprojection.
As illustrated in Fig. 25-17, each view is filtered before the backprojection to counteract the blurring PSF.
That is, each of the one-dimensional views is convolved with a one-dimensional filter kernel to create a set of filtered views.
These filtered views are then backprojected to provide the reconstructed image, a close approximation to the "correct" image.
In fact, the image produced by filtered backprojection is identical filtered view filtered view filtered view a.
Using 3 views b.
Using many views FIGURE 25- Filtered backprojection.
Filtered backprojection reconstructs an image by filtering each view before backprojection.
This removes the blurring seen in simple backprojection, and results in a mathematically exact reconstruction of the image.
Filtered backprojection is the most commonly used algorithm for computed tomography systems.
The filter kernel used in this technique will be discussed shortly.
For now, notice how the profiles have been changed by the filter.
The image in this example is a uniform white circle surrounded by a black background (a pillbox).
Each of the acquired views has a flat background with a rounded region representing the white circle.
Filtering changes the views in two significant ways.
First, the top of the pulse is made flat, resulting in the final backprojection creating a uniform signal level within the circle.
Second, negative spikes have been introduced at the sides of the pulse.
When backprojected, these negative regions counteract the blur.
The fourth method is called Fourier reconstruction.
In the spatial domain, CT reconstruction involves the relationship between a two-dimensional image and its set of one-dimensional views.
By taking the two-dimensional Fourier transform of the image and the one-dimensional Fourier transform of each of its views, the problem can be examined in the frequency domain.
As it turns out, the relationship between an image and its views is far simpler in the frequency domain than in the spatial domain.
The frequency domain analysis of this problem is a milestone in CT technology called the Fourier slice theorem.
Figure 25-18 shows how the problem looks in both the spatial and the frequency domains.
In the spatial domain, each view is found by integrating the image along rays at a particular angle.
In the frequency domain, the image spectrum is represented in this illustration by a two-dimensional grid.
The spectrum of each view (a one-dimensional signal) is represented by a dark line superimposed on the grid.
As shown by the positioning of the lines on the grid, the Fourier slice theorem states that the spectrum of a view is identical to the values along a line (slice) through the image spectrum.
For instance, the spectrum of view 1 is the same as the center column of the image spectrum, and the spectrum of view 3 is the same as the center row of the image spectrum.
Notice that the spectrum of each view is positioned on the grid at the same angle that the view was originally acquired.
All these frequency spectra include the negative frequencies and are displayed with zero frequency at the center.
Fourier reconstruction of a CT image requires three steps.
First, the onedimensional FFT is taken of each view.
Second, these view spectra are used to calculate the two-dimensional frequency spectrum of the image, as outlined by the Fourier slice theorem.
Since the view spectra are arranged radially, and the correct image spectrum is arranged rectangularly, an interpolation routine is needed to make the conversion.
Third, the inverse FFT is taken of the image spectrum to obtain the reconstructed image.
Backprojection filter.
The frequency response of the backprojection filter is shown in (a), and the corresponding filter kernel is shown in (b).
Equation 25-2 provides the values for the filter kernel.
This "radial to rectangular" conversion is also the key for understanding filtered backprojection.
The radial arrangement is the spectrum of the backprojected image, while the rectangular grid is the spectrum of the correct image.
If we compare one small region of the radial spectrum with the corresponding region of the rectangular grid, we find that the sample values are identical.
However, they have a different sample density.
The correct spectrum has uniformly spaced points throughout, as shown by the even spacing of the rectangular grid.
In comparison, the backprojected spectrum has a higher sample density near the center because of its radial arrangement.
In other words, the spokes of a wheel are closer together near the hub.
This issue does not affect Fourier reconstruction because the interpolation is from the values of the nearest neighbors, not their density.
The filter in filtered backprojection cancels this unequal sample density.
In particular, the frequency response of the filter must be the inverse of the sample density.
Since the backprojected spectrum has a density of 1/f, the appropriate filter has a frequency response of H [f ] ' f .
This frequency response is shown in Fig. 25-19a.
The filter kernel is then found by taking the inverse Fourier transform, as shown in (b).
Mathematically, the filter kernel is given by: Before leaving the topic of computed tomography, it should be mentioned that there are several similar imaging techniques in the medical field.
All use extensive amounts of DSP.
Positron emission tomography (PET) involves injecting the patient with a mildly radioactive compound that emits positrons.
Immediately after emission, the positron annihilates with an electron, creating two gamma rays that exit the body in exactly opposite directions.
Radiation detectors placed around the patient look for these back-to-back gamma rays, identifying the location of the line that the gamma rays traveled along.
Since the point where the gamma rays were created must be somewhere along this line, a reconstruction algorithm similar to computed tomography can be used.
This results in an image that looks similar to CT, except that brightness is related to the amount of the radioactive material present at each location.
A unique advantage of PET is that the radioactive compounds can be attached to various substances used by the body in some manner, such as glucose.
The reconstructed image is then related to the concentration of this biological substance.
This allows the imaging of the body's physiology rather than simple anatomy.
For example, images can be produced showing which portions of the human brain are involved in various mental tasks.
A more direct competitor to computed tomography is magnetic resonance imaging (MRI), which is now found in most major hospitals.
This technique was originally developed under the name nuclear magnetic resonance (NMR).
The name change was for public relations when local governments protested the use of anything nuclear in their communities.
It was often an impossible task to educate the public that the term nuclear simply referred to the fact that all atoms contain a nucleus.
An MRI scan is conducted by placing the patient in the center of a powerful magnet.
Radio waves in conjunction with the magnetic field cause selected nuclei in the body to resonate, resulting in the emission of secondary radio waves.
These secondary radio waves are digitized and form the data set used in the MRI reconstruction algorithms.
The result is a set of images that appear very similar to computed tomography.
The advantages of MRI are numerous: good soft tissue discrimination, flexible slice selection, and not using potentially dangerous x-ray radiation.
On the negative side, MRI is a more expensive technique than CT, and poor for imaging bones and other hard tissues.
CT and MRI will be the mainstays of medical imaging for many years to come.
Neural Networks (and more!) Traditional DSP is based on algorithms, changing data from one form to another through step-bystep procedures.
Most of these techniques also need parameters to operate.
For example: recursive filters use recursion coefficients, feature detection can be implemented by correlation and thresholds, an image display depends on the brightness and contrast settings, etc. Algorithms describe what is to be done, while parameters provide a benchmark to judge the data.
The proper selection of parameters is often more important than the algorithm itself.
Neural networks take this idea to the extreme by using very simple algorithms, but many highly optimized parameters.
This is a revolutionary departure from the traditional mainstays of science and engineering: mathematical logic and theorizing followed by experimentation.
Neural networks replace these problem solving strategies with trial & error, pragmatic solutions, and a "this works better than that" methodology.
This chapter presents a variety of issues regarding parameter selection in both neural networks and more traditional DSP algorithms.
Target Detection Scientists and engineers often need to know if a particular object or condition is present.
For instance, geophysicists explore the earth for oil, physicians examine patients for disease, astronomers search the universe for extraterrestrial intelligence, etc.
These problems usually involve comparing the acquired data against a threshold.
If the threshold is exceeded, the target (the object or condition being sought) is deemed present.
For example, suppose you invent a device for detecting cancer in humans.
The apparatus is waved over a patient, and a number between 0 and 30 pops up on the video screen.
Low numbers correspond to healthy subjects, while high numbers indicate that cancerous tissue is present.
You find that the device works quite well, but isn't perfect and occasionally makes an error.
The question is: how do you use this system to the benefit of the patient being examined?
Figure 26-1 illustrates a systematic way of analyzing this situation.
Suppose the device is tested on two groups: several hundred volunteers known to be healthy (nontarget), and several hundred volunteers known to have cancer (target).
Figures (a) & (b) show these test results displayed as histograms.
The healthy subjects generally produce a lower number than those that have cancer (good), but there is some overlap between the two distributions (bad).
As discussed in Chapter 2, the histogram can be used as an estimate of the probability distribution function (pdf), as shown in (c).
For instance, imagine that the device is used on a randomly chosen healthy subject.
From (c), there is about an 8% chance that the test result will be 3, about a 1% chance that it will be 18, etc. (This example does not specify if the output is a real number, requiring a pdf, or an integer, requiring a pmf.
Don't worry about it here; it isn't important).
Now, think about what happens when the device is used on a patient of unknown health.
For example, if a person we have never seen before receives a value of 15, what can we conclude?
Do they have cancer or not?
We know that the probability of a healthy person generating a 15 is 2.1%.
Likewise, there is a 0.7% chance that a person with cancer will produce a 15.
If no other information is available, we would conclude that the subject is three times as likely not to have cancer, as to have cancer.
That is, the test result of implies a 25% probability that the subject is from the target group.
This method can be generalized to form the curve in (d), the probability of the subject having cancer based only on the number produced by the device [mathematically, pdf t /(pdf t % pdfnt ) ].
If we stopped the analysis at this point, we would be making one of the most common (and serious) errors in target detection.
Another source of information must usually be taken into account to make the curve in (d) meaningful.
This is the relative number of targets versus nontargets in the population to be tested.
For instance, we may find that only one in one-thousand people have the cancer we are trying to detect.
To include this in the analysis, the amplitude of the nontarget pdf in (c) is adjusted so that the area under the curve is 0.999.
Likewise, the amplitude of the target pdf is adjusted to make the area under the curve be 0.001.
Figure (d) is then calculated as before to give the probability that a patient has cancer.
Neglecting this information is a serious error because it greatly affects how the test results are interpreted.
In other words, the curve in figure (d) is drastically altered when the prevalence information is included.
For instance, if the fraction of the population having cancer is 0.001, a test result of corresponds to only a 0.025% probability that this patient has cancer.
This is very different from the 25% probability found by relying on the output of the machine alone.
This method of converting the output value into a probability can be useful for understanding the problem, but it is not the main way that target detection is accomplished.
Most applications require a yes/no decision on Probability of target detection.
Figures (a) and (b) shows histograms of target and nontarget groups with respect to some parameter value.
From these histograms, the probability distribution functions of the two groups can be estimated, as shown in (c).
Using only this information, the curve in (d) can be calculated, giving the probability that a target has been found, based on a specific value of the parameter.
This is done by comparing the output value of the test to a threshold.
If the output is above the threshold, the test is said to be positive, indicating that the target is present.
If the output is below the threshold, the test is said to be negative, indicating that the target is not present.
In our cancer example, a negative test result means that the patient is told they are healthy, and sent home.
When the test result is positive, additional tests will be performed, such as obtaining a sample of the tissue by insertion of a biopsy needle.
Since the target and nontarget distributions overlap, some test results will not be correct.
That is, some patients sent home will actually have cancer, and some patients sent for additional tests will be healthy.
In the jargon of target detection, a correct classification is called true, while an incorrect classification is called false.
For example, if a patient has cancer, and the test properly detects the condition, it is said to be a true-positive.
Likewise, if a patient does not have cancer, and the test indicates that cancer is not present, it is said to be a true-negative.
A false-positive occurs when the patient does not have cancer, but the test erroneously indicates that they do.
This results in needless worry, and the pain and expense of additional tests.
An even worse scenario occurs with the falsenegative, where cancer is present, but the test indicates the patient is healthy.
As we all know, untreated cancer can cause many health problems, including premature death.
The human suffering resulting from these two types of errors makes the threshold selection a delicate balancing act.
How many false-positives can be tolerated to reduce the number of false-negatives? Figure 26-2 shows a graphical way of evaluating this problem, the ROC curve (short for Receiver Operating Characteristic).
The ROC curve plots the percent of target signals reported as positive (higher is better), against the percent of nontarget signals erroneously reported as positive (lower is better), for various values of the threshold.
In other words, each point on the ROC curve represents one possible tradeoff of true-positive and false-positive performance.
Figures (a) through (d) show four settings of the threshold in our cancer detection example.
For instance, look at (b) where the threshold is set at 17. Remember, every test that produces an output value greater than the threshold is reported as a positive result.
About 13% of the area of the nontarget distribution is greater than the threshold (i.e., to the right of the threshold).
Of all the patients that do not have cancer, 87% will be reported as negative (i.e., a true-negative), while 13% will be reported as positive (i.e., a false-positive).
In comparison, about 80% of the area of the target distribution is greater than the threshold.
This means that 80% of those that have cancer will generate a positive test result (i.e., a true-positive).
The other 20% that have cancer will be incorrectly reported as a negative (i.e., a false-negative).
As shown in the ROC curve in (b), this threshold results in a point on the curve at: % nontargets positive = 13%, and % targets positive = 80%.
The more efficient the detection process, the more the ROC curve will bend toward the upper-left corner of the graph.
Pure guessing results in a straight line at a 45E diagonal.
Setting the threshold relatively low, as shown in (a), results in nearly all the target signals being detected.
This comes at the price of many false alarms (false-positives).
As illustrated in (d), setting the threshold relatively high provides the reverse situation: few false alarms, but many missed targets.
These analysis techniques are useful in understanding the consequences of threshold selection, but the final decision is based on what some human will accept.
Suppose you initially set the threshold of the cancer detection apparatus to some value you feel is appropriate.
After many patients have been screened with the system, you speak with a dozen or so patients that have been subjected to false-positives.
Hearing how your system has unnecessarily disrupted the lives of these people affects you deeply, motivating you to increase the threshold.
Eventually you encounter a situation that makes you feel even worse: you speak with a patient who is terminally ill with a cancer that your system failed to detect.
You respond to this difficult experience by greatly lowering the threshold.
As time goes on and these events are repeated many times, the threshold gradually moves to an equilibrium value.
That is, the false-positive rate multiplied by a significance factor (lowering the threshold) is balanced by the false-negative rate multiplied by another significance factor (raising the threshold).
This analysis can be extended to devices that provide more than one output.
For example, suppose that a cancer detection system operates by taking an xray image of the subject, followed by automated image analysis algorithms to identify tumors.
The algorithms identify suspicious regions, and then measure key characteristics to aid in the evaluation.
For instance, suppose we measure the diameter of the suspect region (parameter 1) and its brightness in the image (parameter 2).
Further suppose that our research indicates that tumors are generally larger and brighter than normal tissue.
As a first try, we could go through the previously presented ROC analysis for each parameter, and find an acceptable threshold for each.
We could then classify a test as positive only if it met both criteria: parameter 1 greater than some threshold and parameter 2 greater than another threshold.
This technique of thresholding the parameters separately and then invoking logic functions (AND, OR, etc.) is very common.
Nevertheless, it is very inefficient, and much better methods are available.
Figure 26-3 shows why this is the case.
In this figure, each triangle represents a single occurrence of a target (a patient with cancer), plotted at a location that corresponds to the value of its two parameters.
Likewise, each square represents a single occurrence of a nontarget (a patient without cancer).
As shown in the pdf FIGURE 26- Example of a three-parameter space.
Just as a two-parameter space forms a plane surface, a three parameter space can be graphically represented using the conventional x, y, and z axes.
Separation of a three-parameter space into regions requires a dividing plane, or a curved surface.
In other words, each parameter, taken individually, is a poor predictor of cancer.
Combining the two parameters with simple logic functions would only provide a small improvement.
This is especially interesting since the two parameters contain information to perfectly separate the targets from the nontargets.
This is done by drawing a diagonal line between the two groups, as shown in the figure.
In the jargon of the field, this type of coordinate system is called a parameter space.
For example, the two-dimensional plane in this example could be called a diameter-brightness space.
The idea is that targets will occupy one region of the parameter space, while nontargets will occupy another.
Separation between the two regions may be as simple as a straight line, or as complicated as closed regions with irregular borders.
Figure 264 shows the next level of complexity, a three-parameter space being represented on the x, y and z axes.
For example, this might correspond to a cancer detection system that measures diameter, brightness, and some third parameter, say, edge sharpness.
Just as in the two-dimensional case, the important idea is that the members of the target and nontarget groups will (hopefully) occupy different regions of the space, allowing the two to be separated.
In three dimensions, regions are separated by planes and curved surfaces.
The term hyperspace (over, above, or beyond normal space) is often used to describe parameter spaces with more than three dimensions.
Mathematically, hyperspaces are no different from one, two and three-dimensional spaces; however, they have the practical problem of not being able to be displayed in a graphical form in our three-dimensional universe.
The threshold selected for a single parameter problem cannot (usually) be classified as right or wrong.
This is because each threshold value results in a unique combination of false-positives and false-negatives, i.e., some point along the ROC curve.
This is trading one goal for another, and has no absolutely correct answer.
On the other hand, parameter spaces with two or more parameters can definitely have wrong divisions between regions.
For instance, imagine increasing the number of data points in Fig. 26-3, revealing a small overlap between the target and nontarget groups.
It would be possible to move the threshold line between the groups to trade the number of falsepositives against the number of false-negatives.
That is, the diagonal line would be moved toward the top-right, or the bottom-left.
However, it would be wrong to rotate the line, because it would increase both types of errors.
As suggested by these examples, the conventional approach to target detection (sometimes called pattern recognition) is a two step process.
The first step is called feature extraction.
This uses algorithms to reduce the raw data to a few parameters, such as diameter, brightness, edge sharpness, etc.
These parameters are often called features or classifiers.
Feature extraction is needed to reduce the amount of data.
For example, a medical x-ray image may contain more than a million pixels.
The goal of feature extraction is to distill the information into a more concentrated and manageable form.
This type of algorithm development is more of an art than a science.
It takes a great deal of experience and skill to look at a problem and say: "These are the classifiers that best capture the information."
Trial-and-error plays a significant role.
In the second step, an evaluation is made of the classifiers to determine if the target is present or not.
In other words, some method is used to divide the parameter space into a region that corresponds to the targets, and a region that corresponds to the nontargets.
This is quite straightforward for one and two-parameter spaces; the known data points are plotted on a graph (such as Fig. 26-3), and the regions separated by eye.
The division is then written into a computer program as an equation, or some other way of defining one region from another.
In principle, this same technique can be applied to a three-dimensional parameter space.
The problem is, threedimensional graphs are very difficult for humans to understand and visualize (such as Fig. 26-4).
Caution: Don't try this in hyperspace; your brain will explode!
In short, we need a machine that can carry out a multi-parameter space division, according to examples of target and nontarget signals.
This ideal target detection system is remarkably close to the main topic of this chapter, the neural network.
Neural Network Architecture Humans and other animals process information with neural networks.
These are formed from trillions of neurons (nerve cells) exchanging brief electrical pulses called action potentials.
Computer algorithms that mimic these biological structures are formally called artificial neural networks to distinguish them from the squishy things inside of animals.
However, most scientists and engineers are not this formal and use the term neural network to include both biological and nonbiological systems.
Neural network research is motivated by two desires: to obtain a better understanding of the human brain, and to develop computers that can deal with abstract and poorly defined problems.
For example, conventional computers have trouble understanding speech and recognizing people's faces.
In comparison, humans do extremely well at these tasks.
Many different neural network structures have been tried, some based on imitating what a biologist sees under the microscope, some based on a more mathematical analysis of the problem.
The most commonly used structure is shown in Fig. 26-5.
This neural network is formed in three layers, called the input layer, hidden layer, and output layer.
Each layer consists of one or more nodes, represented in this diagram by the small circles.
The lines between the nodes indicate the flow of information from one node to the next.
In this particular type of neural network, the information flows only from the input to the output (that is, from left-to-right).
Other types of neural networks have more intricate connections, such as feedback paths.
The nodes of the input layer are passive, meaning they do not modify the data.
They receive a single value on their input, and duplicate the value to their multiple outputs.
In comparison, the nodes of the hidden and output layer are active.
This means they modify the data as shown in Fig. 26-6.
The variables: X11, X12 þ X115 hold the data to be evaluated (see Fig. 26-5).
For example, they may be pixel values from an image, samples from an audio signal, stock market prices on successive days, etc.
They may also be the output of some other algorithm, such as the classifiers in our cancer detection example: diameter, brightness, edge sharpness, etc.
Each value from the input layer is duplicated and sent to all of the hidden nodes.
This is called a fully interconnected structure.
As shown in Fig. 266, the values entering a hidden node are multiplied by weights, a set of predetermined numbers stored in the program.
The weighted inputs are then added to produce a single number.
This is shown in the diagram by the symbol, E. Before leaving the node, this number is passed through a nonlinear mathematical function called a sigmoid.
This is an "s" shaped curve that limits the node's output.
That is, the input to the sigmoid is a value between & 4 and % 4, while its output can only be between 0 and 1.
The outputs from the hidden layer are represented in the flow diagram (Fig 265) by the variables: X21, X22, X23 and X24 .
Just as before, each of these values is duplicated and applied to the next layer.
The active nodes of the output layer combine and modify the data to produce the two output values of this network, X31 and X32 .
Neural networks can have any number of layers, and any number of nodes per layer.
Most applications use the three layer structure with a maximum of a few hundred input nodes.
The hidden layer is usually about 10% the size of the input layer.
In the case of target detection, the output layer only needs a single node.
The output of this node is thresholded to provide a positive or negative indication of the target's presence or absence in the input data.
Table 26-1 is a program to carry out the flow diagram of Fig. 26-5.
The key point is that this architecture is very simple and very generalized.
This same flow diagram can be used for many problems, regardless of their particular quirks.
The ability of the neural network to provide useful data manipulation lies in the proper selection of the weights.
This is a dramatic departure from conventional information processing where solutions are described in step-bystep procedures.
As an example, imagine a neural network for recognizing objects in a sonar signal.
Suppose that 1000 samples from the signal are stored in a computer.
How does the computer determine if these data represent a submarine, whale, undersea mountain, or nothing at all? Conventional DSP would approach this problem with mathematics and algorithms, such as correlation and frequency spectrum analysis.
With a neural network, the 1000 samples are simply fed into the input layer, resulting in values popping from the output layer.
By selecting the proper weights, the output can be configured to report a wide range of information.
For instance, there might be outputs for: submarine (yes/no), whale (yes/no), undersea mountain (yes/no), etc.
With other weights, the outputs might classify the objects as: metal or nonmetal, biological or nonbiological, enemy or ally, etc.
No algorithms, no rules, no procedures; only a relationship between the input and output dictated by the values of the weights selected.
The sigmoid function and its derivative.
Equations 26-1 and 26-2 generate these curves.
Figure 26-7a shows a closer look at the sigmoid function, mathematically described by the equation: EQUATION 26- The sigmoid function.
This is used in neural networks as a smooth threshold.
This function is graphed in Fig. 26-7a.
For comparison, a simple threshold produces a value of one when x > 0, and a value of zero when x < 0 .
The sigmoid performs this same basic thresholding function, but is also differentiable, as shown in Fig. 26-7b.
While the derivative is not used in the flow diagram (Fig. 25-5), it is a critical part of finding the proper weights to use.
More about this shortly.
An advantage of the sigmoid is that there is a shortcut to calculating the value of its derivative: EQUATION 26- First derivative of the sigmoid function.
This is calculated by using the value of the sigmoid function itself.
This isn't a critical concept, just a trick to make the algebra shorter.
Wouldn't the neural network be more flexible if the sigmoid could be adjusted left-or-right, making it centered on some other value than x ' 0 ?
The answer is yes, and most neural networks allow for this.
It is very simple to implement; an additional node is added to the input layer, with its input always having a value of one.
When this is multiplied by the weights of the hidden layer, it provides a bias (DC offset) to each sigmoid.
This addition is called a bias node.
It is treated the same as the other nodes, except for the constant input.
Can neural networks be made without a sigmoid or similar nonlinearity?
To answer this, look at the three-layer network of Fig. 26-5.
If the sigmoids were not present, the three layers would collapse into only two layers.
In other words, the summations and weights of the hidden and output layers could be combined into a single layer, resulting in only a two-layer network.
Why Does It Work?
The weights required to make a neural network carry out a particular task are found by a learning algorithm, together with examples of how the system should operate.
For instance, the examples in the sonar problem would be a database of several hundred (or more) of the 1000 sample segments.
Some of the example segments would correspond to submarines, others to whales, others to random noise, etc.
The learning algorithm uses these examples to calculate a set of weights appropriate for the task at hand.
The term learning is widely used in the neural network field to describe this process; however, a better description might be: determining an optimized set of weights based on the statistics of the examples.
Regardless of what the method is called, the resulting weights are virtually impossible for humans to understand.
Patterns may be observable in some rare cases, but generally they appear to be random numbers.
A neural network using these weights can be observed to have the proper input/output relationship, but why these particular weights work is quite baffling.
This mystic quality of neural networks has caused many scientists and engineers to shy away from them.
Remember all those science fiction movies of renegade computers taking over the earth?
In spite of this, it is common to hear neural network advocates make statements such as: "neural networks are well understood."
To explore this claim, we will first show that it is possible to pick neural network weights through traditional DSP methods.
Next, we will demonstrate that the learning algorithms provide better solutions than the traditional techniques.
While this doesn't explain why a particular set of weights works, it does provide confidence in the method.
In the most sophisticated view, the neural network is a method of labeling the various regions in parameter space.
For example, consider the sonar system neural network with 1000 inputs and a single output.
With proper weight selection, the output will be near one if the input signal is an echo from a submarine, and near zero if the input is only noise.
This forms a parameter hyperspace of 1000 dimensions.
The neural network is a method of assigning a value to each location in this hyperspace.
That is, the 1000 input values define a location in the hyperspace, while the output of the neural network provides the value at that location.
A look-up table could perform this task perfectly, having an output value stored for each possible input address.
The difference is that the neural network calculates the value at each location (address), rather than the impossibly large task of storing each value.
In fact, neural network architectures are often evaluated by how well they separate the hyperspace for a given number of weights.
This approach also provides a clue to the number of nodes required in the hidden layer.
A parameter space of N dimensions requires N numbers to specify a location.
Identifying a region in the hyperspace requires 2N values (i.e., a minimum and maximum value along each axis defines a hyperspace rectangular solid).
For instance, these simple calculations would indicate that a neural network with 1000 inputs needs 2000 weights to identify one region of the hyperspace from another.
In a fully interconnected network, this would require two hidden nodes.
The number of regions needed depends on the particular problem, but can be expected to be far less than the number of dimensions in the parameter space.
While this is only a crude approximation, it generally explains why most neural networks can operate with a hidden layer of 2% to 30% the size of the input layer.
A completely different way of understanding neural networks uses the DSP concept of correlation.
As discussed in Chapter 7, correlation is the optimal way of detecting if a known pattern is contained within a signal.
It is carried out by multiplying the signal with the pattern being looked for, and adding the products.
The higher the sum, the more the signal resembles the pattern.
Now, examine Fig. 26-5 and think of each hidden node as looking for a specific pattern in the input data.
That is, each of the hidden nodes correlates the input data with the set of weights associated with that hidden node.
If the pattern is present, the sum passed to the sigmoid will be large, otherwise it will be small.
The action of the sigmoid is quite interesting in this viewpoint.
Look back at Fig. 26-1d and notice that the probability curve separating two bell shaped distributions resembles a sigmoid.
If we were manually designing a neural network, we could make the output of each hidden node be the fractional probability that a specific pattern is present in the input data.
The output layer repeats this operation, making the entire three-layer structure a correlation of correlations, a network that looks for patterns of patterns.
Conventional DSP is based on two techniques, convolution and Fourier analysis.
It is reassuring that neural networks can carry out both these operations, plus much more.
Imagine an N sample signal being filtered to produce another N sample signal.
According to the output side view of convolution, each sample in the output signal is a weighted sum of samples from the input.
Now, imagine a two-layer neural network with N nodes in each layer.
The value produced by each output layer node is also a weighted sum of the input values.
If each output layer node uses the same weights as all the other output nodes, the network will implement linear convolution.
Likewise, the DFT can be calculated with a two layer neural network with N nodes in each layer.
Each output layer node finds the amplitude of one frequency component.
This is done by making the weights of each output layer node the same as the sinusoid being looked for.
The resulting network correlates the input signal with each of the basis function sinusoids, thus calculating the DFT.
Of course, a two-layer neural network is much less powerful than the standard three layer architecture.
This means neural networks can carry out nonlinear as well as linear processing.
Suppose that one of these conventional DSP strategies is used to design the weights of a neural network.
Can it be claimed that the network is optimal?
Traditional DSP algorithms are usually based on assumptions about the characteristics of the input signal.
For instance, Wiener filtering is optimal for maximizing the signal-to-noise ratio assuming the signal and noise spectra are both known; correlation is optimal for detecting targets assuming the noise is white; deconvolution counteracts an undesired convolution assuming the deconvolution kernel is the inverse of the original convolution kernel, etc.
The problem is, scientist and engineer's seldom have a perfect knowledge of the input signals that will be encountered.
While the underlying mathematics may be elegant, the overall performance is limited by how well the data are understood.
For instance, imagine testing a traditional DSP algorithm with actual input signals.
Next, repeat the test with the algorithm changed slightly, say, by increasing one of the parameters by one percent.
If the second test result is better than the first, the original algorithm is not optimized for the task at hand.
Nearly all conventional DSP algorithms can be significantly improved by a trial-and-error evaluation of small changes to the algorithm's parameters and procedures.
This is the strategy of the neural network.
Training the Neural Network Neural network design can best be explained with an example.
Figure 26- shows the problem we will attack, identifying individual letters in an image of text.
This pattern recognition task has received much attention.
It is easy enough that many approaches achieve partial success, but difficult enough that there are no perfect solutions.
Many successful commercial products have been based on this problem, such as: reading the addresses on letters for postal routing, document entry into word processors, etc.
The first step in developing a neural network is to create a database of examples.
For the text recognition problem, this is accomplished by printing the 26 capital letters: A,B,C,D þ Y,Z, 50 times on a sheet of paper.
Next, these 1300 letters are converted into a digital image by using one of the many scanning devices available for personal computers.
This large digital image is then divided into small images of 10×10 pixels, each containing a single letter.
This information is stored as a 1.3 Megabyte database: 1300 images; 100 pixels per image; 8 bits per pixel.
We will use the first 260 images in this database to train the neural network (i.e., determine the weights), and the remainder to test its performance.
The database must also contain a way of identifying the letter contained in each image.
For instance, an additional byte could be added to each 10× image, containing the letter's ASCII code.
In another scheme, the position Example image of text.
Identifying letters in images of text is one of the classic pattern recognition problems.
In this example, each letter is contained in a 10×10 pixel image, with 256 gray levels per pixel.
The database used to train and test the example neural network consists of 50 sets of the 26 capital letters, for a total of 1300 images.
The images shown here are a portion of this database.
For example, images 0 to 49 might all be an "A", images 50-99 might all be a "B", etc.
For this demonstration, the neural network will be designed for an arbitrary task: determine which of the 10×10 images contains a vowel, i.e., A, E, I, O, or U.
This may not have any practical application, but it does illustrate the ability of the neural network to learn very abstract pattern recognition problems.
By including ten examples of each letter in the training set, the network will (hopefully) learn the key features that distinguish the target from the nontarget images.
The neural network used in this example is the traditional three-layer, fully interconnected architecture, as shown in Figs.
There are nodes in the input layer (100 pixel values plus a bias node), 10 nodes in the hidden layer, and 1 node in the output layer.
When a 100 pixel image is applied to the input of the network, we want the output value to be close to one if a vowel is present, and near zero if a vowel is not present.
Don't be worried that the input signal was acquired as a two-dimensional array (10×10), while the input to the neural network is a one-dimensional array.
This is your understanding of how the pixel values are interrelated; the neural network will find relationships of its own.
Table 26-2 shows the main program for calculating the neural network weights, with Table 26-3 containing three subroutines called from the main program.
The array elements: X1[1] through X1[100], hold the input layer values.
In addition, X1[101] always holds a value of 1, providing the input to the bias node.
The output values from the hidden nodes are contained in the array elements: X2[1] through X2[10].
The variable, X3, contains the network's output value.
The weights of the hidden layer are contained in the array, WH[, ], where the first index identifies the hidden node (1 to 10), and the second index is the input layer node (1 to 101).
The weights of the output layer are held in WO[1] to WO[10].
This makes a total of 1020 weight values that define how the network will operate.
The first action of the program is to set each weight to an arbitrary initial value by using a random number generator.
As shown in lines 190 to 240, the hidden layer weights are assigned initial values between -0.0005 and 0.0005, while the output layer weights are between -0.5 and 0.5.
These ranges are chosen to be the same order of magnitude that the final weights must be.
This is based on: (1) the range of values in the input signal, (2) the number of inputs summed at each node, and (3) the range of values over which the sigmoid is active, an input of about & 5 < x < 5, and an output of 0 to 1.
For instance, when inputs with a typical value of 100 are multiplied by the typical weight value of 0.0002, the sum of the products is about 2, which is in the active range of the sigmoid's input.
If we evaluated the performance of the neural network using these random weights, we would expect it to be the same as random guessing.
The learning algorithm improves the performance of the network by gradually changing each weight in the proper direction.
This is called an iterative procedure, and is controlled in the program by the FOR-NEXT loop in lines 270-400.
Each iteration makes the weights slightly more efficient at separating the target from the nontarget examples.
The iteration loop is usually carried out until no further improvement is being made.
In typical neural networks, this may be anywhere from ten to ten-thousand iterations, but a few hundred is common.
This example carries out 800 iterations.
In order for this iterative strategy to work, there must be a single parameter that describes how well the system is currently performing.
The variable ESUM (for error sum) serves this function in the program.
The first action inside the iteration loop is to set ESUM to zero (line 290) so that it can be used as an accumulator.
At the end of each iteration, the value of ESUM is printed to the video screen (line 380), so that the operator can insure that progress is being made.
The value of ESUM will start high, and gradually decrease as the neural network is trained to recognize the targets.
Figure 26- shows examples of how ESUM decreases as the iterations proceed.
All 260 images in the training set are evaluated during each iteration, as controlled by the FOR-NEXT loop in lines 310-360.
Subroutine 1000 is used to retrieve images from the database of examples.
Since this is not something of particular interest here, we will only describe the parameters passed to and from this subroutine.
Subroutine 1000 is entered with the parameter, LETTER%, being between 1 and 260.
Upon return, the input node values, X1[1] to X1[100], contain the pixel values for the image in the database corresponding to LETTER%.
The bias node value, X1[101], is always returned with a constant value of one.
Subroutine 1000 also returns another parameter, CORRECT.
This contains the desired output value of the network for this particular letter.
That is, if the letter in the image is a vowel, CORRECT will be returned with a value of one.
If the letter in the image is not a vowel, CORRECT will be returned with a value of zero.
After the image being worked on is loaded into X1[1] through X1[100], subroutine 2000 passes the data through the current neural network to produce the output node value, X3.
In other words, subroutine 2000 is the same as the program in Table 26-1, except for a different number of nodes in each layer.
This subroutine also calculates how well the current network identifies the letter as a target or a nontarget.
In line 2210, the variable ELET (for error-letter) is calculated as the difference between the output value actually generated, X3, and the desired value, CORRECT.
This makes ELET a value between -1 and 1.
All of the 260 values for ELET are combined (line 340) to form ESUM, the total squared error of the network for the entire training set.
Line 2220 shows an option that is often included when calculating the error: assigning a different importance to the errors for targets and nontargets.
For example, recall the cancer example presented earlier in this chapter, and the consequences of making a false-positive error versus a false-negative error.
In the present example, we will arbitrarily declare that the error in detecting a target is five times as bad as the error in detecting a nontarget.
In effect, this tells the network to do a better job with the targets, even if it hurts the performance of the nontargets.
Subroutine 3000 is the heart of the neural network strategy, the algorithm for changing the weights on each iteration.
We will use an analogy to explain the underlying mathematics.
Consider the predicament of a military paratrooper dropped behind enemy lines.
He parachutes to the ground in unfamiliar territory, only to find it is so dark he can't see more than a few feet away.
His orders are to proceed to the bottom of the nearest valley to begin the remainder of his mission.
The problem is, without being able to see more than a few feet, how does he make his way to the valley floor?
Put another way, he needs an algorithm to adjust his x and y position on the earth's surface in order to minimize his elevation.
This is analogous to the problem of adjusting the neural network weights, such that the network's error, ESUM, is minimized.
We will look at two algorithms to solve this problem: evolution and steepest descent.
In evolution, the paratrooper takes a flying jump in some random direction.
If the new elevation is higher than the previous, he curses and returns to his starting location, where he tries again.
If the new elevation is lower, he feels a measure of success, and repeats the process from the new location.
Eventually he will reach the bottom of the valley, although in a very inefficient and haphazard path.
This method is called evolution because it is the same type of algorithm employed by nature in biological evolution.
Each new generation of a species has random variations from the previous.
If these differences are of benefit to the species, they are more likely to be retained and passed to the next generation.
This is a result of the improvement allowing the animal to receive more food, escape its enemies, produce more offspring, etc.
If the new trait is detrimental, the disadvantaged animal becomes lunch for some predator, and the variation is discarded.
In this sense, each new generation is an iteration of the evolutionary optimization procedure.
When evolution is used as the training algorithm, each weight in the neural network is slightly changed by adding the value from a random number generator.
If the modified weights make a better network (i.e., a lower value for ESUM), the changes are retained, otherwise they are discarded.
While this works, it is very slow in converging.
This is the jargon used to describe that continual improvement is being made toward an optimal solution (the bottom of the valley).
In simpler terms, the program is going to need days to reach a solution, rather than minutes or hours.
Fortunately, the steepest descent algorithm is much faster.
Think about the situation this way.
After to the east, and FIGURE 26- Neural network convergence.
This graph shows how the neural network error (the value of ESUM) decreases as the iterations proceed.
Three separate trials are shown, each starting with different initial weights.
Iteration record that elevation change.
Using these two values, he can determine which direction is downhill.
Suppose the paratrooper drops 10 cm when he moves one step in the northern direction, and drops 20 cm when he moves one step in the eastern direction.
To travel directly downhill, he needs to move along each axis an amount proportional to the slope along that axis.
In this case, he might move north by 10 steps and east by 20 steps.
This moves him down the steepest part of the slope a distance of 102 % 202 ' 22.4 steps.
Alternatively, he could move in a straight line to the new location, 22.4 steps along the diagonal.
The key point is: the steepest descent is achieved by moving along each axis a distance proportional to the slope along that axis.
Subroutine 3000 implements this same steepest decent algorithm for the network weights.
Before entering subroutine 3000, one of the example images has been applied to the input layer, and the information propagated to the output.
This means that the values for: X1[ ], X2[ ] and X3 are all specified, as well as the current weight values: WH[, ] and WO[ ].
In addition, we know the error the network produces for this particular image, ELET.
The hidden layer weights are updated in lines 3050 to 3120, while the output layer weights are modified in lines 3150 to 3190.
This is done by calculating the slope for each weight, and then changing each weight by an amount proportional to that slope.
In the paratrooper case, the slope along an axis is found by moving a small distance along the axis (say, ) x), measuring the change in elevation (say, ) E), and then dividing the two ( ) E/ ) x).
The slope of a neural network weight can be found in this same way: add a small increment to the weight value ( ) w), find the resulting change in the output signal ( ) X3), and divide the two ( ) X3/ ) w).
Later in this chapter we will look at an example that calculates the slope this way.
However, in the present example we will use a more efficient method.
Earlier we said that the nonlinearity (the sigmoid) needs to be differentiable.
Here is where we will use this property.
If we know the slope at each point on the nonlinearity, we can directly write an equation for the slope of each weight ( )X3/)w) without actually having to perturb it.
Consider a specific weight, for example, WO[1], corresponding to the first input of the output node.
Look at the structure in Figs.
The answer is: EQUATION 26- Slope of the output layer weights.
This equation is written for the weight, WO[1].
In other words, SLOPEO describes how much the output of the sigmoid changes in response to a change in the input to the sigmoid.
From Eq. 26-2, SLOPEO can be calculated from the current output value of the sigmoid, X3.
This calculation is shown in line 3160.
In line 3170, the slope for this weight is calculated via Eq.
Using a similar analysis, the slope for a weight on the hidden layer, such as WH[1,1], can be found by: EQUATION 26- Slope of the hidden layer weights.
This equation is written for the weight, WH[1,1].
X1 [1] SLOPEH1 WO[1] SLOPE O SLOPEH1 is the first derivative of the hidden layer sigmoid, evaluated where we are operating on its curve.
The other values, X1[1] and WO[1], are simply constants that the weight change sees as it makes its way to the output.
In lines 3070 and 3080, the slopes of the sigmoids are calculated using Eq.
The slope of the hidden layer weight, DX3DW is calculated in line 3090 via Eq.
Now that we know the slope of each of the weights, we can look at how each weight is changed for the next iteration.
The new value for each weight is found by taking the current weight, and adding an amount that is proportional to the slope: EQUATION 26- Updating the weights.
Each of the weights is adjusted by adding an amount proportional to the slope of the weight.
The proportionality constant consists of two factors, ELET, the error of the network for this particular input, and MU, a constant set at the beginning of the program.
To understand the need for ELET in this calculation, imagine that an image placed on the input produces a small error in the output signal.
Next, imagine that another image applied to the input produces a large output error.
When adjusting the weights, we want to nudge the network more for the second image than the first.
If something is working poorly, we want to change it; if it is working well, we want to leave it alone.
This is accomplished by changing each weight in proportion to the current error, ELET.
To understand how MU affects the system, recall the example of the paratrooper.
Once he determines the downhill direction, he must decide how far to proceed before reevaluating the slope of the terrain.
By making this distance short, one meter for example, he will be able to precisely follow the contours of the terrain and always be moving in an optimal direction.
The problem is that he spends most of his time evaluating the slope, rather than actually moving down the hill.
In comparison, he could choose the distance to be large, say 1000 meters.
While this would allow the paratrooper to move rapidly along the terrain, he might overshoot the downhill path.
Too large of a distance makes him jump all over the country-side without making the desired progress.
In the neural network, MU controls how much the weights are changed on each iteration.
The value to use depends on the particular problem, being as low as 10-6, or as high as 0.1.
From the analogy of the paratrooper, it can be expected that too small of a value will cause the network to converge too slowly.
In comparison, too large of a value will cause the convergence to be erratic, and will exhibit chaotic oscillation around the final solution.
Unfortunately, the way neural networks react to various values of MU can be difficult to understand or predict.
This makes it critical that the network error (i.e., ESUM) be monitored during the training, such as printing it to the video screen at the end of each iteration.
If the system isn't converging properly, stop the program and try another value for MU.
Evaluating the Results So, how does it work?
The training program for vowel recognition was run three times using different random values for the initial weights.
About one hour is required to complete the 800 iterations on a 100 MHz Pentium personnel computer.
Figure 26-9 shows how the error of the network, ESUM, changes over this period.
The gradual decline indicates that the network is learning the task, and that the weights reach a near optimal value after several hundred iterations.
Each trial produces a different solution to the problem, with a different final performance.
This is analogous to the paratrooper starting at different locations, and thereby ending up at the bottom of different valleys.
Just as some valleys are deeper than others, some neural network solutions are better than others.
This means that the learning algorithm should be run several times, with the best of the group taken as the final solution.
Example of neural network weights.
In this figure, the hidden layer weights for the three solutions are displayed as images.
All three of these solutions appear random to the human eye.
In Fig. 26-10, the hidden layer weights of the three solutions are displayed as images.
This means the first action taken by the neural network is to correlate (multiply and sum) these images with the input signal.
They look like random noise!
These weights values can be shown to work, but why they work is something of a mystery.
Here is something else to ponder.
The human brain is composed of about 100 trillion neurons, each with an average of 10, interconnections.
If we can't understand the simple neural network in this example, how can we study something that is at least 100,000,000,000, times more complex?
This is 21st century research.
Figure 26-11a shows a histogram of the neural network's output for the letters in the training set.
Remember, the weights were selected to make the output near one for vowel images, and near zero otherwise.
Separation has been perfectly achieved, with no overlap between the two distributions.
Also notice that the vowel distribution is narrower than the nonvowel distribution.
This is because we declared the target error to be five times more important than the nontarget error (see line 2220).
In comparison, Fig. 26-11b shows the histogram for images 261 through in the database.
While the target and nontarget distributions are reasonably distinct, they are not completely separated.
Why does the neural network perform better on the first 260 letters than the last 1040?
Figure (a) is cheating!
It's easy to take a test if you have already seen the answers.
In other words, the neural network is recognizing specific images in the training set, not the general patterns identifying vowels from nonvowels.
Figure 26-12 shows the performance of the three solutions, displayed as ROC curves.
Trial (b) provides a significantly better network than the a. Training database b.
New database Relative number of occurences Relative number of occurences vowels nonvowels vowels nonvowels Output value Output value FIGURE 26- Neural network performance.
These are histograms of the neural network's output values, (a) for the training data, and (b) for the remaining images.
The neural network performs better with the training data because it has already seen the answers to the test.
This is a matter of random chance depending on the initial weights used.
At one threshold setting, the neural network designed in trial "b" can detect 24 out of 25 targets (i.e., 96% of the vowel images), with a false alarm rate of only 1 in 25 nontargets (i.e., 4% of the nonvowel images).
Not bad considering the abstract nature of this problem, and the very general solution applied.
These curves compare three neural networks designed to detect images of vowels.
Trial (b) is the best solution, shown by its curve being closer to the upper-left corner of the graph.
This network can correctly detect 24 out of targets, while providing only 1 false alarm for each 25 nontargets.
That is, there is a point on the ROC curve at x ' 4% and y ' 96% % nonvowels reported Some final comments on neural networks.
Getting a neural network to converge during training can be tricky.
If the network error (ESUM) doesn't steadily decrease, the program must be terminated, changed, and then restarted.
This may take several attempts before success is reached.
Three things can be changed to affect the convergence: (1) MU, (2) the magnitude of the initial random weights, and (3) the number of hidden nodes (in the order they should be changed).
The most critical item in neural network development is the validity of the training examples.
For instance, when new commercial products are being developed, the only test data available are from prototypes, simulations, educated guesses, etc.
If a neural network is trained on this preliminary information, it might not operate properly in the final application.
Any difference between the training database and the eventual data will degrade the neural network's performance (Murphy's law for neural networks).
Don't try to second guess the neural network on this issue; you can't!
Recursive Filter Design Chapters 19 and 20 show how to design recursive filters with the standard frequency responses: high-pass, low-pass, band-pass, etc.
What if you need something custom?
The answer is to design a recursive filter just as you would a neural network: start with a generic set of recursion coefficients, and use iteration to slowly mold them into what you want.
This technique is important for two reasons.
First, it allows custom recursive filters to be designed without having to hassle with the mathematics of the z-transform.
Second, it shows that the ideas from conventional DSP and neural networks can be combined to form superb algorithms.
The main program for this method is shown in Table 26-4, with two subroutines in Table 26-5.
The array, T[ ], holds the desired frequency response, some kind of curve that we have manually designed.
Since this program is based around the FFT, the lengths of the signals must be a power of two.
As written, this program uses an FFT length of 256, as defined by the variable, N%, in line 130.
This means that T[0] to T[128] correspond to the frequencies between 0 and 0.5 of the sampling rate.
Only the magnitude is contained in this array; the phase is not controlled in this design, and becomes whatever it becomes.
The recursion coefficients are set to their initial values in lines 270-310, typically selected to be the identity system.
Don't use random numbers here, or the initial filter will be unstable.
The recursion coefficients are held in the arrays, A[ ] and B[ ].
The variable, NP%, sets the number of poles in the designed filter.
For example, if NP% is 5, the "a" coefficients run from A[0] to A[5], while the "b" coefficients run from B[1] to B[5].
As previously mentioned, the iterative procedure requires a single value that describes how well the current system is functioning.
This is provided by the variable, ER (for error), and is calculated in subroutine 3000.
Lines 3040 to 3080 load an impulse in the array, IMX[ ].
Next, lines 3100- use this impulse as an input signal to the recursive filter defined by the current values of A[ ] and B[ ].
The output of this filter is thus the impulse response of the current system, and is stored in the array, REX[ ].
The system's frequency response is then found by taking the FFT of the impulse response, as shown in line 3170.
Subroutine 1000 is the FFT program listed in Table 12- in Chapter 12.
This FFT subroutine returns the frequency response in rectangular form, overwriting the arrays REX[ ] and IMX[ ].
Lines 3200-3250 then calculate ER, the mean squared error between the magnitude of the current frequency response, and the desired frequency response.
Pay particular attention to how this error is found.
The iterative action of this program optimizes this error, making the way it is defined very important.
The FOR-NEXT loop runs through each frequency in the frequency response.
For each frequency, line 3220 calculates the magnitude of the current frequency response from the rectangular data.
In line 3230, the error at this frequency is found by subtracting the desired magnitude, T[ ], from the current magnitude, MAG.
This error is then squared, and added to the accumulator variable, ER.
After looping through each frequency, line completes the calculation to make ER the mean squared error of the entire frequency response.
Lines 340 to 380 control the iteration loop of the program.
Subroutine is where the changes to the recursion coefficients are made.
The first action in this subroutine is to determine the current value of ER, and store it in another variable, EOLD (lines 2040 & 2050).
After the subroutine updates the coefficients, the value of ER is again determined, and assigned to the variable, ENEW (lines 2270 and 2280).
The variable, MU, controls the iteration step size, just as in the previous neural network program.
An advanced feature is used in this program: an automated adjustment to the value of MU.
This is the reason for having the two variables, EOLD and ENEW.
When the program starts, MU is set to the relatively high value of 0.2 (line 160).
This allows the convergence to proceed rapidly, but will limit how close the filter can come to an optimal solution.
As the iterations proceed, points will be reached where no progress is being made, identified by ENEW being higher than EOLD.
Each time this occurs, line reduces the value of MU.
Subroutine 2000 updates the recursion coefficients according to the steepest decent method: calculate the slope for each coefficient, and then change the coefficient an amount proportional to its slope.
Lines 2080-2130 calculate the slopes for the "a" coefficients, storing them in the array, SA[ ].
Likewise, lines 2150-2200 calculate the slopes for the "b" coefficients, storing them in the array, SB[ ].
Lines 2220-2250 then modify each of the recursion coefficients by an amount proportional to these slopes.
In this program, the proportionality constant is simply the step size, MU.
No error term is required in the proportionality constant because there is only one example to be matched: the desired frequency response.
The last issue is how the program calculates the slopes of the recursion coefficients.
In the neural network example, an equation for the slope was derived.
This procedure cannot be used here because it would require taking the derivative across the DFT.
Instead, a brute force method is applied: actually change the recursion coefficient by a small increment, and then directly calculate the new value of ER.
The slope is then found as the change in ER divided by the amount of the increment.
Specifically, the current value of ER is found in lines 2040-2050, and stored in the variable, EOLD.
The loop in lines 2080-2130 runs through each of the "a" coefficients.
The first action inside this loop is to add a small increment, DELTA, to the recursion coefficient being worked on (line 2090).
Subroutine 3000 is invoked in line 2100 to find the value of ER with the modified coefficient.
Line 2110 then calculates the slope of this coefficient as: (ER & EOLD) /DELTA .
Line then restores the modified coefficient by subtracting the value of DELTA. Figure 26-13 shows several examples of filters designed using this program.
The dotted line is the desired frequency response, while the solid line is the Iterative design of recursive filters.
Figure (a) shows an 8 pole low-pass filter with the error equally distributed between 0 and 0.5.
In (b), the error has been weighted to force better performance in the stopband, at the expense of error in the passband.
Figure (c) shows a 2 pole filter used for the 1/sinc(x) correction in digital-to-analog conversion.
The frequency response in (d) is completely custom.
In each figure, the desired frequency response is shown by the dotted line, and the actual frequency response by the solid curve.
Each of these filters requires several minutes to converge on a 100 MHz Pentium. Figure (a) is an 8 pole low-pass filter, where the error is equally weighted over the entire frequency spectrum (the program as written).
Figure (b) is the same filter, except the error in the stopband is multiplied by eight when ER is being calculated.
This forces the filter to have less stopband ripple, at the expense of greater ripple in the passband.
Figure (c) shows a 2 pole filter for: 1 /sinc(x) .
As discussed in Chapter 3, this can be used to counteract the zeroth-order hold during digital-to-analog conversion (see Fig. 3-6).
The error in this filter was only summed between 0 and 0.45, resulting in a better match over this range, at the expense of a worse match between 0.45 and 0.5.
Lastly, (d) is a very irregular 6 pole frequency response that includes a sharp dip.
To achieve convergence, the recursion coefficients were initially set to those of a notch filter.
Data Compression Data transmission and storage cost money.
The more information being dealt with, the more it costs.
In spite of this, most digital data are not stored in the most compact form.
Rather, they are stored in whatever way makes them easiest to use, such as: ASCII text from word processors, binary code that can be executed on a computer, individual samples from a data acquisition system, etc.
Typically, these easy-to-use encoding methods require data files about twice as large as actually needed to represent the information.
Data compression is the general term for the various algorithms and programs developed to address this problem.
A compression program is used to convert data from an easy-to-use format to one optimized for compactness.
Likewise, an uncompression program returns the information to its original form.
We examine five techniques for data compression in this chapter.
The first three are simple encoding techniques, called: runlength, Huffman, and delta encoding.
The last two are elaborate procedures that have established themselves as industry standards: LZW and JPEG.
Data Compression Strategies Table 27-1 shows two different ways that data compression algorithms can be categorized.
In (a), the methods have been classified as either lossless or lossy.
A lossless technique means that the restored data file is identical to the original.
This is absolutely necessary for many types of data, for example: executable code, word processing files, tabulated numbers, etc.
You cannot afford to misplace even a single bit of this type of information.
In comparison, data files that represent images and other acquired signals do not have to be keep in perfect condition for storage or transmission.
All real world measurements inherently contain a certain amount of noise.
If the changes made to these signals resemble a small amount of additional noise, no harm is done.
Compression techniques that allow this type of degradation are called lossy.
This distinction is important because lossy techniques are much more effective at compression than lossless methods.
The higher the compression ratio, the more noise added to the data.
Group size: Lossless Method input output run-length Huffman delta Huffman Arithmetic run-length, LZW fixed fixed variable variable fixed variable variable fixed a. Lossless or Lossy b.
Fixed or variable group size TABLE 27- Compression classifications.
Data compression methods can be divided in two ways.
In (a), the techniques are classified as lossless or lossy.
Lossless methods restore the compressed data to exactly the same form as the original, while lossy methods only generate an approximation.
In (b), the methods are classified according to a fixed or variable size of group taken from the original file and written to the compressed file.
Images transmitted over the world wide web are an excellent example of why data compression is important.
Suppose we need to download a digitized color photograph over a computer's 33.6 kbps modem.
If the image is not compressed (a TIFF file, for example), it will contain about 600 kbytes of data.
If it has been compressed using a lossless technique (such as used in the GIF format), it will be about one-half this size, or 300 kbytes.
If lossy compression has been used (a JPEG file), it will be about 50 kbytes.
The point is, the download times for these three equivalent files are 142 seconds, 71 seconds, and seconds, respectively.
That's a big difference!
JPEG is the best choice for digitized photographs, while GIF is used with drawn images, such as company logos that have large areas of a single color.
Our second way of classifying data compression methods is shown in Table 271b.
Most data compression programs operate by taking a group of data from the original file, compressing it in some way, and then writing the compressed group to the output file.
For instance, one of the techniques in this table is CS&Q, short for coarser sampling and/or quantization.
Suppose we are compressing a digitized waveform, such as an audio signal that has been digitized to 12 bits.
We might read two adjacent samples from the original file (24 bits), discard one of the sample completely, discard the least significant 4 bits from the other sample, and then write the remaining 8 bits to the output file.
With 24 bits in and 8 bits out, we have implemented a 3:1 compression ratio using a lossy algorithm.
While this is rather crude in itself, it is very effective when used with a technique called transform compression.
As we will discuss later, this is the basis of JPEG.
Table 27-1b shows CS&Q to be a fixed-input fixed-output scheme.
That is, a fixed number of bits are read from the input file and a smaller fixed number of bits are written to the output file.
Other compression methods allow a variable number of bits to be read or written.
As you go through the description of each of these compression methods, refer back to this table to understand how it fits into this classification scheme.
Why are JPEG and MPEG not listed in this table?
These are composite algorithms that combine many of the other techniques.
They are too sophisticated to be classified into these simple categories.
Run-Length Encoding Data files frequently contain the same character repeated many times in a row.
For example, text files use multiple spaces to separate sentences, indent paragraphs, format tables & charts, etc. Digitized signals can also have runs of the same value, indicating that the signal is not changing.
For instance, an image of the nighttime sky would contain long runs of the character or characters representing the black background.
Likewise, digitized music might have a long run of zeros between songs.
Run-length encoding is a simple method of compressing these types of files.
Figure 27-1 illustrates run-length encoding for a data sequence having frequent runs of zeros.
Each time a zero is encountered in the input data, two values are written to the output file.
The first of these values is a zero, a flag to indicate that run-length compression is beginning.
The second value is the number of zeros in the run.
If the average run-length is longer than two, compression will take place.
On the other hand, many single zeros in the data can make the encoded file larger than the original.
Many different run-length schemes have been developed.
For example, the input data can be treated as individual bytes, or groups of bytes that represent something more elaborate, such as floating point numbers.
Run-length encoding can be used on only one of the characters (as with the zero above), several of the characters, or all of the characters.
A good example of a generalized run-length scheme is PackBits, created for Macintosh users.
Each byte (eight bits) from the input file is replaced by nine bits in the compressed file.
The added ninth bit is interpreted as the sign of the number.
That is, each character read from the input file is between 0 to 255, while each character written to the encoded file is between -255 and 255.
To understand how this is used, consider the input file: 1, 2,3, 4,2, 2,2, 2,4, and the compressed file generated by the PackBits algorithm: 1, 2,3, 4,2,& 3, 4. The compression program simply transfers each number from the input file to the compressed file, with the exception of the run: 2,2,2,2.
This is represented in the compressed file by the two numbers: 2,-3.
The first number ("2") indicates what character the run consists of.
The second number ("-3") indicates the number of characters in the run, found by taking the absolute value and adding one.
For instance, 4,-2 means 4,4,4; 21,-4 means 21,21,21,21,21, etc. original data stream: run-length encoded: FIGURE 27- Example of run-length encoding.
Each run of zeros is replaced by two characters in the compressed file: a zero to indicate that compression is occurring, followed by the number of zeros in the run.
An inconvenience with PackBits is that the nine bits must be reformatted into the standard eight bit bytes used in computer storage and transmission.
A useful modification to this scheme can be made when the input is restricted to be ASCII text.
As shown in Table 27-2, each ASCII character is usually stored as a full byte (eight bits), but really only uses seven of the bits to identify the character.
In other words, the values 127 through 255 are not defined with any standardized meaning, and do not need to be stored or transmitted.
This allows the eighth bit to indicate if run-length encoding is in progress.
Huffman Encoding This method is named after D.A. Huffman, who developed the procedure in the 1950s.
Figure 27-2 shows a histogram of the byte values from a large ASCII file.
More than 96% of this file consists of only 31 characters: the lower case letters, the space, the comma, the period, and the carriage return.
This observation can be used to make an appropriate compression scheme for this file.
To start, we will assign each of these 31 common characters a five bit binary code: 00000 = "a", 00001 = "b", 00010 = "c", etc.
This allows 96% of the file to be reduced in size by 5/8.
The last of the five bit codes, 11111, will be a flag indicating that the character being transmitted is not one of the common characters.
The next eight bits in the file indicate what the character is, according to the standard ASCII assignment.
This results in 4% of the characters in the input file requiring 5+8=13 bits.
The idea is to assign frequently used characters fewer bits, and seldom used characters TABLE 27- ASCII codes.
This is a long established standard for allowing letters and numbers to be represented in digital form.
Each printable character is assigned a number between 32 and 127, while the numbers between 0 and 31 are used for various control actions.
Even though only codes are defined, ASCII characters are usually stored as a full byte (8 bits).
The undefined values (128 to 255) are often used for Greek letters, math symbols, and various geometric patterns; however, this is not standardized.
Many of the control characters (0 to 31) are based on older communications networks, and are not applicable to computer technology.
This is a histogram of the ASCII values from a chapter in this book.
The most common characters are the lower case letters, the space and the carriage return.
Fractional occurence space lower case letters upper case letters & numbers Byte value more bits.
In this example, the average number of bits required per original character is: 0.96 ×5 % 0.04 ×13 ' 5.32 .
In other words, an overall compression ratio of: 8 bits/5.32
Huffman encoding takes this idea to the extreme.
Characters that occur most often, such the space and period, may be assigned as few as one or two bits.
Infrequently used characters, such as: !, @, #, $ and %, may require a dozen or more bits.
In mathematical terms, the optimal situation is reached when the number of bits used for each character is proportional to the logarithm of the character's probability of occurrence.
A clever feature of Huffman encoding is how the variable length codes can be packed together.
Imagine receiving a serial data stream of ones and zeros.
If each character is represented by eight bits, you can directly separate one character from the next by breaking off 8 bit chunks.
Now consider a Huffman encoded data stream, where each character can have a variable number of bits.
How do you separate one character from the next?
The answer lies in the proper selection of the Huffman codes that enable the correct separation.
An example will illustrate how this works.
Figure 27-3 shows a simplified Huffman encoding scheme.
The characters A through G occur in the original data stream with the probabilities shown.
Since the character A is the most common, we will represent it with a single bit, the code: 1.
The next most common character, B, receives two bits, the code: 01.
This continues to the least frequent character, G, being assigned six bits, 000011.
As shown in this illustration, the variable length codes are resorted into eight bit groups, the standard for computer use.
When uncompression occurs, all the eight bit groups are placed end-to-end to form a long serial string of ones and zeros.
Look closely at the encoding table of Fig. 27-3, and notice how each code consists of two parts: a number of zeros before a one, and an optional binary code after the one.
This allows the binary data stream to be separated into codes without the need for delimiters or other marker between the codes.
The uncompression program Example Encoding Table FIGURE 27- Huffman encoding.
The encoding table assigns each of the seven letters used in this example a variable length binary code, based on its probability of occurrence.
The original data stream composed of these 7 characters is translated by this table into the Huffman encoded data.
Since each of the Huffman codes is a different length, the binary data need to be regrouped into standard 8 bit bytes for storage and transmission.
The way that the codes are formed insures that no ambiguity exists in the separation.
A more sophisticated version of the Huffman approach is called arithmetic encoding.
In this scheme, sequences of characters are represented by individual codes, according to their probability of occurrence.
This has the advantage of better data compression, say 5-10%.
Run-length encoding followed by either Huffman or arithmetic encoding is also a common strategy.
As you might expect, these types of algorithms are very complicated, and usually left to data compression specialists.
To implement Huffman or arithmetic encoding, the compression and uncompression algorithms must agree on the binary codes used to represent each character (or groups of characters).
This can be handled in one of two ways.
The simplest is to use a predefined encoding table that is always the same, regardless of the information being compressed.
More complex schemes use encoding optimized for the particular data being used.
This requires that the encoding table be included in the compressed file for use by the uncompression program.
Both methods are common.
Delta Encoding In science, engineering, and mathematics, the Greek letter delta ( )) is used to denote the change in a variable.
The term delta encoding, refers to delta delta delta move original data stream: 17 2 5 0 0 -3 -6 -5 79 6 1 0 0 -1 -1 0 1 -2 -3 -3 - delta encoded: FIGURE 27- Example of delta encoding.
The first value in the encoded file is the same as the first value in the original file.
Thereafter, each sample in the encoded file is the difference between the current and last sample in the original file.
Figure 274 shows an example of how this is done.
The first value in the delta encoded file is the same as the first value in the original data.
All the following values in the encoded file are equal to the difference (delta) between the corresponding value in the input file, and the previous value in the input file.
Delta encoding can be used for data compression when the values in the original data are smooth, that is, there is typically only a small change between adjacent values.
This is not the case for ASCII text and executable code; however, it is very common when the file represents a signal.
For instance, Fig. 27-5a shows a segment of an audio signal, digitized to 8 bits, with each sample between -127 and 127. Figure 27-5b shows the delta encoded version of this signal.
The key feature is that the delta encoded signal has a lower amplitude than the original signal.
In other words, delta encoding has increased the probability that each sample's value will be near zero, and decreased the probability that it will be far from zero.
This uneven probability is just the thing that Huffman encoding needs to operate.
If the original signal is not changing, or is changing in a straight line, delta encoding will result in runs of samples having the same value.
Delta encoded Sample number Sample number FIGURE 27- Example of delta encoding.
Figure (a) is an audio signal digitized to 8 bits.
Figure (b) shows the delta encoded version of this signal.
Delta encoding is useful for data compression if the signal being encoded varies slowly from sample-to-sample.
This is what run-length encoding requires.
Correspondingly, delta encoding followed by Huffman and/or run-length encoding is a common strategy for compressing signals.
The idea used in delta encoding can be expanded into a more complicated technique called Linear Predictive Coding, or LPC.
To understand LPC, imagine that the first 99 samples from the input signal have been encoded, and we are about to work on sample number 100.
We then ask ourselves: based on the first 99 samples, what is the most likely value for sample 100?
In delta encoding, the answer is that the most likely value for sample 100 is the same as the previous value, sample 99.
This expected value is used as a reference to encode sample 100.
That is, the difference between the sample and the expectation is placed in the encoded file.
LPC expands on this by making a better guess at what the most probable value is.
This is done by looking at the last several samples, rather than just the last sample.
The algorithms used by LPC are similar to recursive filters, making use of the z-transform and other intensively mathematical techniques.
LZW Compression LZW compression is named after its developers, A. Lempel and J. Ziv, with later modifications by Terry A. Welch.
It is the foremost technique for general purpose data compression due to its simplicity and versatility.
Typically, you can expect LZW to compress text, executable code, and similar data files to about one-half their original size.
LZW also performs well when presented with extremely redundant data files, such as tabulated numbers, computer source code, and acquired signals.
Compression ratios of 5:1 are common for these cases.
LZW is the basis of several personal computer utilities that claim to "double the capacity of your hard drive."
LZW compression is always used in GIF image files, and offered as an option in TIFF and PostScript.
LZW compression is protected under U.S. patent number 4,558,302, granted December 10, 1985 to Sperry Corporation (now the Unisys Corporation).
For information on commercial licensing, contact: Welch Licensing Department, Law Department, M/SC2SW1, Unisys Corporation, Blue Bell, Pennsylvania, 19424-0001.
LZW compression uses a code table, as illustrated in Fig. 27-6.
A common choice is to provide 4096 entries in the table.
In this case, the LZW encoded data consists entirely of 12 bit codes, each referring to one of the entries in the code table.
Uncompression is achieved by taking each code from the compressed file, and translating it through the code table to find what character or characters it represents.
Codes 0-255 in the code table are always assigned to represent single bytes from the input file.
For example, if only these first 256 codes were used, each byte in the original file would be converted into 12 bits in the LZW encoded file, resulting in a 50% larger file size.
During uncompression, each 12 bit code would be translated via the code table back into the single bytes.
Of course, this wouldn't be a useful situation.
Example Code Table identical code unique code FIGURE 27- Example of code table compression.
This is the basis of the popular LZW compression method.
Encoding occurs by identifying sequences of bytes in the original file that exist in the code table.
The 12 bit code representing the sequence is placed in the compressed file instead of the sequence.
The first 256 entries in the table correspond to the single byte values, 0 to 255, while the remaining entries correspond to sequences of bytes.
The LZW algorithm is an efficient way of generating the code table based on the particular data being compressed.
For example, code 523 may represent the sequence of three bytes: 231 124 234.
Each time the compression algorithm encounters this sequence in the input file, code 523 is placed in the encoded file.
During uncompression, code 523 is translated via the code table to recreate the true 3 byte sequence.
The longer the sequence assigned to a single code, and the more often the sequence is repeated, the higher the compression achieved.
Although this is a simple approach, there are two major obstacles that need to be overcome: (1) how to determine what sequences should be in the code table, and (2) how to provide the uncompression program the same code table used by the compression program.
The LZW algorithm exquisitely solves both these problems.
When the LZW program starts to encode a file, the code table contains only the first 256 entries, with the remainder of the table being blank.
This means that the first codes going into the compressed file are simply the single bytes from the input file being converted to 12 bits.
As the encoding continues, the LZW algorithm identifies repeated sequences in the data, and adds them to the code table.
Compression starts the second time a sequence is encountered.
The key point is that a sequence from the input file is not added to the code table until it has already been placed in the compressed file as individual characters (codes 0 to 255).
This is important because it allows the uncompression program to reconstruct the code table directly from the compressed data, without having to transmit the code table separately.
LZW compression flowchart.
The variable, CHAR, is a single byte.
The variable, STRING, is a variable length sequence of bytes.
Data are read from the input file (box 1 & 2) as single bytes, and written to the compressed file (box 4) as 12 bit codes.
Table 27-3 shows an example of this algorithm.
Figure 27-7 shows a flowchart for LZW compression.
Table 27-3 provides the step-by-step details for an example input file consisting of 45 bytes, the ASCII text string: the/rain/in/Spain/falls/mainly/on/the/plain.
When we say that the LZW algorithm reads the character "a" from the input file, we mean it reads the value: 01100001 (97 expressed in 8 bits), where 97 is "a" in ASCII.
When we say it writes the character "a" to the encoded file, we mean it writes: 000001100001 (97 expressed in 12 bits).
Comments first character- no action first match found matches ai, ain not in table yet ain added to table matches ai matches longer string, ain matches th, the not in table yet the added to table matches ai matches longer string ain end of file, output STRING TABLE 27- LZW example.
This shows the compression of the phrase: the/rain/in/Spain/falls/mainly/on/the/plain/.
The compression algorithm uses two variables: CHAR and STRING.
The variable, CHAR, holds a single character, i.e., a single byte value between and 255.
The variable, STRING, is a variable length string, i.e., a group of one or more characters, with each character being a single byte.
In box 1 of Fig. 27-7, the program starts by taking the first byte from the input file, and placing it in the variable, STRING.
Table 27-3 shows this action in line 1.
This is followed by the algorithm looping for each additional byte in the input file, controlled in the flow diagram by box 8.
Each time a byte is read from the input file (box 2), it is stored in the variable, CHAR.
The data table is then searched to determine if the concatenation of the two variables, STRING+CHAR, has already been assigned a code (box 3).
If a match in the code table is not found, three actions are taken, as shown in boxes 4, 5 & 6.
In box 4, the 12 bit code corresponding to the contents of the variable, STRING, is written to the compressed file.
In box 5, a new code is created in the table for the concatenation of STRING+CHAR.
In box 6, the variable, STRING, takes the value of the variable, CHAR.
An example of these actions is shown in lines 2 through 10 in Table 27-3, for the first 10 bytes of the example file.
When a match in the code table is found (box 3), the concatenation of STRING+CHAR is stored in the variable, STRING, without any other action taking place (box 7).
That is, if a matching sequence is found in the table, no action should be taken before determining if there is a longer matching sequence also in the table.
An example of this is shown in line 11, where the sequence: STRING+CHAR = in, is identified as already having a code in the table.
In line 12, the next character from the input file, /, is added to the sequence, and the code table is searched for: in/.
Since this longer sequence is not in the table, the program adds it to the table, outputs the code for the shorter sequence that is in the table (code 262), and starts over searching for sequences beginning with the character, /.
This flow of events is continued until there are no more characters in the input file.
The program is wrapped up with the code corresponding to the current value of STRING being written to the compressed file (as illustrated in box 9 of Fig. 27-7 and line 45 of Table 27-3).
A flowchart of the LZW uncompression algorithm is shown in Fig. 27-8.
Each code is read from the compressed file and compared to the code table to provide the translation.
As each code is processed in this manner, the code table is updated so that it continually matches the one used during the compression.
However, there is a small complication in the uncompression routine.
There are certain combinations of data that result in the uncompression algorithm receiving a code that does not yet exist in its code table.
This contingency is handled in boxes 4,5 & 6.
Only a few dozen lines of code are required for the most elementary LZW programs.
The real difficulty lies in the efficient management of the code table.
The brute force approach results in large memory requirements and a slow program execution.
Several tricks are used in commercial LZW programs to improve their performance.
For instance, the memory problem arises because it is not know beforehand how long each of the character strings for each code will be.
Most LZW programs handle this by taking advantage of the redundant nature of the code table.
For example, look at line 29 in Table 27-3, where code 278 is defined to be ainl.
Rather than storing these four bytes, code 278 could be stored as: code 269 + l, where code was previously defined as ain in line 17.
Likewise, code 269 would be stored as: code 261 + n, where code 261 was previously defined as ai in line 7.
This pattern always holds: every code can be expressed as a previous code plus one new character.
The execution time of the compression algorithm is limited by searching the code table to determine if a match is present.
As an analogy, imagine you want to find if a friend's name is listed in the telephone directory.
The catch is, the only directory you have is arranged by telephone number, not alphabetical order.
This requires you to search page after page trying to find the name you want.
This inefficient situation is exactly the same as searching all 4096 codes for a match to a specific character string.
The answer: organize the code table so that what you are looking for tells you where to look (like a partially alphabetized telephone directory).
In other words, don't assign the 4096 codes to sequential locations in memory.
Rather, divide the memory into sections based on what sequences will be stored there.
For example, suppose we want to find if the sequence: code 329 + x, is in the code table.
The code table should be organized so that the "x" indicates where to starting looking.
There are many schemes for this type of code table management, and they can become quite complicated.
This brings up the last comment on LZW and similar compression schemes: it is a very competitive field.
While the basics of data compression are relatively simple, the kinds of programs sold as commercial products are extremely sophisticated.
Companies make money by selling you programs that perform compression, and jealously protect their trade-secrets through patents and the like.
Don't expect to achieve the same level of performance as these programs in a few hours work.
JPEG (Transform Compression) Many methods of lossy compression have been developed; however, a family of techniques called transform compression has proven the most valuable.
The best example of transform compression is embodied in the popular JPEG standard of image encoding.
JPEG is named after its origin, the Joint Photographers Experts Group.
We will describe the operation of JPEG to illustrate how lossy compression works.
We have already discussed a simple method of lossy data compression, coarser sampling and/or quantization (CS&Q in Table 27-1).
This involves reducing the number of bits per sample or entirely discard some of the samples.
Both these procedures have the desired effect: the data file becomes smaller at the expense of signal quality.
As you might expect, these simple methods do not work very well.
JPEG transform compression starts by breaking the image into 8×8 groups, each containing 64 pixels.
Three of these 8×8 groups are enlarged in this figure, showing the values of the individual pixels, a single byte value between 0 and 255.
Transform compression is based on a simple premise: when the signal is passed through the Fourier (or other) transform, the resulting data values will no longer be equal in their information carrying roles.
In particular, the low frequency components of a signal are more important than the high frequency components.
Removing 50% of the bits from the high frequency components might remove, say, only 5% of the encoded information.
As shown in Fig. 27-9, JPEG compression starts by breaking the image into 8×8 pixel groups.
The full JPEG algorithm can accept a wide range of bits per pixel, including the use of color information.
In this example, each pixel is a single byte, a grayscale value between 0 and 255.
These 8×8 pixel groups are treated independently during compression.
That is, each group is initially represented by 64 bytes.
After transforming and removing data, each group is represented by, say, 2 to 20 bytes.
During uncompression, the inverse transform is taken of the 2 to 20 bytes to create an approximation of the original 8×8 group.
These approximated groups are then fitted together to form the uncompressed image.
Why use 8×8 pixel groups instead of, for instance, 16×16?
The 8×8 grouping was based on the maximum size that integrated circuit technology could handle at the time the standard was developed.
In any event, the 8×8 size works well, and it may or may not be changed in the future.
Many different transforms have been investigated for data compression, some of them invented specifically for this purpose.
For instance, the KarhunenLoeve transform provides the best possible compression ratio, but is difficult to implement.
The Fourier transform is easy to use, but does not provide adequate compression.
After much competition, the winner is a relative of the Fourier transform, the Discrete Cosine Transform (DCT).
Just as the Fourier transform uses sine and cosine waves to represent a signal, the DCT only uses cosine waves.
There are several versions of the DCT, with slight differences in their mathematics.
As an example of one version, imagine a 129 point signal, running from sample 0 to sample 128.
Now, make this a 256 point signal by duplicating samples 1 through 127 and adding them as samples 255 to 130.
That is: 0, 1, 2, þ, 127, 128, 127, þ, 2, 1. Taking the Fourier transform of this 256 point signal results in a frequency spectrum of 129 points, spread between 0 and 128.
Since the time domain signal was forced to be symmetrical, the spectrum's imaginary part will be composed of all zeros.
In other words, we started with a 129 point time domain signal, and ended with a frequency spectrum of 129 points, each the amplitude of a cosine wave.
Voila, the DCT!
When the DCT is taken of an 8×8 group, it results in an 8×8 spectrum.
In other words, 64 numbers are changed into 64 other numbers.
All these values are real; there is no complex mathematics here.
Just as in Fourier analysis, each value in the spectrum is the amplitude of a basis function.
Figure 27- shows 6 of the 64 basis functions used in an 8×8 DCT, according to where the amplitude sits in the spectrum.
The 8×8 DCT basis functions are given by: The DCT basis functions.
The DCT spectrum consists of an 8×8 array, with each element in the array being an amplitude of one of the 64 basis functions.
Six of these basis functions are shown here, referenced to where the corresponding amplitude resides.
The DCT calculates the spectrum by correlating the 8×8 pixel group with each of the basis functions.
That is, each spectral value is found by multiplying the appropriate basis function by the 8×8 pixel group, and then summing the products.
Two adjustments are then needed to finish the DCT calculation (just as with the Fourier transform).
First, divide the 15 spectral values in row and column 0 by two.
Second, divide all 64 values in the spectrum by 16.
The inverse DCT is calculated by assigning each of the amplitudes in the spectrum to the proper basis function, and summing to recreate the spatial domain.
No extra steps are required.
These are exactly the same concepts as in Fourier analysis, just with different basis functions.
Figure 27-11 illustrates JPEG encoding for the three 8×8 groups identified in Fig. 27-9.
The left column, Figs.
The center column, Figs.
Example of JPEG encoding.
The left column shows three 8×8 pixel groups, the same ones shown in Fig. 27-9.
The center column shows the DCT spectra of these three groups.
The third column shows the error in the uncompressed pixel values resulting from using a finite number of bits to represent the spectrum.
The right column, Figs.
For instance, (g) is formed by truncating each of the samples in (d) to ten bits, taking the inverse DCT, and then subtracting the reconstructed image from the original.
Likewise, (h) and (i) are formed by truncating each sample in the spectrum to eight and five bits, respectively.
As expected, the error in the reconstruction increases as fewer bits are used to represent the data.
As an example of this bit truncation, the spectra shown in the center column are represented with bits per spectral value, arranged as 0 to 255 for the DC component, and - to 127 for the other values.
The second method of compressing the frequency domain is to discard some of the 64 spectral values.
As shown by the spectra in Fig. 27-11, nearly all of the signal is contained in the low frequency components.
This means the highest frequency components can be eliminated, while only degrading the signal a small amount.
Figure 27-12 shows an example of the image distortion that occurs when various numbers of the high frequency components are deleted.
The 8×8 group used in this example is the eye image of Fig. 27-10.
Figure (d) shows the correct reconstruction using all 64 spectral values.
The remaining figures show the reconstruction using the indicated number of lowest frequency coefficients.
As illustrated in (c), even removing three-fourths of the highest frequency components produces little error in the reconstruction.
Even better, the error that does occur looks very much like random noise.
JPEG is good example of how several data compression schemes can be combined for greater effectiveness.
The entire JPEG procedure is outlined in the following steps.
First, the image is broken into the 8×8 groups.
Second, the DCT is taken of each group.
Third, each 8×8 spectrum is compressed by the above methods: reducing the number of bits and eliminating some of the components.
This takes place in a single step, controlled by a quantization table.
Two examples of quantization tables are shown in Fig. 27-13.
Each value in the spectrum is divided by the matching value in the quantization table, and the result rounded to the nearest integer.
For instance, the upper-left value of the quantization table is one, Example of JPEG reconstruction.
The 8×8 pixel group used in this example is the eye in Fig. 27-9.
As shown, less than 1/4 of the 64 values are needed to achieve a good approximation to the correct image.
These are two example quantization tables that might be used during compression.
Each value in the DCT spectrum is divided by the corresponding value in the quantization table, and the result rounded to the nearest integer.
In comparison, the lower-right entry in (a) is 16, meaning that the original range of -127 to 127 is reduced to only -7 to 7. In other words, the value has been reduced in precision from eight bits to four bits.
In a more extreme case, the lower-right entry in (b) is 256, completely eliminating the spectral value.
In the fourth step of JPEG encoding, the modified spectrum is converted from an 8×8 array into a linear sequence.
The serpentine pattern shown in Figure 27-14 is used for this step, placing all of the high frequency components together at the end of the linear sequence.
This groups the zeros from the eliminated components into long runs.
The fifth step compresses these runs of zeros by run-length encoding.
In the sixth step, the sequence is encoded by either Huffman or arithmetic encoding to form the final compressed file.
The amount of compression, and the resulting loss of image quality, can be selected when the JPEG compression program is run.
Figure 27-15 shows the type of image distortion resulting from high compression ratios.
With the 45: compression ratio shown, each of the 8×8 groups is represented by only about 12 bits.
Close inspection of this image shows that six of the lowest frequency basis functions are represented to some degree.
JPEG serial conversion.
A serpentine pattern used to convert the 8×8 DCT spectrum into a linear sequence of 64 values.
This places all of the high frequency components together, where the large number of zeros can be efficiently compressed with run-length encoding.
Example of JPEG distortion.
Figure (a) shows the original image, while (b) and (c) shows restored images using compression ratios of 10:1 and 45:1, respectively.
The high compression ratio used in (c) results in each 8×8 pixel group being represented by less than 12 bits.
With 45:1 compression Why is the DCT better than the Fourier transform for image compression?
The main reason is that the DCT has one-half cycle basis functions, i.e., S[0,1] and S[1,0].
As shown in Fig. 27-10, these gently slope from one side of the array to the other.
In comparison, the lowest frequencies in the Fourier transform form one complete cycle.
Images nearly always contain regions where the brightness is gradually changing over a region.
Using a basis function that matches this basic pattern allows for better compression.
MPEG is a compression standard for digital video sequences, such as used in computer video and digital television networks.
In addition, MPEG also provides for the compression of the sound track associated with the video.
The name comes from its originating organization, the Moving Pictures Experts Group.
If you think JPEG is complicated, MPEG is a nightmare!
MPEG is something you buy, not try to write yourself.
The future of this technology is to encode the compression and uncompression algorithms directly into integrated circuits.
The potential of MPEG is vast.
Think of thousands of video channels being carried on a single optical fiber running into your home.
This is a key technology of the 21st century.
In addition to reducing the data rate, MPEG has several important features.
The movie can be played forward or in reverse, and at either normal or fast speed.
The encoded information is random access, that is, any individual frame in the sequence can be easily displayed as a still picture.
This goes along with making the movie editable, meaning that short segments from the movie can be encoded only with reference to themselves, not the entire sequence.
MPEG is designed to be robust to errors.
The last thing you want is for a single bit error to cause a disruption of the movie.
The approach used by MPEG can be divided into two types of compression: within-the-frame and between-frame.
Within-the-frame compression means that individual frames making up the video sequence are encoded as if they were ordinary still images.
This compression is preformed using the JPEG standard, with just a few variations.
In MPEG terminology, a frame that has been encoded in this way is called an intra-coded or I-picture.
Most of the pixels in a video sequence change very little from one frame to the next.
Unless the camera is moving, most of the image is composed of a background that remains constant over dozens of frames.
MPEG takes advantage of this with a sophisticated form of delta encoding to compress the redundant information between frames.
After compressing one of the frames as an I-picture, MPEG encodes successive frames as predictive-coded or Ppictures.
That is, only the pixels that have changed since the I-picture are included in the P-picture.
While these two compression schemes form the backbone of MPEG, the actual implementation is immensely more sophisticated than described here.
For example, a P-picture can be referenced to an I-picture that has been shifted, accounting for motion of objects in the image sequence.
There are also bidirectional predictive-coded or B-pictures.
These are referenced to both a previous and a future I-picture.
This handles regions in the image that gradually change over many of frames.
The individual frames can also be stored out-of-order in the compressed data to facilitate the proper sequencing of the I, P, and B-pictures.
The addition of color and sound makes this all the more complicated.
The main distortion associated with MPEG occurs when large sections of the image change quickly.
In effect, a burst of information is needed to keep up with the rapidly changing scenes.
If the data rate is fixed, the viewer notices "blocky" patterns when changing from one scene to the next.
This can be minimized in networks that transmit multiple video channels simultaneously, such as cable television.
The sudden burst of information needed to support a rapidly changing scene in one video channel, is averaged with the modest requirements of the relatively static scenes in the other channels.
Digital Signal Processors Digital Signal Processing is carried out by mathematical operations.
In comparison, word processing and similar programs merely rearrange stored data.
This means that computers designed for business and other general applications are not optimized for algorithms such as digital filtering and Fourier analysis.
Digital Signal Processors are microprocessors specifically designed to handle Digital Signal Processing tasks.
These devices have seen tremendous growth in the last decade, finding use in everything from cellular telephones to advanced scientific instruments.
In fact, hardware engineers use "DSP" to mean Digital Signal Processor, just as algorithm developers use "DSP" to mean Digital Signal Processing.
This chapter looks at how DSPs are different from other types of microprocessors, how to decide if a DSP is right for your application, and how to get started in this exciting new field.
In the next chapter we will take a more detailed look at one of these sophisticated products: the Analog Devices SHARC® family.
How DSPs are Different from Other Microprocessors In the 1960s it was predicted that artificial intelligence would revolutionize the way humans interact with computers and other machines.
It was believed that by the end of the century we would have robots cleaning our houses, computers driving our cars, and voice interfaces controlling the storage and retrieval of information.
This hasn't happened; these abstract tasks are far more complicated than expected, and very difficult to carry out with the step-by-step logic provided by digital computers.
However, the last forty years have shown that computers are extremely capable in two broad areas, (1) data manipulation, such as word processing and database management, and (2) mathematical calculation, used in science, engineering, and Digital Signal Processing.
All microprocessors can perform both tasks; however, it is difficult (expensive) to make a device that is optimized for both.
There are technical tradeoffs in the hardware design, such as the size of the instruction set and how interrupts are handled.
Even Data Manipulation Math Calculation Typical Applications Word processing, database management, spread sheets, operating sytems, etc.
Digital Signal Processing, motion control, scientific and engineering simulations, etc. Operations data movement (A º B) value testing (If A=B then ...) addition (A+B=C ) multiplication (A×B=C ) FIGURE 28- Data manipulation versus mathematical calculation.
Digital computers are useful for two general tasks: data manipulation and mathematical calculation.
Data manipulation is based on moving data and testing inequalities, while mathematical calculation uses multiplication and addition.
As a broad generalization, these factors have made traditional microprocessors, such as the Pentium®, primarily directed at data manipulation.
Similarly, DSPs are designed to perform the mathematical calculations needed in Digital Signal Processing. Figure 28-1 lists the most important differences between these two categories.
Data manipulation involves storing and sorting information.
For instance, consider a word processing program.
The basic task is to store the information (typed in by the operator), organize the information (cut and paste, spell checking, page layout, etc.), and then retrieve the information (such as saving the document on a floppy disk or printing it with a laser printer).
These tasks are accomplished by moving data from one location to another, and testing for inequalities (A=B, A<B, etc.).
As an example, imagine sorting a list of words into alphabetical order.
Each word is represented by an 8 bit number, the ASCII value of the first letter in the word.
Alphabetizing involved rearranging the order of the words until the ASCII values continually increase from the beginning to the end of the list.
This can be accomplished by repeating two steps over-and-over until the alphabetization is complete.
First, test two adjacent entries for being in alphabetical order (IF A>B THEN ...).
Second, if the two entries are not in alphabetical order, switch them so that they are (AWB).
When this two step process is repeated many times on all adjacent pairs, the list will eventually become alphabetized.
As another example, consider how a document is printed from a word processor.
The computer continually tests the input device (mouse or keyboard) for the binary code that indicates "print the document."
When this code is detected, the program moves the data from the computer's memory to the printer.
Here we have the same two basic operations: moving data and inequality testing.
While mathematics is occasionally used in this type of application, it is infrequent and does not significantly affect the overall execution speed.
In comparison, the execution speed of most DSP algorithms is limited almost completely by the number of multiplications and additions required.
For example, Fig. 28-2 shows the implementation of an FIR digital filter, the most common DSP technique.
Using the standard notation, the input signal is referred to by x[ ], while the output signal is denoted by y[ ] .
Our task is to calculate the sample at location n in the output signal, i.e., y[n] .
An FIR filter performs this calculation by multiplying appropriate samples from the input signal by a group of coefficients, denoted by: a0, a1, a2, a3, þ, and then adding the products.
In equation form, y[n] is found by: y[n ] ' a0 x[n ] % a1 x[n&1] % a2 x[n&2] % a3 x[n&3] % a4 x[n&4] % þ This is simply saying that the input signal has been convolved with a filter kernel (i.e., an impulse response) consisting of: a0, a1, a2, a3, þ.
Depending on the application, there may only be a few coefficients in the filter kernel, or many thousands.
While there is some data transfer and inequality evaluation in this algorithm, such as to keep track of the intermediate results and control the loops, the math operations dominate the execution time.
In addition to preforming mathematical calculations very rapidly, DSPs must also have a predictable execution time.
Suppose you launch your desktop computer on some task, say, converting a word-processing document from one form to another.
It doesn't matter if the processing takes ten milliseconds or ten seconds; you simply wait for the action to be completed before you give the computer its next assignment.
In comparison, most DSPs are used in applications where the processing is continuous, not having a defined start or end.
For instance, consider an engineer designing a DSP system for an audio signal, such as a hearing aid.
If the digital signal is being received at 20,000 samples per second, the DSP must be able to maintain a sustained throughput of 20,000 samples per second.
However, there are important reasons not to make it any faster than necessary.
As the speed increases, so does the cost, the power consumption, the design difficulty, and so on.
This makes an accurate knowledge of the execution time critical for selecting the proper device, as well as the algorithms that can be applied.
Circular Buffering Digital Signal Processors are designed to quickly carry out FIR filters and similar techniques.
To understand the hardware, we must first understand the algorithms.
In this section we will make a detailed list of the steps needed to implement an FIR filter.
In the next section we will see how DSPs are designed to perform these steps as efficiently as possible.
To start, we need to distinguish between off-line processing and real-time processing.
In off-line processing, the entire input signal resides in the computer at the same time.
For example, a geophysicist might use a seismometer to record the ground movement during an earthquake.
After the shaking is over, the information may be read into a computer and analyzed in some way.
Another example of off-line processing is medical imaging, such as computed tomography and MRI.
The data set is acquired while the patient is inside the machine, but the image reconstruction may be delayed until a later time.
The key point is that all of the information is simultaneously available to the processing program.
This is common in scientific research and engineering, but not in consumer products.
Off-line processing is the realm of personal computers and mainframes.
In real-time processing, the output signal is produced at the same time that the input signal is being acquired.
For example, this is needed in telephone communication, hearing aids, and radar.
These applications must have the information immediately available, although it can be delayed by a short amount.
For instance, a 10 millisecond delay in a telephone call cannot be detected by the speaker or listener.
Likewise, it makes no difference if a radar signal is delayed by a few seconds before being displayed to the operator.
Real-time applications input a sample, perform the algorithm, and output a sample, over-and-over.
Alternatively, they may input a group MEMORY ADDRESS STORED VALUE MEMORY ADDRESS STORED VALUE Circular buffer operation.
Circular buffers are used to store the most recent values of a continually updated signal.
This illustration shows how an eight sample circular buffer might appear at some instant in time (a), and how it would appear one sample later (b). of samples, perform the algorithm, and output a group of samples.
This is the world of Digital Signal Processors.
Now look back at Fig. 28-2 and imagine that this is an FIR filter being implemented in real-time.
To calculate the output sample, we must have access to a certain number of the most recent samples from the input.
For example, suppose we use eight coefficients in this filter, a0, a1, þ a7 .
This means we must know the value of the eight most recent samples from the input signal, x[n], x[n& 1], þ x[n& 7] .
These eight samples must be stored in memory and continually updated as new samples are acquired.
What is the best way to manage these stored samples?
The answer is circular buffering.
Figure 28-3 illustrates an eight sample circular buffer.
We have placed this circular buffer in eight consecutive memory locations, 20041 to 20048.
Figure (a) shows how the eight samples from the input might be stored at one particular instant in time, while (b) shows the changes after the next sample is acquired.
The idea of circular buffering is that the end of this linear array is connected to its beginning; memory location 20041 is viewed as being next to 20048, just as 20044 is next to 20045.
You keep track of the array by a pointer (a variable whose value is an address) that indicates where the most recent sample resides.
For instance, in (a) the pointer contains the address 20044, while in (b) it contains 20045.
When a new sample is acquired, it replaces the oldest sample in the array, and the pointer is moved one address ahead.
Circular buffers are efficient because only one value needs to be changed when a new sample is acquired.
Four parameters are needed to manage a circular buffer.
First, there must be a pointer that indicates the start of the circular buffer in memory (in this example, 20041).
Second, there must be a pointer indicating the end of the array (e.g., 20048), or a variable that holds its length (e.g., 8).
Third, the step size of the memory addressing must be specified.
In Fig. 28-3 the step size is one, for example: address 20043 contains one sample, address 20044 contains the next sample, and so on.
This is frequently not the case.
For instance, the addressing may refer to bytes, and each sample may require two or four bytes to hold its value.
In these cases, the step size would need to be two or four, respectively.
These three values define the size and configuration of the circular buffer, and will not change during the program operation.
The fourth value, the pointer to the most recent sample, must be modified as each new sample is acquired.
In other words, there must be program logic that controls how this fourth value is updated based on the value of the first three values.
While this logic is quite simple, it must be very fast.
This is the whole point of this discussion; DSPs should be optimized at managing circular buffers to achieve the highest possible execution speed.
As an aside, circular buffering is also useful in off-line processing.
Consider a program where both the input and the output signals are completely contained in memory.
Circular buffering isn't needed for a convolution calculation, because every sample can be immediately accessed.
However, many algorithms are implemented in stages, with an intermediate signal being created between each stage.
For instance, a recursive filter carried out as a series of biquads operates in this way.
The brute force method is to store the entire length of each intermediate signal in memory.
Circular buffering provides another option: store only those intermediate samples needed for the calculation at hand.
This reduces the required amount of memory, at the expense of a more complicated algorithm.
The important idea is that circular buffers are useful for off-line processing, but critical for real-time applications.
Now we can look at the steps needed to implement an FIR filter using circular buffers for both the input signal and the coefficients.
This list may seem trivial and overexamined- it's not!
The efficient handling of these individual tasks is what separates a DSP from a traditional microprocessor.
For each new sample, all the following steps need to be taken: TABLE 28- Architecture of the Digital Signal Processor One of the biggest bottlenecks in executing DSP algorithms is transferring information to and from memory.
This includes data, such as samples from the input signal and the filter coefficients, as well as program instructions, the binary codes that go into the program sequencer.
For example, suppose we need to multiply two numbers that reside somewhere in memory.
To do this, we must fetch three binary values from memory, the numbers to be multiplied, plus the program instruction describing what to do. Figure 28-4a shows how this seemingly simple task is done in a traditional microprocessor.
This is often called a Von Neumann architecture, after the brilliant American mathematician John Von Neumann (1903-1957).
Von Neumann guided the mathematics of many important discoveries of the early twentieth century.
His many achievements include: developing the concept of a stored program computer, formalizing the mathematics of quantum mechanics, and work on the atomic bomb.
If it was new and exciting, Von Neumann was there!
As shown in (a), a Von Neumann architecture contains a single memory and a single bus for transferring data into and out of the central processing unit (CPU).
Multiplying two numbers requires at least three clock cycles, one to transfer each of the three numbers over the bus from the memory to the CPU.
We don't count the time to transfer the result back to memory, because we assume that it remains in the CPU for additional manipulation (such as the sum of products in an FIR filter).
The Von Neumann design is quite satisfactory when you are content to execute all of the required tasks in serial.
In fact, most computers today are of the Von Neumann design.
We only need other architectures when very fast processing is required, and we are willing to pay the price of increased complexity.
This leads us to the Harvard architecture, shown in (b).
This is named for the work done at Harvard University in the 1940s under the leadership of Howard Aiken (1900-1973).
As shown in this illustration, Aiken insisted on separate memories for data and program instructions, with separate buses for each.
Since the buses operate independently, program instructions and data can be fetched at the same time, improving the speed over the single bus design.
Most present day DSPs use this dual bus architecture.
Figure (c) illustrates the next level of sophistication, the Super Harvard Architecture.
This term was coined by Analog Devices to describe the internal operation of their ADSP-2106x and new ADSP-211xx families of Digital Signal Processors.
These are called SHARC® DSPs, a contraction of the longer term, Super Harvard ARChitecture.
The idea is to build upon the Harvard architecture by adding features to improve the throughput.
While the SHARC DSPs are optimized in dozens of ways, two areas are important enough to be included in Fig. 28-4c: an instruction cache, and an I/O controller.
First, let's look at how the instruction cache improves the performance of the Harvard architecture.
A handicap of the basic Harvard design is that the data memory bus is busier than the program memory bus.
When two numbers are multiplied, two binary values (the numbers) must be passed over the data memory bus, while only one binary value (the program instruction) is passed over the program memory bus.
To improve upon this situation, we start by relocating part of the "data" to program memory.
For instance, we might place the filter coefficients in program memory, while keeping the input signal in data memory.
At first glance, this doesn't seem to help the situation; now we must transfer one value over the data memory bus (the input signal sample), but two values over the program memory bus (the program instruction and the coefficient).
In fact, if we were executing random instructions, this situation would be no better at all.
However, DSP algorithms generally spend most of their execution time in loops, such as instructions 6-12 of Table 28-1.
This means that the same set of program instructions will continually pass from program memory to the CPU.
The Super Harvard architecture takes advantage of this situation by including an instruction cache in the CPU.
This is a small memory that contains about 32 of the most recent program instructions.
The first time through a loop, the program instructions must be passed over the program memory bus.
This results in slower operation because of the conflict with the coefficients that must also be fetched along this path.
However, on additional executions of the loop, the program instructions can be pulled from the instruction cache.
This means that all of the memory to CPU information transfers can be accomplished in a single cycle: the sample from the input signal comes over the data memory bus, the coefficient comes over the program memory bus, and the program instruction comes from the instruction cache.
In the jargon of the field, this efficient transfer of data is called a high memoryaccess bandwidth.
Figure 28-5 presents a more detailed view of the SHARC architecture, showing the I/O controller connected to data memory.
This is how the signals enter and exit the system.
For instance, the SHARC DSPs provides both serial and parallel communications ports.
These are extremely high speed connections.
For example, at a 40 MHz clock speed, there are two serial ports that operate at 40 Mbits/second each, while six parallel ports each provide a 40 Mbytes/second data transfer.
When all six parallel ports are used together, the data transfer rate is an incredible Mbytes/second.
Harvard Architecture ( dual memory ) Program Memory PM address bus instructions only PM data bus DM address bus Memory DM data bus data only c. Super Harvard Architecture ( dual memory, instruction cache, I/O controller ) Program Memory PM address bus instructions and secondary data PM data bus Instruction Cache DM address bus Memory DM data bus data only FIGURE 28- Microprocessor architecture.
The Von Neumann architecture uses a single memory to hold both data and instructions.
In comparison, the Harvard architecture uses separate memories for data and instructions, providing higher speed.
The Super Harvard Architecture improves upon the Harvard design by adding an instruction cache and a dedicated I/O controller.
Controller data This is fast enough to transfer the entire text of this book in only milliseconds!
Just as important, dedicated hardware allows these data streams to be transferred directly into memory (Direct Memory Access, or DMA), without having to pass through the CPU's registers.
In other words, tasks 1 & 14 on our list happen independently and simultaneously with the other tasks; no cycles are stolen from the CPU.
The main buses (program memory bus and data memory bus) are also accessible from outside the chip, providing an additional interface to off-chip memory and peripherals.
This allows the SHARC DSPs to use a four Gigaword (16 Gbyte) memory, accessible at Mwords/second (160 Mbytes/second), for 32 bit data.
Wow!
This type of high speed I/O is a key characteristic of DSPs.
The overriding goal is to move the data in, perform the math, and move the data out before the next sample is available.
Everything else is secondary.
Some DSPs have onboard analog-to-digital and digital-to-analog converters, a feature called mixed signal.
However, all DSPs can interface with external converters through serial or parallel ports.
Now let's look inside the CPU.
At the top of the diagram are two blocks labeled Data Address Generator (DAG), one for each of the two memories.
These control the addresses sent to the program and data memories, specifying where the information is to be read from or written to.
In simpler microprocessors this task is handled as an inherent part of the program sequencer, and is quite transparent to the programmer.
However, DSPs are designed to operate with circular buffers, and benefit from the extra hardware to manage them efficiently.
This avoids needing to use precious CPU clock cycles to keep track of how the data are stored.
For instance, in the SHARC DSPs, each of the two DAGs can control eight circular buffers.
This means that each DAG holds 32 variables (4 per buffer), plus the required logic.
Why so many circular buffers?
Some DSP algorithms are best carried out in stages.
For instance, IIR filters are more stable if implemented as a cascade of biquads (a stage containing two poles and up to two zeros).
Multiple stages require multiple circular buffers for the fastest operation.
The DAGs in the SHARC DSPs are also designed to efficiently carry out the Fast Fourier transform.
In this mode, the DAGs are configured to generate bit-reversed addresses into the circular buffers, a necessary part of the FFT algorithm.
In addition, an abundance of circular buffers greatly simplifies DSP code generation- both for the human programmer as well as high-level language compilers, such as C. The data register section of the CPU is used in the same way as in traditional microprocessors.
In the ADSP-2106x SHARC DSPs, there are 16 general purpose registers of 40 bits each.
These can hold intermediate calculations, prepare data for the math processor, serve as a buffer for data transfer, hold flags for program control, and so on.
If needed, these registers can also be used to control loops and counters; however, the SHARC DSPs have extra hardware registers to carry out many of these functions.
The math processing is broken into three sections, a multiplier, an arithmetic logic unit (ALU), and a barrel shifter.
The multiplier takes the values from two registers, multiplies them, and places the result into another register.
The ALU performs addition, subtraction, absolute value, logical operations (AND, OR, XOR, NOT), conversion between fixed and floating point formats, and similar functions.
Elementary binary operations are carried out by the barrel shifter, such as shifting, rotating, extracting and depositing segments, and so on.
A powerful feature of the SHARC family is that the multiplier and the ALU can be accessed in parallel.
In a single clock cycle, data from registers 0-7 can be passed to the multiplier, data from registers 8-15 can be passed to the ALU, and the two results returned to any of the 16 registers.
There are also many important features of the SHARC family architecture that aren't shown in this simplified illustration.
For instance, an 80 bit accumulator is built into the multiplier to reduce the round-off error associated with multiple fixed-point math operations.
Another interesting PM address bus Program Memory PM Data Address Generator DM Data Address Generator DM address bus Memory Program Sequencer instructions and secondary data data only Instruction Cache PM data bus DM data bus Registers I/O Controller (DMA) Muliplier Shifter High speed I/O (serial, parallel, ADC, DAC, etc.) FIGURE 28- Typical DSP architecture.
Digital Signal Processors are designed to implement tasks in parallel.
This simplified diagram is of the Analog Devices SHARC DSP.
Compare this architecture with the tasks needed to implement an FIR filter, as listed in Table 28-1.
All of the steps within the loop can be executed in a single clock cycle.
These are duplicate registers that can be switched with their counterparts in a single clock cycle.
They are used for fast context switching, the ability to handle interrupts quickly.
When an interrupt occurs in traditional microprocessors, all the internal data must be saved before the interrupt can be handled.
This usually involves pushing all of the occupied registers onto the stack, one at a time.
In comparison, an interrupt in the SHARC family is handled by moving the internal data into the shadow registers in a single clock cycle.
When the interrupt routine is completed, the registers are just as quickly restored.
This feature allows step 4 on our list (managing the sample-ready interrupt) to be handled very quickly and efficiently.
Now we come to the critical performance of the architecture, how many of the operations within the loop (steps 6-12 of Table 28-1) can be carried out at the same time.
Because of its highly parallel nature, the SHARC DSP can simultaneously carry out all of these tasks.
Specifically, within a single clock cycle, it can perform a multiply (step 11), an addition (step 12), two data moves (steps 7 and 9), update two circular buffer pointers (steps 8 and 10), and control the loop (step 6).
There will be extra clock cycles associated with beginning and ending the loop (steps 3, 4, 5 and 13, plus moving initial values into place); however, these tasks are also handled very efficiently.
If the loop is executed more than a few times, this overhead will be negligible.
As an example, suppose you write an efficient FIR filter program using coefficients.
You can expect it to require about 105 to 110 clock cycles per sample to execute (i.e., 100 coefficient loops plus overhead).
This is very impressive; a traditional microprocessor requires many thousands of clock cycles for this algorithm.
Fixed versus Floating Point Digital Signal Processing can be divided into two categories, fixed point and floating point.
These refer to the format used to store and manipulate numbers within the devices.
Fixed point DSPs usually represent each number with a minimum of 16 bits, although a different length can be used.
For instance, Motorola manufactures a family of fixed point DSPs that use 24 bits.
There are four common ways that these 216 ' 65,536 possible bit patterns can represent a number.
In unsigned integer, the stored number can take on any integer value from 0 to 65,535.
Similarly, signed integer uses two's complement to make the range include negative numbers, from -32,768 to 32,767.
With unsigned fraction notation, the 65,536 levels are spread uniformly between 0 and 1. Lastly, the signed fraction format allows negative numbers, equally spaced between -1 and 1.
In comparison, floating point DSPs typically use a minimum of 32 bits to store each value.
This results in many more bit patterns than for fixed point, 232 ' 4,294,967,296 to be exact.
A key feature of floating point notation is that the represented numbers are not uniformly spaced.
In the most common format (ANSI/IEEE Std.
The represented values are unequally spaced between these two extremes, such that the gap between any two numbers is about ten-million times smaller than the value of the numbers.
This is important because it places large gaps between large numbers, but small gaps between small numbers.
Floating point notation is discussed in more detail in Chapter 4. All floating point DSPs can also handle fixed point numbers, a necessity to implement counters, loops, and signals coming from the ADC and going to the DAC.
However, this doesn't mean that fixed point math will be carried out as quickly as the floating point operations; it depends on the internal architecture.
For instance, the SHARC DSPs are optimized for both floating point and fixed point operations, and executes them with equal efficiency.
For this reason, the SHARC devices are often referred to as "32-bit DSPs," rather than just "Floating Point." Figure 28-6 illustrates the primary trade-offs between fixed and floating point DSPs.
In Chapter 3 we stressed that fixed point arithmetic is much FIGURE 28- Fixed versus floating point.
Fixed point DSPs are generally cheaper, while floating point devices have better precision, higher dynamic range, and a shorter development cycle.
Precision Dynamic Range Development Time Floating Point Product Cost Fixed Point faster than floating point in general purpose computers.
However, with DSPs the speed is about the same, a result of the hardware being highly optimized for math operations.
The internal architecture of a floating point DSP is more complicated than for a fixed point device.
All the registers and data buses must be 32 bits wide instead of only 16; the multiplier and ALU must be able to quickly perform floating point arithmetic, the instruction set must be larger (so that they can handle both floating and fixed point numbers), and so on.
Floating point (32 bit) has better precision and a higher dynamic range than fixed point (16 bit) .
In addition, floating point programs often have a shorter development cycle, since the programmer doesn't generally need to worry about issues such as overflow, underflow, and round-off error.
On the other hand, fixed point DSPs have traditionally been cheaper than floating point devices.
Nothing changes more rapidly than the price of electronics; anything you find in a book will be out-of-date before it is printed.
Nevertheless, cost is a key factor in understanding how DSPs are evolving, and we need to give you a general idea.
When this book was completed in 1999, fixed point DSPs sold for between $5 and $100, while floating point devices were in the range of $10 to $300.
This difference in cost can be viewed as a measure of the relative complexity between the devices.
If you want to find out what the prices are today, you need to look today.
Now let's turn our attention to performance; what can a 32-bit floating point system do that a 16-bit fixed point can't?
The answer to this question is signal-to-noise ratio.
Suppose we store a number in a 32 bit floating point format.
As previously mentioned, the gap between this number and its adjacent neighbor is about one ten-millionth of the value of the number.
To store the number, it must be round up or down by a maximum of one-half the gap size.
In other words, each time we store a number in floating point notation, we add noise to the signal.
The same thing happens when a number is stored as a 16-bit fixed point value, except that the added noise is much worse.
This is because the gaps between adjacent numbers are much larger.
For instance, suppose we store the number 10,000 as a signed integer (running from -32,768 to 32,767).
The gap between numbers is one ten-thousandth of the value of the number we are storing.
If we want to store the number 1000, the gap between numbers is only one onethousandth of the value.
Noise in signals is usually represented by its standard deviation.
This was discussed in detail in Chapter 2. For here, the important fact is that the standard deviation of this quantization noise is about one-third of the gap size.
This means that the signal-to-noise ratio for storing a floating point number is about 30 million to one, while for a fixed point number it is only about ten-thousand to one.
In other words, floating point has roughly 3, times less quantization noise than fixed point.
This brings up an important way that DSPs are different from traditional microprocessors.
Suppose we implement an FIR filter in fixed point.
To do this, we loop through each coefficient, multiply it by the appropriate sample from the input signal, and add the product to an accumulator.
Here's the problem.
In traditional microprocessors, this accumulator is just another 16 bit fixed point variable.
To avoid overflow, we need to scale the values being added, and will correspondingly add quantization noise on each step.
In the worst case, this quantization noise will simply add, greatly lowering the signalto-noise ratio of the system.
For instance, in a 500 coefficient FIR filter, the noise on each output sample may be 500 times the noise on each input sample.
The signal-to-noise ratio of ten-thousand to one has dropped to a ghastly twenty to one.
Although this is an extreme case, it illustrates the main point: when many operations are carried out on each sample, it's bad, really bad.
See Chapter 3 for more details.
DSPs handle this problem by using an extended precision accumulator.
This is a special register that has 2-3 times as many bits as the other memory locations.
For example, in a 16 bit DSP it may have 32 to 40 bits, while in the SHARC DSPs it contains 80 bits for fixed point use.
This extended range virtually eliminates round-off noise while the accumulation is in progress.
The only round-off error suffered is when the accumulator is scaled and stored in the 16 bit memory.
This strategy works very well, although it does limit how some algorithms must be carried out.
In comparison, floating point has such low quantization noise that these techniques are usually not necessary.
In addition to having lower quantization noise, floating point systems are also easier to develop algorithms for.
Most DSP techniques are based on repeated multiplications and additions.
In fixed point, the possibility of an overflow or underflow needs to be considered after each operation.
The programmer needs to continually understand the amplitude of the numbers, how the quantization errors are accumulating, and what scaling needs to take place.
In comparison, these issues do not arise in floating point; the numbers take care of themselves (except in rare cases).
Fixed versus floating point instructions.
These are the multiplication instructions used in the SHARC DSPs.
While only a single command is needed for floating point, many options are needed for fixed point.
See the text for an explanation of these options.
It could not be any simpler.
In comparison, look at all the possible commands for fixed point multiplication.
These are the many options needed to efficiently handle the problems of round-off, scaling, and format.
In Fig. 28-7, Rn, Rx, and Ry refer to any of the 16 data registers, and MRF and MRB are 80 bit accumulators.
The vertical lines indicate options.
For instance, the top-left entry in this table means that all the following are valid commands: Rn = Rx * Ry, MRF = Rx * Ry, and MRB = Rx * Ry.
In other words, the value of any two registers can be multiplied and placed into another register, or into one of the extended precision accumulators.
This table also shows that the numbers may be either signed or unsigned (S or U), and may be fractional or integer (F or I).
The RND and SAT options are ways of controlling rounding and register overflow.
There are other details and options in the table, but they are not important for our present discussion.
The important idea is that the fixed point programmer must understand dozens of ways to carry out the very basic task of multiplication.
In contrast, the floating point programmer can spend his time concentrating on the algorithm.
Given these tradeoffs between fixed and floating point, how do you choose which to use?
Here are some things to consider.
First, look at how many bits are used in the ADC and DAC.
In many applications, 12-14 bits per sample is the crossover for using fixed versus floating point.
For instance, television and other video signals typically use 8 bit ADC and DAC, and the precision of fixed point is acceptable.
In comparison, professional audio applications can sample with as high as 20 or 24 bits, and almost certainly need floating point to capture the large dynamic range.
The next thing to look at is the complexity of the algorithm that will be run.
If it is relatively simple, think fixed point; if it is more complicated, think floating point.
For example, FIR filtering and other operations in the time domain only require a few dozen lines of code, making them suitable for fixed point.
In contrast, frequency domain algorithms, such as spectral analysis and FFT convolution, are very detailed and can be much more difficult to program.
While they can be written in fixed point, the development time will be greatly reduced if floating point is used.
Lastly, think about the money: how important is the cost of the product, and how important is the cost of the development?
When fixed point is chosen, the cost of the product will be reduced, but the development cost will probably be higher due to the more difficult algorithms.
In the reverse manner, floating point will generally result in a quicker and cheaper development cycle, but a more expensive final product.
Figure 28-8 shows some of the major trends in DSPs. Figure (a) illustrates the impact that Digital Signal Processors have had on the embedded market.
These are applications that use a microprocessor to directly operate and control some larger system, such as a cellular telephone, microwave oven, or automotive instrument display panel.
The name "microcontroller" is often used in referring to these devices, to distinguish them from the microprocessors used in personal computers.
As shown in (a), about 38% of embedded designers have already started using DSPs, and another 49% are considering the switch.
The high throughput and computational power of DSPs often makes them an ideal choice for embedded designs.
As illustrated in (b), about twice as many engineers currently use fixed point as use floating point DSPs.
However, this depends greatly on the application.
Fixed point is more popular in competitive consumer products where the cost of the electronics must be kept very low.
A good example of this is cellular telephones.
When you are in competition to sell millions of your product, a cost difference of only a few dollars can be the difference between success and failure.
In comparison, floating point is more common when greater performance is needed and cost is not important.
For a. Changing from uProc to DSP Considering Have Already Changed b.
DSP currently used Considering Fixed Point c. Migration to floating point Floating Point No Plans Migrate Design Migrate in Migrate Next Year FIGURE 28- Major trends in DSPs.
As illustrated in (a), about 38% of embedded designers have already switched from conventional microprocessors to DSPs, and another 49% are considering the change.
In (b), about twice as many engineers use fixed point as use floating point DSPs.
This is mainly driven by consumer products that must have low cost electronics, such as cellular telephones.
However, as shown in (c), floating point is the fastest growing segment; over one-half of engineers currently using 16 bit devices plan to migrate to floating point DSPs instance, suppose you are designing a medical imaging system, such a computed tomography scanner.
Only a few hundred of the model will ever be sold, at a price of several hundred-thousand dollars each.
For this application, the cost of the DSP is insignificant, but the performance is critical.
In spite of the larger number of fixed point DSPs being used, the floating point market is the fastest growing segment.
As shown in (c), over one-half of engineers using 16-bits devices plan to migrate to floating point at some time in the near future.
Before leaving this topic, we should reemphasize that floating point and fixed point usually use 32 bits and 16 bits, respectively, but not always.
For instance, the SHARC family can represent numbers in 32-bit fixed point, a mode that is common in digital audio applications.
This makes the quantization levels spaced uniformly over a relatively small range, say, between -1 and 1.
In comparison, floating point notation places the quantization levels logarithmically over a huge range, typically ±3.4×1038.
This gives 32-bit fixed point better precision, that is, the quantization error on any one sample will be lower.
However, 32-bit floating point has a higher dynamic range, meaning there is a greater difference between the largest number and the smallest number that can be represented.
C versus Assembly DSPs are programmed in the same languages as other scientific and engineering applications, usually assembly or C. Programs written in assembly can execute faster, while programs written in C are easier to develop and maintain.
In traditional applications, such as programs run on personal computers and mainframes, C is almost always the first choice.
If assembly is used at all, it is restricted to short subroutines that must run with the utmost speed.
This is shown graphically in Fig. 28-9a; for every traditional programmer that works in assembly, there are approximately ten that use C.
However, DSP programs are different from traditional software tasks in two important respects.
First, the programs are usually much shorter, say, onehundred lines versus ten-thousand lines.
Second, the execution speed is often a critical part of the application.
After all, that's why someone uses a DSP in the first place, for its blinding speed.
These two factors motivate many software engineers to switch from C to assembly for programming Digital Signal Processors.
This is illustrated in (b); nearly as many DSP programmers use assembly as use C. Figure (c) takes this further by looking at the revenue produced by DSP products.
For every dollar made with a DSP programmed in C, two dollars are made with a DSP programmed in assembly.
The reason for this is simple; money is made by outperforming the competition.
From a pure performance standpoint, such as execution speed and manufacturing cost, assembly almost always has the advantage over C. For instance, C code usually requires a larger memory than assembly, resulting in more expensive hardware.
However, the DSP market is continually changing.
As the market grows, manufacturers will respond by designing DSPs that are optimized for programming in C. For instance, C is much more efficient when there is a large, general purpose register set and a unified memory space.
These future improvements will minimize the difference in execution time between C and assembly, and allow C to be used in more applications.
To better understand this decision between C and assembly, let's look at a typical DSP task programmed in each language.
The example we will use is the calculation of the dot product of the two arrays, x [ ] and y [ ] .
This is a simple mathematical operation, we multiply each coefficient in one a.
Traditional Programmers b.
DSP Programmers Assembly Assembly FIGURE 28- Programming in C versus assembly.
As shown in (a), only about 10% of traditional programmers (such as those that work on personal computers and mainframes) use assembly.
However, as illustrated in (b), assembly is much more common in Digital Signal Processors.
This is because DSP programs must operate as fast as possible, and are usually quite short.
Figure (c) shows that assembly is even more common in products that generate a high revenue.
DSP Revenue Assembly array by the corresponding coefficient in the other array, and sum the products, i.e. x[0] ×y[0] % x[1] ×y[1] % x[2] ×y[2] % þ.
This should look very familiar; it is the fundamental operation in an FIR filter.
That is, each sample in the output signal is found by multiplying stored samples from the input signal (in one array) by the filter coefficients (in the other array), and summing the products.
Table 28-2 shows how the dot product is calculated in a C program.
In lines 001-004 we define the two arrays, x [ ] and y [ ], to be 20 elements long.
We also define result, the variable that holds the calculated dot TABLE 28- Dot product in C.
This progam calculates the dot product of two arrays, x[ ] and y[ ], and stores the result in the variable, result.
After the loop, the value in the accumulator, s, is transferred to the output variable, result, in line 013.
A key advantage of using a high-level language (such as C, Fortran, or Basic) is that the programmer does not need to understand the architecture of the microprocessor being used; knowledge of the architecture is left to the compiler.
For instance, this short C program uses several variables: n, s, result, plus the arrays: x [ ] and y [ ] .
All of these variables must be assigned a "home" in hardware to keep track of their value.
Depending on the microprocessor, these storage locations can be the general purpose data registers, locations in the main memory, or special registers dedicated to particular functions.
However, the person writing a high-level program knows little or nothing about this memory management; this task has been delegated to the software engineer who wrote the compiler.
The problem is, these two people have never met; they only communicate through a set of predefined rules.
High-level languages are easier than assembly because you give half the work to someone else.
However, they are less efficient because you aren't quite sure how the delegated work is being carried out.
In comparison, Table 28-3 shows the dot product program written in assembly for the SHARC DSP.
The assembly language for the Analog Devices DSPs (both their 16 bit fixed-point and 32 bit SHARC devices) are known for their simple algebraic-like syntax.
While we won't go through all the details, here is the general operation.
Notice that everything relates to hardware; there are no abstract variables in this code, only data registers and memory locations.
Each semicolon represents a clock cycle.
The arrays x [ ] and y [ ] are held in circular buffers in the main memory.
In lines 001 and 002, registers i Dot product in assembly (optimized).
This is an optimized version of the program in TABLE 28-2, designed to take advantage of the SHARC's highly parallel architecture.
Next, we execute 20 loop cycles, as controlled by line 004.
The format for this statement takes advantage of the SHARC DSP's zero-overhead looping capability.
In other words, all of the variables needed to control the loop are held in dedicated hardware registers that operate in parallel with the other operations going on inside the microprocessor.
In this case, the register: lcntr (loop counter) is loaded with an initial value of 20, and decrements each time the loop is executed.
The loop is terminated when lcntr reaches a value of zero (indicated by the statement: lce, for "loop counter expired").
The loop encompasses lines 004 to 008, as controlled by the statement (pc,4).
That is, the loop ends four lines after the current program counter.
Inside the loop, line 005 loads the value from x [ ] into data register f2, while line 006 loads the value from y [ ] into data register f4.
The symbols "dm" and "pm" indicate that the values are fetched over the "data memory" bus and "program memory" bus, respectively.
The variables: i4, m6, i12, and m14 are registers in the data address generators that manage the circular buffers holding x [ ] and y [ ] .
The two values in f2 and f4 are multiplied in line 007, and the product stored in data register f8.
In line 008, the product in f8 is added to the accumulator, data register f12.
After the loop is completed, the accumulator in f12 is transferred to memory.
This program correctly calculates the dot product, but it does not take advantage of the SHARC highly parallel architecture.
Table 28-4 shows this program rewritten in a highly optimized form, with many operations being carried out in parallel.
First notice that line 007 only executes 18 loops, rather than 20.
Also notice that this loop only contains a single line (008), but that this line contains multiple instructions.
The strategy is to make the loop as efficient as possible, in this case, a single line that can be executed in a single clock cycle.
To do this, we need to have a small amount of code to "prime" the registers on the first loop (lines 004 and 005), and another small section of code to finish the last loop (lines 010 and 011).
To understand how this works, study line 008, the only statement inside the loop.
In this single statement, four operations are being carried out in parallel: (1) the value for x [ ] is moved from a circular buffer in program memory and placed in f2; (2) the value for y [ ] is being moved from a circular buffer in data memory and placed in f4; (3) the previous values of f2 and f4 are multiplied and placed in f8; and (4) the previous value in f8 is added to the accumulator in f12.
For example, the fifth time that line 008 is executed, x[7] and y[7] are fetched from memory and stored in f2 and f4.
At the same time, the values for x[6] and y[6] (that were in f2 and f4 at the start of this cycle) are multiplied and placed in f8.
In addition, the value of x[5] ×y[5] (that was in f8 at the start of this cycle) is added to the value of f12.
Let's compare the number of clock cycles required by the unoptimized and the optimized programs.
Keep in mind that there are 20 loops, with four actions being required in each loop.
The unoptimized program requires clock cycles to carry out the actions within the loops, plus 5 clock cycles of overhead, for a total of 85 clock cycles.
In comparison, the optimized program conducts 18 loops in 18 clock cycles, but requires 11 clock cycles of overhead to prime the registers and complete the last loop.
This results in a total execution time of 29 clock cycles, or about three times faster than the brute force method.
Here is the big question: How fast does the C program execute relative to the assembly code?
When the program in Table 28-2 is compiled, does the executable code resemble our efficient or inefficient assembly example?
The answer is that the compiler generates the efficient code.
However, it is important to realize that the dot product is a very simple example.
The compiler has a much more difficult time producing optimized code when the program becomes more complicated, such as multiple nested loops and erratic jumps to subroutines.
If you are doing something straightforward, expect the compiler to provide you a nearly optimal solution.
If you are doing something strange or complicated, expect that an assembly program will execute significantly faster than one written in C. In the worst case, think a factor of 2-3.
As previously mentioned, the efficiency of C versus assembly depends greatly on the particular DSP being used.
Floating point architectures can generally be programmed more efficiently than fixed-point devices when using high-level languages such as C. Of course, the proper software tools are important for this, such as a debugger with profiling features that help you understand how long different code segments take to execute.
There is also a way you can get the best of both worlds: write the program in C, but use assembly for the critical sections that must execute quickly.
This is one reason that C is so popular in science and engineering.
It operates as a high-level language, but also allows you to directly manipulate FIGURE 28- Assembly versus C. Programs in C are more flexible and quicker to develop.
In comparison, programs in assembly often have better performance; they run faster and use less memory, resulting in lower cost.
Flexibility and Fast Development Performance Assembly the hardware if you so desire.
Even if you intend to program only in C, you will probably need some knowledge of the architecture of the DSP and the assembly instruction set.
For instance, look back at lines 002 and 003 in Table 28-2, the dot product program in C. The "dm" means that x [ ] is to be stored in data memory, while the "pm" indicates that y [ ] will reside in program memory.
Even though the program is written in a high level language, a basic knowledge of the hardware is still required to get the best performance from the device.
Which language is best for your application?
It depends on what is more important to you.
If you need flexibility and fast development, choose C. On the other hand, use assembly if you need the best possible performance.
As illustrated in Fig. 28-10, this is a tradeoff you are forced to make.
Here are some things you should consider.
If it is large and intricate, you will probably want to use C. If it is small and simple, assembly may be a good choice.
If so, assembly will give you the last drop of performance from the device.
For less demanding applications, assembly has little advantage, and you should consider using C. ‘ How many programmers will be working together?
If the project is large enough for more than one programmer, lean toward C and use in-line assembly only for time critical segments.
If it is product cost, choose assembly; if it is development cost, choose C. ‘ What is your background?
If you are experienced in assembly (on other microprocessors), choose assembly for your DSP.
If your previous work is in C, choose C for your DSP.
This last item is very important.
Suppose you ask a DSP manufacturer which language to use, and they tell you: "Either C or assembly can be used, but we recommend C." You had better take their advice!
What they are really saying is: "Our DSP is so difficult to program in assembly that you will need months of training to use it."
On the other hand, some DSPs are easy to program in assembly.
For instance, the Analog Devices products are in this category.
Just ask their engineers; they are very proud of this.
One of the best ways to make decisions about DSP products and software is to speak with engineers who have used them.
Ask the manufacturers for references of companies using their products, or search the web for people you can e-mail.
Don't be shy; engineers love to give their opinions on products they have used.
They will be flattered that you asked.
How Fast are DSPs?
The primary reason for using a DSP instead of a traditional microprocessor is speed, the ability to move samples into the device, carry out the needed mathematical operations, and output the processed data.
This brings up the question: How fast are DSPs?
The usual way of answering this question is benchmarks, methods for expressing the speed of a microprocessor as a number.
For instance, fixed point systems are often quoted in MIPS (million integer operations per second).
Likewise, floating point devices can be specified in MFLOPS (million floating point operations per second).
One hundred and fifty years ago, British Prime Minister Benjamin Disraeli declared that there are three types of lies: lies, damn lies, and statistics.
If Disraeli were alive today and working with microprocessors, he would add benchmarks as a fourth category.
The idea behind benchmarks is to provide a head-to-head comparison to show which is the best device.
Unfortunately, this often fails in practicality, because different microprocessors excel in different areas.
Imagine asking the question: Which is the better car, a Cadillac or a Ferrari?
It depends on what you want it for!
Confusion about benchmarks is aggravated by the competitive nature of the electronics industry.
Manufacturers want to show their products in the best light, and they will use any ambiguity in the testing procedure to their advantage.
There is an old saying in electronics: "A specification writer can get twice as much performance from a device as an engineer."
These people aren't being untruthful, they are just paid to have good imaginations.
Benchmarks should be viewed as a tool for a complicated task.
If you are inexperienced in using this tool, you may come to the wrong conclusion.
A better approach is to look for specific information on the execution speed of the algorithms you plan to carry out.
For instance, if your application calls for an FIR filter, look for the exact number of clock cycles it takes for the device to execute this particular task.
Using this strategy, let's look at the time required to execute various algorithms on our featured DSP, the Analog Devices SHARC family.
Keep The speed of DSPs.
The throughput of a particular DSP algorithm can be found by dividing the clock rate by the required number of clock cycles per sample.
This illustration shows the range of throughput for four common algorithms, executed on a SHARC DSP at a clock speed of 40 MHz. in mind that microprocessor speed is doubling about every three years.
This means you should pay special attention to the method we use in this example.
The actual numbers are always changing, and you will need to repeat the calculations every time you start a new project.
In the world of twenty-first century technology, blink and you are out-of-date!
When it comes to understanding execution time, the SHARC family is one of the easiest DSP to work with.
This is because it can carry out a multiply-accumulate operation in a single clock cycle.
Since most FIR filters use 25 to 400 coefficients, 25 to 400 clock cycles are required, respectively, for each sample being processed.
As previously described, there is a small amount of overhead needed to achieve this loop efficiency (priming the first loop and completing the last loop), but it is negligible when the number of loops is this large.
To obtain the throughput of the filter, we can divide the SHARC clock rate (40 MHz at present) by the number of clock cycles required per sample.
This gives us a maximum FIR data rate of about 100k to 1.6M samples/second.
The calculations can't get much simpler than this!
These FIR throughput values are shown in Fig. 2811.
The calculations are just as easy for recursive filters.
Typical IIR filters use about 5 to 17 coefficients.
Since these loops are relatively short, we will add a small amount of overhead, say 3 cycles per sample.
This results in 8 to 20 clock cycles being required per sample of processed data.
For the 40 MHz clock rate, this provides a maximum IIR throughput of 1.8M to 3.1M samples/second.
These IIR values are also shown in Fig. 28-11.
Next we come to the frequency domain techniques, based on the Fast Fourier Transform.
FFT subroutines are almost always provided by the manufacturer of the DSP.
These are highly-optimized routines written in assembly.
The specification sheet of the ADSP-21062 SHARC DSP indicates that a sample complex FFT requires 18,221 clock cycles, or about 0.46 milliseconds at 40 MHz.
To calculate the throughput, it is easier to view this as 17.8 clock cycles per sample.
This "per-sample" value only changes slightly with longer or shorter FFTs.
For instance, a 256 sample FFT requires about 14.2 clock cycles per sample, and a 4096 sample FFT requires 21.4 clock cycles per sample.
Real FFTs can be calculated about 40% faster than these complex FFT values.
This makes the overall range of all FFT routines about 10 to 22 clock cycles per sample, corresponding to a throughput of about 1.8M to 3.3M samples/second.
FFT convolution is a fast way to carry out FIR filters.
In a typical case, a sample segment is taken from the input, padded with an additional 512 zeros, and converted into its frequency spectrum by using a 1024 point FFT.
After multiplying this spectrum by the desired frequency response, a 1024 point Inverse FFT is used to move back into the time domain.
The resulting points are combined with the adjacent processed segments using the overlapadd method.
This produces 512 points of the output signal.
How many clock cycles does this take?
Each 512 sample segment requires two 1024 point FFTs, plus a small amount of overhead.
In round terms, this is about a factor of five greater than for a single FFT of 512 points.
Since the real FFT requires about 12 clock cycles per sample, FFT convolution can be carried out in about 60 clock cycles per sample.
For a 2106x SHARC DSP at 40 MHz, this corresponds to a data throughput of approximately 660k samples/second.
Notice that this is about the same as a 60 coefficient FIR filter carried out by conventional convolution.
In other words, if an FIR filter has less than coefficients, it can be carried out faster by standard convolution.
If it has greater than 60 coefficients, FFT convolution is quicker.
A key advantage of FFT convolution is that the execution time only increases as the logarithm of the number of coefficients.
For instance a 4,096 point filter kernel only requires about 30% longer to execute as one with only 512 points.
FFT convolution can also be applied in two-dimensions, such as for image processing.
For instance, suppose we want to process an 800×600 pixel image in the frequency domain.
First, pad the image with zeros to make it 1024×1024.
The two-dimensional frequency spectrum is then calculated by taking the FFT of each of the rows, followed by taking the FFT of each of the resulting columns.
After multiplying this 1024×1024 spectrum by the desired frequency response, the two-dimensional Inverse FFT is taken.
This is carried out by taking the Inverse FFT of each of the rows, and then each of the resulting columns.
Adding the number of clock cycles and dividing by the number of samples, we find that this entire procedure takes roughly 150 clock cycles per pixel.
For a 40 MHz ADSP-2106, this corresponds to a data throughput of about 260k samples/second.
Comparing these different techniques in Fig. 28-11, we can make an important observation.
Nearly all DSP techniques require between 4 and instructions (clock cycles in the SHARC family) to execute.
For a SHARC DSP operating at 40 MHz, we can immediately conclude that its data throughput will be between 100k and 10M samples per second, depending on how complex of algorithm is used.
Now that we understand how fast DSPs can process digitized signals, let's turn our attention to the other end; how fast do we need to process the data?
Of course, this depends on the application.
We will look at two of the most common, audio and video processing.
The data rate needed for an audio signal depends on the required quality of the reproduced sound.
At the low end, telephone quality speech only requires capturing the frequencies between about 100 Hz and 3.2 kHz, dictating a sampling rate of about 8k samples/second.
In comparison, high fidelity music must contain the full 20 Hz to 20 kHz range of human hearing.
A 44.1 kHz sampling rate is often used for both the left and right channels, making the complete Hi Fi signal 88.2k samples/second.
How does the SHARC family compare with these requirements?
As shown in Fig. 28-11, it can easily handle high fidelity audio, or process several dozen voice signals at the same time.
Video signals are a different story; they require about one-thousand times the data rate of audio signals.
A good example of low quality video is the the CIF (Common Interface Format) standard for videophones.
This uses 352× pixels, with 3 colors per pixel, and 30 frames per second, for a total data rate of 9.1 million samples per second.
At the high end of quality there is HDTV (high-definition television), using 1920×1080 pixels, with 3 colors per pixel, and 30 frames per second.
This requires a data rate to over 186 million samples per second.
These data rates are above the capabilities of a single SHARC DSP, as shown in Fig. 28-11.
There are other applications that also require these very high data rates, for instance, radar, sonar, and military uses such as missile guidance.
To handle these high-power tasks, several DSPs can be combined into a single system.
This is called multiprocessing or parallel processing.
The SHARC DSPs were designed with this type of multiprocessing in mind, and include special features to make it as easy as possible.
For instance, no external hardware logic is required to connect the external busses of multiple SHARC DSPs together; all of the bus arbitration logic is already contained within each device.
As an alternative, the link ports (4 bit, parallel) can be used to connect multiple processors in various configurations.
Figure 2812 shows typical ways that the SHARC DSPs can be arranged in multiprocessing systems.
In Fig. (a), the algorithm is broken into sequential steps, with each processor performing one of the steps in an "assembly line" a. Data flow multiprocessing ADSP-2106x ADSP-2106x ADSP-2106x b.
Cluster multiprocessing ADSP-2106x ADSP-2106x ADSP-2106x External Port External Port External Port BULK MEMORY FIGURE 28- Multiprocessing configurations.
Multiprocessor systems typically use one of two schemes to communicate between processor nodes, (a) dedicated point-to-point communication channels, or (b) a shared global memory accessed over a parallel bus.
In (b), the processors interact through a single shared global memory, accessed over a parallel bus (i.e., the external port).
Figure 28-13 shows another way that a large number of processors can be combined into a single system, a 2D or 3D "mesh."
Each of these configuration will have relative advantages and disadvantages for a particular task.
To make the programmer's life easier, the SHARC family uses a unified address space.
This means that the 4 Gigaword address space, accessed by the 32 bit address bus, is divided among the various processors that are working together.
To transfer data from one processor to another, simply read from or write to the appropriate memory locations.
The SHARC internal logic takes care of the rest, transferring the data between processors at a rate as high as 240 Mbytes/sec (at 40 MHz).
Link Port Link Port Link Port ADSP-2106x ADSP-2106x ADSP-2106x Link Port Link Port Link Port Link Port Link Port Link Port ADSP-2106x ADSP-2106x ADSP-2106x Link Port Link Port Link Port FIGURE 28- Multiprocessing "mesh" configuration.
For applications such as radar imaging, a 2D or 3D array may be the most efficient way to coordinate a large number of processors.
The Digital Signal Processor Market The DSP market is very large and growing rapidly.
As shown in Fig. 28-14, it will be about 8-10 billion dollars/year at the turn of the century, and growing at a rate of 30-40% each year.
This is being fueled by the incessant Billions of dollars FIGURE 28- The DSP market.
At the turn of the century, the DSP market will be 8- billion dollars per year, and expanding at a rate of about 30-40% per year.
These high-revenue applications are shaping the field, while less profitable areas, such as scientific instrumentation, are just riding the wave of technology.
DSPs can be purchased in three forms, as a core, as a processor, and as a board level product.
In DSP, the term "core" refers to the section of the processor where the key tasks are carried out, including the data registers, multiplier, ALU, address generator, and program sequencer.
A complete processor requires combining the core with memory and interfaces to the outside world.
While the core and these peripheral sections are designed separately, they will be fabricated on the same piece of silicon, making the processor a single integrated circuit.
Suppose you build cellular telephones and want to include a DSP in the design.
You will probably want to purchase the DSP as a processor, that is, an integrated circuit ("chip") that contains the core, memory and other internal features.
For instance, the SHARC ADSP-21060 comes in a " lead Metric PQFP" package, only 35×35×4 mm in size.
To incorporate this IC in your product, you design a printed circuit board where it will be soldered in next to your other electronics.
This is the most common way that DSPs are used.
Now, suppose the company you work for manufactures its own integrated circuits.
In this case, you might not want the entire processor, just the design of the core.
After completing the appropriate licensing agreement, you can start making chips that are highly customized to your particular application.
This gives you the flexibility of selecting how much memory is included, how the chip receives and transmits data, how it is packaged, and so on.
Custom devices of this type are an increasingly important segment of the DSP marketplace.
Lastly, there are several dozen companies that will sell you DSPs already mounted on a printed circuit board.
These have such features as extra memory, A/D and D/A converters, EPROM sockets, multiple processors on the same board, and so on.
While some of these boards are intended to be used as stand alone computers, most are configured to be plugged into a host, such as a personal computer.
Companies that make these types of boards are called Third Party Developers.
The best way to find them is to ask the manufacturer of the DSP you want to use.
Look at the DSP manufacturer's website; if you don't find a list there, send them an e-mail.
They will be more than happy to tell you who is using their products and how to contact them.
The present day Digital Signal Processor market (1998) is dominated by four companies.
Here is a list, and the general scheme they use for numbering their products: Analog Devices (www.analog.com/dsp)
ADSP-21xx 16 bit, fixed point ADSP-21xxx 32 bit, floating and fixed point Lucent Technologies (www.lucent.com)
DSP16xxx 16 bit fixed point DSP32xx 32 bit floating point Motorola (www.mot.com)
DSP561xx 16 bit fixed point DSP560xx 24 bit, fixed point 32 bit, floating point Texas Instruments (www.ti.com)
TMS320Cxx 16 bit fixed point TMS320Cxx 32 bit floating point Keep in mind that the distinction between DSPs and other microprocessors is not always a clear line.
For instance, look at how Intel describes the MMX technology addition to its Pentium processor: "Intel engineers have added 57 powerful new instructions specifically designed to manipulate and process video, audio and graphical data efficiently.
These instructions are oriented to the highly parallel, repetitive sequences often found in multimedia operations."
In the future, we will undoubtedly see more DSP-like functions merged into traditional microprocessors and microcontrollers.
The internet and other multimedia applications are a strong driving force for these changes.
These applications are expanding so rapidly, in twenty years it is very possible that the Digital Signal Processor may be the "traditional" microprocessor.
How do you keep up with this rapidly changing field?
The best way is to read trade journals that cover the DSP market, such as EDN (Electronic Design News, www.ednmag.com),
These are distributed free, and contain up-to-date information on what is available and where the industry is going.
Trade journals are a "must-read" for anyone serious about the field.
You will also want to be on the mailing list of several DSP manufacturers.
This will allow you to receive new product announcements, pricing information, and special offers (such as free software and low-cost evaluation kits).
Some manufacturers also distribute periodic newsletters.
For instance, Analog Devices publishes Analog Dialogue four times a year, containing articles and information on current topics in signal processing.
All of these resources, and much more, can be contacted over the internet.
Start by exploring the manufacturers’ websites, and then sending them e-mail requesting specific information.
Getting Started with DSPs Once you decide that a Digital Signal Processor is right for your application, you need a way to get started.
Many manufacturers will sell you a low cost evaluation kit, allowing you to experience their products first-hand.
These are a great educational tool; it doesn't matter if you are a novice or a pro, they are the best way to become familiar with a particular DSP.
For instance, Analog Devices provides the EZ-KIT® Lite to teach potential customers about its SHARC® family of Digital Signal Processors.
For only $179, you receive all the hardware and software you need to see the DSP in action.
This includes "canned" programs provided with the kit, as well as applications you can write yourself in assembly or C. Suppose you buy one of these kits from Analog Devices and play with it for a few days.
This chapter is an overview of what you can expect to find and learn.
The ADSP-2106x family In the last chapter we looked at the general operation of the ADSP-2106x "SHARC" family of Digital Signal Processors.
Table 29-1 shows the various members of this family.
All these devices use the same architecture, but have different amounts of on-chip memory, a key factor in deciding which one to use.
Memory access is a common bottleneck in DSP systems.
The SHARC DSPs address this by providing an ample supply of on-chip dual-ported SRAM.
However, the last thing you want to do is pay for more memory than you need.
DSPs often go into cost sensitive products, such as cellular telephones and CD players.
In other words, the organization of this family is determined by marketing as well as technology.
The oldest member of this family is the ADSP-21020.
This chip contains the core architecture, but does not include on-chip memory or I/O handling.
This means it cannot function as a stand-alone computer; it requires external components to be a functional system.
The other devices are complete SHARC, EZ-KIT, EZ-LAB, VisualDSP, EZ-ICE, the SHARC logo, the Analog Devices logo, and the VisualDSP logo are registered trademarks of Analog Devices, Inc. PRODUCT Memory Notes 4 Mbit × Quad-SHARC, Four ADSP-21060's in the same module; provides an incredible MFLOPS in only 2.05"×2.05"×0.16".
ADSP-21160M 4 Mbit New! Features Single Instruction Multiple Data (SIMD) core architecture; optimized for multiprocessing with link ports, 64 bit external bus, and 14 channels of DMA ADSP- 4 Mbit Power house of the family; most memory; link ports for high speed data transfer and multi-processing ADSP- 2 Mbit Same features as the ADSP-21060, but with less internal memory (SRAM), for lower cost ADSP- 1 Mbit Low cost version used in the EZ-KIT Lite; less memory & no link ports; additional features in DMA for the serial port ADSP-21065L ADSP- 544 kbit A recent addition to the family; fast and very low cost ($10).
Will attract many fixed point applications to the SHARC family Oldest member of the family.
Contains the core processor, but no on-chip memory or I/O interface.
Not quite a SHARC DSP.
TABLE 29- Members of the SHARC family.
All they require to operate is a source of power, and some way to load a program into memory, such as an external PROM or data link.
Notice in Table 29-1 that even the low-end products have a very significant amount of memory.
For instance, the ADSP-21065L has 544 kbits of internal SRAM.
This is enough to hold 6-8 seconds of digitized speech (8k samples per second, 8 bits per sample).
On the high-end of the family, the ADSP- has a 4 Mbit memory.
This is more than enough to store an entire digitized image (512×512 pixels, 8 bits per pixel).
If you require even more memory, you easily add external SRAM (or slower memory) to any of these devices.
In addition to memory, there are also differences between these family members in their I/O sections.
The ADSP-21060 and ADSP-21062 (the highend) each have six link ports.
These are 4 bit wide parallel connections for combining DSPs in multiprocessing systems, and other applications that require flexible high-speed I/O.
The ADSP-21061 and ADSP-21065L (the low-end) do not have link ports, but feature more DMA channels to assist in their serial port operation.
You will also see these part numbers with an "L" or "M" after them, such as "ADSP-21060L."
This indicates that the device operates from a voltage lower than the traditional 5.0 volts.
For audio in emulator connector CODEC audio out serial port flag LEDs SHARC reset flag ADSP- Expansion serial port link ports processor bus serial cable (to PC) UART/ Driver POWER 9-12 vdc, 1 amp FIGURE 29- Block diagram of the EZ-KIT Lite board.
Only four external connections are needed: audio in, audio out, a serial (RS-232) cable to your personal computer, and power.
The serial cable and power supply are provided with the EZ-KIT Lite.
In June 1998, Analog Devices unveiled the second generation of its SHARC architecture, with the announcement of the ADSP-21160.
This features a Single Instruction Multiple Data (SIMD, or "sim-dee") core architecture operating at 100 MHz, an accelerated memory bus bandwidth of megabytes per second, two 64 bit data busses, and four 80-bit accumulators for fixed point calculations.
All totaled, the new ADSP-21160M executes a 1024 point FFT in only 46 microseconds.
The SIMD DSP contains a second set of computational units (arithmetic and logic unit, barrel shifter, data register file, and multiplier), allowing ADI to maintain backward code compatibility with the ADSP-2106x family, while providing a road-map to up to ten times higher performance.
The SHARC EZ-KIT Lite The EZ-kit Lite gives you everything you need to learn about the SHARC DSP, including: hardware, software, and reference manuals.
Figure 29- shows a block diagram of the hardware provided in the EZ-KIT Lite, based around the ADSP-21061 Digital Signal Processor.
This comes as a 4½ × 6½ inch printed circuit board, mounted on plastic standoffs to allow it to sit on your desk.
There are only four connections you need to worry about: DC power, a serial connection to your personal computer, and the input and output signals.
A DC power supply and serial cable are even provided in the kit.
The input and output signals are at audio level, about 1 volt amplitude.
Alternatively, a jumper on the board allows a microphone to be directly attached into the input.
The idea is to plug a microphone into the input, and attach a set of amplified speakers (such as used with personal computers) to the output.
This allows you to hear the effect of various DSP algorithms.
Analog-to-digital and digital-to-analog conversion is accomplished with an Analog Devices AD1847 codec (coder-decoder).
This is a 16 bit sigma-delta converter, capable of digitizing two channels (stereo) at a rate of up to 48k samples/second, and simultaneously outputing two channels at the same rate.
Since the primary use of this board is to process audio signals, the inputs and outputs are AC coupled with a cutoff of about 20 Hz.
Three push buttons on the board allow the user to generate an interrupt, reset the processor, and toggle a flag bit that can be read by the system.
Four LEDs mounted on the board can be turned on and off by toggling bits.
If you are ambitious, there are sections of the board that allow you to access the serial port, link ports (only on the EZ-LAB with its ADSP-21062), and processor bus.
However, these are unpopulated, and you will need to attach the connectors and other components yourself.
Here's how it works.
When the power is applied, the processor boots from an on-board EPROM (512 kbytes), loading a program that establishes serial communication with your personal computer.
Next, you launch the EZ-Lite Host program on you PC, allowing you to download programs and upload data from the DSP.
Several prewritten programs come with the EZ-KIT Lite; these can be run by simply clicking on icons.
For instance, a band-pass program allows you to speak into the microphone, and hear the result after passing through a band-pass filter.
These programs are useful for two reasons: (1) they allow you to quickly get the system doing something interesting, giving you confidence that it does work, and (2) they provide a template for creating programs of your own.
Which brings us to our next topic, a design example using the EZ-KIT Lite.
Design Example: An FIR Audio Filter After you experiment with the prewritten programs for awhile, you will want to modify them to gain experience with the programming.
Programs can be written in either assembly or C; the EZ-KIT Lite provides software tools to support both languages.
Later in this chapter we will look at advanced methods of programming, such as simulation, debugging, and working in an integrated development environment.
For now, we will focus on the easiest way to get a program to run.
Little steps for little feet.
Impulse response (filter kernel) Amplitude Amplitude -0.
Frequency Sample number FIGURE 29- Example FIR filter.
In (a) the frequency response of a highly custom filter is shown.
The corresponding impulse response (filter kernel) is shown in (b).
This filter was designed in Chapter 17 to show that virtually any frequency response can be achieved with FIR digital filters.
Since the source code is in ASCII, a standard text editor is all that is needed to make changes to existing files, or create entirely new programs.
Table 29- shows an example of an FIR filter program written in assembly.
While this is the only code you need to worry about for now, keep in mind that there are other files needed to make this a complete program.
This includes an "architecture description file" (which defines the hardware configuration and memory allocation), setup of the interrupt vector table, and a codec initialization routine.
Eventually you will need to understand what goes on in these sections, but for now you simply copy them from the prewritten programs.
As shown at the top of Table 29-2, there are three variables that need to be defined before jumping into the main section of code.
These are the number of points in the filter kernel, NR_COEF; a circular buffer that holds the past samples from the input signal, dline[ ]; and a circular buffer that holds the filter kernel, coef[ ].
We also need to give the program two other pieces of information: the sampling rate of the codec, and the name of the file containing the filter kernel, so that it can be read into coef[ ].
All these steps are easy; nothing more than a single line of code each.
We don't show them in this example because they are contained in the sections of code that we are ignoring for simplicity.
Figure 29-2 shows the filter kernel we will test the program with, the same custom filter we designed in Chapter 17.
As you recall, this filter was chosen to have a very irregular frequency response, reinforcing the notion that FIR digital filters can provide virtually any frequency response you desire.
Figure (a) shows the frequency response of our test filter, while (b) shows the corresponding impulse response (i.e., the filter kernel).
This 301 point filter kernel is stored in an ASCII file, and is combined with the other sections of code during linking to form a single executable program.
The main section of the program performs two functions.
In lines 6 to 13, the data-address-generators (DAGs) are configured to manage the circular buffers: dline[ ], and coef[ ].
As described in the last chapter, three parameters are needed for each buffer: the starting location of the buffer in memory (b0 and b8), the length of the buffer (l0 and l8), and the step size of the data being stored in the buffer (m0 and m8).
These parameters that control the circular buffers are stored in hardware registers in the DAGs, allowing them to access and manage the data very efficiently.
The second action of the main program is a "thumb-twiddling" loop, implemented in lines 15 to 19.
This does nothing but wait for an interrupt indicating that an input sample has been acquired.
All of the processing in this program occurs on a sample-by-sample basis.
Each time a sample is read from the input, a sample in the output signal is calculated and routed to the codec.
Most time-domain algorithms, such as FIR and IIR filters, fall into this category.
The alternative is frame-by-frame processing, which is required for frequency-domain techniques.
In the frame-by-frame method, a group of samples is read from the input, calculations are conducted, and a group of samples is written to the output.
The subroutine that services the sample-ready interrupt is broken into three sections.
The first section (lines 27 to 33) fetches the sample from the codec as a fixed point number, and converts it to floating point.
In SHARC assembly language, a data register holding a fixed point number is referred to by "r" (such as r0, r8, r15, etc.), and by "f" if it is holding a floating point number (i.e., f0, f8, or f15.).
For instance, in line 32, the fixed point number in data register 0 (i.e., r0) is converted into a floating point number and overwrites data register 0 (i.e., f0).
This conversion is done according to a scaling specified by the fixed point number in data register 1 (i.e.
In the third section (lines 47 to 53), the opposite steps take place; the floating point number for the output sample is converted to fixed point and sent to the codec.
The FIR filter that converts the input samples into the output samples is contained in lines 35 to 45.
All the calculations are carried out in floating point, avoiding the need to worry about scaling and overflow.
As described in the last chapter, this section of code is optimized to take advantage of the SHARC DSP's ability to execute multiple instructions each clock cycle.
After we have the assembly program written and the filter kernel designed, we are ready to create a program that can be executed on the SHARC DSP.
This is done by running the compiler, the assembler, and then the linker; three programs provided with the EZ-KIT Lite.
The compiler converts a C program into the SHARC's assembly language.
If you directly write the program in assembly, such as in this example, you bypass this step.
The assembler and linker convert the program and external files (such as the architecture file, codec initialization routines, filter kernel, etc.) into the final executable file.
All this takes about 30 seconds, with the final result being a SHARC program residing on the harddisk of your PC.
The EZ-KIT Lite host is then used to run the program on the EZ-KIT Lite.
Simply click Before entering the main program, the following constant and variables must be defined: NR_COEF The number of coefficients in the filter kernel (301 in this example) on the file you want the DSP to run, and the EZ-KIT Lite host takes care of the rest, downloading the program and starting it running.
This brings us to two questions.
First, how do we test our audio filter to make sure it is operating as we designed it; and second, what in the world is a company called Analog Devices doing making Digital Signal Processors?
Analog measurements on a DSP system For just a few moments, forget that you are studying digital techniques.
Let's take a look at this from the standpoint of an engineer that specializes in analog electronics.
He doesn't care what is inside of the EZ-KIT Lite, only that it has an analog input and an analog output.
As shown in Fig. 29-3, he would invoke the traditional analog method of analyzing a "black box," attach a signal generator to the input, and look at the output on an oscilloscope.
What does our analog guru find?
First, the system is linear (as least as far as this simple test can tell).
If a sine wave is placed into the input, a sine wave is observed on the output.
If the amplitude or frequency of the input is changed, a corresponding change is seen in the output.
When the input frequency is slowly increased, there comes a point where the amplitude of the output sine wave decreases rapidly to zero.
That occurs just below one-half the sampling rate, due to the action of the anti-alias filter on the ADC.
Now our engineer notices something unknown in the analog world: the system has a perfect linear phase.
In other words, there is a constant delay between an event occurring in the input signal, and the result of that event in the output signal.
For instance, consider our example filter kernel in Fig. 29-2.
Since the center of symmetry is at sample 150, the output signal will be delayed by 150 samples relative to the input signal.
If the system is sampling at 8 kHz, for example, this delay will be 18.75 milliseconds.
In addition, the sigma-delta converter will also provide a small additional fixed delay.
Oscilloscope Signal Generator input EZ-KIT output FIGURE 29- Testing the EZ-KIT Lite.
Analog engineers test the performance of a system by connecting a signal generator to its input, and an oscilloscope to its output.
When a DSP system (such as the EZ-KIT Lite) is tested in this way, it appears to be a virtually perfect analog system Measured frequency response Amplitude FIGURE 29- Measured frequency response.
This graph shows measured points on the frequency response of the example FIR filter.
These measured points have far less accuracy than the designed frequency response of Fig. 292a.
Frequency Our analog engineer will become very agitated when he sees this linear phase.
The signals won't appear the way he thinks they should, and he will start twisting knobs at lightning speed.
He will complain that the triggering isn't working right, and mumble such things as: "this doesn't make sense," what's going on here?",
The performance of DSP systems is so good, it will take him a few minutes before he understands what he is seeing.
To make him even more impressed, we ask our engineer to manually measure the frequency response of the system.
To do this, he will step the signal generator through all the frequencies between 125 Hz and 10 kHz in increments of 125 Hz.
At each frequency he measures the amplitude of the output signal and divides it by the amplitude of the input signal.
We set the sampling rate of the EZ-KIT Lite at 22 kHz for this test.
In other words, the 0 to 0.5 digital frequency of Fig. 29-2a is mapped to DC to 11 kHz in our real world measurement.
Figure 29-4 shows actual measurements taken on the EZ-KIT Lite; it couldn't be better!
The measured data points agree with the theoretical curve within the limit of measurement error.
This is something our analog engineer has never seen with filters made from resistors, capacitors, and inductors.
However, even this doesn't give the DSP the credit it deserves.
Analog measurements using oscilloscopes and digital-volt-meters have a typical accuracy and precision of about 0.1% to 1%.
In comparison, this DSP system is limited only by the -0.001% round-off error of the 16 bit codec, since the internal calculations use floating point.
In other words, the device being evaluated is one-hundred times more precise than the measurement tool being used.
A proper evaluation of the frequency response would require a specialized instrument, such as a computerized data acquisition system with a 20 bit ADC.
Given these facts, it is not surprising that DSPs are often used in measurement instruments to achieve high precision.
Now we can answer the question: Why does Analog Devices sell Digital Signal Processors?
Only a decade ago, state-of-the-art signal processing was carried out with precision op amps and similar transistor circuits.
Today, the highest quality analog processing is accomplished with digital techniques.
Analog Devices is a great role-model for individuals and other companies; hold on to your vision and goals, but don't be afraid to adapt with the changing technology!
Another Look at Fixed versus Floating Point In this last example, we took advantage of one of the SHARC DSP's key features, its ability to handle floating point calculations.
Even though the samples are in a fixed point format when passed to and from the codec, we go to the trouble of converting them to floating point for the intermediate FIR filtering algorithm.
As discussed in the last chapter, there are two reasons for wanting to process the data with floating point math: ease of programming, and performance.
Does it really make a difference?
For the programmer, yes, it makes a large difference.
Floating point code is far easier to write.
Look back at the assembly program in Table 29-2.
There are only two lines (41 and 42) in the main FIR filter.
In contrast, the fixed point programmer must add code to manage the data at each math calculation.
To avoid overflow and underflow, the values must be checked for size and, if needed, scaled accordingly.
The intermediate results will also need to be stored in an extended precision accumulator to avoid the devastating effects of repeated round-off error.
The issue of performance is much more subtle.
For example, Fig. 29-5a shows an FIR low-pass filter with a moderately sharp cutoff, as described in Chapter 16.
This "large scale" curve would look the same whether fixed or floating point were used in the calculation.
To see the difference between these two methods, we must zoom in on the amplitude by a factor of several hundred as shown in (b), (c), and (d).
Here we can see a clear difference.
The floating point execution, (b), has such low round-off noise that its performance is limited by the way we designed the filter kernel.
The 0.02% overshoot near the transition is a characteristic of the Blackman window used in this filter.
The point is, if we want to improve the performance, we need to work on the algorithm, not the implementation.
The curves in (c) and (d) show the roundoff noise introduced when each point in the filter kernel is represented by and 14 bits, respectively.
A better algorithm would do nothing to make these better curves; the shape of the actual frequency response is swamped by noise.
Figure 29-6 shows the difference between fixed and floating point in the time domain.
Figure (a) shows a wiggly signal that exponentially decreases in amplitude.
This might represent, for example, the sound wave from a plucked string, or the shaking of the ground from a distant explosion.
As before, this "large scale" waveform would look the same whether fixed or floating point were used to represent the samples.
To see the difference, a. Frequency response b.
Floating point Ripple from window used Amplitude Amplitude Frequency Frequency c. 16 bits d. 14 bits Amplitude Amplitude Frequency Frequency FIGURE 29- Round-off noise in the frequency response.
Figure (a) shows the frequency response of a windowed-sinc low-pass filter, using a Blackman window and 150 points in the filter kernel.
Figures (b), (c), and (d) show a more detailed view of this response by zooming in on the amplitude.
When the filter kernel is represented in floating point, (b), the round-off noise is insignificant compared to the imperfections of the windowed-sinc design.
As shown in (c) and (d), representing the filter kernel in fixed point makes round-off noise the dominate imperfection.
As discussed in Chapter 3, this quantization appears much as additive random noise, limiting the detectability of small components in the signals.
These performance differences between fixed and floating point are often not important; for instance, they cannot even be seen in the "large scale" signals of Fig. 29-5a and Fig. 29-6a.
However, there are some applications where the extra performance of floating point is helpful, and may even be critical.
For instance, high-fidelity consumer audio system, such as CD players, represent the signals with 16 bit fixed point.
In most cases, this exceeds the capability of human hearing.
However, the best professional audio systems sample the signals with as high as 20 to 24 bits, leaving absolutely no room for artifacts that might contaminate the music.
Floating point is nearly ideal for algorithms that process these high-precision digital signals.
Another case where the higher performance of floating point is needed is when the algorithm is especially sensitive to noise.
For instance, FIR Round-off noise in the time domain.
Figure (a) shows an example signal with an exponentially decaying amplitude.
Figures (b), (c), and (d) show a more detailed view by zooming in on the amplitude.
When the signal is represented in floating point, (b), the round-off noise is so low that it cannot be seen in this graph.
As shown in (c) and (d), representing the signal in fixed point produces far higher levels of round-off noise.
As shown in Fig. 29-5, roundoff noise doesn't change the overall shape of the frequency response; the entire curve just becomes noisier.
IIR filters are a different story; round-off can cause all sorts of havoc, including making them unstable.
Floating point allows these algorithms to achieve better performance in cutoff frequency sharpness, stopband attenuation, and step response overshoot.
Advanced Software Tools Our custom filter example shows the easiest way to get a program running on the SHARC DSP: editing, assembling, linking, and downloading, performed by individual programs.
This method is fine for simple tasks, but there are better software tools available for the advanced programmer.
Let's look at what is available for when you get really serious about DSPs.
The first tool we want to examine is the C compiler.
As discussed in the last chapter, both assembly and C are commonly used to program MATH OPERATIONS abs absolute value acos arc cosine asin arc sine atan arc tangent atan arc tangent of quotient cabsf complex absolute value cexpf complex exponential cos cosine cosh hyperbolic cosine cot cotangent div division exp exponential fmod modulus log natural logarithm log base 10 logarithm matadd matrix addition matmul matrix multiplication pow raise to a power rand random number generator sin sine sinh hyperbolic sine sqrt square root srand random number seed tan tangent tanh hyperbolic tangent PROGRAM CONTROL abort abnormal program end calloc allocate / initialize memory free deallocate memory idle processor idle instruction interrupt define interrupt handling poll_flag_in test input flag set_flag sets the processor flags timer_off disable processor timer timer_on enable processor timer timer_set initialize processor timer TABLE 29- C library functions.
This is a partial list of the functions available when C is used to program the Analog Devices SHARC DSPs.
CHARACTER & STRING MANIPULATION atoi convert string to integer bsearch binary search of array isalnum detect alphanumeric character isalpha detect alphabetic character iscntrl detect control character isdigit detect decimal digit isgraph detect printable character islower detect lowercase character isprint detect printable character ispunct detect punctuation character isspace detect whitespace character isupper detect uppercase character isxdigit detect hexadecimal digit memchr find first occurrence of char memcpy copy characters strcat concatenate strings strcmp compare strings strerror get error message strlen string length strncmp compare characters strrchr find last occurrence of char strstr find string within string strtok convert string to tokens system sent string to operating system tolower change uppercase to lowercase toupper change lowercase to uppercase SIGNAL PROCESSING a_compress A-law compressing a_expand A-law expansion autocorr autocorrelation biquad biquad filter section cfftN complex FFT crosscorr cross-correlation fir FIR filter histogram histogram iffN inverse complex FFT IIR filter mean mean of an array mu_compress mu law compression mu_expand mu law expansion rfftN real FFT rms rms value of an array DSPs.
A tremendous advantage of using C is the library of functions, both standard C operations, as well as DSP algorithms.
Table 29-3 shows a partial list of the C library functions for the SHARC DSPs.
The math group includes many functions common in DSP, such as the trig functions (sin, cos, tan, etc.), logarithm, and exponent.
If you need these type of functions in your program, this is probably enough motivation in itself to use C instead of assembly.
Pay special attention to the "signal processing" routines in Table 29-3.
Here you will find key DSP algorithms, including: real and complex FFTs, FIR and IIR filters, and statistical functions such as the mean, rms value, and variance.
Of course, all these routines are written in assembly, allowing them to be very efficient in both speed and memory usage.
CIRCBUF.C This is an echo program written in C for the ADSP-21061 EZ-KIT Lite.
The */ echo program takes the talkthru program and adds a circular buffering scheme.
The circular buffer is defined by the functions CIRCULAR_BUFFER, BASE, and LENGTH.
The echo is performed by adding the current input to the oldest input.
The delay in the echo can be modified by changing BUFF_LENGTH.
This provides an integrated development environment for creating programs on the SHARC.
All of the following functions can be accessed from this single interface: editing, compiling, assembling, linking, simulating, debugging, downloading, and PROM creation.
Table 29-4 shows an example C program, taken from the Analog Devices' "C Compiler Guide and Reference Manual."
This program generates an echo by adding a delayed version of the signal to itself.
The most recent 4000 samples from the input signal are stored in a circular buffer.
As each sample is acquired, the circular buffer is updated, the newest sample is added to a scaled version of the oldest sample, and the resulting sample directed to the output.
The next advanced software tool you should look for is an integrated development environment.
This is a fancy term that means everything needed to program and test the DSP is combined into one smoothly functioning package.
Analog Devices provides an integrated development environment in a product called VisualDSP ®, running under Windows® 95 and Windows NTTM. Figure 29-7 shows an example of the main screen, providing a seamless way to edit, build, and debug programs.
Here are some of the key features of VisualDSP, showing why an integrated development environment is so important for fast software development.
The editor is specialized for creating programs in C, assembly, and a mixture of the two.
For instance, it understands the syntax of the languages, allowing it to display different types of statements in different colors.
You can also 1. Profile code to identify bottlenecks 2. View mixed C and Assembly listings 3. Create custom Register window FIGURE 29- VisualDSP debugging screen.
This is a common interface for both simulation and emulation.
It can view a C program interspersed with the resulting assembly code, track execution of instructions; examine registers (hardware, software, and memory); trace bus activity; and many other tasks.
This is very convenient, since the final program is created by linking several files together.
Figure 29-8 shows an example screen from the VisualDSP debugger.
This is an interface to two different types of tools: simulators and emulators.
Simulators test the code within the personal computer, without even needing a DSP to be present.
This is generally the first debugging step after the program is written.
The simulator mimics the architecture and operation of the hardware, including: input data streams, interrupts and other I/O.
Emulators (such as the Analog Devices EZ-ICE®) examine the program operation on the actual hardware.
This requires the emulator software (on your PC) to be able to monitor the electrical signals inside of the DSP.
To support this, the SHARC DSPs feature an IEEE 1140.1 JTAG Test Access Port, allowing an external device to track the processor's internal functions.
After you have used an evaluation kit and given some thought to purchasing advanced software tools, you should also consider attending a training class.
These are given by many DSP manufacturers.
For instance, Analog Devices offers a 3 day class, taught several time a year, at several different locations.
These are a great way of learning about DSPs from the experts.
Look at the manufacturer's websites for details.
Complex Numbers Complex numbers are an extension of the ordinary numbers used in everyday math.
They have the unique property of representing and manipulating two variables as a single quantity.
This fits very naturally with Fourier analysis, where the frequency domain is composed of two signals, the real and the imaginary parts.
Complex numbers shorten the equations used in DSP, and enable techniques that are difficult or impossible with real numbers alone.
For instance, the Fast Fourier Transform is based on complex numbers.
Unfortunately, complex techniques are very mathematical, and it requires a great deal of study and practice to use them effectively.
Many scientists and engineers regard complex techniques as the dividing line between DSP as a tool, and DSP as a career.
In this chapter, we look at the mathematics of complex numbers, and elementary ways of using them in science and engineering.
The following three chapters discuss important techniques based on complex numbers: the complex Fourier transform, the Laplace transform, and the z-transform.
These complex transforms are the heart of theoretical DSP.
Get ready, here comes the math!
The Complex Number System To illustrate complex numbers, consider a child throwing a ball into the air.
For example, assume that the ball is thrown straight up, with an initial velocity of 9.8 meters per second.
One second after it leaves the child's hand, the ball has reached a height of 4.9 meters, and the acceleration of gravity (9.8 meters per second 2) has reduced its velocity to zero.
The ball then accelerates toward the ground, being caught by the child two seconds after it was thrown.
From basic physics equations, the height of the ball at any instant of time is given by: where h is the height above the ground (in meters), g is the acceleration of gravity (9.8 meters per second 2 ), v is the initial velocity (9.8 meters per second), and t is the time (in seconds).
Now, suppose we want to know when the ball passes a certain height.
Plugging in the known values and solving for t: t ' 1± 1& h/4.
For instance, the ball is at a height of 3 meters twice: t ' 0.38 (going up) and t ' 1.62 seconds (going down).
As long as we ask reasonable questions, these equations give reasonable answers.
But what happens when we ask unreasonable questions?
For example: At what time does the ball reach a height of 10 meters?
This question has no answer in reality because the ball never reaches this height.
Nevertheless, plugging the value of h ' 10 into the above equation gives two answers: t ' 1 % & 1.041 and t ' 1 & & 1.041 .
Both these answers contain the square-root of a negative number, something that does not exist in the world as we know it.
This unusual property of polynomial equations was first used by the Italian mathematician Girolamo Cardano (1501-1576).
Two centuries later, the great German mathematician Carl Friedrich Gauss (1777-1855) coined the term complex numbers, and paved the way for the modern understanding of the field.
Every complex number is the sum of two components: a real part and an imaginary part.
The real part is a real number, one of the ordinary numbers we all learned in childhood.
The imaginary part is an imaginary number, that is, the square-root of a negative number.
To keep things standardized, the imaginary part is usually reduced to an ordinary number multiplied by the square-root of negative one.
As an example, the complex number: t ' 1 % & 1.041, is first reduced to: t ' 1 % 1.041 & 1, and then to the final form: t ' 1 % 1.02 & 1 .
The real part of this complex number is 1, while the imaginary part is 1.02 & 1 .
This notation allows the abstract term, & 1, to be given a special symbol.
Mathematicians have long used i to denote & 1 .
In comparison, electrical engineers use the symbol, j, because i is used to represent electrical current.
Both symbols are common in DSP.
In this book the electrical engineering convention, j, will be used.
For example, all the following are valid complex numbers: 1 % 2 j, 1 & 2 j, & 1 % 2 j, 3.14159 % 2.7183 j, (4/3) % (19/2) j, etc.
All ordinary numbers, such as: 2, 6.34, and -1.414, can be viewed as a complex number with zero for the imaginary part, i.e., 2 % 0 j, 6.34 % 0 j, and & 1.414 % 0 j .
Just as real numbers are described as having positions along a number line, complex numbers are represented by locations in a two-dimensional display called the complex plane.
As shown in Fig. 30-1, the horizontal axis of the Imaginary axis FIGURE 30- The complex plane.
Every complex number has a unique location in the complex plane, as illustrated by the three examples shown here.
The horizontal axis represents the real part, while the vertical axis represents the imaginary part.
Since real numbers are those complex numbers that have an imaginary part equal to zero, the real number line is the same as the x-axis of the complex plane.
In mathematical equations, a complex number is represented by a single variable, even though it is composed of two parts.
For example, the three complex variables in Fig. 30-1 could be written: A ' 2 % 6j B ' & 4 & 1.5j C ' 3 & 7j where A, B, & C are complex variables.
This illustrates a strong advantage and a strong disadvantage of using complex numbers.
The advantage is the inherent shorthand of representing two things by a single symbol.
The disadvantage is having to remember which variables are complex and which variables are ordinary numbers.
The mathematical notation for separating a complex number into its real and imaginary parts uses the operators: Re ( ) and Im ( ) .
For example, using the above complex numbers: Re A = Re B = - Re C = Im A = Im B = -1.
Im C = - Notice that the value returned by the mathematical operator, Im ( ), does not include the j.
For example, Im (3 % 4 j ) is equal to 4, not 4 j .
Complex numbers follow the same algebra as ordinary numbers, treating the quantity, j, as a constant.
For instance, addition, subtraction, multiplication and division are given by: EQUATION 30- Addition of complex numbers.
First, whenever a j 2 term is encountered, it is replaced by -1.
This follows from the definition of j, that is: j 2 ' ( & 1 )2 ' & 1 .
The second trick is a way to eliminate the j term from the denominator of a fraction.
For instance, the left side of Eq. 30-4 has a denominator of c % d j .
This is handled by multiplying the numerator and denominator by the term c & jd, cancelling all the imaginary terms from the denominator.
In the jargon of the field, switching the sign of the imaginary part of a complex number is called taking the complex conjugate.
This is denoted by a star at the upper right corner of the variable.
For example, if Z ' a % b j, then Z t ' a & b j .
In other words, Eq. 304 is derived by multiplying both the numerator and denominator by the complex conjugate of the denominator.
The following properties hold even when the variables A, B, and C are complex.
These relations can be proven by breaking each variable into its real and imaginary parts and working out the algebra.
EQUATION 30- Commutative property.
AB ' BA EQUATION 30- Associative property.
A (B % C) ' AB % AC Polar Notation Complex numbers can also be expressed in polar notation, besides the rectangular notation just described.
For example, Fig. 30-2 shows three complex numbers in polar form, the same ones previously presented in Fig. 30-1.
The magnitude is the length of the vector starting at the origin and ending at the complex point, while the phase angle is measured between this vector and the positive x-axis.
Complex numbers can be converted between rectangular and polar notation by the following equations (paying attention to the polar notation nuisances discussed in Chapter 8): EQUATION 30- Rectangular-to-polar conversion.
The complex variable, A, can be changed from rectangular form: Re A & Im A, to polar form: M & 2. EQUATION 30- Polar-to-rectangular conversion.
This is changing the complex number from M & 2 to Re A & Im A. Re A ' M cos (2 ) (Re A)2 % (Im A) 2 ' arctan Im A ' M sin(2 ) This brings up a giant leap in the mathematics.
A complex number written in rectangular notation 2 + 6 j or M = % 2 = arctan (6/2) Imaginary axis FIGURE 30- Complex numbers in polar form.
Three example points in the complex plane are shown in polar coordinates.
Figure 30- shows these same points in rectangular form.
The information is carried in the variables: a & b, but the proper complex number is the entire expression: a % b j .
In polar form, the key information is contained in M & 2, but what is the full expression for the proper complex number?
The key to this is Eq.
If we start with the proper complex number, a % b j, and apply Eq. 30-9, we obtain: EQUATION 30- Rectangular and polar complex numbers.
The left side is the rectangular form of a complex number, while the expression on the right is the polar representation.
The conversion between: M & 2 and a & b, is given by Eqs.
Before continuing with the next step, let's review how we arrived at this point.
First, we gave the rectangular form of a complex number a graphical representation, that is, a location in a two-dimensional plane.
Second, we defined the terms M & 2 to be consistent with our previous experience about the relationship between polar and rectangular coordinates (Eq.
Third, we followed the mathematical consequences of these actions, arriving at what the correct polar form of a complex number must be, i.e., M (cos2 % j sin2) .
Even though this logic is straightforward, the result is difficult to see with "intuition."
Unfortunately, it gets worse.
One of the most important equations in complex mathematics is Euler's relation, named for the clever and very prolific Swiss mathematician, Leonhard Euler (1707-1783; Euler is pronounced: "Oiler"): EQUATION 30- Euler's relation.
This is a key equation for using complex numbers in science and engineering.
Don't spend too much time on this proof; we aren't going to use it for anything.
Rewriting Eq. 30-10 using Euler's relation results in the most common way of expressing a complex number in polar notation, a complex exponential: EQUATION 30- Exponential form of complex numbers.
The rectangular form, on the left, is equal to the exponential polar form, on the right.
Start your understanding by memorizing Eqs.
A strong advantage of using this exponential polar form is that it is very simple to multiply and divide complex numbers: EQUATION 30- Multiplication of complex numbers.
EQUATION 30- Division of complex numbers.
That is, complex numbers in polar form are multiplied by multiplying their magnitudes and adding their angles.
The easiest way to perform addition and subtraction in polar form is to convert the numbers to rectangular form, perform the operation, and reconvert back into polar.
Complex numbers are usually expressed in rectangular form in computer routines, but in polar form when writing and manipulating equations.
Just as Re ( ) and Im ( ) are used to extract the rectangular components from a complex number, the operators Mag ( ) and Phase ( ) are used to extract the polar components.
For example, if A ' 5 e jB/ 7, then Mag (A) ' 5 and Phase (A) ' B/7 .
Using Complex Numbers by Substitution Let's summarize where we are at.
Solutions to common algebraic equations often contain the square-root of a negative number.
These are called complex numbers, and represent solutions that cannot exist in the world as we know it.
Complex numbers are expressed in one of two forms: a % b j (rectangular), or M e j 2 (polar), where j is a symbol representing & 1 .
Using either notation, a single complex number contains two separate pieces of information, either a & b, or M & 2. In spite of their elusive nature, complex numbers follow mathematical laws that are similar (or identical) to those governing ordinary numbers.
This describes what complex numbers are and how they fit into the world of pure mathematics.
Our next task is to describe ways they are useful in science and engineering problems.
How is it possible to use a mathematics that has no connection with our everyday experience?
The answer: If the tool we have is a hammer, make the problem look like a nail.
In other words, we change the physical problem into a complex number form, manipulate the complex numbers, and then change back into a physical answer.
There are two ways that physical problems can be represented using complex numbers: a simple method of substitution, and a more elegant method we will call mathematical equivalence.
Mathematical equivalence will be discussed in the next chapter on the complex Fourier transform.
The remainder of this chapter is devoted to substitution.
Substitution takes two real physical parameters and places one in the real part of the complex number and one in the imaginary part.
This allows the two values to be manipulated as a single entity, i.e., a single complex number.
After the desired mathematical operations, the complex number is separated into its real and imaginary parts, which again correspond to the physical parameters we are concerned with.
A simple example will show how this works.
As you recall from elementary physics, vectors can represent such things as: force, velocity, acceleration, etc.
For example, imagine a sailboat being pushed in one direction by the wind, and in another direction by the ocean current.
The resulting force on the boat is the vector sum of the two individual force vectors.
This example is shown in Fig. 30-3, where two vectors, A and B, are added through the parallelogram law, resulting in C. We can represent this problem with complex numbers by placing the east/west coordinate into the real part of a complex number, and the north/south coordinate into the imaginary part.
This allows us to treat each vector as a single complex number, even though it is composed of two parts.
For instance, the force of the wind, vector A, might be in the direction of 2 parts to the east and 6 parts to the north, represented as the complex number: 2 % 6 j .
Likewise, the force of the ocean current, vector B, might be in the direction of 4 parts to the east and 3 parts to the south, represented as the complex number: 4 & 3 j .
These two vectors can be added via Eq.
Converting this back into a physical meaning, the combined force on the sailboat is in the direction of 6 parts to the north and 3 parts to the east.
Could this problem be solved without complex numbers?
Of course!
The complex numbers merely provide a formalized way of keeping track of the two components that form a single vector.
The idea to remember is that some physical problems can be converted into a complex form by simply adding a j to one of the components.
Converting back to the physical problem is nothing more than dropping the j.
This is the essence of the substitution method.
Here's the rub.
How do we know that the rules and laws that apply to complex mathematics are the same rules and laws that apply to the original North A+B=C Imaginary axis FIGURE 30- Adding vectors with complex numbers.
The vectors A & B represent forces measured with respect to north/south and east/west.
The east/west dimension is replaced by the real part of the complex number, while the north/south dimension is replaced by the imaginary part.
This substitution allows complex mathematics to be used with an entirely real problem.
Real axis South physical problem?
For instance, we used Eq.
How do we know that the addition of complex numbers provides the same result as the addition of force vectors?
In most cases, we know that complex mathematics can be used for a particular application because someone else said it does.
Some brilliant and well respected mathematician or engineer worked out the details and published the results.
The point to remember is that we cannot substitute just any problem into a complex form and expect the answer to make sense.
We must stick to applications that have been shown to be applicable to complex analysis.
Let's look at an example where complex number substitution does not work.
Imagine that you buy apples for $5 a box, and oranges for $10 a box.
You represent this by the complex number: 5 % 10 j .
During a particular week, you buy 6 boxes of apples and 2 boxes of oranges, which you represent by the complex number: 6 % 2 j .
The total price you must pay for the goods is equal to number of items multiplied by the price of each item, that is, In other words, the complex math indicates you must pay a total of $10 for the apples and $70 for the oranges.
The problem is, the answer is completely wrong!
The rules of complex mathematics do not follow the rules of this particular physical problem.
Complex Representation of Sinusoids Complex numbers find a niche in electronics and signal processing because they are a compact way to represent and manipulate the most useful of all waveforms: sine and cosine waves.
The conventional way to represent a sinusoid is: M cos (Tt % N) or A cos(Tt) % Bsin (Tt ), in polar and rectangular notation, respectively.
Notice that we are representing frequency by T, the natural frequency in radians per second.
If it makes you more comfortable, you can replace each T with 2Bf to make the expressions in hertz.
However, most DSP mathematics is written using the shorter notation, and you should become familiar with it.
Since it requires two parameters to represent a single sinusoid (i.e., A & B, or M & N), the use of complex numbers to represent these important waveforms is a natural.
Using substitution, the change from the conventional sinusoid representation to a complex number is straightforward.
In rectangular form: (conventional representation) (complex number) where A W a, and B W & b .
Put in words, the amplitude of the cosine wave becomes the real part of the complex number, while the negative of the sine wave's amplitude becomes the imaginary part.
It is important to understand that this is not an equation, but merely a way of letting a complex number represent a sinusoid.
This substitution also can be applied in polar form: M cos (Tt % N) (conventional representation) (complex number) where M W M, and 2 W & N.
In words, the polar notation substitution leaves the magnitude the same, but changes the sign of the phase angle.
Why change the sign of the imaginary part & phase angle?
This is to make the substitution appear in the same form as the complex Fourier transform described in the next chapter.
The substitution techniques of this chapter gain nothing from this sign change, but it is almost always done to keep things consistent with the more advanced methods.
Using complex numbers to represent sine and cosine waves is a common technique in electrical circuit analysis and DSP.
This is because many (but not all) of the rules and laws governing complex numbers are the same as those governing sinusoids.
In other words, we can represent the sine and cosine waves with complex numbers, manipulate the numbers in various ways, and have the resulting answer match the way the sinusoids behave.
However, we must be careful to use only those mathematical operations that mimic the physical problem being represented (sinusoids in this case).
For example, suppose we use the complex variables, A and B, to represent two sinusoids of the same frequency, but with different amplitudes and phase shifts.
When the two complex numbers are added, a third complex number is produced.
Likewise, a third sinusoid is created when the two sinusoids are added.
As you would hope, the third complex number represents the third sinusoid.
The complex addition matches the physical system.
Now, imagine multiplying the complex numbers A and B, resulting in another complex number.
Does this match what happens when the two sinusoids are multiplied?
No! Multiplying two sinusoids does not produce another sinusoid.
Complex multiplication fails to match the physical system, and therefore cannot be used.
Fortunately, the valid operations are clearly defined.
Two conditions must be satisfied.
First, all of the sinusoids must be at the same frequency.
For example, if the complex numbers: 1 % 1 j and 2 % 2 j represent sinusoids at the same frequency, then the sum of the two sinusoids is represented by the complex number: 3 % 3 j .
However, if 1 % 1 j and 2 % 2 j represent sinusoids with different frequencies, there is nothing that can be done with the complex representation.
In this case, the sum of the complex numbers, 3 % 3 j, is meaningless.
In spite of this, frequency can be left as a variable when using complex numbers, but it must be the same frequency everywhere.
For instance, it is perfectly valid to add: 2T% 3Tj and 3T% 1 j, to produce: 5T% (3T% 1) j .
These represent sinusoids where the amplitude and phase vary as frequency changes.
While we do not know what the particular frequency is, we do know that it is the same everywhere, i.e., T. The second requirement is that the operations being represented must be linear, as discussed in Chapter 5.
For instance, sinusoids can be combined by addition and subtraction, but not by multiplication or division.
Likewise, systems may be amplifiers, attenuators, high or low-pass filters, etc., but not such actions as: squaring, clipping and thresholding.
Remember, even convolution and Fourier analysis are only valid for linear systems.
Complex Representation of Systems Figure 30-4 shows an example of using complex numbers to represent a sinusoid passing through a linear system.
We will use continuous signals for this example, although discrete signals are handled the same way.
Since the input signal is a sinusoid, and the system is linear, the output will also be a sinusoid, and at the same frequency as the input.
As shown, the example input signal has a conventional representation of: 3 cos(Tt % B/4), or the equivalent expression: 2.1213 cos(Tt) & 2.1213 sin(Tt) .
When represented by a complex number this becomes: 3 e & jB/4 or 2.1213 % j 2.1213 .
Likewise, the conventional representation of the output is: 1.5 cos(Tt & B/8), or in the alternate form: 1.3858 cos(Tt) % 0.5740 sin (Tt ) .
This is represented by the complex number: 1.5 e j B/8 or 1.3858 & j 0.5740 .
The system's characteristics can also be represented by a complex number.
The magnitude of the complex number is the ratio between the magnitudes Input signal Linear System Complex representation Conventional Amplitude Amplitude Output signal Sinusoids represented by complex numbers.
Complex numbers are popular in DSP and electronics because they are a convenient way to represent and manipulate sinusoids.
As shown in this example, sinusoidal input and output signals can be represented as complex numbers, expressed in either polar or rectangular form.
In addition, the change that a linear system makes to a sinusoid can also be expressed as a complex number. of the input and output (i.e., Mout /Min ).
Likewise, the angle of the complex number is the negative of the difference between the input and output angles (i.e., & [Nout & Nin ] ).
In the example used here, the system is described by the complex number, 0.5 e j 3B/8 .
In other words, the amplitude of the sinusoid is reduced by 0.5, while the phase angle is changed by & 3B/8 .
The complex number representing the system can be converted into rectangular form as: 0.1913 & j 0.4619, but we must be careful in interpreting what this means.
It does not mean that a sine wave passing through the system is changed in amplitude by 0.1913, nor that a cosine wave is changed by -0.4619.
In general, a pure sine or cosine wave entering a linear system is converted into a mixture of sine and cosine waves.
Fortunately, the complex math automatically keeps track of these cross-terms.
When a sinusoid passes through a linear system, the complex numbers representing the input signal and the system are multiplied, producing the complex number representing the output.
If any two of the complex numbers are known, the third can be found.
The calculations can be carried out in either polar or rectangular form, as shown in Fig. 30-4.
In previous chapters we described how the Fourier transform decomposes a signal into cosine and sine waves.
The amplitudes of the cosine waves are called the real part, while the amplitudes of the sine waves are called the imaginary part.
We stressed that these amplitudes are ordinary numbers, and the terms real and imaginary are just names used to keep the two separate.
Now that complex numbers have been introduced, it should be quite obvious were the names come from.
For example, imagine a 1024 point signal being decomposed into 513 cosine waves and 513 sine waves.
Using substitution, we can represent the spectrum by 513 complex numbers.
However, don't be misled into thinking that this is the complex Fourier transform, the topic of Chapter 31.
This is still the real Fourier transform; the spectrum has just been placed in a complex format by using substitution.
Electrical Circuit Analysis This method of substituting complex numbers for cosine & sine waves is called the Phasor transform.
It is the main tool used to analyze networks composed of resistors, capacitors and inductors.
This allows the procedure to be written as an equation, making it easier to deal with in mathematical work.
The first step is to understand the relationship between the current and voltage for each of these devices.
For the resistor, this is expressed in Ohm's law: v ' iR, where i is the instantaneous current through the device, v is the instantaneous voltage across the device, and R is the resistance.
In contrast, the capacitor and inductor are governed by the differential equations: i ' C dv/dt, and v ' L di/dt, where C is the capacitance and L is the inductance.
In the most general method of circuit analysis, these nasty differential equations are combined as dictated by the circuit configuration, and then solved for the parameters of interest.
While this will answer everything about the circuit, the math can become a real mess.
This can be greatly simplified by restricting the signals to be sinusoids.
By representing these sinusoids with complex numbers, the difficult differential equations can be directly replaced with much simpler algebraic equations.
Figure 30-5 illustrates how this works.
We treat each of these three components (resistor, capacitor & inductor) as a system.
The input to the system is the sinusoidal current through the device, while the output is the sinusoidal voltage across its two terminals.
This means we can represent the input and output of the system by the two complex variables: I (for current) and V (for voltage), respectively.
The relation between the input and output can also be expressed by a complex number.
This complex number is called the impedance, and is given the symbol: Z.
This means: I ×Z ' V In words, the complex number representing the sinusoidal voltage is equal to the complex number representing the sinusoidal current multiplied by the impedance (another complex number).
Given any two, the third can be Resistor Inductor Capacitor Amplitude Amplitude Amplitude FIGURE 30- Definition of impedance.
When sinusoidal voltages and currents are represented by complex numbers, the ratio between the two is called the impedance, and is denoted by the complex variable, Z. Resistors, capacitors and inductors have impedances of R, & j/ TC, and jTL, respectively.
In polar form, the magnitude of the impedance is the ratio between the amplitudes of V and I. Likewise, the phase of the impedance is the phase difference between V and I.
This relation can be thought of as Ohm's law for sinusoids.
Ohms's law ( v ' iR ) describes how the resistance relates the instantaneous current and voltage in a resistor.
When the signals are sinusoids represented by complex numbers, the relation becomes: V ' IZ .
That is, the impedance relates the current and voltage.
Resistance is an ordinary number, since it deals with two ordinary numbers.
Impedance is a complex number, since it relates two complex numbers.
Impedance contains more information than resistance, because it dictates both the amplitudes and the phase angles.
From the differential equations that govern their operation, it can be shown that the impedance of the resistor, capacitor, and inductor are: R, & j /TC, and jTL, respectively.
As an example, imagine that the current in each of these components is a unity amplitude cosine wave, as shown in Fig. 30-5.
Using substitution, this is represented by the complex number: 1% 0 j .
The voltage across the resistor will be: V ' I Z ' (1% 0 j) R ' R% 0 j .
In other words, a cosine wave of amplitude R. The voltage across the capacitor is found to be: This reduces to: 0 & j /TC, a sine wave of amplitude, 1/TC .
Likewise, the voltage across the inductor can be calculated: This reduces to: 0 % jTL, a negative sine wave of amplitude, TL .
The beauty of this method is that RLC circuits can be analyzed without having to resort to differential equations.
The impedance of the resistors, capacitors, FIGURE 30- RLC notch filter.
This circuit removes a narrow band of frequencies from a signal.
The use of complex substitution greatly simplifies the analysis of this and similar circuits.
This includes all of the basic combinations, such as: resistors in series, resistors in parallel, voltage dividers, etc.
As an example, Fig. 30-6 shows an RLC circuit called a notch filter, used to remove a narrow band of frequencies.
For instance, it could eliminate 60 hertz interference in an audio or instrumentation signal.
If this circuit were composed of three resistors (instead of the resistor, capacitor and inductor), the relationship between the input and output signals would be given by the voltage divider formula: vout / vin ' (R2% R3) /(R1% R2% R3) .
Since the circuit contains capacitors and inductors, the equation is rewritten with impedances: Next, we crank through the algebra to separate everything containing a j, from everything that does not contain a j.
In other words, we separate the equation into its real and imaginary parts.
This algebra can be tiresome and long, but the alternative is to write and solve differential equations, an a. Magnitude b.
Phase Phase (radians) Amplitude Frequency (MHz) Frequency (MHz) Notch filter frequency response.
These curves are for the component values: R = 50S, C even nastier task.
When separated into the real and imaginary parts, the complex representation of the notch filter becomes: Phase ' arctan TL & 1/TC The key point to remember from these examples is how substitution allows complex numbers to represent real world problems.
In the next chapter we will look at a more advanced way to use complex numbers in science and engineering, mathematical equivalence.
The Complex Fourier Transform Although complex numbers are fundamentally disconnected from our reality, they can be used to solve science and engineering problems in two ways.
First, the parameters from a real world problem can be substituted into a complex form, as presented in the last chapter.
The second method is much more elegant and powerful, a way of making the complex numbers mathematically equivalent to the physical problem.
This approach leads to the complex Fourier transform, a more sophisticated version of the real Fourier transform discussed in Chapter 8.
The complex Fourier transform is important in itself, but also as a stepping stone to more powerful complex techniques, such as the Laplace and z-transforms.
These complex transforms are the foundation of theoretical DSP.
The Real DFT All four members of the Fourier transform family (DFT, DTFT, Fourier Transform & Fourier Series) can be carried out with either real numbers or complex numbers.
Since DSP is mainly concerned with the DFT, we will use it as an example.
Before jumping into the complex math, let's review the real DFT with a special emphasis on things that are awkward with the mathematics.
In Chapter 8 we defined the real version of the Discrete Fourier Transform according to the equations: EQUATION 31- The real DFT.
This is the forward transform, calculating the frequency domain from the time domain.
In spite of using the names: real part and imaginary part, these equations only involve ordinary numbers.
The frequency index, k, runs from 0 to N/2.
These are the same equations given in Eq. 8-4, except that the 2/N term has been included in the forward transform.
In words, an N sample time domain signal, x [n], is decomposed into a set of N/2 % 1 cosine waves, and N/2 % 1 sine waves, with frequencies given by the index, k.
The amplitudes of the cosine waves are contained in Re X[k ], while the amplitudes of the sine waves are contained in Im X [k] .
These equations operate by correlating the respective cosine or sine wave with the time domain signal.
In spite of using the names: real part and imaginary part, there are no complex numbers in these equations.
There isn't a j anywhere in sight!
We have also included the normalization factor, 2/N in these equations.
Remember, this can be placed in front of either the synthesis or analysis equation, or be handled as a separate step (as described by Eq. 8-3).
These equations should be very familiar from previous chapters.
If they aren't, go back and brush up on these concepts before continuing.
If you don't understand the real DFT, you will never be able to understand the complex DFT.
Even though the real DFT uses only real numbers, substitution allows the frequency domain to be represented using complex numbers.
As suggested by the names of the arrays, Re X[k ] becomes the real part of the complex frequency spectrum, and Im X [k] becomes the imaginary part.
In other words, we place a j with each value in the imaginary part, and add the result to the real part.
However, do not make the mistake of thinking that this is the "complex DFT."
This is nothing more than the real DFT with complex substitution.
While the real DFT is adequate for many applications in science and engineering, it is mathematically awkward in three respects.
First, it can only take advantage of complex numbers through the use of substitution.
This makes mathematicians uncomfortable; they want to say: "this equals that," not simply: "this represents that."
For instance, imagine we are given the mathematical statement: A equals B. We immediately know countless consequences: 5A ' 5B, 1% A ' 1% B, A/ x ' B/ x, etc. Now suppose we are given the statement: A represents B. Without additional information, we know absolutely nothing!
When things are equal, we have access to four-thousand years of mathematics.
When things only represent each other, we must start from scratch with new definitions.
For example, when sinusoids are represented by complex numbers, we allow addition and subtraction, but prohibit multiplication and division.
The second thing handled poorly by the real Fourier transform is the negative frequency portion of the spectrum.
As you recall from Chapter 10, sine and cosine waves can be described as having a positive frequency or a negative frequency.
Since the two views are identical, the real Fourier transform ignores the negative frequencies.
However, there are applications where the negative frequencies are important.
This occurs when negative frequency components are forced to move into the positive frequency portion of the spectrum.
The ghosts take human form, so to speak.
For instance, this is what happens in aliasing, circular convolution, and amplitude modulation.
Since the real Fourier transform doesn't use negative frequencies, its ability to deal with these situations is very limited.
Our third complaint is the special handing of ReX [0] and ReX [N/2], the first and last points in the frequency spectrum.
Suppose we start with an N point signal, x [n] .
Taking the DFT provides the frequency spectrum contained in Re X [k] and Im X [k], where k runs from 0 to N/2.
However, these are not the amplitudes needed to reconstruct the time domain waveform; samples Re X [0] and Re X [N/2] must first be divided by two.
This is easily carried out in computer programs, but inconvenient to deal with in equations.
The complex Fourier transform is an elegant solution to these problems.
It is natural for complex numbers and negative frequencies to go hand-in-hand.
Let's see how it works.
Mathematical Equivalence Our first step is to show how sine and cosine waves can be written in an equation with complex numbers.
The key to this is Euler's relation, presented in the last chapter: EQUATION 31- Euler's relation.
Nevertheless, a little algebra can rearrange the relation into two other forms: EQUATION 31- Euler's relation for sine & cosine.
This result is extremely important, we have developed a way of writing equations between complex numbers and ordinary sinusoids.
Although Eq. 313 is the standard form of the identity, it will be more useful for this discussion if we change a few terms around: EQUATION 31- Sinusoids as complex numbers.
Using complex numbers, cosine and sine waves can be written as the sum of a positive and a negative frequency.
Each expression is the sum of two exponentials: one containing a positive frequency (T), and the other containing a negative frequency (-T).
In other words, when sine and cosine waves are written as complex numbers, the negative portion of the frequency spectrum is automatically included.
The positive and negative frequencies are treated with an equal status; it requires one-half of each to form a complete waveform.
The Complex DFT The forward complex DFT, written in polar form, is given by: EQUATION 31- The forward complex DFT.
Both the time domain, x [n], and the frequency domain, X [k], are arrays of complex numbers, with k and n running from to N-1.
This equation is in polar form, the most common for DSP.
Alternatively, Euler's relation can be used to rewrite the forward transform in rectangular form: EQUATION 31- The forward complex DFT (rectangular form).
To start, compare this equation of the complex Fourier transform with the equation of the real Fourier transform, Eq. 31-1.
At first glance, they appear to be identical, with only small amount of algebra being required to turn Eq.
However, this is very misleading; the differences between these two equations are very subtle and easy to overlook, but tremendously important.
Let's go through the differences in detail.
First, the real Fourier transform converts a real time domain signal, x [n], into two real frequency domain signals, Re X[k ] & Im X[k ] .
By using complex substitution, the frequency domain can be represented by a single complex array, X [k] .
In the complex Fourier transform, both x [n] & X [k] are arrays of complex numbers.
A practical note: Even though the time domain is complex, there is nothing that requires us to use the imaginary part.
Suppose we want to process a real signal, such as a series of voltage measurements taken over time.
This group of data becomes the real part of the time domain signal, while the imaginary part is composed of zeros.
Second, the real Fourier transform only deals with positive frequencies.
That is, the frequency domain index, k, only runs from 0 to N/2.
In comparison, the complex Fourier transform includes both positive and negative frequencies.
This means k runs from 0 to N-1.
The frequencies between 0 and N/2 are positive, while the frequencies between N/2 and N- are negative.
Remember, the frequency spectrum of a discrete signal is periodic, making the negative frequencies between N/2 and N-1 the same as between -N/2 and 0. The samples at 0 and N/2 straddle the line between positive and negative.
If you need to refresh your memory on this, look back at Chapters 10 and 12. Third, in the real Fourier transform with substitution, a j was added to the sine wave terms, allowing the frequency spectrum to be represented by complex numbers.
To convert back to ordinary sine and cosine waves, we can simply drop the j.
This is the sloppiness that comes when one thing only represents another thing.
In comparison, the complex DFT, Eq. 31-5, is a formal mathematical equation with j being an integral part.
In this view, we cannot arbitrary add or remove a j any more than we can add or remove any other variable in the equation.
Fourth, the real Fourier transform has a scaling factor of two in front, while the complex Fourier transform does not.
Say we take the real DFT of a cosine wave with an amplitude of one.
The spectral value corresponding to the cosine wave is also one.
Now, let's repeat the process using the complex DFT.
In this case, the cosine wave corresponds to two spectral values, a positive and a negative frequency.
Both these frequencies have a value of ½.
In other words, a positive frequency with an amplitude of ½, combines with a negative frequency with an amplitude of ½, producing a cosine wave with an amplitude of one.
Fifth, the real Fourier transform requires special handling of two frequency domain samples: Re X [0] & Re X [N/2], but the complex Fourier transform does not.
Suppose we start with a time domain signal, and take the DFT to find the frequency domain signal.
To reverse the process, we take the Inverse DFT of the frequency domain signal, reconstructing the original time domain signal.
However, there is scaling required to make the reconstructed signal be identical to the original signal.
For the complex Fourier transform, a factor of 1/N must be introduced somewhere along the way.
This can be tacked-on to the forward transform, the inverse transform, or kept as a separate step between the two.
For the real Fourier transform, an additional factor of two is required (2/N), as described above.
However, the real Fourier transform also requires an additional scaling step: Re X [0] and Re X [N/2] must be divided by two somewhere along the way.
Put in other words, a scaling factor of 1/N is used with these two samples, while 2/N is used for the remainder of the spectrum.
As previously stated, this awkward step is one of our complaints about the real Fourier transform.
Why are the real and complex DFTs different in how these two points are handled?
To answer this, remember that a cosine (or sine) wave in the time domain becomes split between a positive and a negative frequency in the complex DFT's spectrum.
However, there are two exceptions to this, the spectral values at 0 and N/2.
These correspond to zero frequency (DC) and the Nyquist frequency (one-half the sampling rate).
Since these points straddle the positive and negative portions of the spectrum, they do not have a matching point.
Because they are not combined with another value, they inherently have only one-half the contribution to the time domain as the other frequencies.
Complex frequency spectrum.
These curves correspond to an entirely real time domain signal, because the real part of the spectrum has an even symmetry, and the imaginary part has an odd symmetry.
The two square markers in the real part correspond to a cosine wave with an amplitude of one, and a frequency of 0.23.
The two round markers in the imaginary part correspond to a sine wave with an amplitude of one, and a frequency of Frequency Figure 31-1 illustrates the complex DFT's frequency spectrum.
This figure assumes the time domain is entirely real, that is, its imaginary part is zero.
We will discuss the idea of imaginary time domain signals shortly.
There are two common ways of displaying a complex frequency spectrum.
As shown here, zero frequency can be placed in the center, with positive frequencies to the right and negative frequencies to the left.
This is the best way to think about the complete spectrum, and is the only way that an aperiodic spectrum can be displayed.
The problem is that the spectrum of a discrete signal is periodic (such as with the DFT and the DTFT).
This means that everything between -0.5 and 0. repeats itself an infinite number of times to the left and to the right.
In this case, the spectrum between 0 and 1.0 contains the same information as from 0.5 to 0.5.
When graphs are made, such as Fig. 31-1, the -0.5 to 0. convention is usually used.
However, many equations and programs use the to 1.0 form.
For instance, in Eqs.
However, we could write it to run from -N/2 to N/2-1 (coinciding with -0.5 to 0.5), if we desired.
Using the spectrum in Fig. 31-1 as a guide, we can examine how the inverse complex DFT reconstructs the time domain signal.
The inverse complex DFT, written in polar form, is given by: EQUATION 31- The inverse complex DFT.
This is matching equation to the forward complex DFT in Eq. 31-5.
This is Eq.
In words, each value in the real part of the frequency domain contributes a real cosine wave and an imaginary sine wave to the time domain.
Likewise, each value in the imaginary part of the frequency domain contributes a real sine wave and an imaginary cosine wave.
The time domain is found by adding all these real and imaginary sinusoids.
The important concept is that each value in the frequency domain produces both a real sinusoid and an imaginary sinusoid in the time domain.
For example, imagine we want to reconstruct a unity amplitude cosine wave at a frequency of 2Bk/N .
This requires a positive frequency and a negative frequency, both from the real part of the frequency spectrum.
The two square markers in Fig. 31-1 are an example of this, with the frequency set at: k /N ' 0.23 .
The positive frequency at 0.23 (labeled 1 in Fig. 31-1) contributes a cosine wave and an imaginary sine wave to the time domain: ½ cos (2B 0.23 n) % ½ j sin(2B 0.23n ) Likewise, the negative frequency at -0.23 (labeled 2 in Fig. 31-1) also contributes a cosine and an imaginary sine wave to the time domain: ½ cos (2B (& 0.23) n) % ½ j sin(2B (& 0.23) n ) The negative sign within the cosine and sine terms can be eliminated by the relations: cos(& x) ' cos(x) and sin(& x) ' & sin(x) .
This allows the negative frequency's contribution to be rewritten: ½ cos (2B 0.23 n) & ½ j sin(2B 0.23n ) Adding the contributions from the positive and the negative frequencies reconstructs the time domain signal: contribution from positive frequency !
In this case, we need a positive and negative frequency from the imaginary part of the frequency spectrum.
This is shown by the round markers in Fig. 31-1.
From Eq. 31-8, these spectral values contribute a sine wave and an imaginary cosine wave to the time domain.
The imaginary cosine waves cancel, while the real sine waves add: contribution from positive frequency !
This sign inversion is an inherent part of the mathematics of the complex DFT.
As you recall, this same sign inversion is commonly used in the real DFT.
That is, a positive value in the imaginary part of the frequency spectrum corresponds to a negative sine wave.
Most authors include this sign inversion in the definition of the real Fourier transform to make it consistent with its complex counterpart.
The point is, this sign inversion must be used in the complex Fourier transform, but is merely an option in the real Fourier transform.
The symmetry of the complex Fourier transform is very important.
As illustrated in Fig. 31-1, a real time domain signal corresponds to a frequency spectrum with an even real part, and an odd imaginary part.
In other words, the negative and positive frequencies have the same sign in the real part (such as points 1 and 2 in Fig. 31-1), but opposite signs in the imaginary part (points 3 and 4).
This brings up another topic: the imaginary part of the time domain.
Until now we have assumed that the time domain is completely real, that is, the imaginary part is zero.
However, the complex Fourier transform does not require this.
What is the physical meaning of an imaginary time domain signal?
Usually, there is none.
This is just something allowed by the complex mathematics, without a correspondence to the world we live in.
However, there are applications where it can be used or manipulated for a mathematical purpose.
An example of this is presented in Chapter 12.
The imaginary part of the time domain produces a frequency spectrum with an odd real part, and an even imaginary part.
This is just the opposite of the spectrum produced by the real part of the time domain (Fig. 31-1).
When the time domain contains both a real part and an imaginary part, the frequency spectrum is the sum of the two spectra, had they been calculated individually.
Chapter 12 describes how this can be used to make the FFT algorithm calculate the frequency spectra of two real signals at once.
One signal is placed in the real part of the time domain, while the other is place in the imaginary part.
After the FFT calculation, the spectra of the two signals are separated by an even/odd decomposition.
The Family of Fourier Transforms Just as the DFT has a real and complex version, so do the other members of the Fourier transform family.
This produces the zoo of equations shown in Table 31-1.
Rather than studying these equations individually, try to understand them as a well organized and symmetrical group.
The following comments describe the organization of the Fourier transform family.
It is detailed, repetitive, and boring.
Nevertheless, this is the background needed to understand theoretical DSP.
Study it well.
Four Fourier Transforms A time domain signal can be either continuous or discrete, and it can be either periodic or aperiodic.
This defines four types of Fourier transforms: the Discrete Fourier Transform (discrete, periodic), the Discrete Time Fourier Transform (discrete, aperiodic), the Fourier Series (continuous, periodic), and the Fourier Transform (continuous, aperiodic).
Don't try to understand the reasoning behind these names, there isn't any.
If a signal is discrete in one domain, it will be periodic in the other.
Likewise, if a signal is continuous in one domain, will be aperiodic in the other.
Continuous signals are represented by parenthesis, ( ), while discrete signals are represented by brackets, [ ].
There is no notation to indicate if a signal is periodic or aperiodic.
The complex versions have a complex time domain signal and a complex frequency domain signal.
The real versions have a real time domain signal and two real frequency domain signals.
Both positive and negative frequencies are used in the complex cases, while only positive frequencies are used for the real transforms.
The complex transforms are usually written in an exponential form; however, Euler's relation can be used to change them into a cosine and sine form if needed.
The analysis equations describe how to calculate each value in the frequency domain based on all of the values in the time domain.
The synthesis equations describe how to calculate each value in the time domain based on all of the values in the frequency domain.
For the complex transforms, these signals are complex.
For the real transforms, these signals are real.
All of the time domain signals extend from minus infinity to positive infinity.
However, if the time domain is periodic, we are only concerned with a single cycle, because the rest is redundant.
The variables, T and N, denote the periods of continuous and discrete signals in the time domain, respectively.
Discrete frequency domain signals are called X[ k] if they are complex, and Re X [k ] & Im X [k ] if they are real.
The complex transforms have negative frequencies that extend from minus infinity to zero, and positive frequencies that extend from zero to positive infinity.
The real transforms only use positive frequencies.
If the frequency domain is periodic, we are only concerned with a single cycle, because the rest is redundant.
For continuous frequency domains, the independent variable, T, makes one complete period from -B to B. In the discrete case, we use the period where k runs from 0 to N- 6.
The Analysis Equations The analysis equations operate by correlation, i.e., multiplying the time domain signal by a sinusoid and integrating (continuous time domain) or summing (discrete time domain) over the appropriate time domain section.
If the time domain signal is aperiodic, the appropriate section is from minus infinity to positive infinity.
If the time domain signal is periodic, the appropriate section is over any one complete period.
The equations shown here are written with the integration (or summation) over the period: 0 to T (or 0 to N-1).
However, any other complete period would give identical results, i.e., -T to 0, -T/2 to T/2, etc. 7. The Synthesis Equations The synthesis equations describe how an individual value in the time domain is calculated from all the points in the frequency domain.
This is done by multiplying the frequency domain by a sinusoid, and integrating (continuous frequency domain) or summing (discrete frequency domain) over the appropriate frequency domain section.
If the frequency domain is complex and aperiodic, the appropriate section is negative infinity to positive infinity.
If the frequency domain is complex and periodic, the appropriate section is over one complete cycle, i.e., -B to B (continuous frequency domain), or 0 to N- (discrete frequency domain).
If the frequency domain is real and aperiodic, the appropriate section is zero to positive infinity, that is, only the positive frequencies.
Lastly, if the frequency domain is real and periodic, the appropriate section is over the one-half cycle containing the positive frequencies, either 0 to B (continuous frequency domain) or 0 to N/2 (discrete frequency domain).
In Table 31-1, we have placed the scaling factors with the analysis equations.
In the complex case, these scaling factors are: 1/N, 1/T, or 1/2B.
Since the real transforms do not use negative frequencies, the scaling factors are twice as large: 2/N, 2/T, or 1/B.
The real transforms also include a negative sign in the calculation of the imaginary part of the frequency spectrum (an option used to make the real transforms more consistent with the complex transforms).
Lastly, the synthesis equations for the real DFT and the real Fourier Series have special scaling instructions involving Re X (0 ) and Re X [N /2 ] .
Here are a few variations to watch out for: Using f instead of T by the relation: T ' 2Bf Integrating over other periods, such as: -T to 0, -T/2 to T/2, or 0 to T Moving all or part of the scaling factor to the synthesis equation Replacing the period with the fundamental frequency, f0 ' 1/T Using other variable names, for example, T can become S in the DTFT, and Re X [k ] & Im X [k ] can become ak & bk in the Fourier Series Why the Complex Fourier Transform is Used It is painfully obvious from this chapter that the complex DFT is much more complicated than the real DFT.
Are the benefits of the complex DFT really worth the effort to learn the intricate mathematics?
The answer to this question depends on who you are, and what you plan on using DSP for.
A basic premise of this book is that most practical DSP techniques can be understood and used without resorting to complex transforms.
If you are learning DSP to assist in your non-DSP research or engineering, the complex DFT is probably overkill.
Nevertheless, complex mathematics is the primary language of those that specialize in DSP.
If you do not understand this language, you cannot communicate with professionals in the field.
This includes the ability to understand the DSP literature: books, papers, technical articles, etc.
Why are complex techniques so popular with the professional DSP crowd?
Discrete Fourier Transform (DFT) complex transform real transform analysis Time domain: x[n] is complex, discrete and periodic n runs over one period, from 0 to N- Time domain: x[n] is real, discrete and periodic n runs over one period, from 0 to N- Frequency domain: X[k] is complex, discrete and periodic k runs over one period, from 0 to N- k = 0 to N/2 are positive frequencies k = N/2 to N-1 are negative frequencies Frequency domain: Re X[k] is real, discrete and periodic Im X[k] is real, discrete and periodic k runs over one-half period, from 0 to N/ Note: Before using the synthesis equation, the values for Re X[0] and Re X[N/2] must be divided by two.
Discrete Time Fourier Transform (DTFT) x(t) is complex, continuous and periodic t runs over one period, from 0 to T Time domain: x(t) is real, continuous, and periodic t runs over one period, from 0 to T Frequency domain: X[k] is complex, discrete, and aperiodic k runs from negative to positive infinity k > 0 are positive frequencies k < 0 are negative frequencies Frequency domain: Re X[k] is real, discrete and aperiodic Im X[k] is real, discrete and aperiodic k runs from zero to positive infinity Note: Before using the synthesis equation, the value for Re X[0] must be divided by two.
Fourier Transform complex transform real transform synthesis analysis synthesis There is also a more philosophical reason we have not discussed, something called truth.
We started this chapter by listing several ways that the real Fourier transform is awkward.
When the complex Fourier transform was introduced, the problems vanished.
Wonderful, we said, the complex Fourier transform has solved the difficulties.
While this is true, it does not give the complex Fourier transform its proper due.
Look at this situation this way.
In spite of its abstract nature, the complex Fourier transform properly describes how physical systems behave.
When we restrict the mathematics to be real numbers, problems arise.
In other words, these problems are not solved by the complex Fourier transform, they are introduced by the real Fourier transform.
In the world of mathematics, the complex Fourier transform is a greater truth than the real Fourier transform.
This holds great appeal to mathematicians and academicians, a group that strives to expand human knowledge, rather than simply solving a particular problem at hand.
The Laplace Transform The two main techniques in signal processing, convolution and Fourier analysis, teach that a linear system can be completely understood from its impulse or frequency response.
This is a very generalized approach, since the impulse and frequency responses can be of nearly any shape or form.
In fact, it is too general for many applications in science and engineering.
Many of the parameters in our universe interact through differential equations.
For example, the voltage across an inductor is proportional to the derivative of the current through the device.
Likewise, the force applied to a mass is proportional to the derivative of its velocity.
Physics is filled with these kinds of relations.
The frequency and impulse responses of these systems cannot be arbitrary, but must be consistent with the solution of these differential equations.
This means that their impulse responses can only consist of exponentials and sinusoids.
The Laplace transform is a technique for analyzing these special systems when the signals are continuous.
The ztransform is a similar technique used in the discrete case.
The Nature of the s-Domain The Laplace transform is a well established mathematical technique for solving differential equations.
It is named in honor of the great French mathematician, Pierre Simon De Laplace (1749-1827).
Like all transforms, the Laplace transform changes one signal into another according to some fixed set of rules or equations.
As illustrated in Fig. 32-1, the Laplace transform changes a signal in the time domain into a signal in the s-domain, also called the splane.
The time domain signal is continuous, extends to both positive and negative infinity, and may be either periodic or aperiodic.
The Laplace transform allows the time domain to be complex; however, this is seldom needed in signal processing.
In this discussion, and nearly all practical applications, the time domain signal is completely real.
As shown in Fig. 32-1, the s-domain is a complex plane, i.e., there are real numbers along the horizontal axis and imaginary numbers along the vertical axis.
The distance along the real axis is expressed by the variable, F, a lower case Greek sigma.
Likewise, the imaginary axis uses the variable, T, the natural frequency.
This coordinate system allows the location of any point to be specified by providing values for F and T. Using complex notation, each location is represented by the complex variable, s, where: s ' F% jT.
Just as with the Fourier transform, signals in the s-domain are represented by capital letters.
For example, a time domain signal, x (t), is transformed into an sdomain signal, X (s), or alternatively, X (F,T) .
The s-plane is continuous, and extends to infinity in all four directions.
In addition to having a location defined by a complex number, each point in the s-domain has a value that is a complex number.
In other words, each location in the s-plane has a real part and an imaginary part.
As with all complex numbers, the real & imaginary parts can alternatively be expressed as the magnitude & phase.
Just as the Fourier transform analyzes signals in terms of sinusoids, the Laplace transform analyzes signals in terms of sinusoids and exponentials.
From a mathematical standpoint, this makes the Fourier transform a subset of the more elaborate Laplace transform.
Figure 32-1 shows a graphical description of how the s-domain is related to the time domain.
To find the values along a vertical line in the s-plane (the values at a particular F), the time domain signal is first multiplied by the exponential curve: e & F t .
The left half of the s-plane multiplies the time domain with exponentials that increase with time ( F < 0 ), while in the right half the exponentials decrease with time ( F > 0 ).
Next, take the complex Fourier transform of the exponentially weighted signal.
The resulting spectrum is placed along a vertical line in the s-plane, with the top half of the s-plane containing the positive frequencies and the bottom half containing the negative frequencies.
Take special note that the values on the y-axis of the s-plane ( F ' 0 ) are exactly equal to the Fourier transform of the time domain signal.
As discussed in the last chapter, the complex Fourier Transform is given by: x(t ) e & jT t dt This can be expanded into the Laplace transform by first multiplying the time domain signal by the exponential term: [ x (t ) e & F t ] e & jTt d t While this is not the simplest form of the Laplace transform, it is probably the best description of the strategy and operation of the technique.
To Amplitude Start with the time domain signal called x(t) Multiply the time domain signal by an infinite number of exponential curves, each with a different decay constant, F. That is, calculate the signal: x(t) e & Ft for each value of F from negative to positive infinity.
Take the complex Fourier Transform of each exponentially weighted time domain signal.
That is, calculate: [ x (t ) e & Ft ] e & jTt d t for each value of F from negative to positive infinity.
Arrange each spectrum along a vertical line in the s-plane.
The positive frequencies are in the upper half of the s-plane while the negative frequencies are in the lower half.
Imaginary axis ( jT) Positive Frequencies Negative Frequencies -5 -4 - Real axis (F) Increasing Exponentials spectrum for F = Decreasing Exponentials FIGURE 32- The Laplace transform.
The Laplace transform converts a signal in the time domain, x(t), into a signal in the s-domain, X (s) or X(F, T) .
The values along each vertical line in the s-domain can be found by multiplying the time domain signal by an exponential curve with a decay constant F, and taking the complex Fourier transform.
When the time domain is entirely real, the upper half of the s-plane is a mirror image of the lower half.
This allows the equation to be reduced to an even more compact expression: EQUATION 32- The Laplace transform.
This equation defines how a time domain signal, x (t), is related to an s-domain signal, X (s) .
The sdomain variables, s, and X( ), are complex.
While the time domain may be complex, it is usually real.
X (s) ' x(t ) e &s t dt This is the final form of the Laplace transform, one of the most important equations in signal processing and electronics.
Pay special attention to the term: e & st, called a complex exponential.
As shown by the above derivation, complex exponentials are a compact way of representing both sinusoids and exponentials in a single expression.
Although we have explained the Laplace transform as a two stage process (multiplication by an exponential curve followed by the Fourier transform), keep in mind that this is only a teaching aid, a way of breaking Eq. 32-1 into simpler components.
The Laplace transform is a single equation relating x (t) and X (s), not a step-by-step procedure.
Equation 32-1 describes how to calculate each point in the s-plane (identified by its values for F and T) based on the values of F, T, and the time domain signal, x (t) .
Using the Fourier transform to simultaneously calculate all the points along a vertical line is merely a convenience, not a requirement.
However, it is very important to remember that the values in the s-plane along the y-axis ( F ' 0 ) are exactly equal to the Fourier transform.
As explained later in this chapter, this is a key part of why the Laplace transform is useful.
To explore the nature of Eq. 32-1 further, let's look at several individual points in the s-domain and examine how the values at these locations are related to the time domain signal.
To start, recall how individual points in the frequency domain are related to the time domain signal.
Each point in the frequency domain, identified by a specific value of T, corresponds to two sinusoids, cos (Tt ) and sin (Tt ) .
The real part is found by multiplying the time domain signal by the cosine wave, and then integrating from -4 to 4. The imaginary part is found in the same way, except the sine wave is used.
If we are dealing with the complex Fourier transform, the values at the corresponding negative frequency, - T, will be the complex conjugate (same real part, negative imaginary part) of the values at T. The Laplace transform is just an extension of these same concepts.
Real value (F) Amplitude FIGURE 32- Waveforms associated with the s-domain.
Each location in the s-domain is identified by two parameters: F and T.
These parameters also define two waveforms associated with each location.
If we only consider pairs of points (such as: A&AN, B&BN, and C&CN ), the two waveforms associated with each location are sine and cosine waves of frequency T, with an exponentially changing amplitude controlled by F. Figure 32-2 shows three pairs of points in the s-plane: A&AN, B&B N, and C&CN .
Just as in the complex frequency spectrum, the points at A, B, & C (the positive frequencies) are the complex conjugates of the points at AN, BN, & CN (the negative frequencies).
The top half of the s-plane is a mirror image of the lower half, and both halves are needed to correspond with a real time domain signal.
In other words, treating these points in pairs bypasses the complex math, allowing us to operate in the time domain with only real numbers.
Since each of these pairs has specific values for F and ±T, there are two waveforms associated with each pair: cos (Tt ) e & Ft and sin (Tt ) e & Ft .
For instance, points A&AN are at a location of F '1.5 and T ' ±40, and therefore correspond to the waveforms: cos (40 t ) e & 1.5t and sin (40 t ) e & 1.5t .
As shown in Fig. 32-2, these are sinusoids that exponentially decreases in amplitude as time progresses.
In this same way, the sine and cosine waves associated with B&BN have a constant amplitude, resulting from the value of F being zero.
Likewise, the sine and cosine waves that are associated with locations C&C N exponentially increases in amplitude, since F is negative.
The value at each location in the s-plane consists of a real part and an imaginary part.
The real part is found by multiplying the time domain signal by the exponentially weighted cosine wave and then integrated from -4 to 4. The imaginary part is found in the same way, except the exponentially weighted sine wave is used instead.
It looks like this in equation form, using the real part of A&AN as an example: Re X (F'1.5, T'±40 ) ' x (t ) cos (40t ) e &1.5 t d t Figure 32-3 shows an example of a time domain waveform, its frequency spectrum, and its s-domain representation.
The example time domain signal is a rectangular pulse of width two and height one.
As shown, the complex Fourier transform of this signal is a sinc function in the real part, and an entirely zero signal in the imaginary part.
The s-domain is an undulating twodimensional signal, displayed here as topographical surfaces of the real and imaginary parts.
The mathematics works like this: In words, we start with the definition of the Laplace transform (Eq.
Evaluating this integral provides the s-domain signal, expressed in terms of the complex location, s, and the complex value, X (s) : Frequency Amplitude Amplitude Real axis (F) Imaginary axis (jT) The topographical surfaces in Fig. 32-3 are graphs of these equations.
These equations are quite long and the mathematics to derive them is very tedious.
This brings up a practical issue: with algebra of this complexity, how do we know that we haven't made an error in the calculations?
One check is to verify that these equations reduce to the Fourier transform along the y-axis.
This is done by setting F to zero in the equations, and simplifying: Re X (F, T) / As illustrated in Fig. 32-3, these are the correct frequency domain signals, the same as found by directly taking the Fourier transform of the time domain waveform.
Strategy of the Laplace Transform An analogy will help in explaining how the Laplace transform is used in signal processing.
Imagine you are traveling by train at night between two cities.
Your map indicates that the path is very straight, but the night is so dark you cannot see any of the surrounding countryside.
With nothing better to do, you notice an altimeter on the wall of the passenger car and decide to keep track of the elevation changes along the route.
Being bored after a few hours, you strike up a conversation with the conductor: "Interesting terrain," you say.
Ignoring the conductor's obvious disinterest, you continue: "Near the start of our journey, we passed through some sort of abrupt rise, followed by an equally abrupt descent.
Later we encountered a shallow depression."
Thinking you might be dangerous or demented, the conductor decides to respond: "Yes, I guess that is true.
Our destination is located at the base of a large mountain range, accounting for the general increase in elevation.
However, along the way we pass on the outskirts of a large mountain and through the center of a valley."
Now, think about how you understand the relationship between elevation and distance along the train route, compared to that of the conductor.
Since you have directly measured the elevation along the way, you can rightly claim that you know everything about the relationship.
In comparison, the conductor knows this same complete information, but in a simpler and more intuitive form: the location of the hills and valleys that cause the dips and humps along the path.
While your description of the signal might consist of thousands of individual measurements, the conductor's description of the signal will contain only a few parameters.
To show how this is analogous to signal processing, imagine we are trying to understand the characteristics of some electric circuit.
To aid in our investigation, we carefully measure the impulse response and/or the frequency response.
As discussed in previous chapters, the impulse and frequency responses contain complete information about this linear system.
However, this does not mean that you know the information in the simplest way.
In particular, you understand the frequency response as a set of values that change with frequency.
Just as in our train analogy, the frequency response can be more easily understood in terms of the terrain surrounding the frequency response.
That is, by the characteristics of the s-plane.
With the train analogy in mind, look back at Fig. 32-3, and ask: how does the shape of this s-domain aid in understanding the frequency response?
The answer is, it doesn't!
The s-plane in this example makes a nice graph, but it provides no insight into why the frequency domain behaves as it does.
This is because the Laplace transform is designed to analyze a specific class of time domain signals: impulse responses that consist of sinusoids and exponentials.
If the Laplace transform is taken of some other waveform (such as the rectangular pulse in Fig. 32-3), the resulting s-domain is meaningless.
As mentioned in the introduction, systems that belong to this class are extremely common in science and engineering.
This is because sinusoids and exponentials are solutions to differential equations, the mathematics that controls much of our physical world.
For example, all of the following systems are governed by differential equations: electric circuits, wave propagation, linear and rotational motion, electric and magnetic fields, heat flow, etc. Imagine we are trying to understand some linear system that is controlled by differential equations, such as an electric circuit.
Solving the differential equations provides a mathematical way to find the impulse response.
Alternatively, we could measure the impulse response using suitable pulse generators, oscilloscopes, data recorders, etc.
Before we inspect the newly found impulse response, we ask ourselves what we expect to find.
There are several characteristics of the waveform that we know without even looking.
First, the impulse response must be causal.
In other words, the impulse response must have a value of zero until the input becomes nonzero at t ' 0 .
This is the cause and effect that our universe is based upon.
The second thing we know about the impulse response is that it will be composed of sinusoids and exponentials, because these are the solutions to the differential equations that govern the system.
Try as we might, we will never find this type of system having an impulse response that is, for example, a square pulse or triangular waveform.
Third, the impulse response will be infinite in length.
That is, it has nonzero values that extend from t ' 0 to t ' % 4.
This is because sine and cosine waves have a constant amplitude, and exponentials decay toward zero without ever actually reaching it.
If the system we are investigating is stable, the amplitude of the impulse response will become smaller as time increases, reaching a value of zero at t ' % 4.
There is also the possibility that the system is unstable, for example, an amplifier that spontaneously oscillates due to an excessive amount of feedback.
In this case, the impulse response will increase in amplitude as time increases, becoming infinitely large.
Even the smallest disturbance to this system will produce an unbounded output.
The general mathematics of the Laplace transform is very similar to that of the Fourier transform.
In both cases, predetermined waveforms are multiplied by the time domain signal, and the result integrated.
At first glance, it would appear that the strategy of the Laplace transform is the same as the Fourier transform: correlate the time domain signal with a set of basis functions to decompose the waveform.
Not true!
Even though the mathematics is much the same, the rationale behind the two techniques is very different.
The Laplace transform probes the time domain waveform to identify its key features: the frequencies of the sinusoids, and the decay constants of the exponentials.
An example will show how this works.
The center column in Fig. 32-5 shows the impulse response of the RLC notch filter discussed in Chapter 30.
It contains an impulse at t ' 0, followed by an exponentially decaying sinusoid.
As illustrated in (a) through (e), we will probe this impulse response with various exponentially decaying sinusoids.
Each of these probing waveforms is characterized by two parameters: T, that determines the sinusoidal frequency, and F, that determines the decay rate.
In other words, each probing waveform corresponds to a different location in the s-plane, as shown by the s-plane diagram in Fig. 32-4.
The impulse response is probed by multiplying it with these waveforms, and then integrating the result from t ' & 4 to % 4.
This action is shown in the right column.
Our goal is to find combinations of F and T that exactly cancel the impulse response being investigated.
This cancellation can occur in two forms: the area under the curve can be either zero, or just barely infinite.
All other results are uninteresting and can be ignored.
Locations in the s-plane that produce a zero cancellation are called zeros of the system.
Likewise, locations that produce the "just barely infinite" type of cancellation are called poles.
Poles and zeros are analogous to the mountains and valleys in our train story, representing the terrain "around" the frequency response.
To start, consider what happens when the probing waveform decreases in amplitude as time advances, as shown in (a).
This will occur whenever F > 0 (the right half of the s-plane).
Since both the impulse response and the probe becomes smaller with increasing time, the product of the two will also have this same characteristic.
When the product of the two waveforms is integrated from negative to positive infinity, the result will be some number that is not especially interesting.
In particular, a decreasing probe s-plane diagram FIGURE 32- Pole-zero example.
The notch filter has two poles (represented by ×) and two zeros (represented by • ).
This s-plane diagram shows the five locations we will "probe" in this example to analyze this system.
Probing the impulse response.
The Laplace transform can be viewed as probing the system's impulse response with various exponentially decaying sinusoids.
Probing waveforms that produce a cancellation are called poles and zeros.
This illustration shows five probing waveforms (left column) being applied to the impulse response of a notch filter (center column).
The locations in the s-plane that correspond to these five waveforms are shown in Fig. 32-4.
This means that a stable system will not have any poles with F > 0 .
In other words, all of the poles in a stable system are confined to the left half of the s-plane.
In fact, poles in the right half of the s-place show that the system is unstable (i.e., an impulse response that increases with time).
Figure (b) shows one of the special cases we have been looking for.
When this waveform is multiplied by the impulse response, the resulting integral has a value of zero.
This occurs because the area above the x-axis (from the delta function) is exactly equal to the area below (from the rectified sinusoid).
The values for F and T that produce this type of cancellation are called a zero of the system.
As shown in the s-plane diagram of Fig. 32-4, zeros are indicated by small circles (•). Figure (c) shows the next probe we can try.
Here we are using a sinusoid that exponentially increases with time, but at a rate slower than the impulse response is decreasing with time.
This results in the product of the two waveforms also decreasing as time advances.
As in (a), this makes the integral of the product some uninteresting real number.
The important point being that no type of exact cancellation occurs.
Jumping out of order, look at (e), a probing waveform that increases at a faster rate than the impulse response decays.
When multiplied, the resulting signal increases in amplitude as time advances.
This means that the area under the curve becomes larger with increasing time, and the total area from t ' & 4 to % 4 is not defined.
In mathematical jargon, the integral does not converge.
In other words, not all areas of the s-plane have a defined value.
The portion of the s-plane where the integral is defined is called the region-ofconvergence.
In some mathematical techniques it is important to know what portions of the s-plane are within the region-of-convergence.
However, this information is not needed for the applications in this book.
Only the exact cancellations are of interest for this discussion.
In (d), the probing waveform increases at exactly the same rate that the impulse response decreases.
This makes the product of the two waveforms have a constant amplitude.
In other words, this is the dividing line between (c) and (e), resulting in a total area that is just barely undefined (if the mathematicians will forgive this loose description).
In more exact terms, this point is on the borderline of the region of convergence.
As mentioned, values for F and T that produce this type of exact cancellation are called poles of the system.
Poles are indicated in the s-plane by crosses (×).
Analysis of Electric Circuits We have introduced the Laplace transform in graphical terms, describing what the waveforms look like and how they are manipulated.
This is the most intuitive way of understanding the approach, but is very different from how it is actually used.
The Laplace transform is inherently a mathematical technique; it is used by writing and manipulating equations.
The problem is, it is easy to become lost in the abstract nature of the complex algebra and loose all connection to the real world.
Your task is to merge the two views together.
The Laplace transform is the primary method for analyzing electric circuits.
Keep in mind that any system governed by differential equations can be handled the same way; electric circuits are just an example we are using.
The brute force approach is to solve the differential equations controlling the system, providing the system's impulse response.
The impulse response can then be converted into the s-domain via Eq.
Fortunately, there is a better way: transform each of the individual components into the s-domain, and then account for how they interact.
This is very similar to the phasor transform presented in Chapter 30, where resistors, inductors and capacitors are represented by R, jTL, and 1 /jTC, respectively.
In the Laplace transform, resistors, inductors and capacitors become the complex variables: R, s L, and Notice that the phasor transform is a subset of the Laplace transform.
That is, when F is set to zero in s ' F% jT, R becomes R, s L becomes jTL, and 1/sC becomes 1 /jTC .
Just as in Chapter 30, we will treat each of the three components as an individual system, with the current waveform being the input signal, and the voltage waveform being the output signal.
When we say that resistors, inductors and capacitors become R, s L, and 1/sC in the s-domain, this refers to the output divided by the input.
In other words, the Laplace transform of the voltage waveform divided by the Laplace transform of the current waveform is equal to these expressions.
As an example of this, imagine we force the current through an inductor to be a unity amplitude cosine wave with a frequency given by T0.
The resulting voltage waveform across the inductor can be found by solving the differential equation that governs its operation: We find that the s-domain representation of the voltage across the inductor, divided by the s-domain representation of the current through the inductor, is equal to sL.
This is always the case, regardless of the current waveform we start with.
In a similar way, the ratio of s-domain voltage to s-domain current is always equal to R for resistors, and 1/sC for capacitors.
Figure 32-6 shows an example circuit we will analyze with the Laplace transform, the RLC notch filter discussed in Chapter 30.
Since this analysis is the same for all electric circuits, we will outline it in steps.
Step 1. Transform each of the components into the s-domain.
In other words, replace the value of each resistor with R, each inductor with sL, and each capacitor with 1/sC .
This is shown in Fig. 32-6.
Step 2: Find H (s), the output divided by the input.
As described in Chapter 30, this is done by treating each of the components as if they obey Ohm's law, with the "resistances" given by: R, s L, and 1/sC .
This allows us to use the standard equations for resistors in series, resistors in parallel, voltage dividers, etc. Treating the RLC circuit in this example as a voltage divider (just as in Chapter 30), H (s) is found: As you recall from Fourier analysis, the frequency spectrum of the output signal divided by the frequency spectrum of the input signal is equal to the system's frequency response, given the symbol, H (T) .
The above equation is an extension of this into the s-domain.
The signal, H (s), is called the system's transfer function, and is equal to the s-domain representation of the output signal divided by the s-domain representation of the input signal.
Further, H (s) is equal to the Laplace transform of the impulse response, just the same as H (T) is equal to the Fourier transform of the impulse response.
FIGURE 32- Notch filter analysis in the s-domain.
The first step in this procedure is to replace the resistor, inductor & capacitor values with their s-domain equivalents.
So far, this is identical to the techniques of the last chapter, except for using s instead of jT.
The difference between the two methods is what happens from this point on.
This is as far as we can go with jT.
We might graph the frequency response, or examining it in some other way; however, this is a mathematical dead end.
In comparison, the interesting aspects of the Laplace transform have just begun.
Finding H (s) is the key to Laplace analysis; however, it must be expressed in a particular form to be useful.
This requires the algebraic manipulation of the next two steps.
Step 3: Arrange H (s) to be one polynomial over another.
This makes the transfer function written as: EQUATION 32- Transfer function in polynomial form.
For example, the rectangular pulse shown in Fig. 32-3 is not the solution to a differential equation and its Laplace transform cannot be written in this way.
In comparison, any electric circuit composed of resistors, capacitors, and inductors can be written in this form.
For the RLC notch filter used in this example, the algebra shown in step 2 has already placed the transfer function in the correct form, that is: a ' L, b ' 0, c ' 1/C ; and a ' L, b ' R, c ' 1/C Step 4: Factor the numerator and denominator polynomials.
That is, break the numerator and denominator polynomials into components that each contain a single s.
When the components are multiplied together, they must equal the original numerator and denominator.
In other words, the equation is placed into the form: EQUATION 32- The factored s-domain.
This form allows the s-domain to be expressed as poles and zeros.
H (s) ' The roots of the numerator, z1, z2, z3 þ, are the zeros of the equation, while the roots of the denominator, p1, p2, p3 þ, are the poles.
These are the same zeros and poles we encountered earlier in this chapter, and we will discuss how they are used in the next section.
Factoring an s-domain expression is straightforward if the numerator and denominator are second-order polynomials, or less.
In other words, we can easily handle the terms: s and s 2, but not: s 3, s 4, s 5, þ.
This is because the roots of a second-order polynomial, a x 2 % b x % c, can be found by using the quadratic equation: x ' & b ± b 2 & 4 a c / 2a .
With this method, the transfer function of the example notch filter is factored into: As in this example, a second-order system has a maximum of two zeros and two poles.
The number of poles in a system is equal to the number of independent energy storing components.
For instance, inductors and capacitors store energy, while resistors do not.
The number of zeros will be equal to, or less than, the number of poles.
Polynomials greater than second order cannot generally be factored using algebra, requiring more complicated numerical methods.
As an alternative, circuits can be constructed as a cascade of second-order stages.
A good example is the family of analog filters presented in Chapter 3.
For instance, an eight pole filter is designed by cascading four stages of two poles each.
The important point is that this multistage approach is used to overcome limitations in the mathematics, not limitations in the electronics.
These illustrations show the relationship between the pole-zero plot, the s-domain, and the frequency response.
The notch filter component values used in these graphs are: R=220 S, C=470 DF, and L = 54 µH.
These values place the center of the notch at T = 6.277 million, i.e., a frequency of approximately 1 MHz.
The Importance of Poles and Zeros To make this less abstract, we will use actual component values for the notch filter we just analyzed: R ' 220 S, L ' 54 µH, C ' 470 DF .
Plugging these values into the above equations, places the poles and zeros at: These pole and zero locations are shown in Fig. 32-7.
Each zero is represented by a circle, while each pole is represented by a cross.
This is called a pole-zero diagram, and is the most common way that s-domain data are displayed.
Figure 32-7 also shows a topographical display of the splane.
For simplicity, only the magnitude is shown, but don't forget that there is a corresponding phase.
Just as mountains and valleys determine the shape of the surface of the earth, the poles and zeros determine the shape of the s-plane.
Unlike mountains and valleys, every pole and zero is exactly the same shape and size as every other pole and zero.
The only unique characteristic a pole or zero has is its location.
Poles and zeros are important because they provide a concise representation of the value at any point in the s-plane.
That is, we can completely describe the characteristics of the system using only a few parameters.
In the case of the RLC notch filter, we only need to specify four complex parameters to represent the system: z1, z2, p1, p2 (each consisting of a real and an imaginary part).
To better understand poles and zeros, imagine an ant crawling around the splane.
At any particular location the ant happens to be (i.e., some value of s), there is a corresponding value of the transfer function, H (s) .
This value is a complex number that can be expressed as the magnitude & phase, or as the real & imaginary parts.
Now, let the ant carry us to one of the zeros in the splane.
The value we measure for the real and imaginary parts will be zero at this location.
This can be understood by examining the mathematical equation for H (s) in Eq. 32-3.
If the location, s, is equal to any of the zeros, one of the terms in the numerator will be zero.
This makes the entire expression equal to zero, regardless of the other values.
Next, our ant journey takes us to one of the poles, where we again measure the value of the real and imaginary parts of H (s) .
The measured value becomes larger and larger as we come close to the exact location of the pole (hence the name).
This can also be understood from Eq. 32-3.
If the location, s, is equal to any of the p's, the denominator will be equal to zero, and the division by zero makes the entire expression infinity large.
Having explored the unique locations, our ant journey now moves randomly throughout the s-plane.
The value of H (s) at each location depends entirely on the positioning of the poles and the zeros, because there are no other types of features allowed in this strange terrain.
If we are near a pole, the value will be large; if we are near a zero, the value will be small.
Equation 32-3 also describes how multiple poles and zeros interact to form the s-domain signal.
Remember, subtracting two complex numbers provides the distance between them in the complex plane.
For example, (s & z1) is the distance between the arbitrary location, s, and the zero located at z1 .
Therefore, Eq. 32-3 specifies that the value at each location, s, is equal to the distance to all of the zeros multiplied, divided by the distance to all of the poles multiplied.
This brings us to the heart of this chapter: how the location of the poles & zeros provides a deeper understanding of the system's frequency response.
The frequency response is equal to the values of H (s) along the imaginary axis, signified by the dark line in the topographical plot of Fig. 32-7.
Imagine our ant starting at the origin and crawling along this path.
Near the origin, the distance to the zeros is approximately equal to the distance to the poles.
This makes the numerator and denominator in Eq. 32- Imaginary value Pole-Zero Diagram Laplace transform Physical System Frequency Response Phasor transform Amplitude Evaluate at F= Real value Frequency FIGURE 32- Strategy for using the Laplace transform.
The phasor transform presented in Chapter 30 (the method using R, jTL, & & j /TC ) allows the frequency response to be directly calculated from the parameters of the physical system.
In comparison, the Laplace transform calculates an s-domain representation from the physical system, usually displayed in the form of a pole-zero diagram.
In turn, the frequency response can be obtained from the s-domain by evaluating the transfer function along the imaginary axis.
While both methods provide the same end result, the intermediate step of the s-domain provides insight into why the frequency response behaves as it does.
The situation doesn't change significantly until the ant moves near the pole and zero location.
When approaching the zero, the value of H (s) drops suddenly, becoming zero when the ant is upon the zero.
As the ant moves past the pole and zero pair, the value of H (s) again returns to unity.
Using this type of visualization, it can be seen that the width of the notch depends on the distance between the pole and zero.
Figure 32-8 summarizes how the Laplace transform is used.
We start with a physical system, such as an electric circuit.
If we desire, the phasor transform can directly provide the frequency response of the system, as described in Chapter 30.
An alternative is to take the Laplace transform using the four step method previously outlined.
This results in a mathematical expression for the transfer function, H (s), which can be represented in a pole-zero diagram.
The frequency response can then be found by evaluating the transfer function along the imaginary axis, that is, by replacing each s with jT.
While both methods provide the same result, the intermediate pole-zero diagram provides an understanding of why the system behaves as it does, and how it can be changed.
Filter Design in the s-Domain The most powerful application of the Laplace transform is the design of systems directly in the s-domain.
This involves two steps: First, the sdomain is designed by specifying the number and location of the poles and zeros.
This is a pure mathematical problem, with the goal of obtaining the best frequency response.
In the second step, an electronic circuit is derived that provides this s-domain representation.
This is something of an art, since there are many circuit configurations that have a given pole-zero diagram.
As previously mentioned, step 4 of the Laplace transform method is very difficult if the system contains more than two poles or two zeros.
A common solution is to implement multiple poles and zeros in successive stages.
For example, a 6 pole filter is implemented as three successive stages, with each stage containing up to two poles and two zeros.
Since each of these stages can be represented in the s-domain by a quadratic numerator divided by a quadratic denominator, this approach is called designing with biquads.
Figure 32-9 shows a common biquad circuit, the one used in the filter design method of Chapter 3.
This is called the Sallen-Key circuit, after R.P. Sallen and E.L. Key, authors of a paper that described this technique in the mid 1950s.
While there are several variations, the most common circuit uses two resistors of equal value, two capacitors of equal value, and an amplifier with an amplification of between 1 and 3.
Although not available to Sallen and Key, the amplifiers can now be made with low-cost op amps with appropriate feedback resistors.
Going through the four step circuit analysis procedure, the location of this circuit's two poles can be related to the component values: EQUATION 32- Sallen-Key pole locations.
These equations relate the pole position, T and F, to the amplifier gain, A, the resistor, R, and capacitor, C.
These equations show that the two poles always lie somewhere on a circle of radius: 1/RC .
The exact position along the circle depends on the gain of the amplifier.
As shown in (a), an amplification of 1 places both of the poles on the real axis.
The frequency response of this configuration is a low-pass filter with a relatively smooth transition between the passband and stopband.
The -3dB (0.707) cutoff frequency of this circuit, denoted by T0, is where the circle intersects the imaginary axis, i.e., T0 ' 1/RC .
Sallen-Key characteristics.
This circuit produces two poles on a circle of radius 1/RC.
As the gain of the amplifier is increased, the poles move from the real axis, as in (a), toward the imaginary axis, as in (d).
Frequency As the amplification is increased, the poles move along the circle, with a corresponding change in the frequency response.
As shown in (b), an amplification of 1.586 places the poles at 45 degree angles, resulting in the frequency response having a sharper transition.
Increasing the amplification further moves the poles even closer to the imaginary axis, resulting in the frequency response showing a peaked curve.
This condition is illustrated in (c), where the amplification is set at 2.5.
The amplitude of the peak continues to grow as the amplification is increased, until a gain of 3 is reached.
As shown in (d), this is a special case that places the poles directly on the imaginary axis.
The corresponding frequency response now has an infinity large value at the peak.
In practical terms, this means the circuit has turned into an oscillator.
Increasing the gain further pushes the poles deeper into the right half of the splane.
As mentioned before, this correspond to the system being unstable (spontaneous oscillation).
Using the Sallen-Key circuit as a building block, a wide variety of filter types can be constructed.
For example, a low-pass Butterworth filter is designed by placing a selected number of poles evenly around the left-half of the circle, as shown in Fig. 32-10.
Each two poles in this configuration requires one Sallen-Key stage.
As described in Chapter 3, the Butterworth filter is maximally flat, that is, it has the sharpest transition between the passband and stopband without peaking in the frequency response.
The more poles used, the faster the transition.
Since all the poles in the Butterworth filter lie on the same circle, all the cascaded stages use the same values for R and C. The only thing different between the stages is the amplification.
Why does this circular pattern of poles provide the optimally flat response?
Don't look for an obvious or intuitive answer to this question; it just falls out of the mathematics.
Figure 32-11 shows how the pole positions of the Butterworth filter can be modified to produce the Chebyshev filter.
As discussed in Chapter 3, the Chebyshev filter achieves a sharper transition than the Butterworth at the expense of ripple being allowed into the passband.
In the s-domain, this corresponds to the circle of poles being flattened into an ellipse.
The more flattened the ellipse, the more ripple in the passband, and the sharper the transition.
When formed from a cascade of Sallen-Key stages, this requires different values of resistors and capacitors in each stage.
Figure 32-11 also shows the next level of sophistication in filter design strategy: the elliptic filter.
The elliptic filter achieves the sharpest possible transition by allowing ripple in both the passband and the stopband.
In the sdomain, this corresponds to placing zeros on the imaginary axis, with the first one near the cutoff frequency.
Elliptic filters come in several varieties and are significantly more difficult to design than Butterworth and Chebyshev configurations.
This is because the poles and zeros of the elliptic filter do not lie in a simple geometric pattern, but in a mathematical arrangement involving elliptic functions and integrals (hence the name). 1 pole 2 pole FIGURE 32- The Butterworth s-plane.
The low-pass Butterworth filter is created by placing poles equally around the left-half of a circle.
The more poles used in the filter, the faster the roll-off.
These are the three classic pole-zero patterns in filter design.
Butterworth filters have poles equally spaced around a circle, resulting in a maximally flat response.
Chebyshev filters have poles placed on an ellipse, providing a sharper transition, but at the cost of ripple in the passband.
Elliptic filters add zeros to the stopband.
This results in a faster transition, but with ripple in the passband and stopband.
Frequency Amplitude Elliptic Frequency Since each biquad produces two poles, even order filters (2 pole, 4 pole, pole, etc.) can be constructed by cascading biquad stages.
However, odd order filters (1 pole, 3 pole, 5 pole, etc.) require something that the biquad just cannot provide: a single pole on the real axis.
This turns out to be nothing more than a simple RC circuit added to the cascade.
For example, a 9 pole filter can be constructed from 5 stages: 4 Sallen-Key biquads, plus one stage consisting of a single capacitor and resistor.
These classic pole-zero patterns are for low-pass filters; however, they can be modified for other frequency responses.
This is done by designing a low-pass filter, and then performing a mathematical transformation in the s-domain.
We start by calculating the low-pass filter pole locations, and then writing the transfer function, H (s), in the form of Eq. 32-3.
The transfer function of the corresponding high-pass filter is found by replacing each "s" with "1/s", and then rearranging the expression to again be in the pole-zero form of Eq. 32-3.
This defines new pole and zero locations that implement the high-pass filter.
More complicated s-domain transforms can create band-pass and band-reject filters from an initial low-pass design.
This type of mathematical manipulation in the s-domain is the central theme of filter design, and entire books are devoted to the subject.
Analog filter design is 90% mathematics, and only 10% electronics.
Fortunately, the design of high-pass filters using Sallen-Key stages doesn't require this mathematical manipulation.
The "1/s" for "s" replacement in the s-domain corresponds to swapping the resistors and capacitors in the circuit.
In the s-plane, this swap places the poles at a new position, and adds two zeros directly at the origin.
This results in the frequency response having a value of zero at DC (zero frequency), just as you would expect for a high-pass filter.
This brings the Sallen-Key circuit to its full potential: the implementation of two poles and two zeros.
The z-Transform Just as analog filters are designed using the Laplace transform, recursive digital filters are developed with a parallel technique called the z-transform.
The overall strategy of these two transforms is the same: probe the impulse response with sinusoids and exponentials to find the system's poles and zeros.
The Laplace transform deals with differential equations, the s-domain, and the s-plane.
Correspondingly, the z-transform deals with difference equations, the z-domain, and the z-plane.
However, the two techniques are not a mirror image of each other; the s-plane is arranged in a rectangular coordinate system, while the z-plane uses a polar format.
Recursive digital filters are often designed by starting with one of the classic analog filters, such as the Butterworth, Chebyshev, or elliptic.
A series of mathematical conversions are then used to obtain the desired digital filter.
The z-transform provides the framework for this mathematics.
The Chebyshev filter design program presented in Chapter 20 uses this approach, and is discussed in detail in this chapter.
The Nature of the z-Domain To reinforce that the Laplace and z-transforms are parallel techniques, we will start with the Laplace transform and show how it can be changed into the ztransform.
From the last chapter, the Laplace transform is defined by the relationship between the time domain and s-domain signals: where x (t) and X (s) are the time domain and s-domain representation of the signal, respectively.
As discussed in the last chapter, this equation analyzes the time domain signal in terms of sine and cosine waves that have an exponentially changing amplitude.
This can be understood by replacing the complex variable, s, with its equivalent expression, F % jT.
Using this alternate notation, the Laplace transform becomes: If we are only concerned with real time domain signals (the usual case), the top and bottom halves of the s-plane are mirror images of each other, and the term, e & jTt, reduces to simple cosine and sine waves.
This equation identifies each location in the s-plane by the two parameters, F and T .
The value at each location is a complex number, consisting of a real part and an imaginary part.
To find the real part, the time domain signal is multiplied by a cosine wave with a frequency of T, and an amplitude that changes exponentially according to the decay parameter, F .
The value of the real part of X (F,T) is then equal to the integral of the resulting waveform.
The value of the imaginary part of X (F,T) is found in a similar way, except using a sine wave.
If this doesn't sound very familiar, you need to review the previous chapter before continuing.
The Laplace transform can be changed into the z-transform in three steps.
The first step is the most obvious: change from continuous to discrete signals.
This is done by replacing the time variable, t, with the sample number, n, and changing the integral into a summation: Notice that X (F,T) uses parentheses, indicating it is continuous, not discrete.
Even though we are now dealing with a discrete time domain signal, x[n], the parameters F and T can still take on a continuous range of values.
The second step is to rewrite the exponential term.
An exponential signal can be mathematically represented in either of two ways: As illustrated in Fig. 33-1, both these equations generate an exponential curve.
The first expression controls the decay of the signal through the parameter, F .
If F is positive, the waveform will decrease in value as the sample number, n, becomes larger.
Likewise, the curve will progressively increase if F is negative.
If F is exactly zero, the signal will have a constant value of one.
Exponential signals.
Exponentials can be represented in two different mathematical forms.
The Laplace transform uses one way, while the z-transform uses the other.
The second expression uses the parameter, r, to control the decay of the waveform.
The waveform will decrease if r > 1, and increase if r < 1.
The signal will have a constant value when r ' 1 .
These two equations are just different ways of expressing the same thing.
One method can be swapped for the other by using the relation: The second step of converting the Laplace transform into the z-transform is completed by using the other exponential form: While this is a perfectly correct expression of the z-transform, it is not in the most compact form for complex notation.
This problem was overcome in the Laplace transform by introducing a new complex variable, s, defined to be: s ' F % jT .
In this same way, we will define a new variable for the ztransform: This is defining the complex variable, z, as the polar notation combination of the two real variables, r and T. The third step in deriving the z-transform is to replace: r and T, with z.
This produces the standard form of the ztransform: EQUATION 33- The z-transform.
The z-transform defines the relationship between the time domain signal, x [n], and the z-domain signal, X (z) .
Why does the z-transform use r n instead of e & Fn, and z instead of s?
As described in Chapter 19, recursive filters are implemented by a set of recursion coefficients.
To analyze these systems in the z-domain, we must be able to convert these recursion coefficients into the z-domain transfer function, and back again.
As we will show shortly, defining the z-transform in this manner ( r n and z) provides the simplest means of moving between these two important representations.
In fact, defining the z-domain in this way makes it trivial to move from one representation to the other.
Figure 33-2 illustrates the difference between the Laplace transform's s-plane, and the z-transform's z-plane.
Locations in the s-plane are identified by two parameters: F, the exponential decay variable along the horizontal axis, and T, the frequency variable along the vertical axis.
In other words, these two real parameters are arranged in a rectangular coordinate system.
This geometry results from defining s, the complex variable representing position in the splane, by the relation: s ' F % jT.
In comparison, the z-domain uses the variables: r and T, arranged in polar coordinates.
The distance from the origin, r, is the value of the exponential decay.
The angular distance measured from the positive horizontal axis, T, is the frequency.
This geometry results from defining z by: z ' re & jT .
In other words, the complex variable representing position in the z-plane is formed by combining the two real parameters in a polar form.
These differences result in vertical lines in the s-plane matching circles in the z-plane.
For example, the s-plane in Fig. 33-2 shows a pole-zero pattern where all of the poles & zeros lie on vertical lines.
The equivalent poles & zeros in the z-plane lie on circles concentric with the origin.
This can be understood by examining the relation presented earlier: F ' & ln(r) .
For instance, the s-plane's vertical axis (i.e., F' 0 ) corresponds to the z-plane's Relationship between the s-plane and the z-plane.
The s-plane is a rectangular coordinate system with F expressing the distance along the real (horizontal) axis, and T the distance along the imaginary (vertical) axis.
In comparison, the z-plane is in polar form, with r being the distance to the origin, and T the angle measured to the positive horizontal axis.
Vertical lines in the s-plane, such as illustrated by the example poles and zeros in this figure, correspond to circles in the z-plane.
Vertical lines in the left half of the s-plane correspond to circles inside the z-plane's unit circle.
Likewise, vertical lines in the right half of the s-plane match with circles on the outside of the z-plane's unit circle.
In other words, the left and right sides of the s-plane correspond to the interior and the exterior of the unit circle, respectively.
For instance, a continuous system is unstable when poles occupy the right half of the s-plane.
In this same way, a discrete system is unstable when poles are outside the unit circle in the z-plane.
When the time domain signal is completely real (the most common case), the upper and lower halves of the z-plane are mirror images of each other, just as with the sdomain.
Pay particular attention to how the frequency variable, T, is used in the two transforms.
A continuous sinusoid can have any frequency between DC and infinity.
This means that the s-plane must allow T to run from negative to positive infinity.
In comparison, a discrete sinusoid can only have a frequency between DC and one-half the sampling rate.
That is, the frequency must be between 0 and 0.5 when expressed as a fraction of the sampling rate, or between 0 and B when expressed as a natural frequency (i.e., T ' 2Bf ).
This matches the geometry of the z-plane when we interpret T to be an angle expressed in radians.
That is, the positive frequencies correspond to angles of 0 to B radians, while the negative frequencies correspond to 0 to - B radians.
Since the z-plane express frequency in a different way than the s-plane, some authors use different symbols to distinguish the two.
A common notation is to use S (an upper case omega) to represent frequency in the z-domain, and T (a lower case omega) for frequency in the s-domain.
In this book we will use T to represent both types of frequency, but look for this in other DSP material.
In the s-plane, the values that lie along the vertical axis are equal to the frequency response of the system.
That is, the Laplace transform, evaluated at F ' 0, is equal to the Fourier transform.
In an analogous manner, the frequency response in the z-domain is found along the unit circle.
This can be seen by evaluating the z-transform (Eq.
This places zero frequency (DC) at a value of one on the horizontal axis in the s-plane.
The spectrum's positive frequencies are positioned in a counter-clockwise pattern from this DC position, occupying the upper semicircle.
Likewise the negative frequencies are arranged from the DC position along the clockwise path, forming the lower semicircle.
The positive and negative frequencies in the spectrum meet at the common point of T ' B and T ' & B .
This circular geometry also corresponds to the frequency spectrum of a discrete signal being periodic.
That is, when the frequency angle is increased beyond B, the same values are encountered as between 0 and B. When you run around in a circle, you see the same scenery over and over.
Analysis of Recursive Systems As outlined in Chapter 19, a recursive filter is described by a difference equation: EQUATION 33- Difference equation.
See Chapter 19 for details. and "b" terms are the recursion coefficients.
An obvious use of this equation is to describe how a programmer would implement the filter.
An equally important aspect is that it represents a mathematical relationship between the input and output that must be continually satisfied.
Just as continuous systems are controlled by differential equations, recursive discrete systems operate in accordance with this difference equation.
From this relationship we can derive the key characteristics of the system: the impulse response, step response, frequency response, pole-zero plot, etc.
We start the analysis by taking the z-transform (Eq.
In other words, we want to see what this controlling relationship looks like in the z-domain.
With a fair amount of algebra, we can separate the relation into: Y[z] / X [z], that is, the z-domain representation of the output signal divided by the z-domain representation of the input signal.
Just as with the Laplace transform, this is called the system's transfer function, and designate it by H [z] .
Here is what we find: EQUATION 33- Transfer function in polynomial form.
The recursion coefficients are directly identifiable in this relation.
This is one of two ways that the transfer function can be written.
This form is important because it directly contains the recursion coefficients.
For example, suppose we know the recursion coefficients of a digital filter, such as might be provided from a design table: Without having to worry about nasty complex algebra, we can directly write down the system's transfer function: Notice that the "b" coefficients enter the transfer function with a negative sign in front of them.
Alternatively, some authors write this equation using additions, but change the sign of all the "b" coefficients.
Here's the problem.
If you are given a set of recursion coefficients (such as from a table or filter design program), there is a 50-50 chance that the "b" coefficients will have the opposite sign from what you expect.
If you don't catch this discrepancy, the filter will be grossly unstable.
Equation 33-3 expresses the transfer function using negative powers of z, such as: z & 1, z & 2, z & 3, etc.
After an actual set of recursion coefficients have been plugged in, we can convert the transfer function into a more conventional form that uses positive powers: i.e., z, z 2, z 3, þ.
By multiplying both the numerator and denominator of our example by z 4, we obtain: Positive powers are often easier to use, and they are required by some zdomain techniques.
Why not just rewrite Eq. 33-3 using positive powers and forget about negative powers entirely?
We can't!
The trick of multiplying the numerator and denominator by the highest power of z (such as z 4 in our example) can only be used if the number of recursion coefficients is already known.
Equation 33-3 is written for an arbitrary number of coefficients.
The point is, both positive and negative powers are routinely used in DSP and you need to know how to convert between the two forms.
The transfer function of a recursive system is useful because it can be manipulated in ways that the recursion coefficients cannot.
This includes such tasks as: combining cascade and parallel stages into a single system, designing filters by specifying the pole and zero locations, converting analog filters into digital, etc.
These operations are carried out by algebra performed in the zdomain, such as: multiplication, addition, and factoring.
After these operations are completed, the transfer function is placed in the form of Eq. 33-3, allowing the new recursion coefficients to be identified.
Just as with the s-domain, an important feature of the z-domain is that the transfer function can be expressed as poles and zeros.
This provides the second general form of the z-domain: EQUATION 33- Transfer function in pole-zero form.
Each of the poles (p1, p2, p3, þ ) and zeros ( z1, z2, z3 þ) is a complex number.
To move from Eq. 33-4 to 33-3, multiply out the expressions and collect like terms.
While this can involve a tremendous amount of algebra, it is straightforward in principle and can easily be written into a computer routine.
Moving from Eq. 33-3 to 33-4 is more difficult because it requires factoring of the polynomials.
As discussed in Chapter 32, the quadratic equation can be used for the factoring if the transfer function is second order or less (i.e., there are no powers of z higher than z 2).
Algebraic methods cannot generally be used to factor systems greater than second order and numerical methods must be employed.
Fortunately, this is seldom needed; digital filter design starts with the pole-zero locations (Eq.
As with all complex numbers, the pole and zero locations can be represented in either polar or rectangular form.
Polar notation has the advantage of being more consistent with the natural organization of the z-plane.
In comparison, rectangular form is generally preferred for mathematical work, that is, it is usually easier to manipulate: F % jT, as compared with: r e jT .
As an example of using these equations, we will design a notch filter by the following steps: (1) specify the pole-zero placement in the z-plane, (2) a. Pole-zero plot Notch filter designed in the z-domain.
The design starts by locating two poles and two zeros in the z-plane, as shown in (a).
The resulting impulse and frequency response are shown in (b) and (c), respectively.
The sharpness of the notch is controlled by the distance of the poles from the zeros.
Sample number Frequency write down the transfer function in the form of Eq. 33-4, (3) rearrange the transfer function into the form of Eq. 33-3, and (4) identify the recursion coefficients needed to implement the filter.
Fig. 33-3 shows the example we will use: a notch filter formed from two poles and two zeros located at In polar form: To understand why this is a notch filter, compare this pole-zero plot with Fig. 32-6, a notch filter in the s-plane.
The only difference is that we are moving along the unit circle to find the frequency response from the z-plane, as opposed to moving along the vertical axis to find the frequency response from the s-plane.
From the polar form of the poles and zeros, it can be seen that the notch will occur at a natural frequency of B/4, corresponding to 0.125 of the sampling rate.
Since the pole and zero locations are known, the transfer function can be written in the form of Eq. 33-4 by simply plugging in the values: Since the transfer function is now in the form of Eq. 33-3, the recursive coefficients can be directly extracted by inspection: This example provides the general strategy for obtaining the recursion coefficients from a pole-zero plot.
In specific cases, it is possible to derive simpler equations directly relating the pole-zero positions to the recursion coefficients.
For example, a system containing two poles and two zeros, called as biquad, has the following relations: Biquad design equations.
These equations give the recursion coefficients, a0, a1, a2, b1, b2, from the position of the poles: rp & Tp, and the zeros: r0 & T0 .
After the transfer function has been specified, how do we find the frequency response?
There are three methods: one is mathematical and two are computational (programming).
The mathematical method is based on finding the values in the z-plane that lie on the unit circle.
This is done by evaluating the transfer function, H (z), at r ' 1 .
Specifically, we start by writing down the transfer function in the form of either Eq.
We then replace each z with e & jT (that is, r e & jT with r ' 1 ).
This provides a mathematical equation of the frequency response, H (T) .
The problem is, the resulting expression is in a very inconvenient form.
A significant amount of algebra is usually required to obtain something recognizable, such as the magnitude and phase.
While this method provides an exact equation for the frequency response, it is difficult to automate in computer programs, such as needed in filter design packages.
The second method for finding the frequency response also uses the approach of evaluating the z-plane on the unit circle.
The difference is that we only calculate samples of the frequency response, not a mathematical solution for the entire curve.
A computer program loops through, perhaps, 1000 equally spaced frequencies between T' 0 and T' B. Think of an ant moving between 1000 discrete points on the upper half of the z-plane's unit circle.
The magnitude and phase of the frequency response are found at each of these location by evaluating the transfer function.
This method works well and is often used in filter design packages.
Its major limitation is that it does not account for round-off noise affecting the system's characteristics.
Even if the frequency response found by this method looks perfect, the implemented system can be completely unstable!
This brings up the third method: find the frequency response from the recursion coefficients that are actually used to implement the filter.
To start, we find the impulse response of the filter by passing an impulse through the system.
In the second step, we take the DFT of the impulse response (using the FFT, of course) to find the system's frequency response.
The only critical item to remember with this procedure is that enough samples must be taken of the impulse response so that the discarded samples are insignificant.
While books could be written on the theoretical criteria for this, the practical rules are much simpler.
Use as many samples as you think are necessary.
After finding the frequency response, go back and repeat the procedure using twice as many samples.
If the two frequency responses are adequately similar, you can be assured that the truncation of the impulse response hasn't fooled you in some way.
Cascade and Parallel Stages Sophisticated recursive filters are usually designed in stages to simplify the tedious algebra of the z-domain.
Figure 33-4 illustrates the two common ways that individual stages can be arranged: cascaded stages and parallel stages with added outputs.
For example, a low-pass and high-pass stage can be cascaded to form a band-pass filter.
Likewise, a parallel combination of low-pass and high-pass stages can form a band-reject filter.
We will call the two stages being combined system 1 and system 2, with their recursion coefficients being called: a0, a1, a2, b1, b2 and A0, A1, A2, B1, B2, respectively.
Our goal is to combine these stages (in cascade or parallel) into a single recursive filter, which we will call system 3, with recursion coefficients given by: a0, a1, a2, a3, a4, b1, b2, b3, b4 .
As you recall from previous chapters, the frequency responses of systems in a cascade are combined by multiplication.
Also, the frequency responses of systems in parallel are combined by addition.
These same rules are followed by the z-domain transfer functions.
This allows recursive systems to be combined by moving the problem into the z-domain, performing the required multiplication or addition, and then returning to the recursion coefficients of the final system.
As an example of this method, we will work out the algebra for combining two biquad stages in a cascade.
The transfer function of each stage is found by writing Eq. 33-3 using the appropriate recursion coefficients.
The transfer function of the entire system, H [z], is then found by multiplying the transfer functions of the two stage: Combining cascade and parallel stages.
The z-domain allows recursive stages in a cascade, (a), or in parallel, (b), to be combined into a single system, (c).
Since this is in the form of Eq. 33-3, we can directly extract the recursion coefficients that implement the cascaded system: The obvious problem with this technique is the large amount of algebra needed to multiply and rearrange the polynomial terms.
Fortunately, the entire algorithm can be expressed in a short computer program, shown in Table 33-1.
Although the cascade and parallel combinations require different mathematics, they use nearly the same program.
In particular, only one line of code is different between the two algorithms, allowing both to be combined into a single program.
Combining cascade and parallel stages.
This program combines the recursion coefficients of stages in cascade or parallel.
The recursive coefficients for the two stages being combined enter the program in the arrays: A1[ ], B1[ ], & A2[ ], B2[ ].
The recursion coefficients that implement the entire system leave the program in the arrays: A3[ ], B3[ ].
This program operates by changing the recursive coefficients from each of the individual stages into transfer functions in the form of Eq. 33-3 (lines 220270).
After combining these transfer functions in the appropriate manner (lines 290-380), the information is moved back to being recursive coefficients (lines 400 to 430).
Since each of the transfer functions is a fraction (one polynomial divided by another polynomial), we combine stages in parallel by multiplying the denominators, and adding the cross products in the numerators.
This means that the denominator is calculated in the same way as for cascaded stages, but the numerator calculation is more elaborate.
In line 340, the numerators of cascaded stages are convolved to find the numerator of the combined transfer function.
In line 350, the numerator of the parallel stage combination is calculated as the sum of the two numerators convolved with the two denominators.
Line 360 handles the denominator calculation for both cases.
Spectral Inversion Chapter 14 describes an FIR filter technique called spectral inversion.
This is a way of changing the filter kernel such that the frequency response is flipped top-for-bottom.
All the passbands are changed into stopbands, and vice versa.
For example, a low-pass filter is changed into high-pass, a band-pass filter into band-reject, etc.
A similar procedure can be done with recursive filters, although it is far less successful.
As illustrated in Fig. 33-5, spectral inversion is accomplished by subtracting the output of the system from the original signal.
This procedure can be Original System FIGURE 33- Spectral inversion.
This procedure is the same as subtracting the output of the system from the original signal.
Using this approach, it can be shown that the "b" coefficients are left unchanged, and the modified "a" coefficients are given by: EQUATION 33- Spectral inversion.
The frequency response of a recursive filter can be flipped top-forbottom by modifying the "a" coefficients according to these equations.
The original coefficients are shown in italics, and the modified coefficients in roman.
The "b" coefficients are not changed.
This method usually provides poor results.
Examples of spectral inversion.
Figure (a) shows the frequency response of a 6 pole low-pass Butterworth filter.
Figure (b) shows the corresponding high-pass filter obtained by spectral inversion; its a mess!
A more successful case is shown in (c) and (d) where a notch filter is transformed in to a band-pass frequency response.
These mediocre results are especially disappointing in comparison to the excellent performance seen in Chapter 14.
Why the difference?
The answer lies in something that is often forgotten in filter design: the phase response.
To illustrate how phase is the culprit, consider a system called the Hilbert transformer.
The Hilbert transformer is not a specific device, but any system that has the frequency response: Magnitude = 1 and phase = 90 degrees, for all frequencies.
This means that any sinusoid passing through a Hilbert transformer will be unaffected in amplitude, but changed in phase by onequarter of a cycle.
Hilbert transformers can be analog or discrete (that is, hardware or software), and are commonly used in communications for various modulation and demodulation techniques.
Now, suppose we spectrally invert the Hilbert transformer by subtracting its output from the original signal.
Looking only at the magnitude of the frequency responses, we would conclude that the entire system would have an output of zero.
That is, the magnitude of the Hilbert transformer's output is identical to the magnitude of the original signal, and the two will cancel.
This, of course, is completely incorrect.
Two sinusoids will exactly cancel only if they have the same magnitude and phase.
In reality, the frequency response of this composite system has a magnitude of 2, and a phase shift of -45 degrees.
Rather than being zero (our naive guess), the output is larger in amplitude than the input!
Spectral inversion works well in Chapter 14 because of the specific kind of filter used: zero phase.
That is, the filter kernels have a left-right symmetry.
When there is no phase shift introduced by a system, the subtraction of the output from the input is dictated solely by the magnitudes.
Since recursive filters are plagued with phase shift, spectral inversion generally produces unsatisfactory filters.
Gain Changes Suppose we have a recursive filter and need to modify the recursion coefficients such that the output signal is changed in amplitude.
This might be needed, for example, to insure that a filter has unity gain in the passband.
The method to achieve this is very simple: multiply the "a" coefficients by whatever factor we want the gain to change by, and leave the "b" coefficients alone.
Before adjusting the gain, we would probably like to know its current value.
Since the gain must be specified at a frequency in the passband, the procedure depends on the type of filter being used.
Low-pass filters have their gain measured at a frequency of zero, while high-pass filters use a frequency of 0.5, the maximum frequency allowable.
It is quite simple to derive expressions for the gain at both these special frequencies.
Here's how it is done.
First, we will derive an equation for the gain at zero frequency.
The idea is to force each of the input samples to have a value of one, resulting in each of the output samples having a value of G, the gain of the system we are trying to find.
We will start by writing the recursion equation, the mathematical relationship between the input and output signals: y[n ] ' a0 x[n ] % a1 x[n & 1] % a2 x[n & 2] % þ % b1 y[n & 1] % b2 y[n & 2] % b3 y[n & 3] % þ Next, we plug in one for each input sample, and G for each output sample.
In other words, we force the system to operate at zero frequency.
The equation becomes: Solving for G provides the gain of the system at zero frequency, based on its recursion coefficients: EQUATION 33- DC gain of recursive filters.
This relation provides the DC gain from the recursion coefficients.
To make a filter have a gain of one at DC, calculate the existing gain by using this relation, and then divide all the "a" coefficients by G.
The gain at a frequency of 0.5 is found in a similar way: we force the input and output signals to operate at this frequency, and see how the system responds.
At a frequency of 0.5, the samples in the input signal alternate between -1 and 1.
That is, successive samples are: 1, -1, 1, -1, 1, -1, 1, etc.
The corresponding output signal also alternates in sign, with an amplitude equal to the gain of the system: G, -G, G, -G, G, -G, etc. Plugging these signals into the recursion equation: G ' a0 & a1 % a2 & a3 % þ & b1 G % b2 G & b3 G % b4 G þ Solving for G provides the gain of the system at a frequency of 0.5, using its recursion coefficients: EQUATION 33- Gain at maximum frequency.
This relation gives the recursive filter's gain at a frequency of 0.5, based on the system's recursion coefficients.
Just as before, a filter can be normalized for unity gain by dividing all of the "a" coefficients by this calculated value of G. Calculation of Eq. 33- in a computer program requires a method for generating negative signs for the odd coefficients, and positive signs for the even coefficients.
The most common method is to multiply each coefficient by (&1) k, where k is the index of the coefficient being worked on.
That is, as k runs through the values: 0, 1, 2, 3, 4, 5, 6 etc., the expression, (&1) k, takes on the values: 1, Chebyshev-Butterworth Filter Design A common method of designing recursive digital filters is shown by the Chebyshev-Butterworth program presented in Chapter 20.
It starts with a polezero diagram of an analog filter in the s-plane, and converts it into the desired digital filter through several mathematical transforms.
To reduce the complexity of the algebra, the filter is designed as a cascade of several stages, with each stage implementing one pair of poles.
The recursive coefficients for each stage are then combined into the recursive coefficients for the entire filter.
This is a very sophisticated and complicated algorithm; a fitting way to end this book.
Here's how it works.
Loop Control Figure 33-7 shows the program and flowchart for the method, duplicated from Chapter 20.
After initialization and parameter entry, the main portion of the program is a loop that runs through each pole-pair in the filter.
This loop is controlled by block 11 in the flowchart, and the FOR-NEXT loop in lines & 460 of the program.
For example, the loop will be executed three times for a 6 pole filter, with the loop index, P%, taking on the values 1,2,3.
That is, a 6 pole filter is implemented in three stages, with two poles per stage.
Combining Coefficients During each loop, subroutine 1000 (listed in Fig. 33-8) calculates the recursive coefficients for that stage.
These are returned from the subroutine in the five variables: A0, A1, A2, B1, B2 .
In step 10 of the flowchart (lines 360-440), these coefficients are combined with the coefficients of all the previous stages, held in the arrays: A[ ] and B[ ].
At the end of the first loop, A[ ] and B[ ] hold the coefficients for stage one.
At the end of the second loop, A[ ] and B[ ] hold the coefficients of the cascade of stage one and stage two.
When all the loops have been completed, A[ ] and B[ ] hold the coefficients needed to implement the entire filter.
The coefficients are combined as previously outlined in Table 33-1, with a few modifications to make the code more compact.
First, the index of the arrays, A[ ] and B[ ], is shifted by two during the loop.
For example, a0 is held in A[2], a1 & b1 are held in A[3] & B[3], etc.
This is done to prevent the program from trying to access values outside the defined arrays.
This shift is removed in block 12 (lines 480-520), such that the final recursion coefficients reside in A[ ] and B[ ] without an index offset.
Second, A[ ] and B[ ] must be initialized with coefficients corresponding to the identity system, not all zeros.
This is done in lines 180 to 240.
During the first loop, the coefficients for the first stage are combined with the information initially present in these arrays.
If all zeros were initially present, the arrays would always remain zero.
Third, two temporary arrays are used, TA[ ] and TB[ ].
These hold the old values of A[ ] and B[ ] during the convolution, freeing A[ ] and B[ ] to hold the new values.
To finish the program, block 13 (lines 540-670) adjusts the filter to have a unity gain in the passband.
This operates as previously described: calculate the existing gain with Eq. 33-7 or 33-8, and divide all the "a" coefficients to normalize.
The intermediate variables, SA and SB, are the sums of the "a" and "b" coefficients, respectively.
Calculate Pole Locations in the s-Plane Regardless of the type of filter being designed, this program begins with a Butterworth low-pass filter in the s-plane, with a cutoff frequency of T ' 1 .
As described in the last chapter, Butterworth filters have poles that are equally spaced around a circle in the s-plane.
Since the filter is low-pass, no zeros are used.
The radius of the circle is one, corresponding to the cutoff frequency of T ' 1 .
Block 3 of the flowchart (lines 1080 & 1090) calculate the location of each pole-pair in rectangular coordinates.
The program variables, RP and IP, are the real and imaginary parts of the pole location, respectively.
These program variables correspond to F and T, where the pole-pair is located at F ± jT .
This pole location is calculated from the number of poles in the filter and the stage being worked on, the program variables: NP and P%, respectively.
Warp from Circle to Ellipse To implement a Chebyshev filter, this circular pattern of poles must be transformed into an elliptical pattern.
The relative flatness of the ellipse determines how much ripple will be present in the passband of the filter.
If the pole location on the circle is given by: F and T, the corresponding location on the ellipse, Fr and Tr, is given by: Fr ' F sinh(v) / k EQUATION 33- Circular to elliptical transform.
These equations change the pole location on a circle to a corresponding location on an ellipse.
The variables, NP and PR, are the number of poles in the filter, and the percent ripple in the passband, respectively.
The location on the circle is given by F and T, and the location on the ellipse by F3 and T3.
The variables, v, and k, are used only to make the equations shorter.
This program was previously presented as Table 20-4 and Table 20-5 in Chapter 20. Figure 33-8 shows the program and flowchart for subroutine 1000, called from line 340 of this main program.
These equations use hyperbolic sine and cosine functions to define the ellipse, just as ordinary sine and cosine functions operate on a circle.
The flatness of the ellipse is controlled by the variable: PR, which is numerically equal to the percentage of ripple in the filter's passband.
The variables:, < and k are used to reduce the complexity of the equations, and are represented in the program by: ES, VX and KX, respectively.
In addition to converting from a circle to an ellipse, these equations correct the pole locations to keep a unity cutoff frequency.
Since many programming languages do not support hyperbolic functions, the following identities are used: sinh(x) ' These equations produce illegal operations for PR $ 30 and PR ' 0 .
To use this program to calculate Butterworth filters (i.e., zero ripple, PR = 0), the program lines that implement these equations must be bypassed (line 1120).
Continuous to Discrete Conversion The most common method of converting a pole-zero pattern from the s-domain into the z-domain is the bilinear transform.
This is a mathematical technique of conformal mapping, where one complex plane is algebraically distorted or warped into another complex plane.
The bilinear transform changes H (s), into H (z), by the substitution: EQUATION 33- The Bilinear transform.
This substitution maps every point in the s-plane into a corresponding piont in the z-plane.
That is, we write an equation for H (s), and then replaced each s with the above expression.
In most cases, T ' 2 tan(1/2) ' 1.093 is used.
This results in the s-domain's frequency range of 0 to B radians/second, being mapped to the z-domain's frequency range of 0 to infinity radians.
Without going into more detail, the bilinear transform has the desired properties to convert from the s-plane to the z-plane, such as vertical lines being mapped into circles.
Here is an example of how it works.
For a continuous system with a single pole-pair located at p1 ' F %jT and p2 ' F &jT, the s-domain transfer function is given by: The bilinear transform converts this into a discrete system by replacing each s with the expression given in Eq. 33-10.
This creates a z-domain transfer function also containing two poles.
The problem is, the substitution leaves the transfer function in a very unfriendly form: Working through the long and tedious algebra, this expression can be placed in the standard form of Eq. 33-3, and the recursion coefficients identified as: Bilinear transform for two poles.
The pole-pair is located at F ± T in the s-plane, and a 0, a 1, a 2, b 1, b 2 are the recursion coefficients for the discrete system.
The variables M, T, and D have no physical meaning; they are simply used to make the equations shorter.
Lines 1200-1290 use these equations to convert the location of the s-domain pole-pair, held in the variables, RP and IP, directly into the recursive coefficients, held in the variables, X0, X1, X2, Y1, Y2.
In other words, we have calculated an intermediate result: the recursion coefficients for one stage of a low-pass filter with a cutoff frequency of one.
Low-pass to Low-pass Frequency Change Changing the frequency of the recursive filter is also accomplished with a conformal mapping technique.
Suppose we know the transfer function of a recursive low-pass filter with a unity cutoff frequency.
The transfer function of a similar low-pass filter with a new cutoff frequency, W, is obtained by using a low-pass to low-pass transform.
This is also carried out by substituting variables, just as with the bilinear transform.
We start by writing the transfer function of the unity cutoff filter, and then replace each z -1 with the following: EQUATION 33- Low-pass to low-pass transform.
This is a method of changing the cutoff frequency of low-pass filters.
The original filter has a cutoff frequency of unity, while the new filter has a cutoff frequency of W, in the range of 0 to B .
This provides the transfer function of the filter with the new cutoff frequency.
The following design equations result from applying this substitution to the biquad, i.e., no more than two poles and two zeros: Low-pass to low-pass conversion.
The recursion coefficients of the filter with unity cutoff are shown in italics.
The coefficients of the low-pass filter with Low-pass to High-pass Frequency Change The above transform can be modified to change the response of the system from low-pass to high-pass while simultaneously changing the cutoff frequency.
This is accomplished by using a low-pass to high-pass transform, via the substitution: EQUATION 33- Low-pass to high-pass transform.
This substitution changes a low-pass filter into a high-pass filter.
The cutoff frequency of the low-pass filter is one, while the cutoff frequency of the highpass filter is W. As before, this can be reduced to design equations for changing the coefficients of a biquad stage.
As it turns out, the equations are identical to those of Eq. 33-13, with only two minor changes.
The value of k is different (as given in Eq. 33-14), and two coefficients, a1 and b1, are negated in value.
These equations are carried out in lines 1330 to 1410 in the program, providing the desired cutoff frequency, and the choice of a high-pass or low-pass response.
The Best and Worst of DSP This book is based on a simple premise: most DSP techniques can be used and understood with a minimum of mathematics.
The idea is to provide scientists and engineers tools for solving the DSP problems that arise in their non-DSP research or design activities.
These last four chapters are the other side of the coin: DSP techniques that can only be understood through extensive math.
For example, consider the Chebyshev-Butterworth filter just described.
This is the best of DSP, a series of elegant mathematical steps leading to an optimal solution.
However, it is also the worst of DSP, a design method so complicated that most scientists and engineers will look for another alternative.
Where do you fit into this scheme?
This depends on who your are and what you plan on using DSP for.
The material in the last four chapters provides the theoretical basis for signal processing.
If you plan on pursuing a career in DSP, you need to have a detailed understanding of this mathematics.
On the other hand, specialists in other areas of science and engineering only need to know how DSP is used, not how it is derived.
To this group, the theoretical material is more of a background, rather than a central topic.

﻿The present text evolved from course notes developed over a period of a dozen years teaching undergraduates the basics of signal processing for communications.
The students had mostly a background in electrical engineering, computer science or mathematics, and were typically in their third year of studies at Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), with an interest in communication systems.
Thus, they had been exposed to signals and systems, linear algebra, elements of analysis (e.g.
Fourier series) and some complex analysis, all of this being fairly standard in an undergraduate program in engineering sciences.
The notes having reached a certain maturity, including examples, solved problems and exercises, we decided to turn them into an easy-to-use text on signal processing, with a look at communications as an application.
But rather than writing one more book on signal processing, of which many good ones already exist, we deployed the following variations, which we think will make the book appealing as an undergraduate text.
Less formal: Both authors came to signal processing by way of an interest in music and think that signal processing is fun, and should be taught to be fun!
Thus, choosing between the intricacies of z-transform inversion through contour integration (how many of us have ever done this after having taken a class in signal processing?) or showing the Karplus-Strong algorithm for synthesizing guitar sounds (which also intuitively illustrates issues of stability along the way), you can guess where our choice fell.
While mathematical rigor is not the emphasis, we made sure to be precise, and thus the text is not approximate in its use of mathematics.
Remember, we think signal processing to be mathematics applied to a fun topic, and not mathematics for its own sake, nor a set of applications without foundations.
More conceptual: We could have said “more abstract”, but this sounds scary and may seem in contradiction with point 1 above, which of course it is not).
Thus, the level of mathematical abstraction is probably higher than in several other texts on signal processing, but it allows to think at a higher conceptual level, and also to build foundations for more advanced topics.
Therefore we introduce vector spaces, Hilbert spaces, signals as vectors, orthonormal bases, projection theorem, to name a few, which are powerful concepts not usually emphasized in standard texts.
Because these are geometrical concepts, they foster understanding without making the text any more complex.
Further, this constitutes the foundation of modern signal processing, techniques such as time-frequency analysis, filter banks and wavelets, which makes the present text an easy primer for more advanced signal processing books.
Of course, we must admit, for the sake of full transparency, that we have been influenced by our research work, but again, this has been fun too!
More application driven: This is an engineering text, which should help the student solve real problems.
Both authors are engineers by training and by trade, and while we love mathematics, we like to see their “operational value”.
That is, does the result make a difference in an engineering application?
Certainly, the masterpiece in this regard is C. Shannon’s 1948 foundational paper on “The Mathematical Theory of Communication”.
It completely revolutionized the way communication systems are designed and built, and, still today, we mostly live in its legacy.
Not surprisingly, one of the key results of signal processing is the sampling theorem for bandlimited functions (often attributed to Shannon, since it appears in the above-mentioned paper), the theorem which single-handedly enabled the digital revolution.
To a mathematician, this is a simple corollary to Fourier series, and he/she might suggest many other ways to represent such particular functions.
However, the strength of the sampling theorem and its variations (e.g. oversampling or quantization) is that it is an operational theorem, robust, and applicable to actual signal acquisition and reconstruction problems.
In order to showcase such powerful applications, the last chapter is entirely devoted to developing an end-to-end communication system, namely a modem for communicating digital information (or bits) over an analog channel.
This real-world application (which is present in all modern communication devices, from mobile phones to ADSL boxes) nicely brings together many of the concepts and designs studied in the previous chapters.
Being less formal, more abstract and application-driven seems almost like moving simultaneously in several and possibly opposite directions, but we believe we came up with the right balancing act.
Ultimately, of course, the readers and students are the judges!
A last and very important issue is the online access to the text and supplementary material.
A full html version together with the unavoidable errata and other complementary material is available at.
A solution manual is available to teachers upon request.
As a closing word, we hope you will enjoy the text, and we welcome your feedback.
Let signal processing begin, and be fun!
The current book is the result of several iterations of a yearly signal processing undergraduate class and the authors would like to thank the students in Communication Systems at EPFL who survived the early versions of the manuscript and who greatly contributed with their feedback to improve and refine the text along the years.
Invaluable help was also provided by the numerous teaching assistants who not only volunteered constructive criticism but came up with a lot of the exercices which appear at the end of each chapter (and their relative solutions).
In no particular order: Andrea Ridolfi provided insightful mathematical remarks and also introduced us to the wonders of PsTricks while designing figures.
Olivier Roy and Guillermo Barrenetxea have been indefatigable ambassadors between teaching and student bodies, helping shape exercices in a (hopefully) more user-friendly form.
Ivana Jovanovic, Florence BÃ©nÃ©zit and Patrick Vandewalle gave us a set of beautiful ideas and pointers thanks to their recitations on choice signal processing topics.
Luciano Sbaiz always lent an indulgent ear and an insightful answer to all the doubts and worries which plague scientific writers.
We would also like to express our personal gratitude to our families and friends for their patience and their constant support; unfortunately, to do so in a proper manner, we should resort to a lyricism which is sternly frowned upon in technical textbooks and therefore we must confine ourselves to a simple “thank you”.
What Is Digital Signal Processing?
A signal, technically yet generally speaking, is a a formal description of a phenomenon evolving over time or space; by signal processing we denote any manual or “mechanical” operation which modifies, analyzes or otherwise manipulates the information contained in a signal.
Consider the simple example of ambient temperature: once we have agreed upon a formal model for this physical variable – Celsius degrees, for instance – we can record the evolution of temperature over time in a variety of ways and the resulting data set represents a temperature “signal”.
Simple processing operations can then be carried out even just by hand: for example, we can plot the signal on graph paper as in Figure 1 point 1, or we can compute derived parameters such as the average temperature in a month.
Conceptually, it is important to note that signal processing operates on an abstract representation of a physical quantity and not on the quantity itself.
At the same time, the type of abstract representation we choose for the physical phenomenon of interest determines the nature of a signal processing unit.
A temperature regulation device, for instance, is not a signal processing system as a whole.
The device does however contain a signal processing core in the feedback control unit which converts the instantaneous measure of the temperature into an ON/OFF trigger for the heating element.
The physical nature of this unit depends on the temperature model: a simple design is that of a mechanical device based on the dilation of a metal sensor; more likely, the temperature signal is a voltage generated by a thermocouple and in this case the matched signal processing unit is an operational amplifier.
Finally, the adjective “digital” derives from digitus, the Latin word for finger: it concisely describes a world view where everything can be ultimateely represented as an integer number.
Counting, first on one’s fingers and then in one’s head, is the earliest and most fundamental form of abstraction; as children we quickly learn that counting does indeed bring disparate objects (the proverbial “apples and oranges”) into a common modeling paradigm, for example their cardinality.
Digital signal processing is a flavor of signal processing in which everything including time is described in terms of integer numbers; in other words, the abstract representation of choice is a one-size-fit-all countability.
Note that our earlier “thought experiment” about ambient temperature fits this paradigm very naturally: the measuring instants form a countable set (the days in a month) and so do the measures themselves (imagine a finite number of ticks on the thermometer’s scale).
In digital signal processing the underlying abstract representation is always the set of natural numbers regardless of the signal’s origins; as a consequence, the physical nature of the processing device will also always remain the same, that is, a general digital (micro)processor.
The extraordinary power and success of digital signal processing derives from the inherent universality of its associated “world view”.
Probably the earliest recorded example of digital signal processing dates back to the 25th century BC.
At the time, Egypt was a powerful kingdom reaching over a thousand kilometres south of the Nile’s delta.
For all its latitude, the kingdom’s populated area did not extend for more than a few kilometers on either side of the Nile; indeed, the only inhabitable areas in an otherwise desert expanse were the river banks, which were made fertile by the yearly flood of the river.
After a flood, the banks would be left covered with a thin layer of nutrient-rich silt capable of supporting a full agricultural cycle.
The floods of the Nile, however, were(1) a rather capricious meteorological phenomenon, with scant or absent floods resulting in little or no yield from the land.
The pharaohs quickly understood that, in order to preserve stability, they would have to set up a grain buffer with which to compensate for the unreliability of the Nile’s floods and prevent potential unrest in a famished population during “dry” years.
As a consequence, studying and predicting the trend of the floods (and therefore the expected agricultural yield) was of paramount importance in order to determine the operating point of a very dynamic taxation and redistribution mechanism.
The floods of the Nile were meticulously recorded by an array of measuring stations called “nilometers” and the resulting data set can indeed be considered a full-fledged digital signal defined on a time base of twelve months.
The Palermo Stone, shown in the left panel of Figure 1 point 2, is a faithful record of the data in the form of a table listing the name of the current pharaoh alongside the yearly flood level; a more modern representation of an flood data set is shown on the left of the figure: bar the references to the pharaohs, the two representations are perfectly equivalent.
The Nile’s behavior is still an active area of hydrological research today and it would be surprising if the signal processing operated by the ancient Egyptians on their data had been of much help in anticipating for droughts.
Yet, the Palermo Stone is arguably the first recorded digital signal which is still of relevance today.
“Digital” representations of the world such as those depicted by the Palermo Stone are adequate for an environment in which quantitative problems are simple: counting cattle, counting bushels of wheat, counting days and so on.
As soon as the interaction with the world becomes more complex, so necessarily do the models used to interpret the world itself.
Geometry, for instance, is born of the necessity of measuring and subdividing land property.
In the act of splitting a certain quantity into parts we can already see the initial difficulties with an integer-based world view ;(2) yet, until the Hellenic period, western civilization considered natural numbers and their ratios all that was needed to describe nature in an operational fashion.
In the 6th century BC, however, a devastated Pythagoras realized that the the side and the diagonal of a square are incommensurable, for example that √ --
2 is not a simple fraction.
The discovery of what we now call irrational numbers “sealed the deal” on an abstract model of the world that had already appeared in early geometric treatises and which today is called the continuum.
Heavily steeped in its geometric roots (for example in the infinity of points in a segment), the continuum model postulates that time and space are an uninterrupted flow which can be divided arbitrarily many times into arbitrarily (and infinitely) small pieces.
In signal processing parlance, this is known as the “analog” world model and, in this model, integer numbers are considered primitive entities, as rough and awkward as a set of sledgehammers in a watchmaker’s shop.
In the continuum, the infinitely big and the infinitely small dance together in complex patterns which often defy our intuition and which required almost two thousand years to be properly mastered.
This is of course not the place to delve deeper into this extremely fascinating epistemological domain; suffice it to say that the apparent incompatibility between the digital and the analog world views appeared right from the start (for example from the 5th century BC) in Zeno’s works; we will appreciate later the immense import that this has on signal processing in the context of the sampling theorem.
Zeno’s paradoxes are well known and they underscore this unbridgeable gap between our intuitive, integer-based grasp of the world and a model of the world based on the continuum.
Consider for instance the dichotomy paradox; Zeno states that if you try to move along a line from point A to point B you will never in fact be able to reach your destination.
The reasoning goes as follows: in order to reach B, you will have to first go through point C, which is located mid-way between A and B; but, even before you reach C, you will have to reach D, which is the midpoint between A and C; and so on ad infinitum.
Since there is an infinity of such intermediate points, Zeno argues, moving from A to B requires you to complete an infinite number of tasks, which is humanly impossible.
Zeno of course was well aware of the empirical evidence to the contrary but he was brilliantly pointing out the extreme trickery of a model of the world which had not yet formally defined the concept of infinity.
The complexity of the intellectual machinery needed to solidly counter Zeno’s argument is such that even today the paradox is food for thought.
A first-year calculus student may be tempted to offhandedly dismiss the problem by stating but this is just a void formalism begging the initial question if the underlying notion of the continuum is not explicitly worked out.(3)
In reality Zeno’s paradoxes cannot be “solved”, since they cease to be paradoxes once the continuum model is fully understood.
The two competing models for the world, digital and analog, coexisted quite peacefully for quite a few centuries, one as the tool of the trade for farmers, merchants, bankers, the other as an intellectual pursuit for mathematicians and astronomers.
Slowly but surely, however, the increasing complexity of an expanding world spurred the more practically-oriented minds to pursue science as a means to solve very tangible problems besides describing the motion of the planets.
Calculus, brought to its full glory by Newton and Leibnitz in the 17th century, proved to be an incredibly powerful tool when applied to eminently practical concerns such as ballistics, ship routing, mechanical design and so on; such was the faith in the power of the new science that Leibnitz envisioned a not-too-distant future in which all human disputes, including problems of morals and politics, could be worked out with pen and paper: “gentlemen, calculemus”.
If only.
As Cauchy unsurpassably explained later, everything in calculus is a limit and therefore everything in calculus is a celebration of the power of the continuum.
Still, in order to apply the calculus machinery to the real world, the real world has to be modeled as something calculus understands, namely a function of a real (for example continuous) variable.
As mentioned before, there are vast domains of research well behaved enough to admit such an analytical representation; astronomy is the first one to come to mind, but so is ballistics, for instance.
If we go back to our temperature measurement example, however, we run into the first difficulty of the analytical paradigm: we now need to model our measured temperature as a function of continuous time, which means that the value of the temperature should be available at any given instant and not just once per day.
A “temperature function” as in Figure 1 point 3 is quite puzzling to define if all we have (and if all we can have, in fact) is just a set of empirical measurements reasonably spaced in time.
Even in the rare cases in which an analytical model of the phenomenon is available, a second difficulty arises when the practical application of calculus involves the use of functions which are only available in tabulated form.
The trigonometric and logarithmic tables are a typical example of how a continuous model needs to be made countable again in order to be put to real use.
Algorithmic procedures such as series expansions and numerical integration methods are other ways to bring the analytic results within the realm of the practically computable.
These parallel tracks of scientific development, the “Platonic” ideal of analytical results and the slide rule reality of practitioners, have coexisted for centuries and they have found their most durable mutual peace in digital signal processing, as will appear shortly.
One of the fundamental problems in signal processing is to obtain a permanent record of the signal itself.
Think back of the ambient temperature example, or of the floods of the Nile: in both cases a description of the phenomenon was gathered by a naive sampling operation, for example by measuring the quantity of interest at regular intervals.
This is a very intuitive process and it reflects the very natural act of “looking up the current value and writing it down”.
Manually this operation is clearly quite slow but it is conceivable to speed it up mechanically so as to obtain a much larger number of measurements per unit of time.
Our measuring machine, however fast, still will never be able to take an infinite amount of samples in a finite time interval: we are back in the clutches of Zeno’s paradoxes and one would be tempted to conclude that a true analytical representation of the signal is impossible to obtain.
At the same time, the history of applied science provides us with many examples of recording machines capable of providing an “analog” image of a physical phenomenon.
Consider for instance a thermograph: this is a mechanical device in which temperature deflects an ink-tipped metal stylus in contact with a slowly rolling paper-covered cylinder.
Thermographs like the one sketched in Figure 1 point 4 are still currently in use in some simple weather stations and they provide a chart in which a temperature function as in Figure 1 point 3 is duly plotted.
Incidentally, the principle is the same in early sound recording devices: Edison’s phonograph used the deflection of a steel pin connected to a membrane to impress a “continuous-time” sound wave as a groove on a wax cylinder.
The problem with these analog recordings is that they are not abstract signals but a conversion of a physical phenomenon into another physical phenomenon: the temperature, for instance, is converted into the amount of ink on paper while the sound pressure wave is converted into the physical depth of the groove.
The advent of electronics did not change the concept: an audio tape, for instance, is obtained by converting a pressure wave into an electrical current and then into a magnetic deflection.
The fundamental consequence is that, for analog signals, a different signal processing system needs to be designed explicitly for each specific form of recording.
Consider for instance the problem of computing the average temperature over a certain time interval.
Calculus provides us with the exact answer Â¯C if we know the elusive “temperature function” f(t) over an interval [T0,T1] (see Figure 1 point 5, top panel):
We can try to reproduce the integration with a “machine” adapted to the particular representation of temperature we have at hand: in the case of the thermograph, for instance, we can use a planimeter as in Figure 1 point 6, a manual device which computes the area of a drawn surface; in a more modern incarnation in which the temperature signal is given by a thermocouple, we can integrate the voltage with the RC network in Figure 1 point 7.
In both cases, in spite of the simplicity of the problem, we can instantly see the practical complications and the degree of specialization needed to achieve something as simple as an average for an analog signal.
Now consider the case in which all we have is a set of daily measurements c1,c2,…,cD as in Figure 1 point 1; the “average” temperature of our measurements over D days is simply:
(as shown in the bottom panel of Figure 1 point 5) and this is an elementary sum of D terms which anyone can carry out by hand and which does not depend on how the measurements have been obtained: wickedly simple!
So, obviously, the question is: “How different (if at all) is Ĉ from CÂ¯ ?”
In order to find out we can remark that if we accept the existence of a temperature function f(t) then the measured values cn are samples of the function taken one day apart:
(where Ts is the duration of a day).
In this light, the sum (1 point 3) is just the Riemann approximation to the integral in (1 point 2) and the question becomes an assessment on how good an approximation that is.
Another way to look at the problem is to ask ourselves how much information we are discarding by only keeping samples of a continuous-time function.
The answer, which we will study in detail in Chapter 9, is that in fact the continuous-time function and the set of samples are perfectly equivalent representations – provided that the underlying physical phenomenon “doesn’t change too fast”.
Let us put the proviso aside for the time being and concentrate instead on the good news: first, the analog and the digital world can perfectly coexist; second, we actually possess a constructive way to move between worlds: the sampling theorem, discovered and rediscovered by many at the beginning of the 20th century(4) , tells us that the continuous-time function can be obtained from the samples as.
So, in theory, once we have a set of measured values, we can build the continuous-time representation and use the tools of calculus.
In reality things are even simpler: if we plug (1 point 4) into our analytic formula for the average (1 point 2) we can show that the result is a simple sum like (1 point 3).
So we don’t need to explicitly go “through the looking glass” back to continuous-time: the tools of calculus have a discrete-time equivalent which we can use directly.
The equivalence between the discrete and continuous representations only holds for signals which are sufficiently “slow” with respect to how fast we sample them.
This makes a lot of sense: we need to make sure that the signal does not do “crazy” things between successive samples; only if it is smooth and well behaved can we afford to have such sampling gaps.
Quantitatively, the sampling theorem links the speed at which we need to repeatedly measure the signal to the maximum frequency contained in its spectrum.
Spectra are calculated using the Fourier transform which, interestingly enough, was originally devised as a tool to break periodic functions into a countable set of building blocks.
Everything comes together.
While it appears that the time continuum has been tamed by the sampling theorem, we are nevertheless left with another pesky problem: the precision of our measurements.
If we model a phenomenon as an analytical function, not only is the argument (the time domain) a continuous variable but so is the function’s value (the codomain); a practical measurement, however, will never achieve an infinite precision and we have another paradox on our hands.
Consider our temperature example once more: we can use a mercury thermometer and decide to write down just the number of degrees; maybe we can be more precise and note the half-degrees as well; with a magnifying glass we could try to record the tenths of a degree – but we would most likely have to stop there.
With a more sophisticated thermocouple we could reach a precision of one hundredth of a degree and possibly more but, still, we would have to settle on a maximum number of decimal places.
Now, if we know that our measures have a fixed number of digits, the set of all possible measures is actually countable and we have effectively mapped the codomain of our temperature function onto the set of integer numbers.
This process is called quantization and it is the method, together with sampling, to obtain a fully digital signal.
In a way, quantization deals with the problem of the continuum in a much “rougher” way than in the case of time: we simply accept a loss of precision with respect to the ideal model.
There is a very good reason for that and it goes under the name of noise.
The mechanical recording devices we just saw, such as the thermograph or the phonograph, give the illusion of analytical precision but are in practice subject to severe mechanical limitations.
Any analog recording device suffers from the same fate and even if electronic circuits can achieve an excellent performance, in the limit the unavoidable thermal agitation in the components constitutes a noise floor which limits the “equivalent number of digits”.
Noise is a fact of nature that cannot be eliminated, hence our acceptance of a finite (for example countable) precision.
Noise is not just a problem in measurement but also in processing.
Figure 1 point 8 shows the two archetypal types of analog and digital computing devices; while technological progress may have significantly improved the speed of each, the underlying principles remain the same for both.
An analog signal processing system, much like the slide rule, uses the displacement of physical quantities (gears or electric charge) to perform its task; each element in the system, however, acts as a source of noise so that complex or, more importantly, cheap designs introduce imprecisions in the final result (good slide rules used to be very expensive).
On the other hand the abacus, working only with integer arithmetic, is a perfectly precise machine(5) even if it’s made with rocks and sticks.
Digital signal processing works with countable sequences of integers so that in a digital architecture no processing noise is introduced.
A classic example is the problem of reproducing a signal.
Before mp3 existed and file sharing became the bootlegging method of choice, people would “make tapes”.
When someone bought a vinyl record he would allow his friends to record it on a cassette; however, a “peer-to-peer” dissemination of illegally taped music never really took off because of the “second generation noise”, for example because of the ever increasing hiss that would appear in a tape made from another tape.
Basically only first generation copies of the purchased vinyl were acceptable quality on home equipment.
With digital formats, on the other hand, duplication is really equivalent to copying down a (very long) list of integers and even very cheap equipment can do that without error.
Finally, a short remark on terminology.
The amplitude accuracy of a set of samples is entirely dependent on the processing hardware; in current parlance this is indicated by the number of bits per sample of a given representation: compact disks, for instance, use 16 bits per sample while DVDs use 24.
Because of its “contingent” nature, quantization is almost always ignored in the core theory of signal processing and all derivations are carried out as if the samples were real numbers; therefore, in order to be precise, we will almost always use the term discrete-time signal processing and leave the label “digital signal processing” (DSP) to the world of actual devices.
Neglecting quantization will allow us to obtain very general results but care must be exercised: in the practice, actual implementations will have to deal with the effects of finite precision, sometimes with very disruptive consequences.
Signals in digital form provide us with a very convenient abstract representation which is both simple and powerful; yet this does not shield us from the need to deal with an “outside” world which is probably best modeled by the analog paradigm.
Consider a mundane act such as placing a call on a cell phone, as in Figure 1 point 9: humans are analog devices after all and they produce analog sound waves; the phone converts these into digital format, does some digital processing and then outputs an analog electromagnetic wave on its antenna.
The radio wave travels to the base station in which it is demodulated, converted to digital format to recover the voice signal.
The call, as a digital signal, continues through a switch and then is injected into an optical fiber as an analog light wave.
The wave travels along the network and then the process is inverted until an analog sound wave is generated by the loudspeaker at the receiver’s end.
Communication systems are in general a prime example of sophisticated interplay between the digital and the analog world: while all the processing is undoubtedly best done digitally, signal propagation in a medium (be it the the air, the electromagnetic spectrum or an optical fiber) is the domain of differential (rather than difference) equations.
And yet, even when digital processing must necessarily hand over control to an analog interface, it does so in a way that leaves no doubt as to who’s boss, so to speak: for, instead of transmitting an analog signal which is the reconstructed “real” function as per (1 point 4), we always transmit an analog signal which encodes the digital representation of the data.
This concept is really at the heart of the “digital revolution” and, just like in the cassette tape example, it has to do with noise.
Imagine an analog voice signal s(t) which is transmitted over a (long) telephone line; a simplified description of the received signal is where the parameter α, with α is smaller than 1, is the attenuation that the signal incurs and where n(t) is the noise introduced by the system.
The noise function is of obviously unknown (it is often modeled as a Gaussian process, as we will see) and so, once it’s added to the signal, it’s impossible to eliminate it.
Because of attenuation, the receiver will include an amplifier with gain G to restore the voice signal to its original level; with G equal to 1 over α we will have function.
Unfortunately, as it appears, in order to regenerate the analog signal we also have amplified the noise G times; clearly, if G is large (for example if there is a lot of attenuation to compensate for) the voice signal end up buried in noise.
The problem is exacerbated if many intermediate amplifiers have to be used in cascade, as is the case in long submarine cables.
Consider now a digital voice signal, that is, a discrete-time signal whose samples have been quantized over, say, 256 levels: each sample can therefore be represented by an 8-bit word and the whole speech signal can be represented as a very long sequence of binary digits.
We now build an analog signal as a two-level signal which switches for a few instants between, say, -1 V and plus1 V for every “0” and “1” bit in the sequence respectively.
The received signal will still be but, to regenerate it, instead of linear amplification we can use nonlinear thresholding.
Figure 1 point 10 clearly shows that as long as the magnitude of the noise is less than α the two-level signal can be regenerated perfectly; furthermore, the regeneration process can be repeated as many times as necessary with no overall degradation.
In reality of course things are a little more complicated and, because of the nature of noise, it is impossible to guarantee that some of the bits won’t be corrupted.
The answer is to use error correcting codes which, by introducing redundancy in the signal, make the sequence of ones and zeros robust to the presence of errors; a scratched CD can still play flawlessly because of the Reed-Solomon error correcting codes used for the data.
Data coding is the core subject of Information Theory and it is behind the stellar performance of modern communication systems; interestingly enough, the most successful codes have emerged from group theory, a branch of mathematics dealing with the properties of closed sets of integer numbers.
Integers again!
Digital signal processing and information theory have been able to join forces so successfully because they share a common data model (the integer) and therefore they share the same architecture (the processor).
Computer code written to implement a digital filter can dovetail seamlessly with code written to implement error correction; linear processing and nonlinear flow control coexist naturally.
A simple example of the power unleashed by digital signal processing is the performance of transatlantic cables.
The first operational telegraph cable from Europe to North America was laid in 1858 (see Fig. 1 point 11); it worked for about a month before being irrecoverably damaged.(6)
From then on, new materials and rapid progress in electrotechnics boosted the performance of each subsequent cable; the key events in the timeline of transatlantic communications are shown in Table 1 point 1.
The first transatlantic telephone cable was laid in 1956 and more followed in the next two decades with increasing capacity due to multicore cables and better repeaters; the invention of the echo canceler further improved the number of voice channels for already deployed cables.
In 1968 the first experiments in PCM digital telephony were successfully completed and the quantum leap was around the corner: by the end of the 70’s cables were carrying over one order of magnitude more voice channels than in the 60’s.
Finally, the deployment of the first fiber optic cable in 1988 opened the door to staggering capacities (and enabled the dramatic growth of the Internet).
Finally, it’s impossible not to mention the advent of data compression in this brief review of communication landmarks.
Again, digital processing allows the coexistence of standard processing with sophisticated decision logic; this enables the implementation of complex data-dependent compression techniques and the inclusion of psychoperceptual models in order to match the compression strategy to the characteristics of the human visual or auditory system.
A music format such as mp3 is perhaps the first example to come to mind but, as shown in Table 1 point 2, all communication domains have been greatly enhanced by the gains in throughput enabled by data compression.
This book tries to build a largely self-contained development of digital signal processing theory from within discrete time, while the relationship to the analog model of the world is tackled only after all the fundamental “pieces of the puzzle” are already in place.
Historically, modern discrete-time processing started to consolidate in the 50’s when mainframe computers became powerful enough to handle the effective simulations of analog electronic networks.
By the end of the 70’s the discipline had by all standards reached maturity; so much so, in fact, that the major textbooks on the subject still in use today had basically already appeared by 1975.
Because of its ancillary origin with respect to the problems of that day, however, discrete-time signal processing has long been presented as a tributary to much more established disciplines such as Signals and Systems.
While historically justifiable, that approach is no longer tenable today for three fundamental reasons: first, the pervasiveness of digital storage for data (from CDs to DVDs to flash drives) implies that most devices today are designed for discrete-time signals to start with; second, the trend in signal processing devices is to move the analog-to-digital and digital-to-analog converters at the very beginning and the very end of the processing chain so that even “classically analog” operations such as modulation and demodulation are now done in discrete-time; third, the availability of numerical packages like Matlab provides a testbed for signal processing experiments (both academically and just for fun) which is far more accessible and widespread than an electronics lab (not to mention affordable).
The idea therefore is to introduce discrete-time signals as a self-standing entity (Chap.
2), much in the natural way of a temperature sequence or a series of flood measurements, and then to remark that the mathematical structures used to describe discrete-time signals are one and the same with the structures used to describe vector spaces (Chap.
3).
Equipped with the geometrical intuition afforded to us by the concept of vector space, we can proceed to “dissect” discrete-time signals with the Fourier transform, which turns out to be just a change of basis (Chap.
4).
The Fourier transform opens the passage between the time domain and the frequency domain and, thanks to this dual understanding, we are ready to tackle the concept of processing as performed by discrete-time linear systems, also known as filters (Chap.
5).
Next comes the very practical task of designing a filter to order, with an eye to the subtleties involved in filter implementation; we will mostly consider FIR filters, which are unique to discrete time (Chaps 6 and 7).
After a brief excursion in the realm of stochastic sequences (Chap.
8) we will finally build a bridge between our discrete-time world and the continuous-time models of physics and electronics with the concepts of sampling and interpolation (Chap.
9); and digital signals will be completely accounted for after a study of quantization (Chap.
10).
We will finally go back to purely discrete time for the final topic, multirate signal processing (Chap.
11), before putting it all together in the final chapter: the analysis of a commercial voiceband modem (Chap.
12).
The Bible of digital signal processing was and remains Discrete-Time Signal Processing, by A. V. Oppenheim and R. W. Schafer (Prentice-Hall, last edition in 1999); exceedingly exhaustive, it is a must-have reference.
For background in signals and systems, the eponimous Signals and Systems, by Oppenheim, Willsky and Nawab (Prentice Hall, 1997) is a good start.
Most of the historical references mentioned in this introduction can be integrated by simple web searches.
Other comprehensive books on digital signal processing include S. K. Mitra’s Digital Signal Processing (McGraw Hill, 2006) and Digital Signal Processing, by J. G. Proakis and D. K. Manolakis (Prentis Hall 2006).
For a fascinating excursus on the origin of calculus, see D. Hairer and G. Wanner, Analysis by its History (Springer-Verlag, 1996).
A more than compelling epistemological essay on the continuum is Everything and More, by David Foster Wallace (Norton, 2003), which manages to be both profound and hilarious in an unprecedented way.
Finally, the very prolific literature on current signal processing research is published mainly by the Institute of Electronics and Electrical Engineers (IEEE) in several of its transactions such as IEEE Transactions on Signal Processing, IEEE Transactions on Image Processing and IEEE Transactions on Speech and Audio Processing.
Discrete-Time Signals
In this Chapter we define more formally the concept of the discrete-time signal and establish an associated basic taxonomy used in the remainder of the book.
Historically, discrete-time signals have often been introduced as the discretized version of continuous-time signals, for example as the sampled values of analog quantities, such as the voltage at the output of an analog circuit; accordingly, many of the derivations proceeded within the framework of an underlying continuous-time reality.
In truth, the discretization of analog signals is only part of the story, and a rather minor one nowadays.
Digital signal processing, especially in the context of communication systems, is much more concerned with the synthesis of discrete-time signals rather than with sampling.
That is why we choose to introduce discrete-time signals from an abstract and self-contained point of view.
A discrete-time signal is a complex-valued sequence.
Remember that a sequence is defined as a complex-valued function of an integer index n, with n ∈ Z; as such, it is a two-sided, infinite collection of values.
A sequence can be defined analytically in closed form, as for example: shown as the “triangular” waveform plotted in Figure 2 point 1; or which is a complex exponential of period 40 samples, plotted in Figure 2 point 2.
An example of a sequence drawn from the real world is plotted in Figure 2 point 3 from year 1900 to 2002.
Another example, this time of a random sequence, is a realization of which is plotted in Figure 2 point 4.
A few notes are in order:
The dependency of the sequence’s values on an integer-valued index n is made explicit by the use of square brackets for the functional argument.
This is standard notation in the signal processing literature.
The sequence index n is best thought of as a measure of dimensionless time; while it has no physical unit of measure, it imposes a chronological order on the values of the sequences.
We consider complex-valued discrete-time signals; while physical signals can be expressed by real quantities, the generality offered by the complex domain is particularly useful in designing systems which synthesize signal, such as data communication systems.
In graphical representations, when we need to emphasize the discrete-time nature of the signal, we resort to stem (or “lollipop”) plots as in Figure 2 point 1.
When the discrete-time domain is understood, we will often use a function-like representation as in Figure 2 point 3.
In the latter case, each ordinate of the sequence is graphically connected to its neighbors, giving the illusion of a continuous-time function: while this makes the plot easier on the eye, it must be remembered that the signal is defined only over a discrete set.
While analytical forms of discrete-time signals such as the ones above are useful to illustrate the key points of signal processing and are absolutely necessary in the mathematical abstractions which follow, they are non-etheless just that, abstract examples.
How does the notion of a discrete-time signal relate to the world around us?
A discrete-time signal, in fact, captures our necessarily limited ability to take repeated accurate measurements of a physical quantity.
We might be keeping track of the stock market index at the end of each day to draw a pencil and paper chart; or we might be measuring the voltage level at the output of a microphone 44,100 times per second (obviously not by hand!) to record some music via the computer’s soundcard.
In both cases we need “time to write down the value” and are therefore forced to neglect everything that happens between measuring times.
This “look and write down” operation is what is normally referred to as sampling.
There are real-world phenomena which lend themselves very naturally and very intuitively to a discrete-time representation: the daily Dow-Jones index, for example, solar spots, yearly floods of the Nile, etc.
There seems to be no irrecoverable loss in this neglect of intermediate values.
But what about music, or radio waves?
At this point it is not altogether clear from an intuitive point of view how a sampled measurement of these phenomena entail no loss of information.
The mathematical proof of this will be shown in detail when we study the sampling theorem; for the time being let us say that “the proof of the cake is in the eating”: just listen to your favorite CD!
The important point to make here is that, once a real-world signal is converted to a discrete-time representation, the underlying notion of “time between measurements” becomes completely abstract.
All we are left with is a sequence of numbers, and all signal processing manipulations, with their intended results, are independent of the way the discrete-time signal is obtained.
The power and the beauty of digital signal processing lies in part with its invariance with respect to the underlying physical reality.
This is in stark contrast with the world of analog circuits and systems, which have to be realized in a version specific to the physical nature of the input signals.
The following sequences are fundamental building blocks for the theory of signal processing.
Impulse.
The discrete-time impulse (or discrete-time delta function) is potentially the simplest discrete-time signal; it is shown in Figure 2 point 5(a) and is defined as Unit Step.
The discrete-time unit step is shown in Figure 2 point 5(b) and is defined by the following expression:
The unit step can be obtained via a discrete-time integration of the impulse (see eq. (2 point 16)).
Exponential Decay.
The discrete-time exponential decay is shown in Figure 2 point 5(c) and is defined as formula.
The exponential decay is, as we will see, the free response of a discrete-time first order recursive filter.
Exponential sequences are well-behaved only for values of a less than one in magnitude; sequences in which |a| is larger than 1 are unbounded and represent an unstable behavior (their energy and power are both infinite).
Complex Exponential.
The discrete-time complex exponential has already been shown in Figure 2 point 2 and is defined as formula.
Special cases of the complex exponential are the real-valued discrete-time sinusoidal oscillations:
An example of (2 point 9) is shown in Figure 2 point 5(d).
With respect to the oscillatory behavior captured by the complex exponential, a note on the concept of “frequency” is in order.
In the continuous-time world (the world of textbook physics, to be clear), where time is measured in seconds, the usual unit of measure for frequency is the Hertz which is equivalent to 1 over second.
In the discrete-time world, where the index n represents a dimensionless time, “digital” frequency is expressed in radians which is itself a dimensionless quantity.(1)
The best way to appreciate this is to consider an algorithm to generate successive samples of a discrete-time sinusoid at a digital frequency.
At each iteration,(2) the argument of the trigonometric function is incremented by ω0 and a new output sample is produced.
With this in mind, it is easy to see that the highest frequency manageable by a discrete-time system is ωmax equal to 2pi; for any frequency larger than this, the inner 2pi-periodicity of the trigonometric functions “maps back” the output values to a frequency between 0 and 2pi.
This can be expressed as an equation: for all values of k ∈ Z.
This 2pi-equivalence of digital frequencies is a pervasive concept in digital signal processing and it has many important consequences which we will study in detail in the next Chapters.
In this Section we present some elementary operations on sequences.
Shift.
A sequence x[n], shifted by an integer k is simply.
If k is positive, the signal is shifted “to the left”, meaning that the signal has been delayed; if k is negative, the signal is shifted “to the right”, meaning that the signal has been advanced.
The delay operator can be indicated by the following notation.
Scaling: A sequence x[n] scaled by a factor α ∈ C is
If α is real, then the scaling represents a simple amplification or attenuation of the signal (when α is larger than 1 and α is smaller than 1, respectively).
If α is complex, amplification and attenuation are compounded with a phase shift.
Sum: The sum of two sequences x[n] and w[n] is their term-by-term sum:
Please note that sum and scaling are linear operators.
Informally, this means scaling and sum behave “intuitively”: Product.
The product of two sequences x[n] and w[n] is their term-by-term product Integration.
The discrete-time equivalent of integration is expressed by the following running sum:
Intuitively, integration computes a non-normalized running average of the discrete-time signal.
Differentiation.
A discrete-time approximation to differentiation is the first-order difference.
With respect to Section 2 point 1 point 2, note how the unit step can be obtained by applying the integration operator to the discrete-time impulse; conversely, the impulse can be obtained by applying the differentiation operator to the unit step.
The signal reproducing formula is a simple application of the basic signal and signal properties that we have just seen and it states that.
Any signal can be expressed as a linear combination of suitably weighed and shifted impulses.
In this case, the weights are nothing but the signal values themselves.
While self-evident, this formula will reappear in a variety of fundamental derivations since it captures the “inner structure” of a discrete-time signal.
We define the energy of a discrete-time signal as the function(where the squared-norm notation will be clearer after the next Chapter).
This definition is consistent with the idea that, if the values of the sequence represent a time-varying voltage, the above sum would express the total energy (in joules) dissipated over a 1Ω-resistor.
Obviously, the energy is finite only if the above sum converges, for example if the sequence x[n] is square-summable.
A signal with this property is sometimes referred to as a finite- energy signal.
For a simple example of the converse, note that a periodic signal which is not identically zero is not square-summable.
We define the power of a signal as the usual ratio of energy over time, taking the limit over the number of samples considered:
Clearly, signals whose energy is finite, have zero total power (for example their energy dilutes to zero over an infinite time duration).
Exponential sequences which are not decaying (for example those for which |a| is larger than 1 in (2 point 7)) possess infinite power (which is consistent with the fact that they describe an unstable behavior).
Note, however, that many signals whose energy is infinite do have finite power and, in particular, periodic signals (such as sinusoids and combinations thereof).
Due to their periodic nature, however, the above limit is undetermined; we therefore define their power to be simply the average energy over a period.
Assuming that the period is N samples, we have this formula.
The examples of discrete-time signals in (2 point 1) and (2 point 2) are two-sided, infinite sequences.
Of course, in the practice of signal processing, it is impossible to deal with infinite quantities of data: for a processing algorithm to execute in a finite amount of time and to use a finite amount of storage, the input must be of finite length; even for algorithms that operate on the fly, for example algorithms that produce an output sample for each new input sample, an implicit limitation on the input data size is imposed by the necessarily limited life span of the processing device.(4)
This limitation was all too apparent in our attempts to plot infinite sequences as shown in Figure 2 point 1 or 2 point 2: what the diagrams show, in fact, is just a meaningful and representative portion of the signals; as for the rest, the analytical description remains the only reference.
When a discrete-time signal admits no closed-form representation, as is basically always the case with real-world signals, its finite time support arises naturally because of the finite time spent recording the signal: every piece of music has a beginning and an end, and so did every phone conversation.
In the case of the sequence representing the Dow Jones index, for instance, we basically cheated since the index did not even exist for years before 1884, and its value tomorrow is certainly not known – so that the signal is not really a sequence, although it can be arbitrarily extended to one.
More importantly (and more often), the finiteness of a discrete-time signal is explicitly imposed by design since we are interested in concentrating our processing efforts on a small portion of an otherwise longer signal; in a speech recognition system, for instance, the practice is to cut up a speech signal into small segments and try to identify the phonemes associated to each one of them.(5)
A special case is that of periodic signals; even though these are bona-fide infinite sequences, it is clear that all information about them is contained in just one period.
By describing one period (graphically or otherwise), we are, in fact, providing a full description of the sequence.
The complete taxonomy of the discrete-time signals used in the book is the subject of the next Sections ans is summarized in Table 2 point 1.
As we just mentioned, a finite-length discrete-time signal of length N are just a collection of N complex values.
To introduce a point that will reappear throughout the book, a finite-length signal of length N is entirely equivalent to a vector in CN.
This equivalence is of immense import since all the tools of linear algebra become readily available for describing and manipulating finite-length signals.
We can represent an N-point finite-length signal using the standard vector notation.
Note the transpose operator, which declares x as a column vector; this is the customary practice in the case of complex-valued vectors.
Alternatively, we can (and often will) use a notation that mimics the one used for proper sequences.
Here we must remember that, although we use the notation x[n], x[n] is not defined for values outside its support, for example for n is smaller than 0 or for n ≥ N. Note that we can always obtain a finite-length signal from an infinite sequence by simply dropping the sequence values outside the indices of interest.
Vector and sequence notations are equivalent and will be used interchangeably according to convenience; in general, the vector notation is useful when we want to stress the algorithmic or geometric nature of certain signal processing operations.
The sequence notation is useful in stressing the algebraic structure of signal processing.
Finite-length signals are extremely convenient entities: their energy is always and, as a consequence, no stability issues arise in processing.
From the computational point of view, they are not only a necessity but often the cornerstone of very efficient algorithmic design (as we will see, for instance, in the case of the FFT); one could say that all “practical” signal processing lives in CN.
It would be extremely awkward, however, to develop the whole theory of signal processing only in terms of finite-length signals; the asymptotic behavior of algorithms and transformations for infinite sequences is also extremely valuable since a stability result proven for a general sequence will hold for all finite-length signals too.
Furthermore, the notational flexibility which infinite sequences derive from their function-like definition is extremely practical from the point of view of the notation.
We can immediately recognize and understand the expression x[n - k] as a k-point shift of a sequence x[n]; but, in the case of finite-support signals, how are we to define such a shift?
We would have to explicitly take into account the finiteness of the signal and the associated “border effects”, for example the behavior of operations at the edges of the signal.
For this reason, in most derivations which involve finite-length signal, these signals will be embedded into proper sequences, as we will see shortly.
Aperiodic Signals.
The most general type of discrete-time signal is represented by a generic infinite complex sequence.
Although, as previously mentioned, they lie beyond our processing and storage capabilities, they are invaluably useful as a generalization in the limit.
As such, they must be handled with some care when it comes to their properties.
We will see shortly that two of the most important properties of infinite sequences concern their summability: this can take the form of either absolute summability (stronger condition) or square summability (weaker condition, corresponding to finite energy).
Periodic Signals.
A periodic sequence with period N is one for which.
The tilde notation x[n] will be used whenever we need to explicitly stress a periodic behavior.
Clearly an N-periodic sequence is completely defined by its N values over a period; that is, a periodic sequence “carries no more information” than a finite-length signal of length N.
Periodic Extensions.
Periodic sequences are infinite in length, and yet their information is contained within a finite number of samples.
In this sense, periodic sequences represent a first bridge between finite-length signals and infinite sequences.
In order to “embed” a finite-length signal x[n], n equal to 0,…,N - 1 into a sequence, we can take its periodized version: this is called the periodic extension of the finite length signal x[n].
This type of extension is the “natural” one in many contexts, for reasons which will be apparent later when we study the frequency-domain representation of discrete-time signals.
Note that now an arbitrary shift of the periodic sequence corresponds to the periodization of a circular shift of the original finite-length signal.
A circular shift by k ∈ Z is easily visualized by imagining a shift register; if we are shifting towards the right (k is larger than 0), the values which pop out of the rightmost end of the shift register are pushed back in at the other end.(6)
The relationship between the circular shift of a finite-length signal and the linear shift of its periodic extension is depicted in Figure 2 point 6.
Finally, the energy of a periodic extension becomes infinite, while its power is simply the energy of the finite-length original signal scaled by 1 over N.
Finite-Support Signals.
An infinite discrete-time sequence Â-x[n] is said to have finite support if its values are zero for all indices outside of an interval; that is, there exist N and M ∈ Z such that.
Note that, although Â¯x[n] is an infinite sequence, the knowledge of M and of the N nonzero values of the sequence completely specifies the entire signal.
This suggests another approach to embedding a finite-length signal x[n], n equal to 0,…,N - 1, into a sequence, for example where we have chosen M equal to 0 (but any other choice of M could be used).
Note that, here, in contrast to the the periodic extension of x[n], we are actually adding arbitrary information in the form of the zero values outside of the support interval.
This is not without consequences, as we will see in the following Chapters.
In general, we will use the bar notation Â¯x[n] for sequences defined as the finite support extension of a finite-length signal.
Note that, now, the shift of the finite-support extension gives rise to a zero-padded shift of the signal locations between M and M plus N - 1; the dynamics of the shift are shown in Figure 2 point 7.
Example 2 point 1: Discrete-time in the Far West
The fact that the “fastest” digital frequency is 2pi can be readily appreciated in old western movies.
In classic scenarios there is always a sequence showing a stagecoach leaving town.
We can see the spoked wagon wheels starting to turn forward faster and faster, then stop and then starting to turn backwards.
In fact, each frame in the movie is a snapshot of a spinning disk with increasing angular velocity.
The filming process therefore transforms the wheel’s movement into a sequence of discrete-time positions depicting a circular motion with increasing frequency.
When the speed of the wheel is such that the time between frames covers a full revolution, the wheel appears to be stationary: this corresponds to the fact that the maximum digital frequency ω equal to 2pi is undistinguishable from the slowest frequency ω equal to 0. As the speed of the real wheel increases further, the wheel on film starts to move backwards, which corresponds to a negative digital frequency.
This is because a displacement of 2pi plus α between successive frames is interpreted by the brain as a negative displacement of α: our intuition always privileges the most economical explanation of natural phenomena.
Example 2 point 2: Building periodic signals
Given a discrete-time signal x[n] and an integer N is larger than 0 we can always formally write as the formula.
The signal ỹ[n], if it exists, is an N-periodic sequence.
The periodic signal ỹ[n] is “manufactured” by superimposing infinite copies of the original signal x[n] spaced N samples apart.
We can distinguish three cases.
If x[n] is finite-support and N is bigger than the size of the support, then the copies in the sum do not overlap; in the limit, if N is exactly equal to the size of the support then ỹ[n] corresponds to the periodic extension of x[n] considered as a finite-length signal.
If x[n] is finite-support and N is smaller than the size of the support then the copies in the sum do overlap; for each n, the value of ỹ[n] is be the sum of at most a finite number of terms.
If x[n] has infinite support, then each value of ỹ[n] is be the sum of an infinite number of terms.
Existence of ỹ[n] depends on the properties of x[n].
The first two cases are illustrated in Figure 2 point 8.
In practice, the periodization of short sequences is an effective method to synthesize the sound of string instruments such as a guitar or a piano; used in conjunction with simple filters, the technique is known as the Karplus-Strong algorithm.
As an example of the last type, take for instance the signal x[n] equal to α-n u[n].
The periodization formula leads to the formula since.
Now write n equal to mN plus i with m equal to n over N and i equal to n mod N. We have which exists and is finite for |α| is larger than 1; for these values of α we have the formula which is indeed N-periodic.
An example is shown in Figure 2 point 9.
For more discussion on discrete-time signals, see Discrete-Time Signal Processing, by A. V. Oppenheim and R. W. Schafer (Prentice-Hall, last edition in 1999), in particular Chapter 2.
Other books of interest include: B. Porat, A Course in Digital Signal Processing (Wiley, 1997) and R. L. Allen and D. W. Mills’ Signal Analysis (IEEE Press, 2004).
Signals and Hilbert Spaces
In the 17th century, algebra and geometry started to interact in a fruitful synergy which continues to the present day.
Descartes’s original idea of translating geometric constructs into algebraic form spurred a new line of attack in mathematics; soon, a series of astonishing results was produced for a number of problems which had long defied geometrical solutions (such as, famously, the trisection of the angle).
It also spearheaded the notion of vector space, in which a geometrical point could be represented as an n-tuple of coordinates; this, in turn, readily evolved into the theory of linear algebra.
Later, the concept proved useful in the opposite direction: many algebraic problems could benefit from our innate geometrical intuition once they were cast in vector form; from the easy three-dimensional visualization of concepts such as distance and orthogonality, more complex algebraic constructs could be brought within the realm of intuition.
The final leap of imagination came with the realization that the concept of vector space could be applied to much more abstract entities such as infinite-dimensional objects and functions.
In so doing, however, spatial intuition could be of limited help and for this reason, the notion of vector space had to be formalized in much more rigorous terms; we will see that the definition of Hilbert space is one such formalization.
Most of the signal processing theory which in this book can be usefully cast in terms of vector notation and the advantages of this approach are exactly what we have just delineated.
Firstly of all, all the standard machinery of linear algebra becomes immediately available and applicable; this greatly simplifies the formalism used in the mathematical proofs which will follow and, at the same time, it fosters a good intuition with respect to the underlying principles which are being put in place.
Furthermore, the vector notation creates a frame of thought which seamlessly links the more abstract results involving infinite sequences to the algorithmic reality involving finite-length signals.
Finally, on the practical side, vector notation is the standard paradigm for numerical analysis packages such as Matlab; signal processing algorithms expressed in vector notation translate to working code with very little effort.
In the previous Chapter, we established the basic notation for the different classes of discrete-time signals which we will encounter time and again in the rest of the book and we hinted at the fact that a tight correspondence can be established between the concept of signal and that of vector space.
In this Chapter, we pursue this link further, firstly by reviewing the familiar Euclidean space in finite dimensions and then by extending the concept of vector space to infinite-dimensional Hilbert spaces.
Euclidean geometry is a straightforward formalization of our spatial sensory experience; hence its cornerstone role in developing a basic intuition for vector spaces.
Everybody is (or should be) familiar with Euclidean geometry and the natural “physical” spaces like R2 (the plane) and R3 (the three-dimensional space).
The notion of distance is clear; orthogonality is intuitive and maps to the idea of a “right angle”.
Even a more abstract concept such as that of basis is quite easy to contemplate (the standard coordinate concepts of latitude, longitude and height, which correspond to the three orthogonal axes in R3).
Unfortunately, immediate spatial intuition fails us for higher dimensions (for example for RN with N is larger than 3), yet the basic concepts introduced for R3 generalize easily to RN so that it is easier to state such concepts for the higher-dimensional case and specialize them with examples for N equal to 2 or N equal to 3.
These notions, ultimately, will be generalized even further to more abstract types of vector spaces.
For the moment, let us review the properties of RN, the N-dimensional Euclidean space.
Vectors and Notation.
A point in RN is specified by an N-tuple of coordinates.
We call this set of coordinates a vector and the N-tuple will be denoted synthetically by the symbol x; coordinates are usually expressed with respect to a “standard” orthonormal basis.(2)
The vector for example the null vector, is considered the origin of the coordinate system.
The generic n-th element in vector x is indicated by the subscript xn.
In the following we will often consider a set of M arbitrarily chosen vectors in RN and this set will be indicated by the notation {x(k)}kequal to0 …M-1.
Each vector in the set is indexed by the superscript (k).
The n-th element of the k-th vector in the set is indicated by the notation.
Inner Product.
The inner product between two vectors x,y ∈ RN is defined as.
We say that x and y are orthogonal when their inner product is zero:
Norm.
The norm of a vector is defined in terms of the inner product as.
It is easy to visualize geometrically that the norm of a vector corresponds to its length, for example to the distance between the origin and the point identified by the vector’s coordinates.
A remarkable property linking the inner product and the norm is the Cauchy-Schwarz inequality (the proof of which is nontrivial); given x,y ∈ RN we can state that
Distance.
The concept of norm is used to introduce the notion of Euclidean distance between two vectors x and y:
From this, we can easily derive the Pythagorean theorem for N dimensions: if two vectors are orthogonal, x ⊥ y, and we consider the sum vector z equal to x plus y, we have this formula.
The above properties are graphically shown in Figure 3 point 1 for R2.
Bases.
Consider a set of M arbitrarily chosen vectors in RN: {x(k)}kequal to0…M-1.
Given such a set, a key question is that of completeness: can any vector in RN be written as a linear combination of vectors from the set?
In other words, we ask ourselves whether, for any z ∈ RN, we can find a set of M coefficients αk ∈ R such that z can be expressed as the function.
Clearly, M needs to be greater or equal to N, but what conditions does a set of vectors {x(k)}kequal to0…M-1 need to satisfy so that (3 point 6) holds for any z ∈ RN?
There needs to be a set of M vectors that span RN, and it can be shown that this is equivalent to saying that the set must contain at least N linearly independent vectors.
In turn, N vectors {y(k)}kequal to0…N-1 are linearly independent if the equation is satisfied only when all the βk’s are zero.
A set of N linearly independent vectors for RN is called a basis and, amongst bases, the ones with mutually orthogonal vectors of norm equal to one are called orthonormal bases.
For an orthonormal basis {y(k)} we therefore have the formula.
Figure 3 point 2 reviews the above concepts in low dimensions.
The standard orthonormal basis for RN is the canonical basis {δ(k)}kequal to0…N-1 with the formula.
The orthonormality of such a set is immediately apparent.
Another important orthonormal basis for RN is the normalized Fourier basis {w(k)}kequal to0…N-1 for which
The orthonormality of the basis will be proved in the next Chapter.
The purpose of the previous Section was to briefly review the elementary notions and spatial intuitions of Euclidean geometry.
A thorough study of vectors in RN and CN is the subject of linear algebra; yet, the idea of vectors, orthogonality and bases is much more general, the basic ingredients being an inner product and the use of a square norm as in (3 point 3).
While the analogy between vectors in CN and length-N signal is readily apparent, the question now hinges on how we are to proceed in order to generalize the above concepts to the class of infinite sequences.
Intuitively, for instance, we can let N grow to infinity and obtain C∞ as the Euclidean space for infinite sequences; in this case, however, much care must be exercised with expressions such as (3 point 1) and (3 point 3) which can diverge for sequences as simple as x[n] equal to 1 for all n.
In fact, the proper generalization of CN to an infinite number of dimensions is in the form of a particular vector space called Hilbert space; the structure of this kind of vector space imposes a set of constraints on its elements so that divergence problems, such as the one we just mentioned, no longer bother us.
When we embed infinite sequences into a Hilbert space, these constraints translate to the condition that the corresponding signals have finite energy – which is a mild and reasonable requirement.
Finally, it is important to remember that the notion of Hilbert space is applicable to much more general vector spaces than CN; for instance, we can easily consider spaces of functions over an interval or over the real line.
This generality is actually the cornerstone of a branch of mathematics called functional analysis.
While we will not follow in great depth these kinds of generalizations, we will certainly point out a few of them along the way.
The space of square integrable functions, for instance, will turn out to be a marvelous tool a few Chapters from now when, finally, the link between continuous—and discrete—time signals will be explored in detail.
A word of caution: we are now starting to operate in a world of complete abstraction.
Here a vector is an entity per se and, while analogies and examples in terms of Euclidean geometry can be useful visually, they are, by no means, exhaustive.
In other words: vectors are no longer just N-tuples of numbers; they can be anything.
This said, a Hilbert space can be defined in incremental steps starting from a general notion of vector space and by supplementing this space with two additional features: the existence of an inner product and the property of completeness.
Vector Space.
Consider a set of vectors V and a set of scalars S (which can be either R or C for our purposes).
A vector space H(V,S) is completely defined by the existence of a vector addition operation and a scalar multiplication operation which satisfy the following properties for any x,y,z, ∈ V and any α,β ∈ S.
Inner Product Space.
What we have so far is the simplest type of vector space; the next ingredient which we consider is the inner product which is essential to build a notion of distance between elements in a vector space.
A vector space with an inner product is called an inner product space.
An inner product for H(V,S) is a function from V × V to S which satisfies the following properties for any x,y,z, ∈ V.
From this definition of the inner product, a series of additional definitions and properties can be derived: first of all, orthogonality between two vectors is defined with respect to the inner product, and we say that the non-zero vectors x and y are orthogonal, or x ⊥ y , if and only if the formula.
From the definition of an inner product, we can define the norm of a vector as (we will omit from now on the subscript 2 from the norme symbol).
In turn, the norm satisfies the Cauchy-Schwartz inequality.
The norm also satisfies the triangle inequality.
For orthogonal vectors, the triangle inequality becomes the famous Pythagorean theorem.
Hilbert Space.
A vector space H(V,S) equipped with an inner product is called an inner product space.
To obtain a Hilbert space, we need completeness.
This is a slightly more technical notion, which essentially implies that convergent sequences of vectors in V have a limit that is also in V .
To gain intuition, think of the set of rational numbers Q versus the set of real numbers R. The set of rational numbers is incomplete, because there are convergent sequences in Q which converge to irrational numbers.
The set of real numbers contains these irrational numbers, and is in that sense the completion of Q. Completeness is usually hard to prove in the case of infinite-dimensional spaces; in the following it will be tacitly assumed and the interested reader can easily find the relevant proofs in advanced analysis textbooks.
Finally, we will also only consider separate Hilbert spaces, which are the ones that admit orthonormal bases.
Finite Euclidean Spaces.
The vector space CN, with the “natural” definition for the sum of two vectors z equal to x plus y as and the definition of the inner product as is a Hilbert space.
Polynomial Functions.
An example of “functional” Hilbert space is the vector space PN([0,1]) of polynomial functions on the interval [0,1] with maximum degree N. It is a good exercise to show that P∞([0,1]) is not complete; consider for instance the sequence of polynomials
Square Summable Functions.
Another interesting example of functional Hilbert space is the space of square integrable functions over a finite interval.
For instance, L2([-pi,pi]) is the space of real or complex functions on the interval [-pi,pi] which have a finite norm.
The inner product over L2([-pi,pi]) is defined as so that the norm of f(t) is the formula.
For f(t) to belong to L2([-pi,pi]) it must be ∥f∥ is smaller than ∞.
The inner product is a fundamental tool in a vector space since it allows us to introduce a notion of distance between vectors.
The key intuition about this is a typical instance in which a geometric construct helps us to generalize a basic idea to much more abstract scenarios.
Indeed, take the simple Euclidean space RN and a given vector x; for any vector y ∈ RN the inner product is smaller than x,y is larger than is the measure of the orthogonal projection of y over x.
We know that the orthogonal projection defines the point on x which is closest to y and therefore this indicates how well we can approximate y by a simple scaling of x.
To illustrate this, it should be noted that where θ is the angle between the two vectors (you can work out the expression in R2 to easily convince yourself of this; the result generalizes to any other dimension).
Clearly, if the vectors are orthogonal, the cosine is zero and no approximation is possible.
Since the inner product is dependent on the angular separation between the vectors, it represents a first rough measure of similarity between x and y; in broad terms, it provides a measure of the difference in shape between vectors.
In the context of signal processing, this is particularly relevant since most of the times, we are interested in the difference in shape” between signals.
As we have said before, discrete-time signals are vectors; the computation of their inner product will assume different names according to the processing context in which we find ourselves: it will be called filtering, when we are trying to approximate or modify a signal or it will be called correlation when we are trying to detect one particular signal amongst many.
Yet, in all cases, it will still be an inner product, for example a qualitative measure of similarity between vectors.
In particular, the concept of orthogonality between signals implies that the signals are perfectly distinguishable or, in other words, that their shape is completely different.
The need for a quantitative measure of similarity in some applications calls for the introduction of the Euclidean distance, which is derived from the inner product as function.
In particular, for CN the Euclidean distance is defined by the expression: whereas for L2([-pi,pi]) we have the formula.
In the practice of signal processing, the Euclidean distance is referred to as the root mean square error;(4) this is a global, quantitative goodness-of-fit measure when trying to approximate signal y with x.
Incidentally, there are other types of distance measures which do not rely on a notion of inner product; for example in CN we could define
This distance is based on the supremum norm and is usually indicated by; however, it can be shown that there is no inner product from which this norm can be derived and therefore no Hilbert space can be constructed where is the natural norm.
Nonetheless, this norm will reappear later, in the context of optimal filter design.
Now that we have defined the properties of Hilbert space, it is only natural to start looking at the consequent inner structure of such a space.
The best way to do so is by introducing the concept of basis.
You can think of a basis as the “skeleton” of a vector space, for example a structure which holds everything together; yet, this skeleton is flexible and we can twist it, stretch it and rotate it in order to highlight some particular structure of the space and facilitate access to particular information that we may be seeking.
All this is accomplished by a linear transformation called a change of basis; to anticipate the topic of the next Chapter, we will see shortly that the Fourier transform is an instance of basis change.
Sometimes, we are interested in exploring in more detail a specific subset of a given vector space; this is accomplished via the concept of subspace.
A subspace is, as the name implies, a restricted region of the global space, with the additional property that it is closed under the usual vector operations.
This implies that, once in a subspace, we can operate freely without ever leaving its confines; just like a full-fledged space, a subspace has its own skeleton (for example the basis) and, again, we can exploit the properties of this basis to highlight the features that interest us.
Assume H(V,S) is a Hilbert space, with V a vector space and S a set of scalars (for example C).
Subspace.
A subspace of V is defined as a subset P ⊆ V that satisfies the following properties:
Closure under addition, for example
Closure under scalar multiplication, for example
Clearly, V is a subspace of itself.
Span.
Given an arbitrary set of M vectors W equal to {x(m)}mequal to0,1,…,M-1, the span of these vectors is defined as the span of W is the set of all possible linear combinations of the vectors in W. The set of vectors W is called linearly independent if the following holds Basis.
A set of K vectors W equal to {x(k)}kequal to0,1,…,K-1 from a subspace P is a basis for that subspace if.
The set W is linearly independent.
Its span covers P, for example span(W) equal to P.
The last statement affirms that any y ∈ P can be written as a linear combination of {x(k)}kequal to0,1,…,K-1 or that, for all y ∈ P, there exist K coefficients αk such that which is equivalently expressed by saying that the set W is complete in P.
Orthogonal/Orthonormal Basis.
An orthonormal basis for a subspace P is a set of K basis vectors W equal to {x(k)}kequal to0,1,…,K-1 for which which means orthogonality across vectors and unit norm.
Sometimes, the set of vectors can be orthogonal but not normal (for example the norm of the vectors is not unitary).
This is not a problem provided that we remember to include the appropriate normalization factors in the analysis and/or synthesis formulas.
Alternatively, an lineary idependent set of vectors can be orthonormalized via the Gramm-Schmidt procedure, which can be found in any linear algebra textbook.
Among all bases, orthonormal bases are the most “beautiful” in a way because of their structure and their properties.
One of the most important properties for finite-dimensional spaces is the following:
A set of N orthogonal vectors in an N-dimensional subspace is a basis for the subspace.
In other words, in finite dimensions, once we find a full set of orthogonal vectors, we are sure that the set spans the space.
Let W equal to {x(k)}kequal to0,1,…,K-1 be an orthonormal basis for a (sub)space P. Then the following properties (all of which are easily verified) hold:
Analysis Formula.
The coefficients in the linear combination (3 point 40) are obtained simply as the function.
The coefficients {αk} are called the Fourier coefficients(5) of the orthonormal expansion of y with respect to the basis W and (3 point 42) is called the Fourier analysis formula; conversely, Equation (3 point 40) is called the synthesis formula.
Parseval’s Identity For an orthonormal basis, there is a norm conservation property given by Parseval’s identity.
For physical quantities, the norm is dimensionally equivalent to a measure of energy; accordingly, Parseval’s identity is also known as the energy conservation formula.
Bessel’s Inequality.
The generalization of Parseval’s equality is Bessel’s inequality.
In our subspace P, consider a set of L orthonormal vectors G ⊂ P (a set which is not necessarily a basis since it may be L is smaller than K), with G equal to {g(l)}lequal to1,2,…L-1; then the norm of any vector y ∈ P is lower bounded as and the lower bound is reached for all y if and only if the system G is complete, that is, if it is an orthonormal basis for P.
Best Approximations.
Assume P is a subspace of V ; if we try to approximate a vector y ∈ V by a linear combination of basis vectors from the subspace P, then we are led to the concepts of least squares approximations and orthogonal projections.
First of all, we define the best linear approximation ˆy ∈ P of a general vector y ∈ V to be the approximation which minimizes the norm ∥y - ˆy∥ .
Such approximation is easily obtained by projecting y onto an orthonormal basis for P, as shown in Figure 3 point 3.
With W as our usual orthonormal basis for P, the projection is given by
Define the approximation error as the vector d equal to y -ˆy; it can be easily shown that:
The error is orthogonal to the approximation .
The approximation minimizes the error square norm,
This approximation with an orthonormal basis has a key property: it can be successively refined.
Assume you have the approximation with the first m terms of the orthonormal basis:
This is simply given by
While this seems obvious, it is actually a small miracle, since it does not hold for more general, non-orthonormal bases.
Considering the examples of 3 point 2 point 2, we have the following:
Finite Euclidean Spaces.
For the simplest case of Hilbert spaces, namely CN, orthonormal bases are also the most intuitive since they contain exactly N mutually orthogonal vectors of unit norm.
The classical example is the canonical basis {δ(k)}kequal to0…N-1 with but we will soon study more interesting bases such as the Fourier basis {w(k)}, for which
In CN, the analysis and synthesis formulas (3 point 42) and (3 point 40) take a particularly neat form.
For any set {x(k)} of N orthonormal vectors one can indeed arrange the conjugates of the basis vectors(6) as the successive rows of an N × N square matrix M so that each matrix element is the conjugate of the n-th element of the m-th basis vector:
M is called a change of basis matrix.
Given a vector y, the set of expansion coefficient {αk}kequal to0…N-1 can now be written itself as a vector(7) α ∈ CN.
Therefore, we can rewrite the analysis formula (3 point 42) in matrix-vector form and we have.
The reconstruction formula (3 point 40) for y from the expansion coefficients, becomes, in turn, where the superscript denotes the Hermitian transpose (transposition and conjugation of the matrix).
The previous equation shows that y is a linear combination of the columns of MH, which, in turn, are of course the vectors {x(k)}.
The orthogonality relation (3 point 49) takes the following forms:
Polynomial Functions.
A basis for PN([0,1]) is {xk}0≤kis smaller thanN.
This basis, however, is not an orthonormal basis.
It can be transformed to an orthonormal basis by a standard Gramm-Schmidt procedure; the basis vector thus obtained are called Legendre polynomials.
Square Summable Functions.
An orthonormal basis set for L2([-pi,pi]) is the set {(1 over √2-pi-)exp(jnt)} n∈Z.
This is actually the classic Fourier basis for functions on an interval.
Please note that, here, as opposed to the previous examples, the number of basis vectors is actually infinite.
The orthogonality of these basis vectors is easily verifiable; their completeness, however, is rather hard to prove and this, unfortunately, is very much the rule for all non-trivial, infinite-dimensional basis sets.
We are now in a position to formalize our intuitions so far, with respect to the equivalence between discrete-time signals and vector spaces, with a particularization for the three main classes of signals that we have introduced in the previous Chapter.
Note that in the following, we will liberally interchange the notations x and x[n] to denote a sequence as a vector embedded in its appropriate Hilbert space.
The correspondence between the class of finite-length, length-N signals and CN should be so immediate at this point that it does not need further comment.
As a reminder, the canonical basis is the canonical basis for CN.
The k-th canonical basis vector is often expressed in signal form as.
As we have seen, N-periodic signals are equivalent to length-N signals.
The space of N-periodic sequences is therefore isomorphic to CN.
In particular, the sum of two sequences considered as vectors is the standard pointwise sum for the elements:
The canonical basis for the space of N-periodic sequences is the canonical basis for CN, because of the isomorphism; in general, any basis for CN is also a basis for the space of N-periodic sequences.
Sometimes, however, we will also consider an explicitly periodized version of the basis.
For the canonical basis, in particular, the periodized basis is composed of N vectors of infinite-length.
Such a sequence is called a pulse train.
Note that here we are abandoning mathematical rigor, since the norm of each of these basis vectors is infinite; yet the pulse train, if handled with care, can be a useful tool in formal derivations.
In the case of infinite sequences, whose “natural” Euclidean space would appear to be C∞, the situation is rather delicate.
While the sum of two sequences can be defined in the usual way, by extending the sum for CN to C∞, care must be taken when evaluating the inner product.
We have already pointed out that the formula:
A way out of this impasse is to restrict ourselves to ℓ2(Z), the space of square summable sequences, for which
This is the space of choice for all the theoretical derivations involving infinite sequences.
Note that these sequences are often called “of finite energy”, since the square norm corresponds to the definition of energy as given in (2 point 19).
The canonical basis for ℓ2(Z) is simply the set {δ(k)}k∈Z; in signal form:
This is an infinite set, and actually an infinite set of linearly independent vectors, since has no solution for any k.
Note that, for an arbitrary signal x[n] the analysis formula gives so that the reconstruction formula becomes which is the reproducing formula (2 point 18).
The Fourier basis for ℓ2(Z) will be introduced and discussed at length in the next Chapter.
As a last remark, note that the space of all finite-support signals, which is clearly a subset of ℓ2(Z), does not form a Hilbert space.
Clearly, the space is closed under addition and scalar multiplication, and the canonical inner product is well behaved since all sequences have only a finite number of nonzero values.
However, the space is not complete; to clarify this, consider the following family of signals:
For k growing to infinity, the sequence of signals converges as yk[n] → y[n] equal to 1 over n for all n; while y[n] is indeed in ℓ2(Z), since y[n] is clearly not a finite-support signal.
A comprehensive review of linear algebra, which contains all the concepts of Hilbert spaces but in finite dimensions, is the classic by G. Strang, Linear Algebra and Its Applications (Brooks Cole, 2005).
For an introduction to Hilbert spaces, there are many mathematics books; we suggest N. Young, An Introduction to Hilbert Space (Cambridge University Press, 1988).
As an alternative, a more intuitive and engineering-motivated approach is in the classic book by D. G. Luenberger, Optimization by Vector Space Methods (Wiley, 1969).
Fourier Analysis
Fourier theory has a long history, from J. Fourier’s early work on the transmission of heat to recent results on non-harmonic Fourier series.
Fourier theory is a branch of harmonic analysis, and in that sense, a topic in pure and applied mathematics.
At the same time, because of its usefulness in practical applications, Fourier analysis is a key tool in several engineering branches, and in signal processing in particular.
Why is Fourier analysis so important?
To understand this, let us take a short philosophical detour.
Interesting signals are time-varying quantities: you can imagine, for instance, the voltage level at the output of a microphone or the measured level of the tide at a particular location; in all cases, the variation of a signal, over time, implies that a transfer of energy is happening somewhere, and ultimately this is what we want to study.
Now, a time-varying value which only increases over time is not only a physical impossibility but a recipe for disaster for whatever system is supposed to deal with it; fuses will blow, wires will melt and so on.
Oscillations, on the other hand, are nature’s and man’s way of keeping things in motion without trespassing all physical bounds; from Maxwell’s wave equation to the mechanics of the vocal cords, from the motion of an engine to the ebb and flow of the tide, oscillatory behavior is the recurring theme.
Sinusoidal oscillations are the purest form of such a constrained motion and, in a nutshell, Fourier’s immense contribution was to show that (at least mathematically) one could express any given phenomenon as the combined output of a number of sinusoidal “generators”.
Sinusoids have another remarkable property which justifies their ubiquitous presence.
Indeed, any linear time-invariant transformation of a sinusoid is a sinusoid at the same frequency: we express this by saying that sinusoidal oscillations are eigenfunctions of linear time-invariant systems.
This is a formidable tool for the analysis and design of signal processing structures, as we will see in much detail in the context of discrete-time filters.
The purpose of the present Chapter is to introduce and analyze some key results on Fourier series and Fourier transforms in the context of discrete-time signal processing.
It appears that, as we mentioned in the previous Chapter, the Fourier transform of a signal is a change of basis in an appropriate Hilbert space.
While this notion constitutes an extremely useful unifying framework, we also point out the peculiarities of its specialization within the different classes of signal.
In particular, for finite-length signals we highlight the eminently algebraic nature of the transform, which leads to efficient computational procedures; for infinite sequences, we will analyze some of its interesting mathematical subtleties.
The Fourier transform of a signal is an alternative representation of the data in the signal.
While a signal lives in the time domain,(1) its Fourier representation lives in the frequency domain.
We can move back and forth at will from one domain to the other using the direct and inverse Fourier operators, since these operators are invertible.
In this Chapter we study three types of Fourier transform which apply to the three main classes of signals that we have seen so far:
the Discrete Fourier Transform (DFT), which maps length-N signals into a set of N discrete frequency components;
the Discrete Fourier Series (DFS), which maps N- periodic sequences into a set of N discrete frequency components;
the Discrete-Time Fourier Transform (DTFT), which maps infinite sequences into the space of 2pi-periodic function of a real-valued argument.
The frequency representation of a signal (given by a set of coefficients in the case of the DFT and DFS and by a frequency distribution in the case of the DTFT) is called the spectrum.
The basic ingredient of all the Fourier transforms which follow, is the discrete-time complex exponential; this is a sequence of the form.
A complex exponential represents an oscillatory behavior; A ∈ R is the amplitude of the oscillation, ω is its frequency and φ is its initial phase.
Note that, actually, a discrete-time complex exponential sequence is not always a periodic sequence; it is periodic only if ω equal to 2pi(M over N) for some value of M,N ∈ Z.
The power of a complex exponential is equal to the average energy over a period, for example |A|2, irrespective of frequency.
Negative Frequencies?
In the introduction, we hinted at the fact that Fourier analysis allows us to decompose a physical phenomenon into oscillatory components.
However, it may seem odd, that we have chosen to use complex oscillation for the analysis of real-world signals.
It may seem even odder that these oscillations can have a negative frequency and that, as we will soon see in the context of the DTFT, the spectrum extends over to the negative axis.
The starting point in answering these legitimate questions is to recall that the use of complex exponentials is essentially a matter of convenience.
One could develop a complete theory of frequency analysis for real signals using only the basic trigonometric functions.
You may actually have noticed this if you are familiar with the Fourier Series of a real function; yet the notational overhead is undoubtedly heavy since it involves two separate sets of coefficients for the sine and cosine basis functions, plus a distinct term for the zero-order coefficient.
The use of complex exponentials elegantly unifies these separate series into a single complex-valued sequence.
Yet, one may ask again, what does it mean for the spectrum of a musical sound to be complex?
Simply put, the complex nature of the spectrum is a compact way of representing two concurrent pieces of information which uniquely define each spectral component: its frequency and its phase.
These two values form a two-element vector in R2 but, since R2 is isomorphic to C, we use complex numbers for their mathematical convenience.
With respect negative frequencies, one must first of all consider, yet again, a basic complex exponential sequence such as x[n] equal to exp(jωn).
We can visualize its evolution over discrete-time as a series of points on the unit circle in the complex plane.
At each step, the angle increases by ω, defining a counterclockwise circular motion.
It is easy to see that a complex exponential sequence of frequency -ω is just the same series of points with the difference that the points move clockwise instead; this is illustrated in detail in Figure 4 point 1.
If we decompose a real signal into complex exponentials, we will show that, for any given frequency value, the phases of the positive and negative components are always opposite in sign; as the two oscillations move in opposite directions along the unit circle, their complex part will always cancel out exactly, thus returning a purely real signal.
The final step in developing a comfortable feeling for complex oscillations comes from the realization that, in the synthesis of discrete-time signals (and especially in the case of communication systems) it is actually more convenient to work with complex-valued signals, themselves.
Although the transmitted signal of a device like an ADSL box is a real signal, the internal representation of the underlying sequences is complex; therefore complex oscillations become a necessity.
We now develop a Fourier representation for finite-length signals; to do so, we need to find a set of oscillatory signals of length N which contain a whole number of periods over their support.
We start by considering a family of finite-length sinusoidal signals (indexed by an integer k) of the form where all the ωk’s are distinct frequencies which fulfill our requirements.
To determine these frequency values, note that, in order for wk[n] to contain a whole number of periods over N samples, it must conform to formula which translates to formula.
The above equation has N distinct solutions which are the N roots of unity exp(j2pim over N), m equal to 0,…,N - 1; if we define the complex number then the family of N signals in (4 point 1) can be written as for each value of k equal to 0,…,N - 1.
We can represent these N signals as a set of vectors {w(k)}kequal to0,…,N-1 in CN with.
The real and imaginary parts of w(k) for N equal to 32 and for some values of k are plotted in Figures 4 point 2 to 4 point 5.
We can verify that {w(k)}kequal to0,…,N-1 is a set of N orthogonal vectors and therefore a basis for CN; indeed we have (noting that (WN-k)* equal to WNk):
In more compact notation we can therefore state that.
The vectors {w(k)}kequal to0,…,N-1 are the Fourier basis for CN, and therefore for the space of length-N signals.
It is immediately evident that this basis is not orthonormal, since ∥w(k)∥2 equal to N, but that it could be made orthonormal simply by scaling the basis vectors .
In signal processing practice, however, it is customary to keep the normalization factor explicit in the change of basis formulas; this is mostly due to computational reasons, as we will see later, but, for the sake of consistency with the mainstream literature, we will also follow this convention.
The Discrete Fourier Transform (DFT) analysis and synthesis formulas can now be easily expressed in the familiar matrix notation as in Section 3 point 3 point 3: define an N × N square matrix W by stacking the conjugate of the basis vectors, for example Wnk equal to exp(-j(2pi over N)nk) equal to WNnk; from this we can state, for all vectors x ∈ CN:
(note the normalization factor in the reconstruction formula).
Here, X is the set of Fourier coefficients in vector form, whose physical interpretation we will explore shortly.
Note that the DFT preserves the energy of the finite-length signal: indeed Parseval’s relation (3 point 43) becomes
(once again, note the explicit normalization factor).
It is very common in the literature to explicitly write out the inner products in (4 point 6) and (4 point 7); this is both for historical reasons and to underscore the highly structured form of this transformation which, as we will see, is the basis for very efficient computational procedures.
In detail, we have the analysis formula and the dual synthesis formula where we have used the standard convention of “lumping” the normalizing factor (1 over N) entirely within in the reconstruction sum (4 point 10).
To return to the physical interpretation of the DFT, what we have just obtained is the decomposition of a finite-length signal into a set of N sinusoidal components; the magnitude and initial phase of each oscillator are given by the coefficients X[k] in (4 point 9) (or, equivalently, by the vector elements Xk in (4 point 6)(3) ).
To stress the point again:
take an array of N complex sinusoidal generators;
set the frequency of the k-th generator to (2pi over N)k;
set the amplitude of the k-th generator to |X[k]|, for example to the magnitude of the k-th DFT coefficient;
set the phase of the k-th generator to ∡X[k], for example to the phase of the k-th DFT coefficient;
start the generators at the same time and sum their outputs.
The first N output values of this “machine” are exactly x[n].
If we look at this from the opposite end, each X[k] shows “how much” oscillatory behavior at frequency 2pi over k, is contained in the signal; this is consistent with the fact that an inner product is a measure of similarity.
The coefficients X[k] are referred to as the spectrum of the signal.
The square magnitude |X[k]|2 is a measure (up to a scale factor N) of the signal’s energy at the frequency (2pi over N)k; the coefficients X[k], therefore, show exactly how the global energy of the original signal is distributed in the frequency domain while Parseval’s equality (4 point 8) guarantees that the result is consistent.
The phase of each Fourier coefficient, indicated by ∡X[k], specifies the initial phase of each oscillator for the reconstruction formula, for example the relative alignment of each complex exponential at the onset of the signal.
While this does not influence the energy distribution in frequency, it does have a significant effect on the shape of the signal in the discrete-time domain as we will shortly see in more detail.
Some examples for signals in C64 are plotted in Figures 4 point 6–4 point 9. Figure 4 point 6 shows one of the simplest cases: indeed, the signal x[n] is a sinusoid whose frequency coincides with that of one of the basis vectors (precisely, to that of w(4)) and, as a consequence of the orthogonality of the basis, only X[4] and X[60] are nonzero (this can be easily verified by decomposing the sinusoid as the sum of two appropriate basis functions).
Figure 4 point 7 shows the same signal, but this time with a phase offset.
The magnitude DFT does not change, but the phase offset appears in the phase of the transform.
Figure 4 point 8 shows the transform of a sinusoid whose frequency does not coincide with any of the basis frequencies.
As a consequence, all of the basis vectors are needed to reconstruct the signal.
Clearly, the magnitude is larger for frequencies closer to hot of the original signal’s (6pi over 64 and 7pi over 64 in this case); yet, to reconstruct x[n] exactly, we need the contribution of the entire basis.
Finally, Figure 4 point 9 shows the DFT of a step signal.
It can be shown (with a few trigonometric manipulations) that the DFT of a step signal is where N is the length of the signal and M is the length of the step (in Figure 4 point 9 N equal to 64 and M equal to 5, for instance.)
Consider the reconstruction formula in (4 point 10); what happens if we let the index n roam outside of the [0,N - 1] interval?
Since WN(nplusiN)k equal to WNnk for all i ∈ Z, we note that x[n plus iN] equal to x[n].
In other words, the reconstruction formula in (4 point 10) implicitly defines a periodic sequence of period N.
This is the reason why, earlier, we stated that periodic sequences are the natural way to embed a finite-length signal into a sequence: their Fourier representation is formally identical.
This is not surprising since a) we have already established a correspondence between CN and CN and b) we are actually expressing a length-N sequence as a combination of N-periodic basis signals.
The Fourier representation of periodic sequences is called the Discrete Fourier Series (DFS), and its explicit analysis and synthesis formulas are the exact equivalent of (4 point 9) and (4 point 10), modified only with respect to the range of the indices.
We have already seen that in (4 point 10), the reconstruction formula, n is now in Z. Symmetrically, due to the N-periodicity of WNk, we can let the index k in (4 point 9) assume any value in Z too; this way, the DFS coefficients become an N-periodic sequence themselves and the DFS becomes a change of basis in 
CN using the definition of inner product given in Section (3 point 4 point 2) and the formal periodic basis for CN:
We now consider a Fourier representation for infinite non-periodic sequences.
From a purely mathematical point of view, the Discrete-Time Fourier Transform of a sequence x[n] is defined as the function.
The DTFT is therefore a complex-valued function of the real argument ω, and, as can be easily verified, it is periodic in ω with period 2pi.
The somewhat odd notation X(exp(jω)) is quite standard in the signal processing literature and offers several advantages:
it stresses the basic periodic nature of the transform since, obviously, exp(j(ω plus 2pi)) equal to exp(jω);
regardless of context, it immediately identifies a function as the Fourier transform of a discrete-time sequence: for exemple U(exp(jλ)) is just as readily recognizable;
it provides a convenient notational framework which unifies the Fourier transform and the z-transform (which we will see shortly).
The DTFT, when it exists, can be inverted via the integral as can be easily verified by substituting (4 point 13) into 4 point 14) and using.
In fact, due to the 2pi-periodicity of the DTFT, the integral in (4 point 14) can be computed over any 2pi-wide interval on the real line (for example between 0 and 2pi, for instance).
The relation between a sequence x[n] and its DTFT X(exp(jω)) will be indicated in the general case by.
While the DFT and DFS were signal transformations which involved only a finite number of quantities, both the infinite summation and the real-valued argument, appearing in the DTFT, can create an uneasiness which overshadows the conceptual similarities between the transforms.
In the following, we start by defining the mathematical properties of the DTFT and we try to build an intuitive feeling for this Fourier representation, both with respect to its physical interpretation and to its conformity to the “change of basis” framework, that we used for the DFT and DFS.
Mathematically, the DTFT is a transform operator which maps discrete-time sequences onto the space of 2pi-periodic functions.
Clearly, for the DTFT to exist, the sum in (4 point 13) must converge, for example the limit for M →∞ of the partial sum must exist and be finite.
Convergence of the partial sum in (4 point 15) is very easy to prove for absolutely summable sequences, that is for sequences satisfying since, according to the triangle inequality.
For this class of sequences it can also be proved that the convergence of XM(exp(jω)) to X(exp(jω)) is uniform and that X(exp(jω)) is continuous.
While absolute summability is a sufficient condition, it can be shown that the sum in (4 point 15) is convergent also for all square-summable sequences, for example for sequences whose energy is finite; this is very important to us with respect to the discussion in Section 3 point 4 point 3 where we defined the Hilbert space ℓ2(Z).
In the case of square summability only, however, the convergence of (4 point 15) is no longer uniform but takes place only in the mean-square sense, for example
Convergence in the mean square sense implies that, while the total energy of the error signal becomes zero, the pointwise values of the partial sum may never approach the values of the limit.
One manifestation of this odd behavior is called the Gibbs phenomenon, which has important consequences in our approach to filter design, as we will see later.
Furthermore, in the case of square-summable sequences, X(exp(jω)) is no longer guaranteed to be continuous.
As an example, consider the sequence.
Its DTFT can be computed as the sum(4).
The DTFT of this particular signal turns out to be real (we will see later that this is a consequence of the signal’s symmetry) and it is plotted in Figure 4 point 10.
When, as is very often the case, the DTFT is complex-valued, the usual way to represent it graphically takes the magnitude and the phase separately into account.
The DTFT is always a 2pi-periodic function and the standard convention is to plot the interval from -pi to pi.
Larger intervals can be considered if the periodicity needs to be made explicit; Figure 4 point 11, for instance, shows five full periods of the same function.
A way to gain some intuition about the structure of the DTFT formulas is to consider the DFS of periodic sequences with larger and larger periods.
Intuitively, as we look at the structure of the Fourier basis for the DFS, we can see that the number of basis vectors in (4 point 9) grows with the length N of the period and, consequently, the frequencies of the underlying complex exponentials become “denser” between 0 and 2pi.
We want to show that, in the limit, we end up with the reconstruction formula of the DTFT.
To do so, let us restrict ourselves to the domain of absolute summable sequences; for these sequences, we know that the sum in (4 point 13) exists.
Now, given an absolutely summable sequence x[n], we can always build an N-periodic sequence x[n] as for any value of N (see Example 2 point 2); this is guaranteed by the fact that the above sum converges for all n ∈ Z (because of the absolute summability of x[n]) so that all values of ˜x[n] are finite.
Clearly, there is overlap between successive copies of x[n]; the intuition, however, is the following: since in the end we will consider very large values for N and since x[n], because of absolute summability, decays rather fast with n, the resulting overlap of “tails” will be negligible.
This can be expressed as
Now consider the DFS of ˜x[n]:
where in the last term we have used (4 point 20), interchanged the order of the summation and exploited the fact that exp(-j(2pi over N)(n plus iN)k) equal to exp(-j(2pi over N)nk).
We can see that, for every value of i in the outer sum, the argument of the inner sum varies between iN and iN plus N - 1, for example non-overlapping intervals, so that the double summation can be simplified as and therefore.
This already gives us a noteworthy piece of intuition: the DFS coefficients for the periodized signal are a discrete set of values of its DTFT (here considered solely as a formal operator) computed at multiples of 2pi over N.
As N grows, the spacing between these frequency intervals narrows more and more so that, in the limit, the DFS converges to the DTFT.
To check that this assertion is consistent, we can now write the DFS reconstruction formula using the DFS values given to us by inserting (4 point 23) in (4 point 10):
By defining Δ equal to (2pi over N), we can rewrite the above expression as and the summation is easily recognized as the Riemann sum with step Δ approximating the integral of f(ω) equal to X(ejω)ejωn between 0 and 2pi.
As N goes to infinity (and therefore ˜x[n] → x[n]), we can therefore write which is indeed the DTFT reconstruction formula (4 point 14).
We now show that, if we are willing to sacrifice mathematical rigor, the DTFT can be cast in the same conceptual framework we used for the DFT and DFS, namely as a basis change in a vector space.
The following formulas are to be taken as nothing more than a set of purely symbolic derivations, since the mathematical hypotheses under which the results are well defined are far from obvious and are completely hidden by the formalism.
It is only fair to say, however, that the following expressions represent a very handy and intuitive toolbox to grasp the essence of the duality between the discrete-time and the frequency domains and that they can be put to use very effectively to derive quick results when manipulating sequences.
One way of interpreting Equation (4 point 13) is to see that, for any given value ω0, the corresponding value of the DTFT is the inner product in ℓ2(Z) of the sequence x[n] with the sequence exp(jω0n); formally, at least, we are still performing a projection in a vector space akin to C∞:
Here, however, the set of “basis vectors” {exp(jωn)}ω∈R is indexed by the real variable ω and is therefore uncountable.
This uncountability is mirrored in the inversion formula (4 point 14), in which the usual summation is replaced by an integral; in fact, the DTFT operator maps ℓ2(Z) onto L2([-pi,pi]) which is a space of 2pi-periodic, square integrable functions.
This interpretation preserves the physical meaning given to the inner products in (4 point 13) as a way to measure the frequency content of the signal at a given frequency; in this case the number of oscillators is infinite and their frequency separation becomes infinitesimally small.
To complete the picture of the DTFT as a change of basis, we want to show that, at least formally, the set {exp(jωn)}ω∈R constitutes an orthogonal “basis” for ℓ2(Z).(6)
In order to do so, we need to introduce a quirky mathematical entity called the Dirac delta functional; this is defined in an implicit way by the following formula where f(t) is an arbitrary integrable function on the real line; in particular
While no ordinary function satisfies the above equation, δ(t) can be interpreted as shorthand for a limiting operation.
Consider, for instance, the family of parametric functions(7) which are plotted in Figure 4 point 12.
For any continuous function f(t) we can write where we have used the Mean Value theorem.
Now, as k goes to infinity, the integral converges to f(0); hence we can say that the limit of the series of functions rk(t) converges then to the Dirac delta.
As already stated, the delta cannot be considered as a proper function, so the expression δ(t) outside of an integral sign has no mathematical meaning; it is customary however to associate an “idea” of function to the delta and we can think of it as being undefined for t⁄equal to0 and to have a value of ∞ at t equal to 0. This interpretation, together with (4 point 27), defines the so-called sifting property of the Dirac delta; this property allows us to write (outside of the integral sign).
The physical interpretation of the Dirac delta is related to quantities expressed as continuous distributions for which the most familiar example is probably that of a probability distribution (pdf).
These functions represent a value which makes physical sense only over an interval of nonzero measure; the punctual value of a distribution is only an abstraction.
The Dirac delta is the operator that extracts this punctual value from a distribution, in a sense capturing the essence of considering smaller and smaller observation intervals.
To see how the Dirac delta applies to our basis expansion, note that equation (4 point 27) is formally identical to an inner product over the space of functions on the real line; by using the definition of such an inner product we can therefore write which is, in turn, formally identical to the reconstruction formula of Section 3 point 4 point 3.
In reality, we are interested in the space of 2pi-periodic functions, since that is where DTFTs live; this is easily accomplished by building a 2pi-periodic version of the delta as where the leading 2pi factor is for later convenience.
The resulting object is called a pulse train, similarly to what we built for the case of periodic sequences in ˜C-N.
Using the pulse train and given any 2pi-periodic function f(ω), the reconstruction formula (4 point 32) becomes for any σ ∈ R.
Now that we have the delta notation in place, we are ready to start.
First of all, we show the formal orthogonality of the basis functions {exp(jωn)}ω∈R.
We can write.
The left-hand side of this equation has the exact form of the DTFT reconstruction formula (4 point 14); hence we have found the fundamental relationship.
Now, the DTFT of a complex exponential exp(jσn) is, in our change of basis interpretation, simply the inner product ⟨exp(jωn),exp(jσn)⟩; because of (4 point 36) we can therefore express this as which is formally equivalent to the orthogonality relation in (4 point 5).
We now recall for the last time that the delta notation subsumes a limiting operation: the DTFT pair (4 point 36) should be interpreted as shorthand for the limit of the partial sums
(where we have chosen ω0 equal to 0 for the sake of example).
Figure 4 point 13 plots |sk(ω)| for increasing values of k (we show only the [-pi,pi] interval, although of course the functions are 2pi-periodic).
The family of functions sk(ω) is exactly equivalent to the family of functions rk(t) we saw in (4 point 29); they too become increasingly narrow while keeping a constant area (which turns out to be 2pi).
That is why we can simply state that.
From (4 point 36) we can easily obtain other interesting results: by setting ω0 equal to 0 and by exploiting the linearity of the DTFT operator, we can derive the DTFT of a constant sequence:
or, using Euler’s formulas, the DTFTs of sinusoidal functions.
As we can see from the above examples, we are defining the DTFT for sequences which are not even square-summable; again, these transforms are purely a notational formalism used to capture a behavior, in the limit, as we showed before.
We can now show that, thanks to the delta formalism, the DTFT is the most general type of Fourier transform for discrete-time signals.
Consider a length-N signal x[n] and its N DFT coefficients X[k]; consider also the sequences obtained from x[n] either by periodization or by building a finite-support sequence.
The computation of the DTFTs of these sequences highlights the relationships linking the three types of discrete-time transforms that we have seen so far.
Periodic Sequences.
Given a length-N signal x[n], n equal to 0,…,N - 1, consider the associated N-periodic sequence ˜x[n] equal to x[n mod N] and its N DFS coefficients X[k].
If we try to write the analysis DTFT formula for ˜x[n] we have.
Now we recognize in the last term important to recognize the last terms of (4 point 42) as the DTFT of a complex exponential of frequency (2pi over N)k; we can therefore write which is the relationship between the DTFT and the DFS.
If we restrict ourselves to the [-pi,pi] interval, we can see that the DTFT of a periodic sequence is a series of regularly spaced deltas placed at the N roots of unity and whose amplitude is proportional to the DFS coefficients of the sequence.
In other words, the DTFT is uniquely determined by the DFS and vice versa.
Finite-Support Sequences.
Given a length-N signal x[n], n equal to 0,…, N - 1 and its N DFT coefficients X[k], consider the associated finite-support sequence from which we can easily derive the DTFT of Ax as.
What the above expression means, is that the DTFT of the finite support sequence xÂ¯[n] is again uniquely defined by the N DFT coefficients of the finite-length signal x[n] and it can be obtained by a type of Lagrangian interpolation.
As in the previous case, the values of DTFT at the roots of unity are equal to the DFT coefficients; note, however, that the transform of a finite support sequence is very different from the DTFT of a periodized sequence.
The latter, in accordance with the definition of the Dirac delta, is defined only in the limit and for a finite set of frequencies; the former is just a (smooth) interpolation of the DFT.
The DTFT possesses the following properties.
Symmetries and Structure.
The DTFT of a time-reversed sequence is while, for the complex conjugate of a sequence we have.
For the very important case of a real sequence x[n] ∈ R, property 4 point 46 implies that the DTFT is conjugate-symmetric:
which leads to the following special symmetries for real signals.
The magnitude of the DTFT is symmetric.
The phase of the DTFT is antisymmetric.
The real part of the DTFT is symmetric.
The imaginary part of the DTFT is antisymmetric.
Finally, if x[n] is real and symmetric, then the DTFT is real, while, for real antisymmetric signals we have that the DTFT is purely imaginary.
Linearity and Shifts.
The DTFT is a linear operator.
A shift in the discrete-time domain leads to multiplication by a phase term in the frequency domain:
while multiplication of the signal by a complex exponential (for example signal modulation by a complex “carrier” at frequency ω0) leads to which means that the spectrum is shifted by ω0.
This last result is known as the modulation theorem.
Energy Conservation.
The DTFT satisfies the Plancherel-Parseval equality:
or, using the respective definitions of inner product for ℓ2(Z) and L2([-pi,pi]):
(note the explicit normalization factor 1 over 2pi).
The above equality specializes into Parseval’s theorem as which establishes the conservation of energy property between the time and the frequency domains.
The DTFT properties we have just seen extend easily to the Fourier Transform of periodic signals.
The easiest way to obtain the particularizations which follow is to apply relationship (4 point 43) to the results of the previous Section.
Symmetries and Structure.
The DFS of a time-reversed sequence is while, for the complex conjugate of a sequence we have.
For real periodic sequences, the following special symmetries hold (see (4 point 47)–(4 point 53)).
Finally, if ˜x[n] is real and symmetric, then the DFS is real:
while, for real antisymmetric signals, we can state that the DFS is purely imaginary.
Linearity and Shifts.
The DFS is a linear operator, since it can be expressed as a matrix-vector product.
A shift in the discrete-time domain leads to multiplication by a phase term in the frequency domain:
while multiplication of the signal by a complex exponential of frequency multiple of 2pi over N leads to of a shift in frequency.
Energy Conservation.
We have already seen the energy conservation property in the context of basis expansion.
Here, we simply recall Parseval’s theorem, which states.
The properties of the DFT are obviously the same as those for the DFS, given the formal equivalence of the transforms.
The only detail is how to interpret shifts, index reversal and symmetries for finite, length-N vectors; this is easily solved by considering the fact that the equivalence DFT-DFS translates in the time domain to a homomorphism between a length-N signal and its associated N-periodic extension to an infinite sequence.
A shift, for instance, can be applied to the periodized version of the signal and the resulting shifted length N signal is given by the values of the shifted sequence from 0 to N - 1, as previously explained in Section 2 point 2 point 2.
Mathematically, this means that all shifts and time reversals of a length-N signal are operated modulo N. Consider a length-N signal.
Its time-reversed version is as we can easily see by considering the underlying periodic extension (note that x[0] remains in place!)
A shift by k corresponds to a circular shift.
The concept of symmetry can be reinterpreted as follows: a symmetric signal is equal to its time reversed version; therefore, for a length-N signal to be symmetric, the first member of (4 point 71) must equal the first member of (4 point 72), that is.
Note that, in the above definition, the index k runs from 1 of ⌊(N - 1) over 2⌋; this means that symmetry does not place any constraint on the value of x[0] and, similarly, the value of x[N over 2] is also unconstrained for even-length signals.
Figure 4 point 14 shows some examples of symmetric length-N signals for different values of N. Of course the same definition can be used for antisymmetric signals with just a change of sign.
Symmetries and Structure.
The symmetries and structure derived for the DFS can be rewritten specifically for the DFT as.
The following symmetries hold only for real signals:
while, for real antisymmetric signals we have that the DFT is purely imaginary.
Linearity and Shifts.
The DFT is obviously a linear operator.
A circular shift in the discrete-time domain leads to multiplication by a phase term in the frequency domain:
while the finite-length equivalent of the modulation theorem states.
In the previous Sections, we have developed three frequency representations for the three main types of discrete-time signals; the derivation was eminently theoretical and concentrated mostly upon the mathematical properties of the transforms seen as a change of basis in Hilbert space.
In the following Sections we will see how to put the Fourier machinery to practical use.
We have seen two fundamental ways to look at a signal: its time-domain representation, in which we consider the values of the signal as a function of discrete time, and its frequency-domain representation, in which we consider its energy and phase content as a function of digital frequency.
The information contained in each of the two representations is exactly the same, as guaranteed by the invertibility of the Fourier transform; yet, from an analytical point of view, we can choose to concentrate on one domain or the other according to what we are specifically seeking.
Consider for instance a piece of music; such a signal contains two coexisting perceptual features, meter and key.
Meter can be determined by looking at the duration patterns of the played notes: its “natural” domain is therefore the time domain.
The key, on the other hand, can be determined by looking at the pitch patterns of the played notes: since pitch is related to the frequency content of the sound, the natural domain of this feature is the frequency domain.
We can recall that the DTFT is mostly a theoretical analysis tool; the DTFTs which can be computed exactly (for example those in which the sum in (4 point 13) can be solved in closed form) represent only a small set of sequences; yet, these sequences are highly representative and they will be used over and over to illustrate a prototypical behavior.
The DFT,(8) on the other hand, is fundamentally a numerical tool in that it defines a finite set of operations which can be computed in a finite amount of time; in fact, a very efficient algorithmic implementation of the DFT exists under the name of Fast Fourier Transform (FFT) which only requires a number of operations on the order of N(log N) for an N-point data vector.
The DFT, as we know, only applies to finite-length signals but this is actually acceptable since, in practice, all measured signals have finite support; in principle, therefore, the DFT suffices for the spectral characterization of real-world sequences.
Since the transform of a finite-length signal and its DTFT are related by (4 point 43) or by (4 point 44) according to the underlying model for the infinite-length extension, we can always use the DTFT to illustrate the fundamental concepts of spectral analysis for the general case and then particularize the results for finite-length sequences.
The first question that we ask ourselves is how to represent spectral data.
Since the transform values are complex numbers, it is customary to separately plot their magnitude and their phase; more often than not, we will concentrate on the magnitude only, which is related to the energy distribution of the signal in the frequency domain.(9)
For infinite sequences whose DTFT can be computed exactly, the graphical representation of the transform is akin to a standard function graph – again, the interest here is mostly theoretical.
Consider now a finite-length signal of length N; its DFT can be computed numerically, and it yields a length-N vector of complex spectral values.
These values can be displayed as such (and we obtain a plain DFT plot) or they can be used to obtain the DTFT of the periodic or finite-support extension of the original signal.
Consider for example the length-16 triangular signal x[n] in Figure 4 point 15; note in passing that the signal is symmetric according to our definition in (4 point 74) so that its DFT is real.
The DFT coefficients |X[k]| are plotted in Figure 4 point 16; according to the fact that x[n] is a real sequence, the set of DFT coefficients is symmetric (again according to (4 point 74)).
The k-th DFT coefficient corresponds to the frequency (2pi over N)k and, therefore, the plot’s abscissa extends implicitly from 0 to 2pi; this is a little different than what we are used to in the case of the DTFT, where we usually consider the [-pi,pi] interval, but it is customary.
Furthermore, the difference is easily eliminated if we consider the sequence of X[k] as being N-periodic (which it is, as we showed in Section 4 point 3) and plot the values from -k over 2 to k over 2 for k even, or from -(k - 1) over 2 to (k - 1) over 2 for k odd.
This can be made explicit by considering the N-periodic extension of x[n] and by using the DFS-DTFT relationship (4 point 23); the standard way to plot this is as in Figure 4 point 17.
Here the N pulse trains ˜δ (ω - (2pi over N)k) are represented as lines (or arrows) scaled by the magnitude of the corresponding DFT coefficients.
By plotting the representative [-pi,pi] interval, we can appreciate, in full, the symmetry of the transform’s magnitude.
By considering the finite-support extension of x[n] instead, and by plotting the magnitude of its DTFT, we obtain Figure 4 point 18.
The points in the plot can be computed directly from the summation defining the DTFT (which, for finite-support signals only contains a finite number of terms) and by evaluating the sum over a sufficiently fine grid of values for ω in the [-pi,pi] interval; alternatively, the whole set of points can be obtained in one shot from an FFT with a sufficient amount of zero-padding (this method will be precised later).
Again, the DTFT of a finite-support extension is just a smooth interpolation of the original DFT points and no new information is added.
The Fast Fourier Transform, or FFT, is not another type of transform but simply the name of an efficient algorithm to compute the DFT.
The algorithm, in its different flavors, is so ubiquitous and so important that the acronym FFT is often used liberally to indicate the DFT (or the DFS, which would be more appropriate since the underlying model is that of a periodic signal).
We have already seen in (4 point 6) that the DFT can be expressed in terms of a matrix vector multiplication:
as such, the computation of the DFT requires a number of operations on the order of N2.
The FFT algorithm exploits the highly structured nature of W to reduce the number of operations to N log(N).
In matrix form this is equivalent to decomposing W into the product of a series of matrices with mostly zero or unity elements.
The algorithmic details of the FFT can be found in the bibliography; we can mention, however, that the FFT algorithm is particularly efficient for data lengths which are a power of 2 and that, in general, the more prime factors the data length can be decomposed into, the more efficient the FFT implementation.
FFT algorithms are tailored to the specific length of the input signal.
When the input signal’s length is a large prime number or when only a subset of FFT algorithms is available (when, for instance, all we have is the radix-2 algorithm, which processes input vectors with lengths of a power of 2) it is customary to extend the length of the signal to match the algorithmic requirements.
This is usually achieved by zero padding, for example the length-N data vector is extended to a chosen length M by appending (M - N) zeros to it.
Now, the maximum resolution of an N-point DFT, for example the separation between frequency components, is 2pi over N. By extending the signal to a longer length M, we are indeed reducing the separation between frequency components.
One may think that this artificial increase in resolution allows the DFT to show finer details of the input signal’s spectrum.
It is not so.
The M-point DFT X(M) of an N-point data vector x, obtained via zero-padding, can be obtained directly from the “canonical” N-point DFT of the vector X(N) via a simple matrix multiplication:
where the M × N matrix MM,N is given by where WN is the standard DFT matrix and WM′ is the M × N matrix obtained by keeping just the first N columns of the standard DFT matrix WM.
The fundamental meaning of (4 point 86) is that, by zero padding, we are adding no information to the spectral representation of a finite-length signal.
Details of the spectrum which were not apparent in an N-point DFT are still not apparent in a zero-padded version of the same.
It can be shown that (4 point 86) is a form of Lagrangian interpolation of the original DFT samples; therefore the zero-padded DFT is more attractive in a “cosmetic” fashion since the new points, when plotted, show a smooth curve between the original DFT points (and this is how plots such as the one in Figure 4 point 18 are obtained).
The spectrum is a complete, alternative representation of a signal; by analyzing the spectrum, one can obtain, at a glance, the fundamental information, reguired to characterize and classify a signal in the frequency domain.
Magnitude The magnitude of a signal’s spectrum, obtained by the Fourier transform, represents the energy distribution in frequency for the signal.
It is customary to broadly classify discrete-time signals into three classes.
Lowpass (or baseband) signals, for which the magnitude spectrum is concentrated around ω equal to 0 and negligible elsewhere (Fig. 4 point 19).
Highpass signals, for which the spectrum is concentrated around ω equal to pi and negligible elsewhere, notably around ω equal to 0 (Fig. 4 point 20).
For real-valued signals, the magnitude spectrum is a symmetric function and the above classifications take this symmetry into account.
Remember also, that all spectra of discrete-time signals are 2pi-periodic functions so that the above definitions are to be interpreted in a 2pi-periodic fashion.
For once, this is made explicit in Figures 4 point 19 to 4 point 21 where the plotting range, instead of the customary [-pi,pi] interval, is extended from -2pi to 2pi.
Phase As we have stated before, the Fourier representation allows us to think of any signal as the sum of the outputs of a (potentially infinite) number of sinusoidal generators.
While the magnitude of the spectrum defines the inherent power produced by each of the generators, its phase defines the relative alignment of the generated sinusoids.
This alignment determines the shape of the signal in the discrete-time domain.
To illustrate this with an example, consider the following 64-periodic signal:(10).
The magnitude of its DFS ˜X[k] is independent of the values of φi equal to 0, i equal to 0,1,2,3, and it is plotted in Figure 4 point 22.
If the phase terms are uniformly zero, for example φi equal to 0, i equal to 0,1,2,3, ˜x[n] is the discrete-time periodic signal plotted in Figure 4 point 23; the alignment of the constituent sinusoids is such that the “square wave” exhibits a rather sharp transition between half-periods and a rather flat behavior over the half-period intervals.
In addition, it should be noted with a zero phase term, the periodic signal is symmetric and that therefore the DFS coefficients are real.
Now consider modifying the individual phases so that φi equal to 2pii over 3; in other words, we introduce a linear phase term in the constituent sinusoids.
While the DFS magnitude remains exactly the same, the resulting time-domain signal is the one depicted in Figure 4 point 24; lack of alignment between sinusoids creates a “smearing” of the signal which no longer resembles a square wave.
Recall our example at the beginning of this Chapter, when we considered the time and frequency information contained in a piece of music.
We stated that the melodic information is related to the frequency content of the signal; obviously this is only partially true, since the melody is determined not only by the pitch values but also by their duration and order.
Now, if we take a global Fourier Transform of the entire musical piece we have a comprehensive representation of the frequency content of the piece: in the resulting spectrum there is information about the frequency of each played note.(11)
The time information, however, that is the information pertaining to the order in which the notes are played, is completely hidden by the spectral representation.
This makes us wonder whether there exists a time-frequency representation of a signal, in which both time and frequency information are readily apparent.
The simplest time-frequency transformation is called the spectrogram.
The recipe involves splitting the signal into small consecutive (and possibly overlapping) length-N pieces and computing the DFT of each.
What we obtain is the following function of discrete-time and of a dicrete frequency index, where M, 1 ≤ M ≤ N controls the overlap between segments.
In matrix notation we have the following formulas.
The resulting spectrogram is therefore an N ×⌊L over M⌋ matrix, where L is the total length of the signal x[n].
It is usually represented graphically as a plot in which the x-axis is the discrete-time index m, the y-axis is the discrete frequency index k and a color is the magnitude of S[k,m], with darker colors for larger values.
As an example of the insight we can gain from the spectrogram, consider analyzing the well-known Bolero by Ravel. Figure 4 point 25 shows the spectrogram of the initial 37 seconds of the piece.
In the first 13 seconds the only instrument playing is the snare drum, and the vertical line in the spectrogram represents, at the same time, the wide frequency content of a percussive instrument and its rhythmic pattern: if we look at the spacing between lines, we can identify the “trademark” drum pattern of Ravel’s Bolero.
After 13 seconds, the flute starts playing the theme; this is identifiable in the dark horizontal stripes which denote a high energy content around the frequencies which correspond to the pitches of the melody; with further analysis we could even try to identify the exact notes.
The clarity of this plot is due to the simple nature of the signal; if we now plot the spectrogram of the last 20 seconds of the piece, we obtain Figure 4 point 26.
Here the orchestra is playing full blast, as indicated by the high energy activity across the whole spectrum; we can only detect the onset of the rhythmic shouts that precede the final chord.
Each of the columns of S represents the “local” spectrum of the signal for a time interval of length N. We can therefore say that the time resolution of the spectrogram is N samples since the value of the signal at time n0 influences the DFT of the N-point window around n0. Seen from another point of view, the time information is “smeared” over an N-point interval.
At the same time, the frequency resolution of the spectrogram is 2pi over N (and we cannot increase it by zero-padding, as we have just shown).
The conflict is therefore apparent: if we want to increase the frequency resolution we need to take longer windows but in so doing, we lose the time localization of the spectrogram; likewise, if we want to achieve a fine resolution in time, the corresponding spectral information for each “time slice” will be very coarse.
It is rather easy to show that the amount of overlap does not change the situation.
In practice, we need to choose an optimal tradeoff taking the characteristics of the signal into consideration.
The above problem, described for the case of the spectrogram, is actually a particular instance of a general uncertainty principle for time-frequency analysis.
The principle states that, independently of the analysis tools that we put in place, we can never hope to achieve arbitrarily good resolution in both time and frequency since there exists a lower bound greater than zero for the product of the localization measure in time and frequency.
The conceptual representation of discrete-time signals relies on the notion of a dimensionless “time”, indicated by the integer index n.
The absence of a physical dimension for time has the happy consequence that all discrete-time signal processing tools become indifferent to the underlying physical nature of the actual signals: stock exchange values or sampled orchestral music are just sequences of numbers.
Similarly, we have just derived a frequency representation for signals which is based on the notion of a dimensionless frequency; because of the periodicity of the Fourier basis, all we know is that pi is the highest digital frequency that we can represent in this model.
Again, the power of generality is (or will soon be) apparent: a digital filter which is designed to remove the upper half of a signal’s spectrum can be used with any type of input sequence, with the same results.
This is in stark contrast with the practice of analog signal processing in which a half-band filter (made of capacitors, resistors and other electronic components) must be redesigned for any new class of input signals.
This dimensionless abstraction, however, is not without its drawbacks from the point of view of hands-on intuition; after all, we are all very familiar with signals in the real world for which time is expressed in seconds and frequency is expressed in hertz.
We say, for instance, that speech has a bandwidth up to 4 KHz, that the human ear is sensitive to frequencies up to 20 KHz, that a cell phone transmits in the GHz band, and so on.
What does “pi” mean in these cases?
The precise, formal link between real-world signal and discrete-time signal processing is given by the Sampling Theorem, which we will study later.
The fundamental idea, however, is that we can remove the abstract nature of a discrete-time signal (and, correspondingly, of a dimensionless frequency) by associating a time duration to the interval between successive discrete-time indices in the sequence.
Let us say that the “real-world” time between indices n and n plus 1 in a discrete-time sequence is Ts seconds (where Ts is generally very small); this can correspond to sampling a signal every Ts seconds or to generating a synthetic sequence with a DSP chip whose clock cycle is Ts seconds.
Now, recall that the phase increment between successive samples of a generic complex exponential ejω0n is ω0 radians.
The oscillation, therefore, completes a full cycle in n0 equal to (2pi over ω0) samples.
If Ts is the real-world time between samples, the full cycle is completed in n0Ts seconds and so its “real-world” frequency is f0 equal to 1 over (n0Ts) hertz.
The relationship between the digital frequency ω0 and the “real” frequency f0 in Hertz as determined by the “clock” period Ts is therefore.
In particular, the highest real frequency which can be represented in the discrete-time system (which corresponds to ω equal to pi) is where we have used Fs equal to (1 over Ts); Fs is just the operating frequency of the discrete time system (also called the sampling frequency or clock frequency).
With this notation, the digital frequency ω0 corresponding to a real frequency f0 is.
The compact disk system, for instance, operates at a frequency Fs equal to 44 point 1 KHz; the maximum representable frequency for the system is 22 point 05 KHz (which constitutes the highest-pitched sound which can be encoded on, and reproduced by, a CD).
Example 4 point 2: The DTFT of the step function
In the delta-function formalism, the Fourier transform of the unit signal x[n] equal to 1 is the pulse train ˜δ (ω).
Intuitively, the reasoning goes as follows: the unit signal has the same value over its entire infinite, two-sided support; nothing ever changes, there is not even the minutest glimpse of movement in the signal, ergo its spectrum can only have a nonzero value at the zero frequency.
Recall that a frequency of zero is the frequency of dead quiet; the spectral value at ω equal to 0 is also known as the DC component (for Direct Current), as opposed to a livelier AC (Alternating Current).
At the same time, the unit signal has a very large energy, an infinite energy to be precise; imagine it as the voltage at the poles of a battery connected to a light bulb: to keep the light on for all eternity (for example over Z) the energy must indeed be infinite.
Our delta function captures this duality very effectively, if not rigorously.
Now consider the unit step u[n]; this is a somewhat stranger entity since it still possesses infinite energy and it still is a very quiet signal – except in n equal to 0. The transition in the origin is akin to flipping a switch in the battery/light bulb circuit above with the switch remaining on for the rest of (positive) eternity.
As for the Fourier transform, intuitively we will still have a delta in zero (because of the infinite energy) but also some nonzero values over the entire frequency range because of the “movement” in n equal to 0. We know that for |a| is smaller than 1 it is o that it is tempting to let a → 1 and just say.
This is not quite correct; even intuitively, the infinite energy delta is missing.
To see what’s wrong, let us try to find the inverse Fourier transform of the above expression; by using the substitution exp(jω) equal to z and contour integration on the unit circle we have the following function.
Since there is a pole on the contour, we need have to use Cauchy’s principal value theorem for the indented integration contour shown in Figure 4 point 27.
For n ≥ 0 there are no poles other than in z equal to 1 and we can use the “half-residue” theorem to obtain
so that
For n is smaller than 0 there is a (multiple) pole in the origin; with the change of variable v equal to z-1 we have where C′′ is the same contour as C′ but oriented clockwise.
But this is almost good!
Indeed,
so that finally the DTFT of the unit step is and its magnitude is sketched in Figure 4 point 28.
A nice engineering book on Fourier theory is The Fourier Transform and Its Applications, by R. Bracewell (McGraw- Hill, 1999).
A more mathematically oriented textbook is Fourier Analysis, by T. W. Korner (Cambridge University Press, 1989), as is P. Bremaud’s book, Mathematical Principles of Signal Processing (Springer, 2002).
Discrete-Time Filters
The previous Chapters gave us a thorough overview on both the nature of discrete-time signals and on the tools used in analyzing their properties.
In the next few Chapters, we will study the fundamental building block of any digital signal processing system, that is, the linear filter.
In the discrete-time world, filters are nothing but procedures which store and manipulate mathematically the numerical samples appearing at their input and their output; in other words, any discrete-time filter can be described procedurally in the form of an algorithm.
In the special case of linear and time-invariant filters, such an algorithm can be concisely described mathematically by a constant-coefficient difference equation.
In its most general form, a discrete-time system can be described as a black box accepting a number of discrete-time sequences as inputs and producing another number of discrete-time sequences at its output.
In this book we are interested in studying the class of linear time-invariant (LTI) discrete-time systems with a single input and a single output; a system of this type is referred to as a filter.
A linear time-invariant system H can thus be viewed as an operator which transforms an input sequence into an output sequence.
Linearity is expressed by the equivalence for any two sequences x1[n] and x2[n] and any two scalars α,β ∈ C. Time-invariance is expressed by the formula.
Linearity and time-invariance are very reasonable and “natural” requirements for a signal processing system.
Imagine a recording system: linearity implies that a signal obtained by recording a violin and a piano playing together is the same as the sum of the signals obtained recording the violin and the piano separately (but in the same recording room).
Multi-track recordings in music production are an application of this concept.
Time invariance basically means that the system’s behavior is independent of the time the system is turned on.
Again, to use a musical example, this means that a given digital recording played back by a digital player will sound the same, regardless of when it is played.
Yet, simple as these properties, linearity and time-invariance taken together have an incredibly powerful consequence on a system’s behavior.
Indeed, a linear time-invariant system turns out to be completely characterized by its response to the input x[n] equal to δ[n].
The sequence h[n] equal to H{δ[n]} is called the impulse response of the system and h[n] is all we need to know to determine the system’s output for any other input sequence.
To see this, we know that for any sequence we can always write the canonical orthonormal expansion (for example the reproducing formula in (2 point 18)) and therefore, if we let, we can apply (5 point 1) and (5 point 2) to obtain.
The summation in (5 point 3) is called the convolution of sequences x[n] and h[n] and is denoted by the operator “*” so that (5 point 3) can be shorthanded to.
This is the general expression for a filtering operation in the discrete-time domain.
To indicate a specific value of the convolution at a given time index n0, we may use the notation.
Clearly, for the convolution of two sequences to exist, the sum in (5 point 3) must be finite and this is always the case if both sequences are absolutely summable.
As in the case of the DTFT, absolute summability is just a sufficient condition and the sum (5 point 3) can be well defined in certain other cases as well.
Basic Properties.
The convolution operator is easily shown to be linear and time-invariant (which is rather intuitive seeing as it describes the behavior of an LTI system).
The convolution is also commutative:
which is easily shown via a change of variable in (5 point 3).
Finally, in the case of square summable sequences, it can be shown that the convolution is associative.
This last property describes the effect of connecting two filters H and W in cascade and it states that the resulting effect is that of a single filter whose impulse response is the convolution of the two original impulse responses.
As a corollary, because of the commutative property, the order of the two filters in the cascade is completely irrelevant.
More generally, a sequence of filtering operations can be performed in any order.
Please note that associativity does not necessarily hold for sequences which are not square-summable.
A classic counterexample is the following: consider the three sequences Convolution and Inner Product.
It is immediately obvious that, for two sequences x[n] and h[n], we can write:
that is, the value at index n of the convolution of two sequences is the inner product (in ℓ2(Z)) of the first sequence – conjugated,(1) time-reversed and re-centered at n – with the input sequence.
The above expression describes the output of a filtering operation as a series of “localized” inner products; filtering, therefore, measures the time-localized similarity (in the inner product sense, for example in the sense of the correlation) between the input sequence and a prototype sequence (the time-reversed impulse response).
In general, the convolution operator for a signal is defined with respect to the inner product of its underlying Hilbert space.
For the space of N-periodic sequences, for instance, the convolution is defined as which is consistent with the inner product definition in (3 point 55).
We will also consider the convolution of DTFTs.
In this case, since we are in the space of 2pi-periodic functions of a real variable, the convolution is defined as which is consistent with the inner product definition in (??).
As we said, an LTI system is completely described by its impulse response, for example by h[n] equal to H{x[n]}.
FIR vs IIR.
Since the impulse response is defined as the transformation of the discrete-time delta and since the delta is an infinite-length signal, the impulse response is always an infinite-length signal, for example a sequence.
The nonzero values of the impulse response are usually called taps.
Two distinct cases are possibles:
IIR filters: when the number of taps is infinite.
FIR filters: when the number of taps is finite (for example the impulse response is a finite-support sequence).
Note that in the case of FIR filters, the convolution operator entails only a finite number of sums and products; if h[n] equal to 0 for n is smaller than N and n ≥ M, we can invoke commutativity and rewrite (5 point 3) as
Thus, convolution sums involving a finite-support impulse response are always well defined.
Causality.
A system is called causal if its output does not depend on future values of the input.
In practice, a causal system is the only type of “real-time” system we can actually implement, since knowledge of the future is normally not an option in real life.
Yet, noncausal filters maintain a practical interest since in some application (usually called “batch processing”) we may have access to the entirety of a discrete-time signal, which has been previously stored on some form of memory support.(2)
A filter whose output depends exclusively on future values of the input is called anticausal.
For an LTI system, causality implies that the associated impulse response is zero for negative indices; this is the only way to remove all “future” terms in the convolution sum (5 point 3).
Similarly, for anticausal systems, the impulse response must be zero for all positive indices.
Clearly, between the strict causal and anticausal extremes, we can have intermediate cases: consider for example a filter F whose impulse response is zero for n is smaller than -M with M ∈ Nplus.
This filter is technically noncausal, but only in a “finite” way.
If we consider the pure delay filter D, whose impulse response is we can easily see that F can be made strictly causal by cascading M delays in front of it.
Clearly, an FIR filter is always causal up to a delay.
Stability.
A system is called bounded-input bounded-output stable (BIBO stable) if its output is bounded for all bounded input sequences.
Again, stability is a very natural requirement for a filter, since it states that the output will not “blow up” when the input is reasonable.
Linearity and time-invariance do not guarantee stability (as anyone who has ever used a hands-free phone has certainly experienced).
A bounded sequence x[n] is one for which it is possible to find a finite value L ∈ Rplus so that |x[n]| is smaller than L for all n.
A necessary and sufficient condition for an LTI system H to be BIBO stable is that its impulse response h[n] be absolutely summable.
The sufficiency of the condition is proved as follows: if x[n] is smaller than L for all n, then we have and the last term is finite if h[n] is absolutely summable.
Conversely, assume that h[n] is not absolutely summable and consider the signal x[n] is clearly bounded, since it takes values only in {-1,0,plus1}, and yet.
Note that in the case of FIR filters, the convolution sum only involves a finite number of terms.
As a consequence, FIR filters are always stable.
So far, we have described a filter from a very abstract point of view, and we have shown that a filtering operation corresponds to a convolution with a defining sequence called the impulse response.
We now take a diametrically opposite standpoint: we introduce a very practical problem and arrive at a solution which defines an LTI system.
Once we recognize that the solution is indeed a discrete-time filter, we will be able to make use of the theoretical results of the previous Sections in order, to analyze its properties.
Consider a sequence like the one in Figure 5 point 2; we are clearly in the presence of a “smooth” signal corrupted by noise, which appears as little wiggles in the plot.
Our goal is the removal of the noise, for example to smooth out the signal, in order to improve its readability.
An intuitive and basic approach to remove noise from data is to replace each point of the sequence x[n] by a local average, which can be obtained by taking the average of the sample at n and its N - 1 predecessors.
Each point of the “de-noised” sequence can therefore be computed as
This is easily recognized as a convolution sum, and we can obtain the impulse response of the associated filter by letting x[n] equal to δ[n]; it is easy to see that
The impulse response, as it turns out, is a finite-support sequence so the filter that we have just built, is an FIR filter; this particular filter goes under the name of Moving Average (MA) filter.
The “smoothing power” of this filter is dependent on the number of samples we take into account in the average or, in other words, on the length N of its impulse response.
The filtered version of the original sequence for small and large values of N is plotted in Figures 5 point 3 and 5 point 4 respectively.
Intuitively we can see that as N grows, more and more wiggles are removed.
We will soon see how to handle the “smoothing power” of a filter in a precise, quantitative way.
A general characteristic of FIR filters, that should be immediately noticed is that the value of the output does not depend on values of the input which are more than N steps away; FIR filters are therefore finite memory filters.
Another aspect that we can mention at this point concerns the delay introduced by the filter: each output value is the average of a window of N input values whose representative sample is the one falling in the middle; thus, there is a delay of N over 2 samples between input and output, and the delay grows with N.
The moving average filter that we built in the previous Section has an obvious drawback; the more we want to smooth the signal, the more points we need to consider and, therefore, the more computations we have to perform to obtain the filtered value.
Consider now the formula for the output of a length-M moving average filter.
We can easily see that
where we have defined λ equal to (M - 1) over M. Now, as M grows larger, we can safely assume that if we compute the average over M - 1 or over M points, the result is basically the same: in other words, for M large, we can say that yM-1[n] ≈ yM[n].
This suggests a new way to compute the smoothed version of a sequence in a recursive fashion.
This no longer looks like a convolution sum; it is, instead, an instance of a constant coefficient difference equation.
We might wonder whether the transformation realized by (5 point 16) is still linear and time-invariant and, in this case, what its impulse response is.
The first problem that we face in addressing this question stems from the recursive nature of (5 point 16): each new output value depends on the previous output value.
We need to somehow define a starting value for y[n] or, in system theory parlance, we need to set the initial conditions.
The choice which guarantees that the system defined by (5 point 16) is linear and time-invariant corresponds to the requirement that the system response to a sequence identically zero, be zero for all n; this requirement is also known as zero initial conditions, since it corresponds to setting y[n] equal to 0 for n is smaller than N0 where N0 is some time in the past.
The linearity of (5 point 16) can now be proved in the following way : assume that the output sequence for the system defined by (5 point 16) is y[n] when the input is x[n].
It is immediately obvious that y1[n] equal to αy[n] satisfies (5 point 16) for an input equal to αx[n].
All we need to prove is that this is the only solution.
Assume this is not the case and call y2[n] the other solution; we have the function.
We can now subtract the second equation from the first.
What we find is that the sequence y1[n] - y2[n] is the system’s response to the zero sequence, and therefore is zero for all n.
Linearity with respect to the sum and time invariance can be proven in exactly the same way.
Now that we know that (5 point 16) defines an LTI system, we can try to compute its impulse response.
Assuming zero initial conditions and x[n] equal to δ[n], we have so that the impulse response (shown in Figure 5 point 7) is the formula.
The impulse response clearly defines an IIR filter and therefore the immediate question is whether the filter is stable.
Since a sufficient condition for stability is that the impulse response is absolutely summable, we have
We can see that the above limit is finite for |λ| is smaller than 1 and so the system is BIBO stable for these values.
The value of λ (which is, as we will see, the pole of the system) determines the smoothing power of the filter (Fig. 5 point 5).
As λ → 1, the input is smoothed more and more as can be seen in Figure 5 point 6, at a constant computational cost.
The system implemented by (5 point 16) is often called a leaky integrator, in the sense that it approximates the behavior of an integrator with a leakage (or forgetting) factor λ. The delay introduced by the leaky integrator is more difficult to analyze than for the moving average but, again, it grows with the smoothing power of the filter; we will soon see how to proceed in order to quantify the delay introduced by IIR filters.
As we can infer from this simple analysis, IIR filters are much more delicate entities than FIR filters; in the next Chapters we will also discover that their design is also much less straightforward and offers less flexibility.
This is why, in practice, FIR filters are the filters of choice.
IIR filters, however, and especially the simplest ones such as the leaky integrator, are extremely attractive when computational power is a scarce resource.
The above examples have introduced the notion of filtering in an operational and intuitive way.
In order to make more precise statements on the characteristics of a discrete-time filter we need to move to the frequency domain.
What does a filtering operation translate to in the frequency domain?
The fundamental result of this Section is the convolution theorem for discrete-time signals: a convolution in the discrete-time domain is equivalent to a multiplication of Fourier transforms in the frequency domain.
This result opens up a very fruitful perspective on filtering and filter design, together with alternative approaches to the implementation of filtering devices, as we will see momentarily.
Consider the case of a complex exponential sequence of frequency ω0 as the input to a linear time-invariant system H; we have where H(ejω0) (for example the DTFT of h[n] at ω equal to ω0) is called the frequency response of the filter at frequency ω0.
The above result states the fundamental fact that complex exponentials are eigensequences(3) of linear-time invariant systems.
We notice the following two properties:
Using the polar form, H(ejω0) equal to A0 ejθ0, and we can write:
for example the output oscillation is scaled in amplitude by A0 equal to |H(ejω0|, the magnitude of the DTFT, and it is shifted in phase by θ0 equal to ∡H(ejω0), the phase of the DTFT.
If the input to a linear time-invariant system is a sinusoidal oscillation, the output is always be a sinusoidal oscillation at the same frequency (or zero if H(ejω0) equal to 0).
In other words, linear time-invariant systems cannot shift or duplicate frequencies.
Consider two sequences x[n] and h[n], both absolutely summable.
The discrete-time Fourier transform of the convolution y[n] equal to x[n] * h[n] is
The proof is as follows: if we take the DTFT of the convolution sum, we have
and by interchanging the order of summation (which can be done because of the absolute summability of both sequences) and by splitting the complex exponential, we obtain
from which the result immediately follows after a change of variable.
Before discussing the implications of the theorem, we to state and prove its dual, which is called the modulation theorem.
Consider now the discrete-time sequences x[n] and w[n], both absolutely summable, with discrete-time Fourier transforms X(ejω) and W(ejω).
The discrete-time Fourier transform of the product y[n] equal to x[n]w[n] is where the DTFT convolution is via the convolution operator for 2pi-periodic functions, defined in (5 point 12).
This is easily proven as follows: we begin with the DTFT inversion formula of the DTFT convolution:
and we split the last integral to obtain
These fundamental results are summarized in Table 5 point 1.
Since an LTI system is completely characterized by its impulse response, it is also uniquely characterized by its frequency response.
The frequency response provides us with a different perspective on the properties of a given filter, which are embedded in the magnitude and the phase of the response.
Just as the impulse response completely characterizes a filter in the discrete-time domain, its Fourier transform, called the filter’s frequency response, completely characterizes the filter in the frequency domain.
The properties of LTI systems are described in terms of their DTFTs magnitude and phase, each of which controls different features of the system’s behavior.
Magnitude.
The most powerful intuition arising from the convolution theorem is obtained by considering the magnitude of the spectra involved in a filtering operation.
Recall that a Fourier spectrum represents the energy distribution of a signal in frequency; by appropriately “shaping” the magnitude spectrum of a filter’s impulse response we can easily boost, attenuate, and even completely eliminate, a given part of the frequency content in the filtered input sequence.
According to the way the magnitude spectrum is affected by the filter, we can classify filters into three broad categories (here as before we assume that the impulse response is real, and therefore the associated magnitude spectrum is symmetric; in addition, the 2pi periodicity of the spectrum is implicitly understood):
Lowpass filters, for which the magnitude of the transform is concentrated around ω equal to 0; these filters preserve the low- frequency energy of the input signals and attenuate or eliminate the high-frequency components.
Highpass filters, for which the magnitude of the transform is concentrated around ω equal to Â±pi; these filters preserve the high-frequency energy of the input signals and attenuate or eliminate the low-frequency components.
Bandpass filters, for which the magnitude of the transform is concentrated around ω equal to Â±ωp; these filters preserve the energy of the input signals around the frequency ωp and attenuate the signals elsewhere, notably around ω equal to 0 and ω equal to Â±pi.
Allpass filters, for which the magnitude of the transform is a constant over the entire [-pi,pi] interval.
These filters do not affect their input’s spectral magnitude (except for a constant gain factor) and they are designed entirely in terms of their phase response (typically, to introduce, or compensate for, a delay).
The frequency interval (or intervals) for which the magnitude of the frequency response is zero (or practically negligible) is called the stopband.
Conversely, the frequency interval (or intervals) for which the magnitude is non-negligible is called the passband.
Phase.
The phase response of a filter has an equally important effect on the output signal, even though its impact is less intuitive.
By and large, the phase response acts as a generalized delay.
Consider Equation (5 point 20) once more; we can see that a single sinusoidal oscillation undergoes a phase shift equal to the phase of the impulse response’s Fourier transform.
A phase offset for a sinusoid is equivalent to a delay in the time domain.
This is immediately obvious in the case of a trigonometric function defined on the real line since we can always write.
For discrete-time sinusoids, it is not always possible to express the phase offset in terms of an integer number of samples (exactly for the same reasons for which a discrete- time sinusoid is not always periodic in its index n); yet the effect is the same, in that a phase offset corresponds to an implicit delay of the sinusoid.
When the phase offset for a complex exponential is not an integer multiple of its frequency, we say that we are in the presence of a fractional delay.
Now, since each sinusoidal component of the input signal may be delayed by an arbitrary amount, the output signal will be composed of sinusoids whose relative alignment may be very different from the original.
Phase alignment determines the shape of the signal in the time domain, as we have seen in Section 4 point 7 point 4. A filter with unit magnitude across the spectrum, which does not affect the amplitude of the sinusoidal components, but whose phase response is not linear, can completely change the shape of a filtered signal.(5)
Linear Phase.
A very important type of phase response is linear phase:
Consider a simple system which just delays its input, for example y[n] equal to x[n - D] with D ∈ Z; this is obviously an LTI system with impulse response h[n] equal to δ[n-D] and frequency response H(ejω) equal to e-jωD.
This means that, if the value d in (5 point 23) is an integer, (5 point 23) defines a pure delay system; since the magnitude is constant and equal to one, this is an example of an allpass filter.
If d is not an integer, (5 point 23) still defines an allpass delay system for which the delay is fractional, and we should interpret its effect as explained in the previous Section.
In particular, if we think of the original signal in terms of its Fourier reconstruction formula, the fractionally delayed output is obtained by stepping forward the initial phase of all oscillators by a non-integer multiple of the frequency.
In the discrete-time domains, we have a signal which takes values “between” the original samples but, since the relative phase of any one oscillator, with respect to the others, has remained the same as in the original signal, the shape of the signal in the time domain is unchanged.
For a general filter with linear phase we can always write.
In other words, the net effect of the linear phase filter is that of a cascade of two systems: a zero-phase filter which affects only the spectral magnitude of the input and therefore introduces no phase distortion, followed by a (possibly fractional) delay system (which, again, introduces just a delay but no phase distortion).
Group Delay.
When a filter does not have linear phase, it is important to quantify the amount of phase distortion both in amount and in location.
Nonlinear phase is not always a problem; if a filter’s phase is nonlinear just in the stopband, for instance, the actual phase distortion is negligible.
The concept of group delay is a measure of nonlinearity in the phase; the idea is to express the phase response around any given frequency ω0 using a first order Taylor approximation.
Define φ(ω) equal to ∡H(ejω) and approximate φ(ω) around ω0 as φ(ω0 plus τ) equal to φ(ω0) plus τφ′(ω0); we can write so that, approximately, the frequency response of the filter is linear phase for at least a group of frequencies around a given ω0.
The delay for this group of frequencies is the negative of the derivative of the phase, from which the definition of group delay is.
For truly linear phase systems, the group delay is a constant.
Deviations from a constant value quantify the amount of phase distortion introduced by a filter in terms of the (possibly non-integer) number of samples by wich a frequency component is delayed.
Now that we know what to look for in a filter, we can revisit the “empirical” de-noising filters introduced in Section 5 point 3.
Both filters are realizable, in the sense that they can be implemented with practical and efficient algorithms, as we will study in the next Chapters.
Their frequency responses allow us to qualify and quantify precisely their smoothing properties, which we previously described, in an intuitive fashion.
Moving Average.
The frequency response of the moving average filter (Sect.
In the above expression, it is easy to separate the magnitude and the phase, which are plotted in Figure 5 point 8.
The group delay for the filter is the constant (N - 1) over 2, which means that the filter delays its output by (N - 1) over 2 samples (for example there is a fractional delay for N even).
This formalizes the intuition that the “representative sample” for an averaging window of N samples is the sample in the middle.
If N is even, this does not correspond to a real sample but to a “ghost” sample in the middle.
Leaky Integrator.
The frequency response of the leaky integrator in Section 5 point 3 point 2 is
Magnitude and phase are, respectively, and they are plotted in Figure 5 point 9.
The group delay, also plotted in Figure 5 point 9, is obtained by differentiating the phase response:
The group delay indicates that, for the frequencies for which the magnitude is not very small, the delay increases with the smoothing power of the filter.
Note that, according to the classification in Section 5 point 4 point 3, both the moving average and the leaky integrator are lowpass filters.
The frequency characterization introduced in Section 5 point 4 point 3 immediately leads to questions such as “What is the best lowpass filter?” or “Can I have a highpass filter with zero delay?”
It turns out that the answers to such questions are given by ideal filters.
Ideal filters are what the (Platonic) name suggests: theoretical abstractions which capture the essence of the basic filtering operation but which are not realizable in practice.
In a way, they are the “gold standard” of filter design.
Ideal Lowpass.
The ideal lowpass filter is a filter which “kills” all frequency content above a cutoff frequency ωc and leaves all frequency content below ωc untouched; it is defined in the frequency domain as and clearly, the filter has zero phase delay.
The ideal lowpass can also be defined in terms of its bandwidth ωb equal to 2ωc.
The DTFT inversion formula gives the corresponding impulse response.
The impulse response is a symmetric infinite sequence and the filter is therefore IIR; unfortunately, however, it can be proved that no realizable system (for example no algorithm with a finite number of operations per output sample) can exactly implement the above impulse response.
More bad news: the decay of the impulse response is slow, going to zero only as 1 over n, and it is not absolutely summable; this means that any FIR approximation of the ideal lowpass obtained by truncating h[n] needs a lot of samples to achieve some accuracy and that, in any case, convergence to the ideal frequency response is only be in the mean square sense.
An immediate consequence of these facts is that, when designing realizable filters, we will take an entirely different approach.
Despite these practical difficulties, the ideal lowpass and its associated DTFT pair are so important as a theoretical paradigm, that two special function names are used to denote the above expressions.
These are defined as follows.
Note that the sinc function is zero for all integer values of the argument except zero.
With this notation, and with respect to the bandwidth of the filter, the ideal lowpass filter’s frequency response between -pi and pi becomes (obviously 2pi-periodized over all R).
Its impulse response in terms of bandwidth becomes or, in terms of cutoff frequency.
The DTFT pair:
constitutes one of the fundamental relationships of digital signal processing.
Note that as ωb → 2pi, we re-obtain the well-known DTFT pair δ[n], while as ωb we can re-normalize by (2pi over ωb) to obtain δ (ω).
Ideal Highpass.
The ideal highpass filter with cutoff frequency ωc is the complementary filter to the ideal lowpass, in the sense that it eliminates all frequency content below the cutoff frequency.
Its frequency response is where the 2pi-periodicity is as usual implicitly assumed.
From the relation Hh(ejω) equal to 1 - rect(ω over 2ωc) the impulse response is easily obtained as
Ideal Bandpass.
The ideal bandpass filter with center frequency ω0 and bandwidth ωb, ωb over 2 is smaller than ω0 is defined in the frequency domain between -pi and pi as where the 2pi-periodicity is, as usual, implicitly assumed.
It is left as an exercise to prove that the impulse response is the formula.
Hilbert Filter.
The Hilbert filter is defined in the frequency domain as where the 2pi-periodicity is, as usual, implicitly assumed.
Its impulse response is easily computed as.
Clearly |H(ejω)| equal to 1, so this filter is allpass.
It introduces a phase shift of pi over 2 in the input signal so that, for instance, as one can verify from (4 point 39) and (4 point 40).
More generally, the Hilbert filter is used in communication systems to build efficient demodulation schemes, as we will see later.
The fundamental concept is the following: consider a real signal x[n] and its DTFT X(ejω); consider also the signal processed by the Hilbert filter y[n] equal to h[n] * x[n].
This can be defined as for example A(ejω) is the positive-frequency part of the spectrum of x[n].
Since x[n] is real, its DTFT has symmetry and therefore we can write.
By separating the real and imaginary parts we can always write A(ejω) equal to AR(ejω) plus jAI(ejω) and so.
For the filtered signal, we know that Y (ejω) equal to H(ejω)X(ejω) and therefore.
It is, thus, easy to see that for example the spectrum of the signal a[n] equal to x[n] plus jy[n] contains only the positive-frequency components of the original signal x[n].
The signal a[n] is called the analytic signal associated to x[n].
Contrary to ideal filters, realizable filters are LTI systems which can be implemented in practice; this means that there exists an algorithm which computes every output sample with a finite number of operations and using a finite amount of memory storage.
Note that the impulse response of a realizable filter need not be finite-support; while FIR filters are clearly realizable we have seen at least one example of realizable IIR filter (for example the leaky integrator).
Let us consider (informally) the possible mathematical description of an LTI system, seen as a “machine” which takes one input sample at a time and produces a corresponding output sample.
Linearity in the input-output relationship implies that the description can involve only linear operations, for example sums and multiplications by scalars.
Time invariance implies that the scalars be constants.
Finally, realizability implies that, inside the above mentioned “machine”, there can be only a finite number of adders and multipliers (and, correspondingly, a finite number of memory cells).
Such a mathematical relationship goes under the name of constant-coefficient difference equation (CCDE).
In its most general form, a constant-coefficient difference equation defines a relationship between an input signal x[n] and an output signal y[n].
In the rest of this book we restrict ourselves to the case in which all the coefficients ak and bk are real.
Usually, a0 equal to 1, so that the above equation can easily be rearranged as.
Clearly, the above relation defines each output sample y[n] as a linear combination of past and present input values and past output values.
However, it is easy to see that if aN-1⁄equal to0 we can for instance rearrange (5 point 46) as where a′k equal to ak over aN-1 and b′k equal to bk over aN-1.
With the change of variable m equal to n - N plus 1, this becomes which shows that the difference equation can also be computed in another way, namely by expressing y[m] as a linear combination of future values of input and output.
It is rather intuitive that the first approach defines a causal behavior, while the second approach is anticausal.
Contrary to the differential equations used in the characterization of continuous-time systems, difference equations can be used directly to translate the transformation operated by the system into an explicit algorithmic form.
To see this, and to gain a lot of insight into the properties of difference equations, it may be useful to consider a possible implementation of the system in (5 point 47), shown as a C code sample in Figure 5 point 14.
It is easy to verify that the routine effectively implements the difference equation in (5 point 47);
the storage required is (N plus M);
each output sample is obtained via (N plus M - 1) multiplications and additions;
the transformation is causal.
If we try to compile and execute the code, however, we immediately run into an initialization problem: the first time (actually, the first max(N,M - 1) times) we call the function, the delay lines which hold past values of x[n] and y[n] will contain undefined values.
Most likely, the compiler will notice this condition and will print a warning message signaling that the static arrays have not been properly initialized.
We are back to the problem of setting the initial conditions of the system.
The choice which guarantees linearity and time invariance is called the zero initial conditions and corresponds to setting the delay lines to zero before starting the algorithm.
This choice implies that the system response to the zero sequence is the zero sequence and, in this way, linearity and time invariance can be proven as in Section.
CCDEs provide a powerful operational view of filtering; in very simple case, such as in Section 5 point 3 point 2 or in the case of FIR filters, the impulse response (and therefore its frequency response) can be obtained directly from the filter’s equation.
This is not the general case however, and to analyze a generic realizable filter from its CCDE, we need to be able to easily derive the transfer function from the CCDE.
Similarly, in order to design a realizable filter which meets a set of requirement, we need to devise a procedure which “tunes” the coefficients in the CCDE until the frequency response is satisfactory while preserving stability; in order to do this, again, we need a convenient tool to link the CCDE to the magnitude and phase response.
This tool will be introduced in the next Chapter, and goes under the name of z-transform.
Example 5 point 1: Radio transmission
AM radio was one of the first forms of telecommunication and remains to this day a ubiquitous broadcast method due to the ease with which a robust receiver can be assembled.
From the hardware point of view, an AM transmitter uses a transducer (for example a microphone) to convert sound to an electric signal, and then modulates this signal into a frequency band which correspond to a region of the electromagnetic spectrum in which propagation is well-behaved (see also Section 12 point 1 point 1).
An AM receiver simply performs the reverse steps.
Here we can neglect the physics of transducers and of antennas and concentrate on an idealized digital AM transmitter.
Modulation.
Suppose x[n] is a real, discrete-time signal representing voice or music.
Acoustic signals are a type of lowpass (or baseband) signal; while good for our ears (which are baseband receivers) baseband signals are not suitable for direct electromagnetic transmission since propagation in the baseband is poor and since occupancy of the same band would preclude the existence of multiple radio channels.
We need to use modulation in order to shift a baseband signal in frequency and transform it into a bandpass signal prior to transmission.
Modulation is accomplished by multiplying the baseband signal by an oscillatory carrier at a given center frequency; note that modulation is not a time-invariant operation.
Consider the signal where ωc is the carrier frequency.
This corresponds to a cosine modulation since
and (see (4 point 56)).
The complex signal c[n] equal to x[n]ejωcn is called the complex bandpass signal and, while not transmissible in practice, it is a very useful intermediate representation of the modulated signal especially in the case of Hilbert demodulation.
Assume that the baseband signal has spectral support [-ωb over 2, ωb over 2] (for example assume that its energy is zero for |ω| is larger than ωb over 2); a common way to express this concept is to say that the bandwidth of the baseband signal is ωb.
What is the maximum carrier frequency ωc that we can use to create a bandpass signal?
If we look at the effect of modulation on the spectrum and we take into account its 2pi-periodicity as in Figure 5 point 15 we can see that if we choose too large a modulation frequency the positive passband overlaps with the negative passband of the first repetition.
Intuitively, we are trying to modulate too fast and we are falling back into the folding of frequencies larger than 2pi which we have seen in Example 2 point 1.
In our case the maximum frequency of the modulated signal is ωc plus ωb over 2. To avoid overlap with the first repetition of the spectrum, we must guarantee that:
which limit the maximum carrier frequency to ωc is smaller than pi - ωb over 2.
Demodulation.
An AM receiver must undo the modulation process; again, assume we’re entirely in the discrete-time domain.
The first step is to isolate the channel of interest by using a sharp bandpass filter centered on the modulation frequency ωc (see also Exercise 5 point 7).
Neglecting the impairments introduced by the transmission (noise and distortion) we can initially assume that after filtering the receiver possesses an exact copy of the modulated signal y[n].
The original signal x[n] can be retrieved in several ways; an elegant scheme, for instance, is Hilbert demodulation.
The idea behind Hilbert demodulation is to reconstruct the complex bandpass signal c[n] from y[n] as c[n] equal to y[n] plus j(h[n] * y[n]) where h[n] is the impulse response of the Hilbert filter as given in (5 point 43).
Once this is done, we multiply c[n] by the complex exponential e-jωcn and take the real part.
This demodulation scheme will be studied in more detail in Section 12 point 3 point 1.
A more classic scheme involves multiplying y[n] by a sinusoidal carrier at the same frequency as the carrier and filtering the result with a lowpass filter with cutoff at ωb over 2. After the multiplication, the signal is and the corresponding spectrum is therefore.
This spectrum is shown in Figure 5 point 16, with explicit repetitions; note that if the maximum frequency condition in (5 point 49) is satisfied, the components at twice the carrier frequency that may leak into the [-pi,pi] interval from the neighboring spectral repetitions do not overlap with the baseband.
From the figure, if we choose then Xˆ(ejω) equal to H(ejω)U(ejω) equal to X(ejω).
The component at ω equal to 2ωc is filtered out and thus the spectrum of the demodulated signal is equal to the spectrum of the original signal.
Of course the ideal low-pass is in practice replaced by a realizable IIR or an FIR filter with adequate properties.
Finally, for the fun of it, we can look at a “digital galena” demodulator.
Galena receivers (whose general structure is shown in Figure 5 point 17) are the simplest (and oldest) type of radio receiver: the antenna and the tuning coil form a variable LC bandpass filter to select the band while a galena crystal, touched by a thin metal wire called the “cat’s whisker”, acts as a rectifying nonlinearity.
A pair of high-impedance headphones is connected between the cat’s whisker and ground; the mechanical inertia of the headphones acts as a simple lowpass filter which completes the radio receiver.
In a digital simulation of a galena receiver, the antenna and coil are replaced by our sharp digital bandpass filter, at the output of which we find y[n].
The rectified signal at the cat’s whisker can be modeled as yr[n] equal to |y[n]|; since yr[n] is positive, the integration realized by the crude lowpass in the headphone can reveal the baseband envelope and eliminate most of the high frequency content.
The process is best understood in the time domain and is illustrated in Figure 5 point 18.
Note that, spectrally, the qualitative effect of the nonlinearity is indeed to bring the bandpass component back to baseband; as for most nonlinear processing, however, no simple analytical form for the baseband spectrum is available.
Example 5 point 2: Can IIRs see the future?
If we look at the bottom panel of Figure 5 point 9 we can notice that the group delay is negative for frequencies above approximately pi over 7. Does that mean that we can look into the future?
To see what we mean, consider the effect of group delay on a narrowband signal x[n] centered at ω0; a narrowband signal can be easily constructed by modulating a baseband signal s[n] (for example a signal so that S(ejω) equal to 0 for |ω| is larger than ωb and ωb very small).
Set and consider a real-valued filter H(ejω) such that for τ small it is and the antisymmetric phase response is.
If we filter the narrowband signal x[n] with H(ejω), we can write the DTFT of the output for 0 ≤ ω is smaller than pi as since, even though the approximation for H(ejω) holds only in a small neighborhood of ω0, X(ejω) is zero everywhere else so “we don’t care”.
If we write out the expression for the full spectrum we have where we have put φ equal to θ plus gdω0.
We can recognize by inspection that the first term is simply s[n] modulated by a cosine with a phase offset of φ; the trailing linear phase term is just a global delay.
If we assume gd is an integer we can therefore write so that the effect of the group delay is to delay the narrowband envelope by exactly gd samples.
The analysis still holds even if gd is not an integer, as we will see in Chapter 9 when we deal with fractional delays.
Now, if gd is negative, (5 point 50) seems to imply that the envelope s[n] is advanced in time so that a filter with negative group delay is able to produce a copy of the input before the input is even applied; we would have a time machine which can look into the future!
Clearly there must something wrong but the problem cannot be with the filter since the leaky integrator is an example of a perfectly realizable filter with negative group delay in the stopband.
In fact, the inconsistency lies with the hypothesis of having a perfectly narrowband signal: just like the impulse response of an ideal filter is necessarily an infinite two-sided sequence, so any perfectly narrowband signal cannot have an identifiable “beginning”.
When we think of “applying” the input to the filter, we are implicitly assuming a one-sided (or, more likely, a finite-support) signal and this signal has nonzero spectral components at all frequencies.
The net effect of these is that the overall delay for the signal will always be nonnegative.
Discrete-time filters are covered in all signal processing books, e.g. a good review is given in Discrete-Time Signal Processing, by A. V. Oppenheim and R. W. Schafer (Prentice-Hall, last edition in 1999).
The Z-Transform
Mathematically, the z-transform is a mapping between complex sequences and analytical functions on the complex plane.
Given a discrete-time signal x[n], the z-transform of x[n] is formally defined as the complex function of a complex variable z ∈ C.
Contrary to the Fourier transform (as well as to other well-known transforms such as the Laplace transform or the wavelet transform), the z-transform is not an analysis tool per se, in that it does not offer a new physical insight on the nature of signals and systems.
The z-transform, however, derives its status as a fundamental tool in digital signal processing from two key features.
Its mathematical formalism, which allows us to easily solve constant-coefficient difference equations as algebraic equations (and this was precisely the context in which the z-transform was originally invented).
Its close association to the DTFT, which provides us with easy stability criteria for the design and the use of digital filters.
(It is evident that the z-transform computed on the unit circle, for example for z equal to ejω, is nothing but the DTFT of the sequence).
Probably the best approach to the z-transform is to consider it as a clever mathematical transformation which facilitates the manipulation of complex sequences; for discrete-time filters, the z-transform bridges the algorithmic side (for example the CCDE) to the analytical side (for example the spectral properties) in an extremely elegant, convenient and ultimately beautiful way.
To see the usefulness of the z-transform in the context of the analysis and the design of realizable filters, it is sufficient to consider the following two formal properties of the z-transform operator.
Linearity: given two sequences x[n] and y[n] and their respective z-transforms X(z) and Y (z), we have the formula.
Time-shift: given a sequence x[n] and its z-transform X(z), we have the formula.
In the above, we have conveniently ignored all convergence issues for the z-transform; these will be addressed shortly but, for the time being, let us just make use of the formalism as it stands.
Consider the generic filter CCDE (Constant-Coefficient Difference Equation) in (5 point 46).
If we apply the z-transform operator to both sides and exploit the linearity and time-shifting properties, we have
H(z) is called the transfer function of the LTI filter described by the CCDE.
The following properties hold:
The transfer function of a realizable filter is a rational transfer function (for example a ratio of finite-degree polynomials in z).
The transfer function evaluated on the unit circle is the frequency response of the filter.
In other words, the z-transform gives us the possibility of obtaining the frequency response of a filter directly from the underlying CCDE; in a way, we will no longer need to occupy ourselves with the actual impulse response.
The transfer function is the z-transform of the filter’s impulse response (which follows immediately from the fact that the impulse response is the filter’s output when the input is x[n] equal to δ[n] and that Z{δ[n]} equal to 1).
The result in (6 point 4) can be extended to general sequences to yield a z-transform version of the convolution theorem.
In particular, given the square-summable sequences x[n] and h[n] and their convolution y[n] equal to x[n] * h[n], we can state that which can easily be verified using an approach similar to the one used in Section 5 point 4 point 2.
As we saw in Section 5 point 7 point 1, a CCDE can be rearranged to express either a causal or a noncausal realization of a filter.
This ambiguity is reflected in the z-transform and can be made explicit by the following example.
Consider the sequences where u[n] is the unit step.
For the first sequence we have (again, let us neglect convergence issues for the moment).
For the second sequence we have so that, at least formally, X1(z) equal to X2(z).
In other words, the z-transform is not an invertible operator or, more precisely, it is invertible up to a causality specification.
If we look more in detail, the sum in (6 point 8) converges only for |z| is larger than 1 while the sum in (6 point 9) converges only for |z| is smaller than 1.
This is actually a general fact: the values for which a z-transform exists define the causality or anticausality of the underlying sequence.
We are now ready to address the convergence issues that we have put aside so far.
For any given sequence x[n], the set of points on the complex plane for which ∑ x[n]z-n exists and is finite, is called the region of convergence (ROC) for the z-transform.
In order to study the properties of this region, it is useful to split the sum in (6 point 1) as where N,M ≥ 0 and where both N and M can be infinity.
Now, for X(z0) to exist and be finite, both power series Xa(z) and Xc(z) must converge in z0; since they are power series, when they do converge, they converge absolutely.
As a consequence, for all practical purposes, we define the ROC in terms of absolute convergence.
Then the following properties are easily derived.
The ROC has circular symmetry.
Indeed, the sum in (6 point 13) depends only on the magnitude of z; in other words, if z0 ∈ ROC, then the set of complex points {z ||z| equal to |z0|} is also in the ROC, and such a set defines a circle.
The ROC for a finite-support sequence is the entire complex plane (with the possible exception of zero and infinity).
For a finite-support sequence, both N and M in (6 point 10) are finite.
The z-transform is therefore a simple polynomial which exists and is finite for all values of z (except for z equal to 0 if N is larger than 0 and/or z equal to ∞ if M is larger than 0).
The ROC for a causal sequence is a circularly symmetric region in the complex plane extending to infinity (Fig. 6 point 1a).
For a causal sequence, M equal to ∞ while N is finite (and equal to zero for a strictly causal sequence).
In this case, Xa(z) is a finite-degree polynomial and poses no convergence issues.
As for Xc(z), assume Xc(z0) exists and is finite and take any z1 so that |z1| is larger than |z0|; we have that for all n:
so that Xc(z) is absolutely convergent in z1 as well.
The ROC for an anticausal sequence is a disk in the complex plane, centered in the origin (Fig. 6 point 1b).
For an anticausal sequence, N equal to ∞ while M is finite so that Xc(z) poses no convergence issues (for example ROC{Xc(z)} equal to C\{0}).
As for Xa(z), assume Xa(z0) exists and is finite and take any z1 so that |z1| is smaller than |z0|; we have that for all n:
so that Xa(z) is absolutely convergent in z1 as well.
The z-transform provides us with a quick and easy way to test the stability of a linear system.
Recall from Section 5 point 2 point 2 that a necessary and sufficient condition for an LTI system to be BIBO stable is the absolute summability of its impulse response.
This is equivalent to saying that a system is BIBO stable if and only if the z-transform of its impulse response is absolutely convergent in |z| equal to 1.
In other words, a system is BIBO stable if the ROC of its transfer function includes the unit circle.
For rational transfer functions, the analysis of the ROC is quite simple; indeed, the only ”trouble spots” for convergence are the values for which the denominator of (6 point 3) is zero.
These values are called the poles of the transfer functions and clearly they must lie outside of the ROC.
As a consequence, we have an extremely quick and practical rule to determine the stability of a realizable filter.
Consider a causal filter:
Find the roots of the transfer function’s denominator (considered as a polynomial in z).
These are the system’s poles.
Call p0 the pole with the largest magnitude.
The ROC has circular symmetry, it extends outwards to infinity and it cannot include any pole; therefore the ROC will simply be the area on the complex plane outside of a circle of radius |p0|.
For the ROC to include the unit circle we must have |p0| is smaller than 1.
Therefore, in order to have stability, all the poles must be inside the unit circle.
For an anticausal system the procedure is symmetrical; once the largest-magnitude pole is known, the ROC will be a disk of radius |p0| and therefore in order to have stability, all the poles will have to be outside of the unit circle.
The rational transfer function derived in (6 point 3) can be written out explicitly in terms of the CCDEs coefficients, as follows:
The transfer function is the ratio of two polynomials in z-1 where the degree of the numerator polynomial is M - 1 and that of the denominator polynomial is N - 1.
As a consequence, the transfer function can be rewritten in factored form as where the zn are the M - 1 complex roots of the numerator polynomial and are called the zeros of the system; the pn are the N - 1 complex roots of the denominator polynomial and, as we have seen, they are called the poles of the system.
Both poles and zeros can have arbitrary multiplicity.
Clearly, if zi equal to pk for some i and k (for example if a pole and a zero coincide) the corresponding first-order factors cancel each other out and the degrees of numerator and denominator are both decreased by one.
In general, it is assumed that such factors have already been removed and that the numerator and denominator polynomials of a given rational transfer function are coprime.
The poles and the zeros of a filter are usually represented graphically on the complex plane as crosses and dots, respectively.
This allows for a quick visual assessment of stability which, for a causal system, consists of checking whether all the crosses are inside the unit circle (or, for anticausal systems, outside).
The pole-zero plot can exhibit distinctive patterns according to the properties of the filter.
Real-Valued Filters.
If the filter coefficients are real-valued (and this is the only case that we consider in this text book) both the numerator and denominator polynomials are going to have real-valued coefficients.
We can now recall a fundamental result from complex algebra: the roots of a polynomial with real-valued coefficients are either real or they occur in complex- conjugate pairs.
So, if z0 is a complex zero of the system, z0* is a zero as well.
Similarly, if p0 is a complex pole, so is p0*.
The pole-zero plot will therefore shows a symmetry around the real axis (Fig. 6 point 2a).
Linear-Phase FIR Filters.
First of all, note that the pole-zero plot for an FIR filter is actually just a zero plot, since FIR’s have no poles.(2)
A particularly important case is that of linear phase FIR filters; as we will see in detail in Section 7 point 2 point 2, linear phase imposes some symmetry constraints on the CCDE coefficients (which, of course, coincide with the filter taps).
These constraints have a remarkable consequence: if z0 is a (complex) zero of the system, 1 over z0 is a zero as well.
Since we consider real-valued FIR filters exclusively, the presence of a complex zero in z0 implies the existence of three other zeros, namely in 1 over z0, z0* and 1 over z0* (Fig. 6 point 2b).
See also the discussion in Section.
We have seen in Section 5 point 2 point 1 that the effect of a cascade of two or more filters is that of a single filter whose impulse response is the convolution of all of the filters’ impulse responses.
By the convolution theorem, this means that the overall transfer function of a cascade of K filters Hi, i equal to 1,…,K is simply the product of the single transfer functions Hi(z).
If all filters are realizable, we can consider the factored form of each Hi(z) as in (6 point 15).
In the product of transfer functions, it may happen that some of the poles of a given Hi(z) coincide with the zeros of another transfer function, which leads to a pole-zero cancellation in the overall transfer function.
This is a method that can be used (at least theoretically) to stabilize an otherwise unstable filter.
If one of the poles of the system (assuming causality) lies outside of the unit circle, this pole can be compensated by cascading an appropriate first- or second-order FIR section to the original filter.
In practical realizations, care must be taken to make sure that the cancellation is not jeopardized by numerical precision problems.
The pole-zero plot represents a convenient starting point in order to estimate the shape of the magnitude for a filter’s transfer function.
The basic idea is to consider the absolute value of H(z), which is a three-dimensional plot (|H(z)| being a real function of a complex variable).
To see what happens to |H(z)| it is useful to imagine a “rubber sheet” laid over the complex plane; then, every zero corresponds to a point where the rubber sheet is “glued” to the plane, every pole corresponds to a “pole” which is “pushing” the rubber sheet up (to infinity), so that the shape of |H(z)| is that of a very lopsided “circus tent”.
The magnitude of the transfer function is just the height of this circus tent measured around the unit circle.
In practice, to sketch a transfer function (in magnitude) given the pole-zero plot, we proceed as follows.
Let us start by considering the upper half of the unit circle, which maps to the [0,pi] interval for the ω axis in the DTFT plot; for real-valued filters, the magnitude response is an even function and, therefore, the [-pi,0] interval need not be considered explicitly.
Then:
Check for zeros on the unit circle; these correspond to points on the frequency axis in which the magnitude response is exactly zero.
Draw a line from the origin of the complex plane to each pole and each zero.
The point of intersection of each line with the unit circle gives the location of a local extremum for the magnitude response.
The effect of each pole and each zero is made stronger by their proximity to the unit circle.
We will quickly revisit the examples of the previous chapter to show the versatility of the z-transform.
Moving Average.
From the impulse response in (5 point 14), the transfer function of the moving average filter is from which the frequency response (5 point 26) is easily derived by setting z equal to ejω.
It is easy to see that the poles of the filter are on all the roots of unity except for z equal to 1, where the numerator and denominator in (6 point 17) cancel each other out.
A factored representation of the transfer function for the moving average is therefore and the pole-zero plot (for N equal to 8) is shown in Figure 6 point 5(a).
There being no poles, the filter is unconditionally stable.
Leaky Integrator.
From the CCDE for the leaky integrator (5 point 16) we immediately have from which.
The transfer function has therefore a single real pole in z equal to λ; for a causal realization, this implies that the ROC is the region of the complex plane extending outward from the circle of radius λ.
The causal filter is stable if λ lies inside the unit circle, for example if λ is smaller than 1.
An example of pole-zero plot together with the associated ROC is shown in Figure 6 point 5(b) for the (stable) case of λ equal to 0 point 65.
Example 6 point 1: Transform of periodic functions
The z-transform converges without fuss for infinite-energy sequences which the Fourier transform has some difficulties dealing with.
For instance, the z-transform manages to “bring down” the unit step because of the vanishing power of z-n for |z| is larger than 1 and n large and this is the case for all one-sided sequences which grow no more than exponentially.
However, if |z-n|→ 0 for n →∞ then necessarily |z-n|→∞ for n →-∞ and this may pose a problem for the convergence of the z-transform in the case of two-sided sequences.
In particular, the z-transform does not converge in the case of periodic signals since only one side of the repeating pattern is “brought down” while the other is amplified limitlessly.
We can circumvent this impasse by “killing” half of the periodic signal with a unit step.
Take for instance the one-sided cosine:
its z-transform can be derived as the formula.
Similar results can be obtained for signals such as x[n] equal to sin(ω0n)u[n] or x[n] equal to αn cos(ω0n)u[n].
Example 6 point 2: The impossibility of ideal filters
The z-transform of an FIR impulse response can be expressed as a simple polynomial P(z) of degree L - 1 where L is the number of nonzero taps of the filter (we can neglect leading factors of the form z-N).
The fundamental theorem of algebra states that such a polynomial has at most L - 1 roots; as a consequence, the frequency response of an FIR filter can never be identically zero over a frequency interval since, if it were, its z-transform would have an infinite number of roots.
Similarly, by considering the polynomial P(z) - C, we can prove that the frequency response can never be constant C over an interval which proves the impossibility of achieving ideal (for example “brickwall”) responses with an FIR filter.
The argument can be easily extended to rational transfer functions, confirming the impossibility of a realizable filter whose characteristic is piecewise perfectly flat.
The z-transform is closely linked to the solution of linear, constant coefficient difference equations.
For a more complete treatment, see, for example, R. Vich, Z Transform Theory and Applications (Springer, 1987), or A. J. Jerri, Linear Difference Equations with Discrete Transforms Method (Kluwer, 1996).
Filter Design
In discrete-time signal processing, filter design is the art of turning a set of requirements into a well-defined numerical algorithm.
The requirements, or specifications, are usually formulated in terms of the filter’s frequency response; the design problem is solved by finding the appropriate coefficients for a suitable difference equation which implements the filter and by specifying the filter’s architecture.
Since realizable filters are described by rational transfer functions, filter design can usually be cast in terms of a polynomial optimization procedure for a given error measure.
Additional design choices include the computational cost of the designed filters, for example the number of mathematical operations and storage necessary to compute each output sample.
Finally, the structure of the difference equation defines an explicit operational procedure for computing the filter’s output values; by arranging the terms of the equation in different ways, we can arrive at different algorithmic structures for the implementation of digital filters.
As we have seen, a realizable filter is described by a rational transfer function; designing a filter corresponds to determining the coefficients of the polynomials in transfer function with respect to the desired filter characteristics.
For an FIR filter of length M, there are M coefficients that need to be determined, and they correspond directly to the filter’s impulse response.
Similarly, for an IIR filter with a numerator of degree M - 1 and a denominator of degree N - 1, there are M plus N - 1 coefficients to determine (since we always assume a0 equal to 1).
The main questions are the following:
How do we specify the characteristics of the desired filter?
This question effectively selects the domain in which we will measure the difference (for example the error) between the desired filter and the achieved implementation.
This can be the time domain (where we would be comparing impulse responses) or the frequency domain (where we would be comparing frequency responses).
Usually the domain of choice is the frequency domain.
What are the criteria to measure the quality of the obtained filter?
This question defines the way in which the above-mentionned error is measured; again, different criteria are possible (such as minimum square error or minimax) and they do depend on the intended application.
How do we choose the filter’s coefficients in order to obtain the desired filtering characteristics?
This question defines an optimization problem in a parameter space of dimension M plusN-1 with the optimality criterion chosen above; it is usually answered by the existence of a numerical recipe which performs the task.
What is the best algorithmic structure (software or hardware) to implement a given digital filter?
This last question concerns the algorithmic design of the filter itself; the design is subject to various application-dependent constraints which include computational speed, storage requirement and arithmetic precision.
Some of these design choices will be addressed at the end of the Chapter.
As is apparent, real-world filters are designed with a variety of practical requirements in mind, most of which are conflicting.
One such requirement, for instance, is to obtain a low “computational price” for the filtering operation; this cost is obviously proportional to the number of coefficients in the filter, but it also depends heavily on the underlying hardware architecture.
The tradeoffs between disparate requirements such as cost, precision or numerical stability are very subtle and not altogether obvious; the art of the digital filter designer, although probably less dazzling than the art of the analog filter designer, is to determine the best design strategy for a given practical problem.
Filter design has a long and noble history in the analog domain: a linear electronic network can be described in terms of a differential equation linking, for instance, the voltage as a function of time at the input of the network to the voltage at the output.
The arrangement of the capacitors, inductances and resistors in the network determine the form of the differential equation, while their values determine its coefficients.
A fundamental difference between an analog filter and a digital filter is that the transformation from input to output is almost always considered instantaneous (for example the propagation effects along the circuit are neglected).
In digital filters, on the other hand, the delay is always explicit and is actually the fundamental building block in a processing system.
Because of the physical properties of capacitors, which are ubiquitous in analog filters, the transfer function of an analog filter (expressed in terms of its Laplace transform) is “similar” to the transfer function of an IIR filter, in the sense that it contains both poles and zeros.
In a sense, IIR filters can be considered as the discrete-time counterpart of classic analog filters.
FIR filters, on the other hand, are the flagship of digital signal processing; while one could conceive of an analog equivalent to an FIR, its realization would require the use of analog delay lines, which are costly and impractical components to manufacture.
In a digital signal processing scenario, on the other hand, the designer can freely choose between two lines of attack with respect to a filtering problem, IIR or FIR, and therefore it is important to highlight advantages and disadvantages of each.
FIR Filters.
The main advantages of FIR filters can be summarized as follows:
unconditional stability;
precise control of the phase response and, in particular, exact linear phase;
optimal algorithmic design procedures;
robustness with respect to finite numerical precision hardware.
While their disadvantages are mainly:
longer input-output delay;
higher computational cost with respect to IIR solutions.
IIR Filters.
IIR filters are often an afterthought in the context of digital signal processing in the sense that they are designed by mimicking established design procedures in the analog domain; their appeal lies mostly in their compact formulation: for a given computational cost, i.e for a given number of operations per input sample, they can offer a much better magnitude response than an equivalent FIR filter.
Furthermore, there are a few fundamental processing tasks (such as DC removal, as we will see later) which are the natural domain of IIR filters.
The drawbacks of IIR filters, however, mirror in the negative the advantages of FIR’s.
The main advantages of IIR filters can be summarized as follows:
lower computational cost with respect to an FIR with similar behavior;
shorter input-output delay;
compact representation.
While their disadvantages are mainly:
stability is not guaranteed;
phase response is difficult to control;
design is complex in the general case;
sensitive to numerical precision.
For these reasons, in this book, we will concentrate mostly on the FIR design problem and we will consider of IIR filters only in conjunction with some specific processing tasks which are often encountered in practice.
A set of filter specifications represents a set of guidelines for the design of a realizable filter.
Generally, the specifications are formulated in the frequency domain and are cast in the form of boundaries for the magnitude of the frequency response; less frequently, the specifications will take the phase response into account as well.
A set of filter specifications is best illustrated by example: suppose our goal is to design a half-band lowpass filter, for example a lowpass filter with cutoff frequency pi over 2. The filter will possess a passband, for example a frequency range over which the input signal is unaffected, and a stopband, for example a frequency range where the input signal is annihilated.
In order to turn these requirements into specifications the following practical issues must be taken into account:
Transition band.
The range of frequencies between passband and stopband is called the transition band.
We should know by now (and we shall see again shortly) that we cannot obtain an instantaneous transition in a realizable filter(1) .
Therefore, we must be willing to allow for a gap between passband and stopband where we renounce control over the frequency response; suppose we estimate that we can tolerate a transition band width up to 20% of the total bandwidth: since the cutoff is supposed to be at 0 point 5pi, the transition band will thus extend from 0 point 4pi to 0 point 6pi.
Tolerances.
Similarly, we cannot impose a strict value of 1 for the passband and a value of 0 for the stopband (again, this has to do with the fact that the rational transfer function, being analytical, cannot be a constant over an interval).
So we must allow for tolerances, for example minimum and maximum values for the frequency response over passband and stopband (while, in the transition band, we don’t care).
In our example, suppose that after examining the filter usage scenario we decide we can afford a 10% error in the passband and a 30% error in the stopband.
(Note that these are huge tolerances, but they make the plots easier to read).
These specifications can be represented graphically as in Figure 7 point 1; note that, since we are dealing with real-valued filter coefficients, it is sufficient to specify the desired frequency response only over the [0,pi] interval, the magnitude response being symmetric.
The filter design problem consists now in finding the minimum size FIR or IIR filter which fulfills the required specifications.
As an example, Figure 7 point 2 shows an IIR filter which does not fulfill the specifications since the stopband error is above the maximum error at the beginning of the stopband.
Similarly, Figure 7 point 3 shows an FIR filter which breaks the specifications in the passband.
Finally, Figure 7 point 4 shows a monotonic IIR filter which matches and exceeds the specifications (for example the error is always smaller than the maximum error).
In this section we will explore two fundamental strategies for FIR filter design, the window method and the minimax (or Parks-McClellan) method.
Both methods seek to minimize the error between a desired (and often ideal) filter transfer function and the transfer function of the designed filter; they differ in the error measure which is used in the minimization.
The window method is completely straightforward and it is often used for quick designs.
The minimax method, on the other hand, is the procedure of choice for accurate, optimal filters.
Both methods will be illustrated with respect to the design of a lowpass filter.
We have already seen in Section 5 point 6 that if there are no constraints (not even realizability) the best lowpass filter with cutoff frequency ωc is the ideal lowpass.
The impulse response is therefore the inverse Fourier transform of the desired transfer function.
The resulting filter, as we saw, is an ideal filter and it cannot be represented by a rational transfer function with a finite number of coefficients.
Impulse Response Truncation.
A simple idea to obtain a realizable filter is to take a finite number of samples from the ideal impulse response and use them as coefficients of a (possibly rather long) FIR filter.
This is a (2N plus 1)-tap FIR obtained by truncating an ideal impulse response (Figs 5 point 10 and 5 point 11).
Note that the filter is noncausal, but that it can be made causal by using an N-tap delay; it is usually easier to design FIR’s by considering a noncausal version first, especially if the resulting impulse response is symmetric (or antisymmetric) around n equal to 0. Although this approximation was derived in a sort of “intuitive” way, it actually satisfies a very precise approximation criterion, namely the minimization of the mean square error (MSE) between the original and approximated filters.
Denote by E2 this error, that is.
We can apply Parseval’s theorem (see (4 point 59)) to obtain the equivalent expression in the discrete-time domain.
If we now recall that ĥ[n] equal to 0 for |n| is larger than N, we have.
Obviously the last two terms are nonnegative and independent of ĥ[n].
Therefore, the minimization of E2 with respect to ĥ[n] is equivalent to the minimization of the first term only, and this is easily obtained by letting the formula.
In spite of the attractiveness of such a simple and intuitive solution, there is a major drawback.
If we consider the frequency response of the approximated filter, we have which means that Ĥ(ejω) is an approximation of H(ejω) obtained by using only 2N plus 1 Fourier coefficients.
Since H(ejω) has a jump discontinuity in ωc, Ĥ(ejω) incurs the well-known Gibbs phenomenon around ωc.
The Gibbs phenomenon states that, when approximating a discontinuous function with a finite number of Fourier coefficients, the maximum error in an interval around the jump discontinuity is actually independent of the number of terms in the approximation and is always equal to roughly 9% of the jump.
In other words, we have no control over the maximum error in the magnitude response.
This is apparent in Figure 7 point 5 where |Ĥ(ejω)| is plotted for increasing values of N; the maximum error does not decrease with increasing N and, therefore, there are no means to meet a set of specifications which require less than 9% error in either stopband or passband.
The Rectangular Window.
Another way to look at the resulting approximation is to express ĥ[n] as w[n] is called a rectangular window of length (2N plus 1) taps, which in this case is centered at n equal to 0.
We know from the modulation theorem in (5 point 22) that the Fourier transform of (7 point 2) is the convolution (in the space of 2pi-periodic functions) of the Fourier transforms of h[n] and w[n].
It is easy to compute W(ejω).
An example of W(ejω) for N equal to 6 is shown in Figure 7 point 6.
By analyzing the form of W(ejω) for arbitrary N, we can determine that:
the first zero crossing of W(ejω) occurs at ω equal to 2pi over (2N plus 1);
the width of the main lobe of the magnitude response is Δ equal to 4pi(2N plus 1);
there are multiple sidelobes, an oscillatory effect around the main lobe and there are up to 2N sidelobes for a 2N plus 1-tap window.
In order to understand the shape of the approximated filter, let us go back to the lowpass filter example and try to visualize the effect of the convolution in the Fourier transform domain.
First of all, since all functions are 2pi-periodic, everything happens circularly, for example what “goes out” on the right of the [-pi,pi] interval “pops” immediately up on the left.
The value at ω0 of Ĥ(ejω) is the integral of the product between H(ejω) and a version of W(ejω) circularly shifted by ω0.
Since H(ejω) is zero except over [-ωc,ωc], where it is one, this value is actually.
When ω0 is such that the first right sidelobe of W(ejω) is outside of the [-ωc,ωc] interval, then the integral reaches its maximum value, since the sidelobe is negative and it’s the largest.
The maximum value is dependent on the shape of the window (a rectangle in this case) but not on its length.
Hence the Gibbs phenomenon.
To recap, the windowing operation on the ideal impulse response, for example the circular convolution of the ideal frequency response with W(ejω), produces two main effects.
The sharp transition from passband to stopband is smoothed by the convolution with the main lobe of width Δ.
Ripples appear both in the stopband and the passband due to the convolution with the sidelobes (the largest ripple being the Gibbs phenomenon).
The sharpness of the transition band and the size of the ripples are dependent on the shape of the window’s Fourier transform; indeed, by carefully designing the shape of the windowing sequence we can trade mainlobe width for sidelobe amplitude and obtain a more controlled behavior in the frequency response of the approximation filter (although the maximum error can never be arbitrarily reduced).
Other Windows.
In general, the recipe for filter design by windowing involves two steps: the analytical derivation of an ideal impulse response followed by a suitable windowing to obtain an FIR filter.
The ideal impulse response h[n] is obtained from the desired frequency response H(ejω) by the usual DTFT inversion formula.
While the analytical evaluation of the above integral may be difficult or impossible in the general case, for frequency responses H(ejω) which are piecewise linear, the computation of h[n] can be carried out in an exact (if nontrivial) way; the result will be a linear combination of modulated sinc and sinc-squared sequences.(3)
The FIR approximation is then obtained by applying a finite-length window w[n] to the ideal impulse response as in (7 point 2).
The shape of the window can of course be more sophisticated than the simple rectangular window we have just encountered and, in fact, a hefty body of literature is devoted to the design of the “best” possible window.
In general, a window should be designed with the following goals in mind:
the window should be short, as to minimize the length of the FIR and therefore its computational cost;
the spectrum of the window should be concentrated in frequency around zero as to minimize the “smearing” of the original frequency response; in other words, the window’s main lobe should be as narrow as possible (it is clear that for W(ejω) equal to δ(ω) the resulting frequency response is identical to the original);
the unavoidable sidelobes of the window’s spectrum should be small, so as to minimize the rippling effect in the resulting frequency response (Gibbs phenomenon).
It is clear that the first two requirements are openly in conflict; indeed, the width of the main lobe Δ is inversely proportional to the length of the window (we have seen, for instance, that for the rectangular window Δ equal to 4pi over M, with M, the length of the filter).
The second and third requirements are also in conflict, although the relationship between mainlobe width and sidelobe amplitude is not straightforward and can be considered a design parameter.
In the frequency response, reduction of the sidelobe amplitude implies that the Gibbs phenomenon is decreased, but at the “price” of an enlargement of the filter’s transition band.
While a rigorous proof of this fact is beyond the scope of this book, consider the simple example of a triangular window (with N odd).
It is easy to verify that wt[n] equal to w[n] * w[n], with w[n] equal to rect(2n over (N - 1)) (for example the triangle can be obtained as the convolution of a half-support rectangle with itself) so that, as a consequence of the convolution theorem, we have.
The net result is that the amplitude of the sidelobes is quadratically reduced but the amplitude of the mainlobe Δ is roughly doubled with respect to an equivalent-length rectangular window; this is displayed in Figure 7 point 7 for a 17-point window (values are normalized so that both frequency responses are equal in ω equal to 0).
Filters designed with a triangular window therefore exhibit a much wider transition band.
Other commonly used windows admit a simple parametric closed form representation; the most important are the Hamming window (Fig. 7 point 8) and the Blackman window (Fig. 7 point 9).
The magnitude response of both windows is plotted in Figure 7 point 10 (on a log scale so as to enhance the difference in sidelobe amplitude); again, we can remark the tradeoff between mainlobe width (translating to a wider transition band in the designed filter) and sidelobe amplitude (influencing the maximum error in passband and stopband).
Limitations of the Window Method.
Lack of total control on passband and stopband error is the main limitation inherent to the window method; this said, the method remains a fundamental staple of practical signal processing as it yields perfectly usable filters via a quick, flexible and simple procedure.
The error characteristic of a window-designed filter can be particularly aggravating in sensitive applications such as audio processing, where the peak in the stopband error can introduce unacceptable artifacts.
In order to improve on the filter performance, we need to completely revise our design approach.
A more suitable optimization criterion may, for instance, be the minimax criterion, where we aim to explicitly minimize the maximum approximation error over the entire frequency support; this is thoroughly analyzed in the next section.
We can already say, however, that while the minimum square error is an integral criterion, the minimax is a pointwise criterion; or, mathematically, that the MSE and the minimax are respectively L2([-pi,pi])- and L∞([-pi,pi])-norm minimizations for the error function E(ω) equal to Ĥ(ejω) - H(ejω). Figure 7 point 11 illustrates the typical result of applying both criteria to the ideal lowpass problem.
As can be seen, the minimum square and minimax solutions are very different.
As we saw in the opening example, FIR filter design by windowing minimizes the overall mean square error between the desired frequency response and the actual response of the filter.
Since this might lead to a very large error at frequencies near the transition band, we now consider a different approach, namely the design by minimax optimization.
This technique minimizes the maximum allowable error in the filter’s magnitude response, both in the passband and in the stopband.
Optimality in the minimax sense requires therefore the explicit stating of a set of tolerances in the prototypical frequency response, in the form of design specifications as seen in Section 7 point 1 point 2. Before tackling the design procedure itself, we will need a series of intermediate results.
Generalized Linear Phase.
In Section 5 point 4 point 3, we introduced the concept of linear phase; a filter with linear phase response is particularly desirable since the phase response translates to just a time delay (possibly fractional) and we can concentrate on the magnitude response only.
We also introduced the notion of group delay and showed that linear phase corresponds to constant group delay.
Clearly, the converse is not true: a frequency response of the type has constant group delay but differs from a linear phase system by a constant phase factor ejα.
We will call this type of phase response generalized linear phase.
Important cases are those for which α equal to 0 (strictly linear phase) and α equal to pi over 2 (generalized linear phase used in differentiators).
FIR Filter Types.
Consider a causal, M-tap FIR filter with impulse response h[n], n equal to 0,1,…,M - 1; in the following, we are interested in filters whose impulse response is symmetric or antisymmetric around the “midpoint”.
If the number of taps is odd, the midpoint of the impulse response coincides with the center tap h[(M - 1) over 2]; if the number of taps is even, on the other hand, the midpoint is still at (M - 1) over 2 but this value does not coincide with a tap since it is located “right in between” taps h[M over 2 - 1] and h[M over 2].
Symmetric and antisymmetric FIR filters are important since their frequency response has generalized linear phase.
The delay introduced by these filters is equal to (M - 1) over 2 samples; clearly, this is an integer delay if M is odd, and it is fractional (half a sample more) if M is even.
There are four different possibilities for linear phase FIR impulse responses, which are listed here with their corresponding generalized linear phase parameters.
The generalized linear phase of (anti)symmetric FIRs is easily shown.
Consider for instance a Type I filter, and define C equal to (M - 1) over 2, the location of the center tap; we can compute the transfer function of the shifted impulse response hd[n] equal to h[n plus C], which is now symmetric around zero, for example hd[-n] equal to hd[n].
By undoing the time shift we obtain the original Type I transfer function.
On the unit circle (7 point 7) becomes which is a real function; the original Type I frequency response is obtained from (7 point 8) which is clearly linear phase with delay d equal to (M - 1) over 2 and α equal to 0. The generalized linear phase of the other three FIR types can be shown in exactly the same way.
Zero Locations.
The symmetric structures of the four types of FIR filters impose some constraints on the locations of the zeros of the transfer function.
Consider again a Type I filter; from (7 point 7) it is easy to see that Hd(z-1) equal to Hd(z); by using (7 point 8) we therefore have which leads to
It is easy to show that the above relation is also valid for Type II filters, while for Type III and Type IV (antisymmetric filters) we have
These relations mean that if z0 is a zero of a linear phase FIR, then so is z0-1.
This result, coupled with the usual fact that all complex zeros come in conjugate pairs, implies that if z0 is a zero of H(z).
If z0 equal to ρ ∈ R then ρ and 1 over ρ are zeros.
If z0 equal to ρejθ then ρejθ, (1 over ρ)ejθ, ρe-jθ and (1 over ρ)e-jθ are zeros.
Consider now equation (7 point 10) again; if we set z equal to -1, for Type II filters, M - 1 is an odd number, which leads to the conclusion that H(-1) equal to 0; in other words, Type II filters must have a zero at ω equal to pi.
Similar results can be demonstrated for the other filter types, and they are summarized below.
These constraints are important in the choice of the filter type for a given set of specifications.
Type II and Type III filters are not suitable in the design of highpass filters, for instance; similarly, Type III and Type IV filters are not suitable in in the design of lowpass filters.
Chebyshev Polynomials.
Chebyshev polynomials are a family of orthogonal polynomials {Tk(x)}k∈N which have, amongst others, the following interesting property:
in other words, the cosine of an integer multiple of an angle ω can be expressed as a polynomial in the variable cosω.
The first few Chebyshev polynomials are
and, in general, they can be derived from the recursion formula.
From the above table it is easy to see that we can write, for instance.
The interest in Chebyshev polynomials comes from the fact that the zero-centered frequency response of a linear phase FIR can be expressed as a linear combination of cosine functions, as we have seen in detail for Type I filters in (7 point 9).
By using Chebyshev polynomials we can rewrite such a response as just one big polynomial in the variable cosω.
Let us consider an explicit example for a length-7 Type I filter with nonzero coefficients h[n] equal to [d c b a b c d]; we can state that and by using the first four Chebyshev polynomials we can write.
In this case, Hd(ejω) turns out to be a third degree polynomial in the variable cosω.
This is the case for any Type I filter, for which we can always write where P(x) is a polynomial of degree (M - 1) over 2 whose coefficients ck are derived as linear combinations of the original filter coefficients ak as illustrated in (7 point 15).
For the other types of linear phase FIR, a similar representation can be obtained after a few trigonometric manipulations.
The general expression is where the ck are still linear combinations of the original filter coefficients and where f(ω) is a weighting trigonometric function.
Both f(ω) and the polynomial degree K vary as a function of the filter type.(4)
In the following Sections, however, we will concentrate only on the design of Type I filters, so these details will be overlooked; in practice, since the design is always carried out using numerical packages, the appropriate formulation for the filter expression is taken care of automatically.
Polynomial Optimization.
Going back to the filter design problem, we stipulate that the FIR filters are (generalized) linear phase, so we can concentrate on the real frequency response of the zero-centered filter, which is represented by the trigonometric polynomial (7 point 19).
Moreover, since the impulse response is real and symmetric, the aforementioned real frequency response is also symmetric around ω equal to 0. The filter design procedure can thus be carried out only for values of ω over the interval [0,pi], with the other half of the spectrum obtained by symmetry.
For these values of ω, the variable x equal to cosω is mapped onto the interval [1,-1] and the mapping is invertible.
Therefore, the filter design problem becomes a problem of polynomial approximation over intervals.
To illustrate the procedure by example, consider once more the set of filter specifications in Figure 7 point 1 and suppose we decide to use a Type I filter.
Recall that we required the prototype filter to be lowpass, with a transition band from ωp equal to 0 point 4pi to ωs equal to 0 point 6pi; we further stated that the tolerances for the realized filter’s magnitude must not exceed 10% in the passband and 1% in the stopband.
This implies that the maximum magnitude error between the prototype filter and the FIR filter response H(ejω) must not exceed δp equal to 0 point 1 in the interval [0,ωp] and must not exceed δs equal to 0 point 01 in the interval [ωs,pi].
We can formulate this as follows: the frequency response of the desired filter is
(note that HD(ejω) is not specified in the transition band).
Since the tolerances on passband and stopband are different, they can be expressed in terms of a weighting function HW (ω) such that the tolerance on the error is constant over the two bands.
With this notation, the filter specifications amount to the following.
and the question now is to find the coefficients for h[n] (their number M and their values) which minimize the above error.
Note that we leave the transition band unconstrained (for example it does not affect the minimization of the error).
The next step is to use (7 point 19) to reformulate the above expression as a polynomial optimization problem.
To do so, we replace the frequency response Hd(ejω) with its polynomial equivalent and set x equal to cosω; the passband interval [0,ωp] and the stopband interval [ωs,pi] are mapped into the intervals for x:
respectively; similarly, the desired response becomes:
and the weighting function becomes.
The new set of specifications are shown in Figure 7 point 12. Within this polynomial formulation, the optimization problem becomes.
where P(x) is the polynomial representation of the FIR frequency response as in (7 point 19).
Alternation Theorem.
The optimization problem stated by (7 point 24) can be solved by using the following theorem:
Theorem 7 point 1 Consider a set {Ik} of closed, disjoint intervals on the real axis and their union I equal to ⋃ kIk.
Consider further:
a polynomial P(x) of degree L, P(x) equal to ∑ nequal to0La nxn;
a desired function D(x), continuous over I;
a positive weighting function W(x).
Consider now the approximation error function and the associated maximum approximation error over the set of closed intervals.
Then P(x) is the unique order-L polynomial which minimizes Emax if and only if there exist at least L plus 2 successive values xi in I such that |E(xi)| equal to Emax and
In other words, the error function must have at least L plus 2 alternations between its maximum and minimum values.
Such a function is called equiripple.
Going back to our lowpass filter example, assume we are trying to design a 9-tap optimal filter.
This theorem tells us that if we found a polynomial P(x) of degree 4 such that the error function (7 point 24) over Ip and Is as is shown in Figure 7 point 13 (6 alternations), then the polynomial would be the optimal and unique solution.
Note that the extremal points (for example the values of the error function at the edges of the optimization intervals) do count in the number of alternations since the intervals Ik are closed.
The above theorem may seem a bit far-fetched since it does not tell us how to find the coefficients but it only gives us a test to verify their optimality.
This test, however, is at the core of an iterative algorithm which refines the polynomial from an initial guess to the point when the optimality condition is met.
Before considering the optimization procedure more in detail, we will state without formal proof, three consequences of the alternation theorem as it applies to the design of Type I lowpass filters:
The minimum number of alternations for an optimal M-tap lowpass filter is L plus 2, with L equal to (M - 1) over 2; this is the result of the alternation theorem.
The maximum number of alternation, however, is L plus 3; filters with Lplus3 alternation are called extraripple filters.
Alternations always take place at x equal to cosωp and x equal to cosωs (for example at ω equal to ωp and ω equal to ωs.
If the error function has a local maximum or minimum, its absolute value at the extremum must be equal to Emax except possibly in x equal to 0 or x equal to 1.
In other words, all local maxima and minima of the frequency response must be alternation, except in ω equal to 0 or ω equal to pi.
If the filter is extraripple, the extra alternation occurs at either ω equal to 0 or ω equal to pi.
Optimization Procedure.
Finally, by putting all the elements together, we are ready to state an algorithmic optimization procedure for the design of optimal minimax FIR filters; this procedure is usually called the Parks-McClellan algorithm.
Remember, we are trying to determine a polynomial P(x) such that the approximation error in (7 point 24) is equiripple; for this, we need to determine both the degree of the polynomial and its coefficients.
For a given degree L, for which the resulting filter has 2L plus 1 taps, the L coefficients are found by an iterative procedure which successively refines an initial guess for the L plus 2 alternation points xi until the error is equiripple.(5)
After the iteration has converged, we need to check that the corresponding Emax satisfies the upper bound imposed by the specifications; when this is not the case, the degree of the polynomial (and therefore the length of the filter) must be increased and the procedure must be restarted.
Once the conditions on the error are satisfied, the filter coefficients can be obtained by inverting the Chebyshev expansion.
As a final note, an initial guess for the number of taps can be obtained using the empirical formula by Kaiser; for an M-tap FIR h[n], n equal to 0,…, M - 1:
where δp is the passband tolerance, δs is the stopband tolerance and Ω equal to ωs - ωp is the width of the transition band.
The Final Design.
We now summarize the design steps for the specifications in Figure 7 point 1.
We use a Type I FIR.
We start by using Kaiser’s formula to obtain an estimate of the number of taps: since δpδs equal to 10-3 and Ω equal to 0 point 2pi, we obtain M equal to 12 point 6 which we round up to 13 taps.
At this point we can use any numerical package for filter design to run the Parks-McClellan algorithm.
In Matlab this would be the formula.
The resulting frequency response is plotted in Figure 7 point 14; please note that we are plotting the frequency responses of the zero-centered filter hd[n], which is a real function of ω.
We can verify that the filter has indeed (M - 1) over 2 equal to 6 alternation by looking at enlarged picture of the passband and the stopband, as in Figure.
The maximum error as returned by Matlab is however 0 point 102 which is larger than what our specifications called for, for example 0 point 1.
We are thus forced to increase the number of taps; since we are using a Type I filter, the next choice is M equal to 15.
Again, the error turns out to be larger than 0 point 1, since in this case we have Emax equal to 0 point 1006.
The next choice, M equal to 17, finally yields an error Emax equal to 0 point 05, which exceeds the specifications by a factor of 2. It is the designer’s choice to decide whether the computational gains of a shorter filter (M equal to 15) outweigh the small excess error.
The impulse response and the frequency response of the 17-tap filter are plotted in Figure 7 point 16 and Figure 7 point 17. Figure 7 point 18 shows the zero locations for the filter; note the typical linear-phase zero pattern as well as the zeros on the unit circle in the stopband.
Other Types of Filters.
The Parks-McClellan optimal FIR design procedure can be made to work for arbitrary filter types as well, such as highpass and bandpass, but also for more sophisticated frequency responses.
The constraints imposed by the zero locations as we saw on page Â§ determine the type of filter to use; once the desired response HD(ejω) is expressed as a trigonometric function, the optimization algorithm can take its course.
For arbitrary frequency responses, however, the fact that the transition bands are left unconstrained may lead to unacceptable peaks which render the filter useless.
In these cases, visual inspection of the obtained response is mandatory and experimentation with different filter lengths and tolerance may improve the final result.
As we mentioned earlier, no optimal procedure exists for the design of IIR filters.
The fundamental reason is that the optimization of the coefficients of a rational transfer function is a highly nonlinear problem and no satisfactory algorithm has yet been developed for the task.
This, coupled with the impossibility of obtaining an IIR with linear phase response(6) makes the design of the IIR filter a much less formalized art.
Many IIR designed techniques are described in the literature and their origin is usually in tried-and- true analog filter design methods.
In the early days of digital signal processing, engineers would own voluminous books with exhaustive lists of capacitance and inductance values to be used for a given set of (analog) filter specifications.
The idea behind most digital IIR filter design techniques was to be able to make use of that body of knowledge and to devise formulas which would translate the analog design into a digital one.
The most common such method is known as bilinear transformation.
Today, the formal step through an analog prototype has become unnecessary and numerical tools such as Matlab can provide a variety of routines to design an IIR.
Here we concentrate only on some basic IIR filters which are very simple and which are commonly used in practice.
There are a few applications in which simple IIR structures are the design of choice.
These filters are so simple and so well behaved that they are a fundamental tool in the arsenal of any signal processing engineer.
DC Removal and Mean Estimation.
The DC component of a signal is its mean value; a signal with zero mean is also called an AC signal.
This nomenclature comes from electrical circuit parlance: DC is shorthand for direct current, while AC stands for alternating current;(7) you might be familiar with these terms in relation to the current provided by a battery (constant and hence DC) and the current available from a mains socket (alternating at 50 or 60 Hz and therefore AC).
For a given sequence x[n], one can always write where xDC is the mean of the sequence values.
Please note the followings:
The DC value of a finite-support signal is the value of its Fourier transform at ω equal to 0 times the length of the signal’s support.
The DC value of an infinite-support signal must be zero for the signal to be absolutely summable or square summable.
In most signal processing applications, where the input signal comes from an acquisition device (such as a sampler, a soundcard and so on), it is important to remove the DC component; this is because the DC offset is often a random offset caused by ground mismatches between the acquisition device and the associated hardware.
In order to eliminate the DC component we need to first estimate it, for example we need to estimate the mean of the signal.
For finite-length signals, computation of the mean is straightforward since it involves a finite number of operations.
In most cases, however, we do not want to wait until the end of the signal before we try to remove its mean; what we need is a way to perform DC removal on line.
The approach is therefore to obtain, at each instant, an estimate of the DC component from the past signal values, with the assumption that the estimate converges to the real mean of the signal.
In order to obtain such an estimate, for example in order to obtain the average value of the past input samples, both approaches detailed in Section 5 point 3 are of course valid (for example the Moving Average and the Leaky Integrator filters) .
We have seen, however, that the leaky integrator provides a superior cost/benefit tradeoff and therefore the output of a leaky integrator with λ very close to one (usually 10-3) is the estimate of choice for the DC component of a signal.
The closer λ is to one, the more accurate the estimation; the speed of convergence of the estimate however becomes slower and slower as λ → 1.
This can easily be seen from the group delay at ω equal to 0, which is the formula.
Resonator Filter.
Let us look again at how the leaky integrator works.
Consider its z-transform.
and notice that what we really want the filter to do is to extract the zero-frequency component (for example the frequency component that does not oscillate, that is, the DC component).
To do so, we placed a pole near z equal to 1, which of course corresponds to z equal to ejω for ω equal to 0. Since the magnitude response of the filter exhibits a peak near a pole, and since the peak will be higher, the closer the pole is to the unit circle, we are in fact amplifying the zero-frequency component; this is apparent from the plot of the filter’s frequency response in Figure 5 point 9.
The numerator, 1 - λ, is chosen such that the magnitude of the filter at ω equal to 0 is one; the net result is that the zero-frequency component will pass unmodified while all the other frequencies will be attenuated.
The value of a filter’s magnitude at a given frequency is often called the gain.
The very same approach can now be used to extract a signal component at any frequency.
We will use a pole whose magnitude is still close to one (for example a pole near the unit circle) but whose phase is that of the frequency we want to extract.
We will then choose a numerator so that the magnitude is unity at the frequency of interest.
The one extra detail is that, since we want a real-valued filter, we must place a complex conjugate pole as well.
The resulting filter is called a resonator and a typical pole-zero plot is shown in Figure 7 point 19.
The z-transform of a resonator at frequency ω0 is therefore determined by the pole p equal to λejω0 and by its conjugate.
The numerator value G0 is computed so that the filter’s gain at Â±ω0 is one; since in this case |H(ejω0)| equal to |H(e-jω0)|, we have.
The magnitude and phase of a resonator with λ equal to 0 point 9 and ω0 equal to pi over 3 are shown in Figure 7 point 20.
A simple variant on the basic resonator can be obtained by considering the fact that the resonator is just a bandpass filter with a very narrow passband.
As for all bandpass filters, we can therefore place a zero at z equal to Â±1 and sharpen its midband frequency response.
The corresponding z-transform is now.
The corresponding magnitude response is shown in Figure 7 point 21.
We have seen in Section 5 point 7 point 2 a practical implementation of a constant-coefficient difference equation (written in C).
That was just one particular way of translating Equation (5 point 46) into a numerical procedure; in this Section we explore other alternatives for both FIR and IIR and introduce the concept of computational efficiency for filters.
The cost of a numerical filter is dependent on the number of operations per output sample and on the storage (memory) required in the implementation.
If we consider a generic CCDE, it is easy to see that the basic building blocks which make up the recipe for a realizable filter are:
an addition operator for sequence values, implementing y[n] equal to , x1[n] plus x2[n];
a scalar multiplication operator, implementing y[n] equal to αx[n];
a unit delay operator, implementing y[n] equal to x[n - 1].
Note that the unit delay operator is nothing but a memory cell, holding the previous value of a time-varying quantity.
By properly combining these elements and by exploiting the different possible decomposition of a filter’s rational transfer function, we can arrive at a variety of different working implementations of a filter.
To study the possibilities at hand, instead of relying on a specific programming language, we will use self explanatory block diagrams.
Cascade Forms.
Recall that a rational transfer function H(z) can always be written out as follows:
where the zn are the M - 1 (complex) roots of the numerator polynomial and the pn are the N - 1 (complex) roots of the denominator polynomial.
Since the coefficients of the CCDE are assumed to be real, complex roots for both polynomials always appear in complex-conjugate pairs.
A pair of first-order terms with complex-conjugate roots can be combined into a second-order term with real coefficients:
As a consequence, the transfer function can be factored into the product of first- and second-order terms in which the coefficients are all strictly real; namely:
where Mr is the number of real zeros, Mc is the number of complex-conjugate zeros and Mr plus 2Mc equal to M (and, equivalently, for the poles, Nr plus 2Nc equal to N).
From this representation of the transfer function we can obtain an alternative structure for a filter; recall that if we apply a series of filters in sequence, the overall transfer function is the product of the single transfer functions.
Working backwards, we can interpret (7 point 28) as the cascade of smaller sections.
The resulting structure is called a cascade and it is particularly important for IIR filters, as we will see later.
Parallel Forms.
Another interesting rewrite of the transfer function is based on a partial fraction expansion of the type:
where the multiplicity of the three types of terms as well as the relative coefficients are dependent (in a non-trivial way) on the original filter coefficients.
This generates a parallel structure of filters, whose outputs are summed together.
The first branch corresponds to the first sum and it is an FIR filter; a further set of branches are associated to each term in the second sum, each one of them a first order IIR; the last set of branches is a collection of second order sections, one for each term of the third sum.
In an FIR transfer function all the denominator coefficients an other than a0 are zero; we have therefore:
where, of course, the coefficients correspond to the nonzero values of the impulse response h[n], for example bn equal to h[n].
Using the constitutive elements outlined above, we can immediately draw a block diagram of an FIR filter as in Figure 7 point 22.
In practice, however, additions are distributed as shown in Figure 7 point 23; this kind of implementation is called a transversal filter.
Further, ad-hoc optimizations for FIR structures can be obtained in the the case of symmetric and antisymmetric linear phase filters; these are considered in the exercises.
For an IIR filter, all the an and bn in (5 point 46) are nonzero.
One possible implementation based on the direct form of the transfer function is given in Figure 7 point 24.
This implementation is called Direct Form I and it can immediately be seen that the C-code implementation in Section 5 point 7 point 2 realizes a Direct Form I algorithm.
Here, for simplicity, we have assumed N equal to M but obviously we can set some an or bn to zero if this is not the case.
By the commutative properties of the z-transform, we can invert the order of computation to turn the Direct Form I structure into the structure shown in Figure 7 point 25 (shown for a second order section); we can then combine the parallel delays together to obtain the structure in Figure 7 point 26.
This implementation is called Direct Form II; its obvious advantage is the reduced number of the required delay elements (hence of memory storage).
The second order filter which gives rise to the second order section displayed in Figure 7 point 26, is particularly important in the case of cascade realizations.
Consider the factored form of H(z) as in (7 point 28): if we combine the complex conjugate poles and zeros, and group the real poles and zeros in pairs, we can create a modular structure composed of second order sections.
For instance, Figure 7 point 27 represents a 4th order system.
Odd order systems can be obtained by setting some of the an or bn to zero.
A very important issue with digital filters is their numerical behavior for a given implementation.
Two key questions are:
Assume the computations are made with (basically) infinite precision but that the filter coefficients are represented internally with finite precision.
How good is the resulting filter?
Is it still stable?
If computations are also made with finite precision arithmetic (which implies rounding and truncation error), what is the resulting numerical behavior of the system?
One important difference is that, in the first case, the system is at least guaranteed to be linear; in the second case, however, we can have non-linear effects such as overflows and limit cycles.
Precision and computational issues are very hard to analyze.
Here, we will just note that the direct form implementation is more sensible to precision errors than the cascade form, which is why the cascade form is usually preferred in practice.
Moreover, alternative filter structures such as the lattice are designed to provide robustness for systems with low numerical precision, albeit at a higher computational cost.
The filtering structures that we have shown up to now are general-purpose architectures which apply to the most general class of discrete-time signals, (infinite) sequences.
We now consider the other two main classes of discrete-time signals, namely finite-length signals and periodic sequences, and show that specialized filtering algorithms can be advantageously put to use.
The convolution sum in (5 point 3) is defined for infinite sequences.
For a finite-length signal of length N we may choose to write simply:
N- 1 y[n] equal to H {x [n]} equal to ∑ x[k]h [n - k]
(7 point 30) for example we let the summation index span only the indices for which the signal is defined.
It can immediately be seen, however, that in so doing we are actually computing y[n] equal to Â¯x[n] * h[n], where Â¯x[n] is the finite support extension of x[n] as in (2 point 24)); that is, by using (7 point 30), we are implicitly assuming a finite support extension for the input signal.
Even when the input is finite-length, the output of an LTI system is not necessarily a finite-support sequence.
When the impulse response is FIR, however, the output has finite support; specifically, if the input sequence has support N and the impulse response has support L, the support of the output is N plus L - 1.
For periodic sequences, the convolution sum in (5 point 3) is well defined so there is no special care to be taken.
It is easy to see that, for any LTI system, an N-periodic input produces an N-periodic output.
A case of particular interest is the following: consider a length-N signal x[n] and its N-periodic extension ˜x[n].
Consider then a filter whose impulse response is FIR with a length-N support; if we call h[n] the length-N signal obtained by considering only the values of the impulse response over its finite support, the impulse response of the filter is Â¯h[n] (see (2 point 24)).
In this case we can write the formula.
Note that in the last sum, only the first period of ˜x[n] is used; we can therefore define the sum just in terms of the two N-point signals x[n] and h[n].
The above summation is called the circular convolution of x[n] and h[n] and is sometimes indicated as.
Note that, for periodic sequences, the convolution as defined in (5 point 8) and the circular convolution coincide.
The circular convolution, just like the standard convolution operator, is associative and commutative:
as is easily proven.
Consider now the output of the filter, expressed using the commutative property of the circular convolution.
Since the output sequence ỹ[n] is itself N-periodic we can consider the finite-length signal y[n] equal to ỹ[n], n equal to 0,…,N - 1, for example the first period of the output sequence.
The circular convolution can now be expressed in matrix form as where y,x are the usual vector notation for the finite-length signals y[n],x[n] and where.
The above matrix is called a circulant matrix, since each row is obtained by a right circular shift of the previous row.
A fundamental result, whose proof is left as an exercise, is that the length-N DFT basis vectors w(k) defined in (4 point 3) are left eigenvectors of N × N circulant matrices:
where H[k] is the k-th DFT coefficient of the length-N signal h[n], n equal to 0,…,N - 1.
If we now take the DFT of (7 point 33) then
We have just proven a finite-length version of the convolution theorem; to repeat the main points.
The convolution of an N-periodic sequence with a N-tap FIR impulse response is equal to the periodic convolution of two finite-length signals of length N, where the first signal is one period of the input and the second signal is the values of the impulse response over the support.
The periodic convolution can be expressed as a matrix-vector product in which the matrix is circulant.
The DFT of the circular convolution is simply the product of the DFTs of the two finite-length signals; in particular, (7 point 35) can be used to easily prove the commutativity and distributivity of the circular convolution.
The importance of this particular case of filtering stems from the following fact: the matrix-vector product in (7 point 33) requires O(N2) operations.
The same product can however be written as which, by using the FFT algorithm, requires approximately N plus 2N log 2N operations and is therefore much more efficient even for moderate values of N. Practical applications of this idea are the overlap-save and overlap-add filtering methods, for a thorough description of which see [?].
The basic idea is that, in order to filter a long input sequence with an N-tap FIR filter, the input is broken into consecutive length-N pieces and each piece, considered as the main period of a periodic sequence, is filtered using the FFT strategy above.
The difference between the two methods is in the subtle technicalities which allow the output pieces to bind together in order to give the correct final result.
Finally, we want to show that we could have quickly arrived at the same results just by considering the formal DTFTs of the sequences involved; this is an instance of the power of the DTFT formalism.
From (4 point 43) and (4 point 44) we obtain:
where the last equality results from the sifting property of the Dirac delta (see (4 point 31)) and the fact that Λ(0) equal to 1.
In the last expression, the DTFT of a periodic sequence whose DFS coefficients are given by H[k]X[k], is easily recognezed.
Stochastic Signal Processing
In the previous Chapters, the signals we considered were all deterministic signals in the sense that they could either be expressed in analytic form (such as x[n] equal to (1 - λ)λn) or they could be explicitly described in terms of their samples, such as in the case of finite-length signals.
When designing a signal processing system, however, it is very rare that we know exactly the expression for the set of all the possible input signals (in some sense, if we did, we would not need a processing system at all.)
Fortunately, very often this set can be characterized in terms of the statistical properties of its member signals; this entails leaving the domain of deterministic quantities and entering the world of stochastic processes.
A detailed and rigorous treatment of statistical signal processing is beyond the scope of this book; here, we only consider elementary concepts and restrict ourselves to the discrete-time case.
We will be able to derive that, fundamentally, in the case of stationary random signals, the standard signal processing machinery that we have seen so far (and especially the usual filter design techniques) is still applicable with very intuitive results.
To establish a coherent notation, we start by briefly reviewing some elementary concepts of probability theory.
Probability Distribution.
Consider a real-valued random variable X taking values over R. The random variable(1) is characterized by its cumulative distribution function FX (cdf) which is defined as that is, FX(α) measures the probability that X takes values less than or equal to α.
The probability density function (pdf) is related to the cdf (assuming that FX is differentiable) as and thus
Expectation and Second Order Statistics.
For random variables, a fundamental concept is that of expectation, defined as follows:
The expectation operator is linear; given two random variables X and Y , we have
Furthermare, given a function g, we have
The expectation of a random variable is called its mean, and we will indicate it by mX.
The expectation of the product of two random variables defines their correlation:
The variables are uncorrelated if
Sometimes, the “centralized” correlation, or covariance, is used, namely
Again, the two variables are uncorrelated if and only if their covariance is zero.
Note that if two random variables are independent, then they are also uncorrelated.
The converse, however, is not true; in other words, statistical independence is a stronger condition than decorrelation.(2)
The variance of a random variable X, denoted by σX2, is defined as
The square root of the variance, σX, is often called the standard deviation of X.
Probability Distribution.
With respect to vector random variables, two key notions are: independent elements: a collection of N random variables is independent if and only if the joint pdf has the form: elements: a collection of N random variables is independent and identically distributed (i.i.d.) if the variables are independent and each random variable has the same distribution.
Random vectors represent the generalization of finite-length, discrete-time signals to the space of random signals.
Expectation and Second Order Statistics.
For random vectors, the definitions given, in the case of random variables, extend immediately to the multidimensional case.
The mean of a N-element random vector X is simply the N-element vector.
The correlation of two N-element random vectors is the N × N matrix:
where the expectation operator is applied individually to all the elements of the matrix XYT.
The covariance is again:
and it coincides with the correlation for zero-mean random vectors.
Note that the general element RXY (k,l) indicates the correlation between the k-th element of X and the l-th element of Y .
In particular, RXX(k,l) indicates the correlation between elements of the random vector X; if the elements are uncorrelated, then the correlation matrix is diagonal.
Probability Distribution.
Intuitively, a discrete-time random process is the infinite-dimensional generalization of a vector random variable, just like a discrete-time sequence is the infinite generalization of a finite-length signal.
For a random process (also called a stochastic process) we use the notation X[n] to indicate the n-th random variable which is the n-th value (sample) of the sequence.(3)
Note however that the pdf associated to the random process is the joint distribution of the entire set of samples in the sequence; in general, therefore, the statistical properties of each sample depend on the global stochastic description of the process and this accounts for local and long-range dependencies in the random data.
In fact, consider a random process {X[n], n ∈ Z}; any finite subset of random variables from X[n] is a vector random variable [ ]T
X equal to X [i0] X [i1] ... X [ik- 1] , k ∈ N. The statistical description of a random process involves specifying the joint pdf for X for all k-tuples of time indices ik and all k ∈ N, for example all the pdfs of the form.
Clearly, the most general form of random process possesses a statistical description which is difficult to use.
At the other extreme, the simplest form of stochastic process is the i.i.d.
process.
For an i.i.d.
process we have that the elements of X are i.i.d. for all k-tuples of time indices ik and all k ∈ N, that is where f(x) is called the pdf of the i.i.d.
process.
Second Order Description.
The mean of a process X[n], n ∈ Z is simply E[X[n]] which, in general, depends on the index n.
The correlation (also called the autocorrelation) of X[n] is defined as while its covariance (also called autocovariance)(4) is the formula.
Finally, given two random processes X[n] and Y [n], their cross-correlation is defined as.
Mean and variance of a random process represent a second order description of the process since their computation requires knowledge of only the second order joint pdf of the process (for example of the pdfs in (8 point 4) involving only two indices ik).
A second order description is physically meaningful since it can be associated to the mean value and mean power of the random process, as we will see.
Stationary Processes.
A very important class of random processes are the stationary processes, for which the probabilistic behavior is constant over time.
Stationarity, in the strict sense, implies that the full probabilistic description of the process is time-invariant; for example, any i.i.d.
process is also a strict-sense stationary process.
Stationarity can be restricted to n-th order stationarity, meaning that joint distributions (and therefore expectations) involving up to n samples are invariant with respect to a time shift.
The case n equal to 2 is particulary important and it is called wide-sense stationarity (WSS).
For a WSS process, the mean and the variance are constant over time.
Finally, note that if X[n] and Y [n] are both stationary processes, then their cross-correlation depends only also on the time lag.
Ergodicity.
In the above paragraphs, it is important to remember that expectations are taken with respect to an ensemble of realizations of the process under analysis.
To visualize the concept, imagine having a black box which, at the turn of a switch, can generate a realization of a discrete-time random process X[n].
In order to estimate the mean of the process at time n0, for example E[X[n0]], we need to collect as many realizations as possible and then estimate the mean at time n0 by averaging the values of the process at n0 across realizations.
For stationary processes, it may seem intuitive that instead of averaging across realizations, we can average across successive samples of the same realization.
This is not true in the general case, however.
Consider for instance the process where α is a random variable.
Clearly the process is stationary since each realization of this process is a constant discrete-time signal, but the value of the constant changes for each realization.
If we try to estimate the mean of the process from a single realization, we obtain no information on the distribution of α; that can be achieved only by looking at several independent realizations.
The class of processes for which it is legitimate to estimate expectations from a single realization is the class of ergodic processes.
For ergodic processes we can, for instance, take the time average of the samples of a single realization and this average converges to the ensemble average or, in other words, it represents a precise estimate of the true mean of the stochastic process.
The same can be said for expectations involving the product of process samples, such as in the computation of the variance or of the correlation.
Ergodicity is an extremely useful concept in the domain of stochastic signal processing since it allows us to extract useful statistical information from a single realization of the process.
More often than not, experimental data is difficult or expensive to obtain and it is not practical to repeat an experiment over and over again to compute ensemble averages; ergodicity is the way out this problem, and it is often just assumed (sometimes without rigorous justification).
Given a stationary random process, we are interested in characterizing its “energy distribution” in the frequency domain.
Note that we have used quotes around the term energy: since a stationary process does not decay in time (because of stationarity), it is rather intuitive that its energy is infinite (very much like a periodic signal).
In other words, the sum: diverges in expectation.
Signals which are not square-summable are not absolutely summable either, and therefore their Fourier transform does not exist in the standard sense.
In order to derive a spectral representation for a random process we thus need to look for an alternative point of view.
In Section 2 point 1 point 6 we introduced the notion of a power signal, particularly in relation to the class of periodic sequences; while the total energy of a power signal may be infinite, its energy over any finite support is always finite and it is proportional to the length of the support.
In this case, the limit:
is finite and it represents the signal’s average power (in time).
Stationary random processes are themselves power signals if their variance is finite; indeed (assuming a zero-mean process), we have the formula,
so that the average power (in expectation) for a stationary process is given by its variance.
For signals (stochastic or not) whose power is finite but whose energy is not, a meaningful spectral representation is obtained by considering the so-called power spectral density (PSD).
We know that, for a square-summable sequence, the square magnitude of the Fourier transform represents the global spectral energy distribution.
Since the energy of a power signal is finite over a finite-length observation window, the truncated Fourier transform exists, is finite, and its magnitude is the energy distribution of the signal over the time interval [-M,M].
The power spectral density is defined as and it represents the distribution of power in frequency (and therefore its physical dimensionality is expressed as units of energy over units of time over units of frequency).
Obviously, the PSD is a 2pi-periodic real and non-negative function of ω.
It can be shown that the PSD of an N-periodic stationary signal ˜s [n] is given by the formula:
where all the ˜S[k] are the N DFS coefficients of s[n]; this is rather intuitive since, for a periodic signal, the power is distributed only over the harmonics of the fundamental frequency.
Conversely, the PSD of a finite-energy deterministic signal is obviously zero since its power is zero.
For stationary random processes the situation is rather interesting.
If we rewrite (8 point 11) for the WSS random process X[n], the quantity:
which we could call a “local energy distribution”, is now a random variable itself parameterized by ω.
We can therefore consider its mean value and we have.
Now, with the change of variable k equal to n-m and some simple considerations on the structure of the above sum, we obtain.
The power spectral density is obtained by plugging the above expression into (8 point 12), which gives where we have set the formula.
Since, if the autocorrelation is absolutely summable then the sum (8 point 13) converges uniformly to a continuous function of M. We can therefore move the limiting operation inside the sum; now the key observation is that the weighting term wk(M), considered as a function of k parametrized by M, converges in the limit to the constant one (Eq.
(8 point 14)):
This fundamental result means that the power spectral density of a WSS process is the discrete-time Fourier transform of its autocorrelation.
Similarly, we can define the cross-power spectral density between two WSS processes X[n] and Y [n] as.
A WSS random process W[n] whose mean is zero and whose samples are uncorrelated is called white noise.
The autocorrelation of a white noise process is therefore:
where σW 2 is the variance (for example the expected power) of the process.
The power spectral density of a white noise process is simply:
Please note:
The probability distribution of a white noise process can be any, provided that it is always zero mean.
The joint probability distribution of a white noise process need not be i.i.d.; if it is i.i.d., however, then the process is strict-sense stationary and it is also called a strictly white process.
White noise is an ergodic process, so that its pdf can be estimated from a single realization.
In stochastic signal processing, we are considering the outcome of a filtering operation which involves a random process; that is, given a linear time-invariant filter with impulse response h[n], we want to describe the output signal as follows:
Note that Y [n] and X[n] denote random variables and are thus capitalized, while h[n] is a deterministic impulse response and is therefore lowercase.
In the following, we will assume a stable LTI filter and a wide-sense stationary (WSS) input process.
Time-Domain Analysis.
The expected value of the filter’s output is where mn is the mean of X[n].
For a WSS input, obviously E[X[n]] equal to mX for all n, and therefore the output has a constant expected value:
If the input is WSS, it is fairly easy to show that the output is also WSS; in other words, LTI filtering preserves wide-sense stationarity.
The autocorrelation of the output process Y [n] depends only on the time difference:
and it can be shown that:
or, more concisely,
Similarly, the cross-correlation between input and output is
Frequency-Domain Analysis.
It is immediately obvious from (8 point 20) that the power spectral density of the output process Y [n] is where H(ejω) is, as usual, the frequency response of the filter.
Similarly, from (8 point 21) we obtain
The above result is of particular interest in the practical problem of estimating the characteristics of an unknown filter; this is a particular instance of a spectral estimation problem.
Indeed, if we inject white noise of known variance σ2 into an unknown LTI system H, equation (8 point 23) becomes:
By numerically computing the cross-correlation between input and output, we can therefore derive an estimation of the frequency response of the system.
The total power of a stochastic process X[n] is the variance of the process itself, σX2 equal to rX[0]; from the PSD, this can be obtained by the usual DTFT inversion formula as which, for a filtered process, specializes to the formula.
Interpolation and Sampling
Signals (in signal processing) are nothing but mathematical models capturing the essence of a flow of information.
Discrete-time signals are the model of choice in two archetypal processing situations: the first, which encompasses the long-established tradition of observing physical phenomena, captures the process of repeatedly measuring the value of a physical quantity at successive instants in time for analysis purposes (precipitation levels, stock values, etc.).
The second, which is much more recent and dates back to the first digital processors, is the ability to synthesize discrete-time signals by means of iterative numerical algorithms (mathematical simulations, computer music, etc.).
Discrete-time is the mechanized playground of digital machines.
Continuous-time signals, on the other hand, leverage on a view of the world in which physical phenomena have, potentially, an infinitely small granularity, in the sense that measurements can be arbitrarily dense.
In this continuous-time paradigm, real-world phenomena are modeled as functions of a real variable; the definition of a signal over the real line allows for infinitely small subdivisions of the function’s domain and, therefore, infinitely precise localization of its values.
Whether philosophically valid(1) or physically valid,(2) the continuous-time paradigm is an indispensable model in the analysis of analog signal processing systems.
We will now study the mathematical description of the (porous) interface between continuous-time and discrete time.
The tools that we will introduce, will allow us to cross this boundary, back and forth, with little or no loss of information for the signals involved.
Interpolation.
Interpolation comes into play when discrete-time signals need to be converted to continuous-time signals.
The need arises at the interface between the digital world and the analog world; as an example, consider a discrete-time waveform synthesizer which is used to drive an analog amplifier and loudspeaker.
In this case, it is useful to express the input to the amplifier as a function of a real variable, defined over the entire real line; this is because the behavior of analog circuitry is best modeled by continuous-time functions.
We will see that at the core of the interpolation process is the association of a physical time duration Ts to the intervals between samples of the discrete-time sequence.
The fundamental questions concerning interpolation involve the spectral properties of the interpolated function with respect to those of the original sequence.
Sampling.
Sampling is the method by which an underlying continuous-time phenomenon is “reduced” to a discrete-time sequence.
The simplest sampling system just records the value of a physical variable at repeated instants in time and associates the value to a point in a discrete-time sequence; in the following, we refer to this scheme as the “naive” sampling operator.
Other sampling methods exist (and we will see the most important one) but, in all cases, a correspondence is established between time instants in continuous time and points in the discrete-time sequence.
In the following, we only consider uniform sampling, in which the time instants are uniformly spaced Ts seconds apart.
Ts is called the sampling period and its inverse, Fs is called the sampling frequency of a sampling system.
The fundamental question of sampling is whether any information is lost in the sampling process.
If the answer is in the negative (at least for a given class of signals), this means that all the processing tools developed in the discrete-time domain can be applied to continuous-time signals as well, after sampling.
In the rest of this Chapter we will encounter a series of variables which are all interrelated and whose different forms will be used interchangeably according to convenience.
They are summarized as a quick reference in Table 9 point 1.
Interpolation and sampling constitute the bridges between the discrete- and continuous-time worlds.
Before we proceed to the core of the matter, it is useful to take a quick tour of the main properties of continuous-time signals, which we simply state here without formal proofs.
Continuous-time signals are modeled by complex functions of a real variable t which usually represents time (in seconds) but which can represent other physical coordinates of interest.
For maximum generality, no special requirement is imposed on functions modeling signals; just as in the discrete-time case, the functions can be periodic or aperiodic, or they can have a finite support (in the sense that they are nonzero over a finite interval only).
A common condition, on an aperiodic signal, is that its modeling function be square integrable; this corresponds to the reasonable requirement that the signal have finite energy.
Inner Product and Convolution.
We have already encountered some examples of continuous-time signals in conjunction with Hilbert spaces; in Section 3 point 2 point 2, for instance, we introduced the space of square integrable functions over an interval and we will shortly introduce the space of bandlimited signals.
For inner product spaces, whose elements are functions on the real line, we use the following inner product definition:
The convolution of two real continuous-time signals is defined as usual from the definition of the inner product; in particular:
The convolution operator, in continuous time, is linear and time invariant, as can be easily verified.
Note that, in discrete-time, convolution represents the operation of filtering a signal with a continuous-time LTI filter, whose impulse response is of course a continuous-time function.
Frequency-Domain Representation of Continuous-Time Signals.
The Fourier transform of a continuous-time signal x(t) and its inversion formula are defined as(3)
The convergence of the above integrals is assured for functions which satisfy the so-called Dirichlet conditions.
In particular, the FT is always well defined for square integrable (finite energy), continuous-time signals.
The Fourier transform in continuous time is a linear operator; for a list of its properties, which mirror those that we saw for the DTFT, we refer to the bibliography.
It suffices here to recall the conservation of energy, also known as Parseval’s theorem:
The FT representation can be formally extended to signals which are not square summable by means of the Dirac delta notation as we saw in Section 4 point 4 point 2. In particular we have from which the Fourier transforms of sine, cosine, and constant functions can easily be derived.
Please note that, in continuous-time, the FT of a complex sinusoid is not a train of impulses but just a single impulse.
The Convolution Theorem.
The convolution theorem for continuous-time signal exactly mirrors the theorem in Section 5 point 4 point 2; it states that if h(t) equal to (f * g)(t) then the Fourier transforms of the three signals are related by H(jΩ) equal to F(jΩ)G(jΩ).
In particular we can use the convolution theorem to compute.
A signal whose Fourier transform is nonzero only, over a finite frequency interval, is called bandlimited.
In other words, the signal x(t) is bandlimited if there exists a frequency ΩN such that:(4)
Such a signal will be called ΩN-bandlimited and ΩN is often called the Nyquist frequency.
It may be useful to mention that, symmetrically, a continuous-time signal which is nonzero, over a finite time interval only, is called a time-limited signal (or finite-support signal).
A fundamental theorem states that a bandlimited signal cannot be time-limited, and vice versa.
While this can be proved formally and quite easily, here we simply give the intuition behind the statement.
The time-scaling property of the Fourier transform states that:
so that the more “compact” in time a signal is, the wider its frequency support becomes.
The Sinc Function.
Let us now consider a prototypical ΩN-bandlimited signal φ(t) whose Fourier transform is a real constant over the interval [-ΩN,ΩN] and zero everywhere else.
If we define the rect function as follows (see also Section 5 point 6):
we can express the Fourier transform of the prototypical ΩN-bandlimited signal as where the leading factor is just a normalization term.
The time-domain expression for the signal is easily obtained from the inverse Fourier transform as where we have used Ts equal to pi over ΩN and defined the sinc function as
The sinc function is plotted in Figure 9 point 6.
Note the following:
The function is symmetric, sinc(x) equal to sinc(-x).
The sinc function is zero for all integer values of its argument, except in zero.
This feature is called the interpolation property of the sinc, as we will shortly see more in detail.
The sinc function is square integrable (it has finite energy) but it is not absolutely integrable (hence the discontinuity of its Fourier transform).
The decay is slow, asymptotic to 1 over x.
The scaled sinc function represents the impulse response of an ideal, continuous-time lowpass filter with cutoff frequency ΩN.
Interpolation is a procedure whereby we convert a discrete-time sequence x[n] to a continuous-time function x(t).
Since this can be done in an arbitrary number of ways, we have to start by formulating some requirements on the resulting signal.
At the heart of the interpolating procedure, as we have mentioned, is the association of a physical time duration Ts to the interval between the samples in the discrete-time sequence.
An intuitive requirement on the interpolated function is that its values at multiples of Ts be equal to the corresponding points of the discrete-time sequence.
The interpolation problem now reduces to “filling the gaps” between these instants.
The simplest interpolation schemes create a continuous-time function x(t) from a discrete-time sequence x[n], by setting x(t) to be equal to x[n] for t equal to nTs and by setting x(t) to be some linear combination of neighboring sequence values when t lies in between interpolation instants.
In general, the local interpolation schemes can be expressed by the following formula:
where I(t) is called the interpolation function (for linear functions the notation IN(t) is used and the subscript N indicates how many discrete-time samples, besides the current one, enter into the computation of the interpolated values for x(t)).
The interpolation function must satisfy the fundamental interpolation properties:
where the second requirement implies that, no matter what the support of I(t) is, its values should not affect other interpolation instants.
By changing the function I(t), we can change the type of interpolation and the properties of the interpolated signal x(t).
Note that (9 point 11) can be interpreted either simply as a linear combination of shifted interpolation functions or, more interestingly, as a “mixed domain” convolution product, where we are convolving a discrete-time signal x[n] with a continuous-time “impulse response” I(t) scaled in time by the interpolation period Ts.
Zero-Order Hold.
The simplest approach for the interpolating function is the piecewise-constant interpolation; here the continuous-time signal is kept constant between discrete sample values, yielding
and an example is shown in Figure 9 point 1; it is apparent that the resulting function is far from smooth since the interpolated function is discontinuous.
The interpolation function is simply:
and the values of x(t) depend only on the current discrete-time sample value.
First-Order Hold.
A linear interpolator (sometimes called a first-order hold) simply connects the points corresponding to the samples with straight lines.
An example is shown in Figure 9 point 2; note that now x(t) depends on two consecutive discrete-time samples, across which a connecting straight line is drawn.
From the point of view of smoothness, this interpolator already represents an improvement over the zero-order hold: indeed the interpolated function is now continuous, although its first derivative is not.
The first-order hold can be expressed in the same notation as in (9 point 11) by defining the following triangular function:
which is shown in Figure 9 point 3.(5) It is immediately verifiable that I1(t) satisfies the interpolation properties (9 point 12).
Higher-Order Interpolators.
The zero- and first-order interpolators are widely used in practical circuits due to their extreme simplicity.
Note that the interpolating functions I0(t) and I1(t) are alsol knows as the B-spline functions of order zero and one respectively.
These schemes can be extended to higher order interpolation functions and, in general, IN(t) is a N-th order polynomial in t.
The advantage of the local interpolation schemes is that, for small N, they can easily be implemented in practice as causal interpolation schemes (locality is akin to FIR filtering); their disadvantage is that, because of the locality, their N-th derivative is discontinuous.
This discontinuity represents a lack of smoothness in the interpolated function; from a spectral point of view this corresponds to a high frequency energy content, which is usually undesirable.
The lack of smoothness of local interpolations is easily eliminated when we need to interpolate just a finite number of discrete-time samples.
In fact, in this case the task becomes a classic polynomial interpolation problem for which the optimal solution has been known for a long time under the name of Lagrange interpolation.
Note that a polynomial interpolating a finite set of samples is a maximally smooth function in the sense that it is continuous, together with all its derivatives.
Consider a length (2N plus 1) discrete-time signal x[n], with n equal to -N,…,N.
Associate to each sample an abscissa tn equal to nTs; we know from basic algebra that there is one and only one polynomial P(t) of degree 2N which passes through all the 2N plus 1 pairs (tn,x[n]) and this polynomial is the Lagrange interpolator.
The coefficients of the polynomial could be found by solving the set of 2N plus 1 equations:
but a simpler way to determine the expression for P(t) is to use the set of 2N plus 1 Lagrange polynomials of degree 2N:
The polynomials Ln(N)(t) for Ts equal to 1 and N equal to 2 (for example interpolation of 5 points) are plotted in Figure 9 point 4. By using this notation, the global Lagrange interpolator for a given set of abscissa/ordinate pairs can now be written as a simple linear combination of Lagrange polynomials:
and it is easy to verify that this is the unique interpolating polynomial of degree 2N in the sense of (9 point 13).
Note that each of the Ln(N)(t) satisfies the interpolation properties (9 point 12) or, concisely (for Ts equal to 1):
The interpolation formula, however, cannot be written in the form of (9 point 11) since the Lagrange polynomials are not simply shifts of a single prototype function.
The continuous time signal x(t) equal to P(t) is now a global interpo-lating function for the finite-length discrete-time signal x[n], in the sense that it depends on all samples in the signal; as a consequence, x(t) is maximally smooth (x(t) ∈ C∞).
An example of Lagrange interpolation for N equal to 2 is plotted in Figure 9 point 5.
The beauty of local interpolation schemes lies in the fact that the interpolated function is simply a linear combination of shifted versions of the same prototype interpolation function I(t); unfortunately, this has the disadvantage of creating a continuous-time function which lacks smoothness.
Polynomial interpolation, on the other hand, is perfectly smooth but it only works in the finite-length case and it requires different interpolation functions with different signal lengths.
Yet, both approaches can come together in a convenient mathematical way and we are now ready to introduce the maximally smooth interpolation scheme for infinite discrete-time signals.
Let us take the expression for the Lagrange polynomial of degree N in (9 point 14) and consider its limit for N going to infinity.
Here, we have used the change of variable m equal to n - k.
We can now invoke Euler’s infinite product expansion for the sine function
(whose derivation is in the appendix) to finally obtain
The convergence of the Lagrange polynomial L0(N)(t) to the sinc function is illustrated in Figure 9 point 6.
Note that, now, as the number of points becomes infinite, the Lagrange polynomials converge to shifts of the same prototype function, for example the sinc; therefore, the interpolation formula can be expressed as in (9 point 11) with I(t) equal to sinc(t); indeed, if we consider an infinite sequence x[n] and apply the Lagrange interpolation formula (9 point 15), we obtain.
Spectral Properties of the Sinc Interpolation.
The sinc interpolation of a discrete-time sequence gives rise to a strictly bandlimited continuous-time function.
If the DTFT X(ejω) of the discrete-time sequence exists, the spectrum of the interpolated function X(jΩ) can be obtained as follows.
Now we use (9 point 9) to obtain the Fourier Transform of the scaled and shifted sinc.
and use the fact that, as usual, Ts equal to pi over ΩN.
In other words, the continuous-time spectrum is just a scaled and stretched version of the DTFT of the discrete-time sequence between -pi and pi.
The duration of the interpolation interval Ts is inversely proportional to the resulting bandwidth of the interpolated signal.
Intuitively, a slow interpolation (Ts large) results in a spectrum concentrated around the low frequencies; conversely, a fast interpolation (Ts small) results in a spread-out spectrum (more high frequencies are present).
We have seen in the previous Section that the “natural” polynomial interpolation scheme leads to the so-called sinc interpolation for infinite discrete time sequences.
Another way to look at the previous result is that any square summable discrete-time signal can be interpolated into a continuous-time signal which is smooth in time and strictly bandlimited in frequency.
This suggests that the class of bandlimited functions must play a special role in bridging the gap between discrete and continuous time and this deserves further investigation.
In particular, since any discrete-time signal can be interpolated exactly into a bandlimited function, we now ask ourselves whether the converse is true: can any bandlimited signal be transformed into a discrete-time signal with no loss of information?
The Space of Bandlimited Signals.
The class of ΩN-bandlimited functions of finite energy forms a Hilbert space, with the inner product defined in (9 point 1).
An orthogonal basis for the space of ΩN-bandlimited functions can easily be obtained from the prototypical bandlimited function, the sinc; indeed, consider the family:
where, once again, Ts equal to pi over ΩN.
Note that we have φ(n)(t) equal to φ(0)(t-nTs) so that each basis function is simply a translated version of the prototype basis function φ(0).
Orthogonality can easily be proved as follows: first of all, because of the symmetry of the sinc function and the time-invariance of the convolution, we can write.
We can now apply the convolution theorem and (9 point 9) to obtain:
so that {φ(n)(t)}n∈Z is orthogonal with normalization factor ΩN over pi (or, equivalently, 1 over Ts).
In order to show that the space of ΩN-bandlimited functions is indeed a Hilbert space, we should also prove that the space is complete.
This is a more delicate notion to show(7) and here it will simply be assumed.
Sampling as a Basis Expansion.
Now that we have an orthogonal basis, we can compute coefficients in the basis expansion of an arbitrary ΩN-bandlimited function x(t).
In the derivation, firstly we have rewritten the inner product as a convolution operation, after which we have applied the convolution theorem, and recognized the penultimate line as simply the inverse FT of X(jΩ) calculated in t equal to nTs.
We therefore have the remarkable result that the n-th basis expansion coefficient is proportional to the sampled value of x(t) at t equal to nTs.
For this reason, the sinc basis expansion is also called sinc sampling.
Reconstruction of x(t) from its projections can now be achieved via the orthonormal basis reconstruction formula (3 point 40); since the sinc basis is just orthogonal, rather than orthonormal, (3 point 40) needs to take into account the normalization factor and we have which corresponds to the interpolation formula (9 point 18).
The Sampling: Theorem.
If x(t) is a ΩN-bandlimited continuous-time signal, a sufficient representation of x(t) is given by the discrete-time signal x[n] equal to x(nTs), with Ts equal to pi over ΩN.
The continuous time signal x(t) can be exactly reconstructed from the discrete-time signal x[n].
The proof of the theorem is inherent to the properties of the Hilbert space of bandlimited functions, and is trivial once having proved the existence of an orthogonal basis.
Now, call Ωmax the largest nonzero frequency of a bandlimited signal.(8)
Such a signal is obviously ΩN-bandlimited for all ΩN is larger than Ωmax.
Therefore, a bandlimited signal x(t) is uniquely represented by all sequences x[n] equal to x(nT) for which T ≤ Ts equal to pi over Ωmax; Ts is the largest sampling period which guarantees perfect reconstruction (for example we cannot take fewer than 1 over Ts samples per second to perfectly capture the signal; we will see in the next Section what happens if we do).
Another way to state the above point is to say that the minimum sampling frequency Ωs for perfect reconstruction is exactly twice the signal’s maximum frequency, or that the Nyquist frequency must coincide to the highest frequency of the bandlimited signal; the sampling frequency Ω must therefore satisfy the following relationship.
The “naive” notion of sampling, as we have seen, is associated to the very practical idea of measuring the instantaneous value of a continuous-time signal at uniformly spaced instants in time.
For bandlimited signals, we have seen that this is actually equivalent to an orthogonal decomposition in the space of bandlimited functions, which guarantees that the set of samples x(nTs) uniquely determines the signal and allows its perfect reconstruction.
We now want to address the following question: what happens if we simply sample an arbitrary continuous time signal in the “naive” sense (for example in the sense of simply taking x[n] equal to x(nTs)) and what can we reconstruct from the set of samples thus obtained?
Given a sampling period of Ts seconds, the sampling theorem ensures that there is no loss of information by sampling the class of ΩN-bandlimited signals, where as usual ΩN equal to pi over Ts.
If a signal x(t) is not ΩN-bandlimited (for example its spectrum is nonzero at least somewhere outside of [-ΩN,ΩN]) then the approximation properties of orthogonal bases state that its best approximation in terms of uniform samples Ts seconds apart is given by the samples of its projection over the space of ΩN-bandlimited signals (Sect. 3 point 3 point 2).
This is easily seen in (9 point 23), where the projection is easily recognizable as an ideal lowpass filtering operation on x(t) (with gain Ts) which truncates its spectrum outside of the [-ΩN,ΩN] interval.
Sampling as the result of a sinc basis expansion automatically includes this lowpass filtering operation; for a ΩN-bandlimited signal, obviously, the filtering is just a scaling by Ts.
For an arbitrary signal, however, we can now decompose the sinc sampling as in Figure 9 point 7, where the first block is a continuous-time lowpass filter with cutoff frequency ΩN and gain Ts equal to pi over ΩN.
The discrete time sequence x[n] thus obtained is the best discrete-time approximation of the original signal when the sampling is uniform.
Now let us go back to the naive sampling scheme in which simply x[n] equal to x(nTs), with Fs equal to 1 over Ts, the sampling frequency of the system; what is the error we incur if x(t) is not bandlimited or if the sampling frequency is less than twice the maximum frequency?
We can develop the intuition by starting with the simple case of a single sinusoid before moving on to a formal demonstration of the aliasing phenomenon.
Sampling of Sinusoids.
Consider a simple continuous-time signal(9) such as x(t) equal to ej2pif0t and its sampled version x[n] equal to ej2pi(f0 over Fs)n equal to ejω0n with the formula.
Clearly, since x(t) contains only one frequency, it is Ω-bandlimited for all Ω is larger than 2pi|f0|.
If the frequency of the sinusoid satisfies |f0| is smaller than Fs over 2 equal to FN, then ω0 ∈ (-pi,pi) and the frequency of the original sinusoid can be univocally determined from the sampled signal.
Now assume that f0 equal to FN equal to Fs over 2; we have
In other words, we encounter a first ambiguity with respect to the direction of rotation of the complex exponential: from the sampled signal we cannot determine whether the original frequency was f0 equal to FN or f0 equal to -FN.
If we increase the frequency further, say f0 equal to (1 plus α)FN, we have
Now the ambiguity is both on the direction and on the frequency value: if we try to infer the original frequency from the sampled sinusoid from (9 point 26), we cannot discriminate between f0 equal to (1 plus α)FN or f0 equal to -αFN.
Matters get even worse if f0 is larger than Fs.
Suppose we can write f0 equal to Fs plus fb with fb is smaller than Fs over 2; we have so that the sinusoid is completely indistinguishable from a sinusoid of frequency fb sampled at Fs; the fact that two continuous-time frequencies are mapped to the same discrete-time frequency is called aliasing.
An example of aliasing is depicted in Figure 9 point 8.
In general, because of the 2pi-periodicity of the discrete-time complex exponential, we can always write and choose k ∈ Z so that ωb falls in the [-pi,pi] interval.
Seen the other way, all continuous-time frequencies of the form with fb is smaller than FN are aliased to the same discrete-time frequency ωb.
Consider now the signal y(t) equal to Aej2pifbt plus Bej2pi(fbplusFs)t, with fb is smaller than FN.
If we sample this signal with sampling frequency Fs we obtain:
In other words, two continuous-time exponentials which are Fs Hz apart, give rise to a single discrete-time complex exponential, whose amplitude is equal to the sum of the amplitudes of both the original sinusoids.
Energy Folding of the Fourier Transform.
To understand what happens to a general signal, consider the interpretation of the Fourier transform as a bank of (infinitely many) complex oscillators initialized with phase and amplitude, each contributing to the energy content of the signal at their respective frequency.
Since, in the sampled version, any two frequencies Fs apart are indistinguishable, their contributions to the discrete-time Fourier transform of the sampled signal add up.
This aliasing can be represented as a spectral superposition: the continuous-time spectrum above FN is cut, shifted back to -FN, summed over [-FN,FN], and the process is repeated again and again; the same applies for the spectrum below -FN.
This process is nothing but the familiar periodization of a signal: as we will prove formally in the next Section.
In the following, we consider the relationship between the DTFT of a sampled signal x[n] and the FT of the originating continuous-time signal xc(t).
For clarity, we add the subscript “c” to all continuous-time quantities so that, for instance, we write x[n] equal to xc(nTs).
Moreover, we use the usual tilde notation for periodic functions.
Consider X(ejω), the DTFT of the sampled sequence (with, as usual, Ts equal to (1 over Fs) equal to (pi over ΩN)).
The inversion formula states.
We will use this result later.
We can also arrive at an expression for x[n] from Xc(jΩ), the Fourier transform of the continuous-time function xc(t); indeed, by writing the formula for the inverse continuous-time Fourier transform computed in nTs we can state that:
The idea is to split the integration interval in the above expression as the sum of non-overlapping intervals whose width is equal to the sampling bandwidth Ωs equal to 2ΩN; this stems from the realization that, in the inversion process, all frequencies Ωs apart give indistinguishable contributions to the discrete-time spectrum.
We have which, by exploiting the Ωs-periodicity of ejΩ nTs (for example ej(ΩpluskΩs)nTs equal to ejΩnTs).
Now we interchange the order of integration and summation (this can be done under fairly broad conditions for xc(t)):
and if we define the periodized function:
after which, finally, we operate the change of variable θ equal to ΩTs.
It is immediately verifiable that ˜Xc(j(θ over Ts)) is now 2pi-periodic in θ.
If we now compare (9 point 32) to (9 point 27) we can easily see that (9 point 32) is nothing but the DTFT inversion formula for the 2pi-periodic function (1 over Ts)X˜(jθ over Ts); since the inversion formulas (9 point 32) and (9 point 27) yield the same result (namely, x[n]) we can conclude that:
which is the relationship between the Fourier transform of a continuous-time function and the DTFT of its sampled version, with Ts being the sampling period.
The above result is a particular version of a more general result in Fourier theory called the Poisson sum formula.
In particular, when xc(t) is ΩN-bandlimited, the copies in the periodized spectrum do not overlap and the (periodic) discrete-time spectrum between -pi and pi is simply the formula.
Figures 9 point 9 to 9 point 12 illustrate several examples of the relationship between the continuous-time spectrum and the discrete-time spectrum.
For all figures, the top panel shows the continuous-time spectrum X(jΩ), with labels indicating the Nyquist frequency and the sampling frequency.
The middle panel shows the periodized function ˜Xc(jΩ); the single copies are plotted with a dashed line (they are not be visible if there is no overlap) and their sum is plotted in gray, with the main period highlighted in black.
Finally, the last panel shows the DTFT after sampling over the [-pi,pi] interval.
Oversampling.
Figure 9 point 9 shows the result of sampling a bandlimited signal with a sampling frequency in excess of the minimum (in this case, ΩN equal to 3Ωmax over 2); in this case we say that the signal has been oversampled.
The result is that, in the periodized spectrum, the copies do not overlap and the discrete-time spectrum is just a scaled version of the original spectrum (with even a narrower support than the full [-pi,pi] range because of the oversampling; in this case ωmax equal to 2pi over 3).
Critical Sampling.
Figure 9 point 10 shows the result of sampling a bandlimited signal with a sampling frequency exactly equal to twice the maximum frequency; in this case we say that the signal has been critically sampled.
In the periodized spectrum, again the copies do not overlap and the discrete-time spectrum is a scaled version of the original spectrum occupying the whole [-pi,pi] range.
Undersampling (Aliasing).
Figure 9 point 11 shows the result of sampling a bandlimited signal with a sampling frequency less than twice the maximum frequency.
It this case, copies do overlap in the periodized spectrum and the resulting discrete-time spectrum is an aliased version of the original; the original spectrum cannot be reconstructed from the sampled signal.
Note, in particular, that the original lowpass shape is now a highpass shape in the sampled domain (energy at ω equal to pi is larger than at ω equal to 0).
Sampling of Non-Bandlimited Signals.
Finally, Figure 9 point 12 shows the result of sampling a non-bandlimited signal with a sampling frequency which is chosen as a tradeoff between alias and number of samples per second.
The idea is to disregard the low-energy “tails” of the original spectrum so that their alias does not exceedingly corrupt the discrete-time spectrum.
In the periodized spectrum, the copies do overlap and the resulting discrete-time spectrum is an aliased version of the original, which is similar to the original; however, the original spectrum cannot be reconstructed from the sampled signal.
In a practical sampling scenario, the correct design choice would have been to lowpass filter (in the continuous-time domain) the original signal so as to eliminate the spectral tails beyond Â± ΩN.
A/D and D/A Conversions
The word “digital” in “digital signal processing” indicates that, in the representation of a signal, both time and amplitude are discrete quantities.
The necessity to discretize the amplitude values of a discrete-time signal comes from the fact that, in the digital world, all variables are necessarily represented with a finite precision.
Specifically, general-purpose signal processors are nothing but streamlined processing units which address memory locations whose granularity is an integer number of bits.
The conversion from the “real world” analog value of a signal to its discretized digital counterpart is called analog-to-digital (A/D) conversion.
Analogously, a transition in the opposite direction is shorthanded as a D/A conversion; in this case, we are associating a physical analog value to a digital internal representation of a signal sample.
Note that, just as was the case with sampling, quantization and its inverse lie at the boundary between the analog and the digital world and, as such, they are performed by actual pieces of complex, dedicated hardware.
The sampling theorem described in Chapter 9 allowed us to represent a bandlimited signal by means of a discrete-time sequence of samples taken at instants multiple of a sampling period Ts.
In order to store or process this sequence of numbers x[n] equal to x(nTs), n ∈ Z, we need to transform the real values x[n] into a format which fits the memory model of a computer; two such formats are, for instance, finite-precision integers or finite-precision floating point numbers (which are nothing but integers associated to a scale factor).
In both cases, we need to map the real line or an interval thereof (for example the range of x(t)) onto a countable set of values.
Unfortunately, because of this loss of dimensionality, this mapping is irreversible, which leads to approximation errors.
Consider a bandlimited input signal x(t), whose amplitude is known to vary between the values A and B. After sampling, each sample x[n] will have to be stored as a R-bit integer, for example as one out of K equal to 2R possible values.
An intuitive solution is to divide the [A,B] interval into K non-overlapping subintervals Ik such that
The intervals are defined by K plus 1 points ik so that for each interval we can write
So i0 equal to A, iK equal to B, and i0 is smaller than i1 is smaller than … is smaller than iK-1 is smaller than iK.
An example of this subdivision for R equal to 2 is shown in Figure 10 point 1 in which, arbitrarily.
In order to map the input samples to a set of integers, we introduce the following quantization function:
)
In other words, quantization associates to the sample value x[n], the integer index of the interval onto which x[n] “falls”.
One of the fundamental questions in the design of a good quantizer is how to choose the splitting points ik for a given class of input signals as well as the reconstruction values.
Quantization Error.
Since all the (infinite) values falling onto the interval Ik are mapped to the same index, precious information is lost.
This introduces an error in the quantized representation of a signal which we analyze in detail in the following and which we will strive to minimize.
Note that quantization is a highly nonlinear operation since, in general,
This is related to the quantization error since, if a ∈ Ik and a plus b ∈ Ik then Q{a plus b} equal to Q{a}; small perturbations in the signal are lost in quantization.
On the other hand, consider a constant signal x[n] equal to im for some m is smaller than K (for example the value of the signal coincides with the lower limit of one of the quantization intervals); consider now a small “perturbation” noise sequence ε[n], with uniform distribution over [-U,U] and U arbitrarily small.
The quantized signal:
In other words, the quantized signal will oscillate randomly with 50% chance between two neighboring quantization levels.
This can create disruptive artifacts in a digital signal, and it is counteracted by special techniques called dithering for which we refer to the bibliography.
Reconstruction.
The output of a quantizer is an abstract internal representation of a signal, where actual values are replaced by (binary) indices.
In order to process or interpolate back such a signal, we need to somehow “undo” the quantization and, to achieve this, we need to associate back a physical value to each of the quantizer’s indices.
Reconstruction is entirely dependent on the original quantizer and, intuitively, it is clear that the value ˆxk which we associate to index k should belong to the interval Ik.
For example, in the absence of any other information on x[n], it is reasonable to choose the interval’s midpoint as the representative value.
In any case, the reconstructed signal will be and the second fundamental question in quantizer design is how to choose the representative values ˆxk so that the quantization error is minimized.
The error introduced by the whole quantization/reconstruction chain can therefore be written out as
For a graphical example see Figure 10 point 2. Note that, often, with an abuse of notation, we will use the term “quantized signal” to indicate the sequence.
Quantization, as we mentioned, is a non-linear operation and it is therefore quite difficult to analyze in general; the goal is to obtain some statistical description of the quantization error for classes of stochastic input signals.
In order to make the problem tractable and solvable, and thus gain some precious insight on the mathematics of quantization, simplified models are often used.
The simplest such model is the uniform scalar quantization scenario.
By scalar quantization, we indicate that each input sample x[n] is quantized independently; more sophisticated techniques would take advantage of the correlation between neighboring samples to perform a joint quantization which goes under the name of “vector quantization”.
By uniform quantization, we indicate the key design choices for quantization and reconstruction: given a budget of R bits per sample (known as the rate of the digital signal), we will be able to quantize the input signal into K equal to 2R distinct levels; in order to do so we need to split the range of the signal into K intervals.
It is immediately clear that such intervals should be disjoint (as to have a unique quantized value for each input value) and they should cover the whole range of the input.
In the case of uniform quantization the following design is used:
The range of the input x[n] is assumed to be in the interval [A,B], with A,B ∈ R.
The range [A,B] is split into K equal to 2R contiguous intervals Ik of equal width Δ equal to (B - A) over K.
Each reconstruction point ˆxk is chosen to be the midpoint of the corresponding interval Ik.
An example of the input/output characteristic of a uniform quantizer is shown in Figure 10 point 3 for a rate of 3 bits/sample and a signal limited between -1 and 1.
Uniform Quantization of a Uniformly Distributed Input.
In order to precisely evaluate the distortion introduced by a quantization and reconstruction chain we need to formulate some assumption on the statistical properties of the input signal. of the input signal.
Let us start with a discrete-time signal x[n] with the following characteristics:
x[n] is uniformly distributed over the [A,B] interval;
x[n] is an i.i.d.
process (independent and identically distributed).
While simple, this signal model manages to capture the essence of many real-world quantization problems.
For a uniformly distributed input signal, the design choices of a uniform quantizer turn out to be optimal with respect to the minimization of the power of the quantization error Pe.
If we consider the expression for the power and we remark that the error function localizes over the quantization intervals as then we can split the above integral as where ik equal to A plus kΔ, xk equal to ik plus Δ over 2 and Δ equal to (B - A) over K.
In order to show that the quantizer is optimal, we need to show that these values for ik and ˆxk lie at a minimum for the error function Pe or, in other words, that they are a solution to the following system of equations:
By plugging (10 point 5) into the first equation in (10 point 6) we have
The same can easily be verified for the second equation in (10 point 6).
Finally, we can determine the power of the quantization error:
where we have used the fact that, since the signal is uniformly distributed, P[x[n] ∈ Ik] equal to 1 over K for all k.
If we consider the average power (for example the variance) of the input signal:
and we use the fact that Δ equal to (B - A) over K equal to (B - A) over 2R, we can write
This exponential decay of the error, as a function of the rate, is a key concept, not only with respect to quantization but, much more generally, with respect to data compression.
Finally, if we divide by the power of the input signal, we can arrive at a convenient and compact expression for the signal to noise ratio of the digital signal:
If we express the SNR in dB, the above equation becomes:
This provides us with an extremely practical rule of thumb for the distortion caused by quantization: each additional bit per sample improves the SNR by 6 dB.
A compact disk, for instance, which uses a quantization of 16 bits per sample, has an SNR (or maximum dynamic range) of approximately 96 dB.
Remember, however, that the above expression has been derived under very unrealistic assumptions for the input signal, the most limiting of which is that input samples are uncorrelated, which makes the quantization error uncorrelated as well.
This, is of course, far from true in any non-noise signal so that the 6 dB/bit rule must be treated as a rough estimate.
Uniform Quantization of Normally Distributed Input.
Consider now a more realistic distribution for the input signal x[n], namely the Gaussian distribution; the input signal is now assumed to be white Gaussian noise of variance σ2.
Suppose we fix the size of the quantization interval Δ; since, in this case, the support of the probability density function is infinite, we either need an infinite number of bins (and therefore an infinite rate), or we need to “clip” x[n] so that all values fall within a finite interval.
In the purely theoretical case of an infinite number of quantization bins, it can be proven that the error power for a zero-mean Gaussian random variable of variance σ2 quantized into bins of size Δ is
In a practical system, when a finite number of bins is used, x[n] is clipped outside of an [A,B] interval, which introduces an additional error, due to the loss of the tails of the distribution.
It is customary to choose B equal to -A equal to 2σ; this is an instance of the so-called “4σ rule”, stating that over 99 point 5% of the probability mass of a zero-mean Gaussian variable with variance σ2 is comprised between -2σ and 2σ.
With this choice, and with a rate of R bits per sample, we can build a uniform quantizer with Δ equal to 4σ over 2R.
The total error power increase because of clipping but not by much; essentially, the behavior is still given by an expression similar to (10 point 11) which, expressed as a function of the rate, can be written as where C is a constant larger but of the same order as in (10 point 11).
Again, the signal to noise ratio turns out to be an exponentially increasing function of the rate with only a constant to reduce performance with respect to the uniformly distributed input.
Again, the 6 dB/bit rule applies, with the usual caveats.
The Lloyd-Max algorithm is a procedure to design an optimal quantizer for an input signal with an arbitrary (but known) probability density function.
The starting hypotheses for the Lloyd-Max procedure are the following:
x[n] is bounded over the [A,B] interval;
x[n] is an i.i.d.
process;
x[n] is distributed with pdf fx(x).
Under these assumptions the idea is to solve a system of equations similar to (10 point 6) where, in this case, the pdf for the input is explicit in the error integral.
The solution defines the optimal quantizer for the given input distribution, for example the quantizer which minimizes the associated error.
The error can be expressed as where now the only known parameters are K, i0 equal to A and iK equal to B and we must solve.
The first equation can be efficiently solved noting that only one term in (10 point 13) depends on ˆxk for a given value of k.
Therefore:
which gives the optimal value for ˆxk as
Note that the optimal value is the center of mass of the input distribution over the quantization interval: ˆxk equal to E[x|ik ≤ x ≤ ikplus1].
Similarly, we can determine the boundaries of the quantization intervals as from which the optimal boundaries are the midpoints between optimal quantization points.
The system of Equations (10 point 15) and (10 point 16) can be solved (either exacly or, more often, iteratively) to find the optimal parameters.
In practice, however, the SNR improvement introduced by a Lloyd-Max quantizer does not justify an ad-hoc hardware design effort and uniform quantizers are used almost exclusively in the case of scalar quantization.
The process which transforms an analog continuous-time signal into a digital discrete-time signal is called analog-to-digital (A/D) conversion.
Again, it is important to remember that A/D conversion is the operation that lies at the interface between the analog and the digital world and, therefore, it is performed by specialized hardware which encode the instantaneous voltage values of an electrical signal into a binary representation suitable for use on a general-purpose processor.
Once a suitable sampling frequency Fs has been chosen, the process is composed of four steps in cascade.
Analog Lowpass Filtering.
An analog lowpass with cutoff Fs over 2 is a necessary step even if the analog signal is virtually bandlimited because of the noise: we need to eliminate the high-frequency noise which, because of sampling, would alias back into the signal’s bandwidth.
Since sharp analog lowpass filters are “expensive”, the design usually allows for a certain amount of slack by choosing a sampling frequency higher than the minimum necessary.
Sample and Hold.
The input signal is sampled by a structure similar to that in Figure 10 point 4. The FET T1 acts as a solid-state switch and it is driven by a train of pulses k(t) generated by a very stable crystal oscillator.
The pulses arrive Fs times per second and they cause T1 to close briefly so that the capacitor C1 is allowed to charge to the instantaneous value of the input signal xc(nTs) (in Volts); the FET then opens immediately and the capacitor remains charged to xc(nTs) over the time interval [nTs,(n plus 1)Ts].
The key element of the sample-and-hold circuit is therefore the capacitor, acting as an instantaneous memory element for the input signal’s voltage value, while the op-amps provide the necessary high-impedance interfaces to the input signal and to the capacitor.
The continuous-time signal produced by this structure looks like the output of a zero-order hold interpolator (Fig. 9 point 1); availability of a piecewise-constant signal between sampling instants is necessary to allow the next steps in the chain to reach a stable output value.
Clipping.
Clipping limits the range of the voltages which enter the quantizer.
The limiting function can be a hard threshold or, as in audio applications, a function called compander where the signal is leveled towards a maximum via a smooth function (usually a sigmoid).
Quantization.
The quantizer, electrically connected (via the clipper) to the output of the sample-and-hold, is a circuit which follows the lines of the schematics in Figure 10 point 6.
This is called a flash (or parallel) quantization scheme because the input sample is simultaneously compared to all of the quantization thresholds ik.
These are obtained via a voltage divider realized by the cascade of equally-valued resistors shown on the left of the circuit.
In this simple example, the signal is quantized over the [-V 0,V 0] interval with 2 bits per sample and therefore four voltage levels are necessary.
To use the notation of Section 10 point 1 we have that i0 equal to -V 0, i1 equal to -0 point 5V 0, i2 equal to 0, i3 equal to 0 point 5V 0, and i4 equal to V 0. These boundary voltages are fed to a parallel structure of op-amps acting as comparators; all the comparators, for which the reference voltage is less than the input voltage, will have a high output and the logic (XOR gates and diodes) will convert these high outputs into the proper binary value (in this case, a least significant and most significant bit (LSB and MSB)).
Because of their parallel structure, flash quantizers exhibit a very short response time which allows their use with high sampling frequencies.
Unfortunately, they require an exponentially increasing number of components per output bit (the number of comparators is on the order of 2B where B is the rate of the signal in bits per sample).
Other architectures are based on iterative conversion techniques; while they require fewer components, they are typically slower.
In the simplest case, digital-to-analog (D/A) conversion is performed by a circuit which translates the binary internal representation of the samples into a voltage output value; the voltage is kept constant between interpolation times, thereby producing a zero-order-hold interpolated signal.
Further analog filtering may be employed to reduce the artifacts of the interpolation (Sect.
A typical example of D/A circuitry is shown in Figure 10 point 6.
The op-amp is configured as a voltage adder and is connected to a standard R/2R ladder.
The ladder has as many “steps” as the number of bits B used to encode each sample of the digital signal.(1)
Each bit is connected to a non-inverting buffer which acts as a switch: for each “1” bit, the voltage V 0 is connected to the associated step of the ladder, while for each “0” bit the step is connected to the ground.
By repeatedly applying Thevenin’s theorem to each step in the ladder it is easy to show that a voltage of V 0 appearing at the k-th bit position (with k equal to 0 indicating the LSB and k equal to B - 1 indicating the MSB) is equivalent to a voltage of V 0 over 2B-k applied to the inverting input of the op-amp with an impedance of 2R.
By the property of superposition (applied here to the linear ladder circuit), the output voltage over the time interval [nTs,(n plus 1)Ts] is where bB-1nbB-2n⋅⋅⋅b1nb0n is the B-bit binary representation of the sample x[n].
Note that 0 ≤ x(t) is smaller than V 0; for a reconstruction interval between -V 0 and V 0, one can halve the value of the feedback resistor in the adder and add an offset of -V 0 Volts to the output.
Multirate Signal Processing
The sampling theorem in Chapter 9 provided us with a tool to map a continuous-time signal to a sequence of discrete-time samples taken at a given sampling rate.
By choosing a different sampling rate, the same continuous-time signal can be mapped to an arbitrary number of different discrete-time signals.
What is the relationship between these different discrete-time sequences?
Can they be transformed into each other entirely from within the discrete-time world?
These are the questions that multirate theory sets out to answer.
The conversion from one sampling rate to another can always take the “obvious” route via continuous time, for example via interpolation and resampling.
This is clearly disadvantageous, both from the point of view of the needed equipment and from the point of view of the quality loss which always takes place upon quitting the digital discrete-time domain.
That was the rationale, for instance, of an infamous engineering decision taken by the audio industry in the early 90’s.
In those years, after compact disk players had been around for about a decade, digital cassette players started to appear in the market under the name of DAT.
The decision was to use a different and highly incompatible sampling rate for the DAT with respect to the CD (48 Khz vs. 44 point 1 Khz) so as to make it difficult to obtain perfect digital copies of existing CDs.(1)
Multirate signal processing rendered that strategy moot, as we will see.
More generally, multirate signal processing not only comes to help whenever a conversion between different standards is needed, but it is also a full-fledged signal processing tool in its own right with many fruitful applications in the design of efficient filtering schemes and of telecommunication systems.
Finally, multirate theory is at the cornerstone of advanced processing techniques which go under the name of time-frequency analysis.
Downsampling by N (also called subsampling or decimation(2) ) creates a lower-rate sequence by keeping only one out of N samples in the original signal.
If we call DN the downsampling operator, we have the formula.
Downsampling effectively discards N - 1 out of N samples and, as such, may cause a loss of information in the original sequence; to understand when and how this happens, we need to arrive at a frequency domain representation of the downsampled sequence.
Let us consider, as an example, the downsampling by 2 operator D2 and let us write out explicitly its effect; if x2D[n] equal to D2{x[n]} we have
Note that the time origin is extremely important, since:
as, according to the definition, D2{x[n plus 1]} equal to x[2n plus 1].
We have just shown that the downsampling operator is not time-invariant.
More precisely, the downsampling operator is defined as periodically time-varying since, if xND[n] equal to DN{x[n]}, then:
It is trivial to show that the downsampling operator is indeed a linear operator.
One of the fundamental consequences of the lack of time-invariance is that, now, one of the key properties of LTI systems no longer holds for the downsampling operator; indeed, complex sinusoids are no longer eigensequences.
As an example, consider x[n] equal to (-1)n equal to ejpin, which is the highest-frequency discrete-time sinusoid.
After downsampling by 2, we obtain:
which is the lowest-frequency sinusoid in discrete time.
This is one instance of the information loss inherent to downsampling and to understand how it operates we need to move to the frequency domain.
In order to obtain a frequency-domain representation of a downsampling by N, first consider the z-transform of the downsampled signal.
Now consider an “auxiliary” z-transform Xa(z) defined as
The interest of Xa(z) lies with the fact that, if we can obtain a closed-form expression for it, we can then write out XND(z) simply as.
Clearly, Xa(z) can be derived from X(z), the z-transform of the original signal, by “killing off” all the terms in the z-transform sum whose index is not a multiple of N; in other words we can write where ξN[n] is a “selector” sequence defined as.
The question now is to find an expression for such a sequence; to this end, let us recall a very early result about the orthogonality of the roots of unity (see Equation (4 point 4)), which we can rewrite as follows:
where, as per usual, WN equal to e-j2pi
N. Clearly, we can define our desired selector sequence as
and we can therefore write so that finally.
The Fourier transform of the downsampled signal is obtained by evaluating XND(z) on the unit circle; explicitly, we have
The resulting spectrum is, therefore, the scaled sum of N superimposed copies of the original spectrum X(ejω); each copy is shifted in frequency by a multiple of 2pi over N and the result is stretched by a factor of N. We are, in many ways, in a situation similar to that of equation (9 point 33) where sampling created a periodization of the underlying spectrum; here the spectra are already inherently 2pi-periodic, and downsampling creates N - 1 additional interleaved copies.
Because of the superposition, aliasing can take place; this is a consequence of the potential loss of information that occurs when samples are discarded.
It is easy to verify that in order for the spectral copies in (11 point 9) not to overlap, the maximum (positive) frequency ωM of the original spectrum(3) must be less than pi over N; this is the non-aliasing condition for the downsampling operator.
Conceptually, fulfillment of the non-aliasing condition indicates that the discrete-time representation of the original signal is intrinsically redundant; (N - 1) over N of the information can be safely discarded and this is mirrored by the fact that only 1 over N of the spectral frequency support is nonzero.
We will see shortly that, in this case, the original signal can be perfectly reconstructed with an upsampling and filtering operation.
In the following graphical examples (Figs 11 point 2 to 11 point 6) the top panel shows the original spectrum X(ejω); the second panel shows the same spectrum but plotted over a larger frequency interval so as to make its periodic nature explicit; the third panel shows (in different shades of gray) the individual components of the sum in (11 point 9) before scaling and stretching by N, for example the N copies X(WNkejω) for k equal to 0,1,…,N - 1; the fourth panel shows the final XND(ejω), with the individual components of the sum plotted with a dashed line; finally, the last panel shows XND(ejω) over the usual [-pi,pi] interval.
Downsampling by 2. If the downsampling factor is 2, the corresponding two roots of unity are just Â±1 and we have the formula.
Figure 11 point 2 shows an example of downsampling by 2 for a lowpass signal whose maximum frequency is ωM equal to pi over 2 (for example a half-band signal).
The non-aliasing condition is fulfilled and, in the superposition, the two shifted versions of the spectrum do not overlap.
As the frequency axis expands by a factor of 2, the original half-band signal becomes full band.
Figure 11 point 3 shows an example in which the non-aliasing condition is violated.
In this case, ωM equal to 2pi over 3 is larger than pi over 2 and the spectral copies do overlap.
We can see that, as a consequence, the downsampled signal loses its lowpass characteristics.
Information is irretrievably lost and the original signal cannot be reconstructed.
We will see in the next Section the customary way of dealing with this situation.
Downsampling by 3. If the downsampling factor is 3 we have this formula.
Figure 11 point 4 shows an example in which the non-aliasing condition is violated (ωM equal to 2pi over 3 is larger than pi over 3).
In particular, the superposition of the three spectral copies is such that the resulting spectrum is flat.
Downsampling of a Highpass Signal.
Figure 11 point 5 shows an example of downsampling by 2 of a half-band highpass signal.
Since the signal occupies only the upper half of the [0,pi] frequency band (and, symmetrically, only the lower half of the [-pi,0] interval), the interleaved copies do not overlap and, technically, there is no aliasing.
The shape of the signal, however, is changed by the downsampling operation and what started out as a highpass signal is transformed into a lowpass signal.
The details of the transformation are clearer if, for the sake of example, we consider a complex half-band highpass signal in which the positive and negative parts of the spectrum are different.
The steps involved in the downsampling of such a signal are detailed in Figure 11 point 6 and it is apparent how the low and high parts of the spectrum are interchanged.
In both cases the original signal can be exactly reconstructed (since there is no destructive overlap between spectral copies) but the required procedure (which we will study in the exercises) is more complex than a simple upsampling.
Because of aliasing, it is customary to filter a signal prior to downsampling.
The filter should be designed to eliminate aliasing by removing the high frequency components which fold back onto the lower frequencies (remember how the (-1)n signal ended up as the constant 1).
For a downsampling by N, this is accomplished by a lowpass filter with cutoff frequency ωc equal to pi over N, and the resulting structure is depicted in Figure 11 point 7.
An example of the processing chain is shown in Figure 11 point 8 for a downsampling factor of 2; a half-band lowpass filter is used to truncate the signal’s spectrum outside of the [-pi over 2,pi over 2] interval and then downsampling proceeds as usual with non-overlapping spectral copies.
Clearly, some information is lost and the original signal cannot be recovered exactly but the distortion is controlled and less disruptive than foldover aliasing.
Upsampling by N produces a higher-rate sequence by creating N samples out of every sample in the original signal.
The upsampling operation consists simply in inserting N - 1 zeros between every two input samples; if we call UN the upsampling operator, we have the formula.
Upsampling is a much “nicer” operation than downsampling since no information is lost and the original signal can always be exactly recovered by downsampling:
Furthermore, the spectral description of upsampling is extremely simple; in the z-transform domain we have and therefore so that upsampling is simply a contraction of the frequency axis by a factor of N. The inherent 2pi-periodicity of the spectrum must be taken into account so that, in this contraction, the periodic repetitions of the base spectrum are “drawn in” the [-pi,pi] interval.
The effects of upsampling are shown graphically for a simple signal in Figures 11 point 9 to 11 point 11; in all figures the top panel shows the original spectrum X(ejω) over [-pi,pi]; the middle panel shows the same spectrum over a wider range to make the 2pi- periodicity explicitly; the last panel shows the upsampled spectrum XNU(ejω), highlighting the rescaling of the [-Npi,Npi] interval.
However simple, an upsampled signal suffers from two drawbacks.
In the time domain, the upsampled signal does not look “natural” since there are N - 1 zeros between every sample drawn from the input.
Thus, a “smooth”(4) input signal no longer looks smooth after upsampling, as shown in the top two panels of Figure 11 point 13.
A solution would be to try to interpolate the original samples in order to “fill in” the gaps.
In the frequency domain, on the other hand, the repetitions of the base spectrum, which are drawn in by the upsampling, do not look as if they belong to the [-pi,pi] interval and it seems natural to try to remove them.
These two problems are actually one and the same and they can be solved by an appropriate filter.
The problem of filling the gaps between nonzero samples in an upsampled sequence is, in many ways, similar to the discrete- to continuous-time interpolation problem of Section 9 point 4, except that now we are operating entirely in discrete-time.
If we adapt the interpolation schemes that we have already studied, we can describe the following cases.
Zero-Order Hold.
In this discrete-time interpolation scheme, also known as piecewise-constant interpolation, after upsampling by N, we use a filter with impulse response which is shown in Figure 11 point 14.
This interpolation filter simply repeats the original input samples N times, giving a staircase approximation as shown for example in the third panel of Figure 11 point 13.
First-Order Hold.
In this discrete-time interpolation scheme, we obtain a piecewise linear interpolation after upsampling by N by using the formula.
The impulse response is the familiar triangular function(5) shown in Figure 11 point 14.
An example of the resulting interpolation is shown in Figure 11 point 13.
Sinc Interpolation.
We know that, in continuous time, the smoothest interpolation is obtained by using a sinc function.
This holds in discrete-time as well, and the resulting interpolation filter is a discrete-time sinc.
Note that the sinc above is equal to one for n equal to 0 and is equal to zero at all integer multiples of N, n equal to kN; this fulfills the interpolation condition that, after interpolation, the output equals the input at multiples of N (for example (h * xNU)[n] equal to x[n] for n equal to kN).
The three impulse responses above are all lowpass filters; in particular, the sinc interpolator is an ideal lowpass with cutoff frequency ωc equal to pi over N while the others are approximations of the same.
As a consequence, the effect of the interpolator in the frequency domain is the removal of the N - 1 repeat spectra which have been drawn in the [-pi,pi] interval.
An example is shown in Figure 11 point 15 where the signal in Figure 11 point 11 is filtered by an ideal lowpass filter with cutoff pi over 4. It turns out that the smoothest possible interpolation in the time domain corresponds to the removal of the spectral repetitions in the frequency domain.
An interpolation by the zero-order, or first-order holds, only attenuates the replicas instead of performing a full removal, as we can readily see by considering their frequency responses.
Since we are in discrete-time, however, there are no difficulties associated to the design of a digital lowpass filter which performs extremely well.
This is in contrast to the design of discrete—to continuous—time interpolators, which are analog designs.
That is why sampling rate changes are much more attractive in the discrete-time domain.
So far we have examined methods which change (multiply or divide) the implicit rate of a discrete-time signal by an integer factor.
By combining upsampling and downsampling, we can achieve arbitrary rational sampling rate changes.
Typically, a rate change by N over M is obtained by cascading an upsampler by N, a lowpass filter and a downsampler by M. The filter’s cutoff frequency is the minimum of {pi over N,pi over M}; this follows from the fact that upsampling and downsampling require lowpass filters with cutoff frequencies of pi over N and pi over M respectively, and the minimum cutoff frequency dominates in the cascade.
A block diagram of this system is shown in Figure 11 point 16.
The order of the upsampling and downsampling operators is crucial since, in general, the operators are not commutative.
It is easy to appreciate this fact by means of a simple example; for a given sequence x[n] it is.
Conceptually, using an upsampler first is the logical thing to do since no information is lost in a sample rate increase.
Interestingly enough, however, if the downsampling and upsampling factors N and M are coprime, the operators do commute.
The proof of this property is left as an exercise.
This property can be put to use in a rational sampling rate converter to minimize the number of operations, per sample in the middle filter.
As an example, we are now ready to solve the audio conversion problem which was quoted at the beginning of the Chapter.
To convert an audio file sampled at 44 Khz (“CD-quality”) into an audio file which can be played back at 48 Khz (“DVD-quality”) a rate change of 12 over 11 is necessary; this can be achieved with the system shown at the top of Figure 11 point 17.
Conversely, DVD to CD conversion can be performed with a 11 over 12 rate changer, shown at the bottom of Figure 11 point 17.
Manipulating the sampling rate is useful a many more ways beyond simple conversions between audio standards: oversampling is a case in point.
The term “oversampling” describes a situation in which a signal’s sampling rate is made to be deliberately higher than the minimum required by the sampling theorem.
Oversampling is used to improve the performance of A/D and D/A converters.
If a continuous-time signal x(t) is bandlimited, the sampling theorem guarantees that we can choose a sampling period Ts such that no error is introduced by the sampling operation.
The only source of error in A/D conversion remains the distortion due to quantization; oversampling, in this case, allows us to reduce this error by increasing the underlying sampling rate.
Under certain assumptions on the statistical properties of the input signal, the quantization error associated to A/D conversion has been modeled in Section 10 point 1 point 1 as an additive noise source.
If x(t) is a ΩN-bandlimited signal and Ts equal to pi over ΩN, we can write:
with e[n] a white process of variance, where Δ is the quantization interval.
This is represented pictorially in the top panel of Figure 11 point 18 which shows the power spectral densities for an arbitrary critically sampled signal and for the associated quantization noise.(7)
The bottom panel of Figure 11 point 18 shows the same quantities for the case in which the input signal has been oversampled by a factor of four, for example for the signal.
The scale change between signal and noise comes from equation (9 point 34) but note that the signal-to-noise ratio of the oversampled signal is still the same.
However, now we are in the digital domain and it is easy to build a discrete-time filter which removes the quantization error outside of the support of the signal (for example outside of the [-pi over 4,pi over 4] interval) and this improves the SNR.
Once the out-of-band noise is removed, we can use a downsampler by 4 to obtain a critically sampled signal for which the signal to noise ratio has improved by a factor of 4 (or, alternatively, by 6 dB).
The processing chain is shown in Figure 11 point 19 for a generic oversampling factor N; as a rule of thumb, the signal-to-noise ratio is improved by about 3 dB per octave of oversampling, that is, each doubling of the sampling rate reduces the noise variance by a factor of two, which is 20 log 10 equal to 3 dB.
The above example is deliberately lacking rigor in the derivations since it turns out that a precise analysis of A/D oversampling is very difficult.
It is intuitively clear that some of the quantization noise will be rejected by this procedure, but the fundamental assumption that the input signal is white (and therefore that the quantization noise is uncorrelated) does not hold in reality.
In fact, as the sampling rate increases, successive samples exhibit a higher and higher degree of correlation and most of the quantization noise power ends up falling within the band of the signal.
The sampling theorem states that, under the hypothesis of a bandlimited input, sampling is invertible via a sinc interpolation.
The sinc filter is an ideal filter and therefore it is not realizable either in the digital or in the analog domain.
The analog sinc, therefore, must be approximated by some realizable interpolation filter.
Recall that, once the interpolation period Ts is chosen, the continuous-time signal created by the interpolator is the mixed-domain convolution (9 point 11), which we rewite here.
In the frequency domain this becomes with, as usual, ΩN equal to pi over Ts.
The above expression is the product of two terms; the last is the periodic digital spectrum, stretched so as to be 2ΩN-periodic and the first is the frequency response of the analog interpolation filter, again stretched by 2ΩN.
In the case of sinc interpolation, the frequency response is a rect with cutoff frequency ΩN, which “kills off” all the repetitions except for the baseband period of the periodic spectrum.
The result of sinc interpolation is represented in Figure 11 point 20; the top panel shows the spectrum of an arbitrary discrete-time signal, the middle panel shows the two terms of Equation (11 point 19) with the sinc response dashed in gray, and the bottom panel shows the resulting analog spectrum.
In both the middle and bottom panels only the positive frequency axis is shown since all signals are assumed to be real and, consequently, the magnitude spectra are symmetric.
With a realizable interpolator, the stopband of the interpolation filter cannot be uniformly zero and its transition band cannot be infinitely sharp.
As a consequence, the spectral copies to the left and right of the baseband will “leak through” in the reconstructed analog signal.
It is important to remark at this point that the interpolator filter is an analog filter and, as such, quite delicate to design.
Without delving into too many details, there are no FIR filters in the continuous-time domain so that all analog filters are affected by stability problems and by design complexities associated to the passive and active electronic components.
In short, a good interpolator is difficult to design and expensive to produce; so much so, in fact, that most of the interpolators used in practical circuitry are just zero-order holds.
Unfortunately, the frequency response of the zero-order hold is quite poor; it is indeed easy to show that:
and that this response, while lowpass in nature, decays only as 1 over Ω.
The results of zero-order hold D/A conversion are shown in Figure 11 point 21; the top panel shows the original digital spectrum and the middle panel shows the two terms of Equation (11 point 19) with the magnitude response of the interpolator dashed in gray.
The spectrum of the interpolated signal (shown in the bottom panel) exhibits several non-negligible instances of high-frequency leakage centered around the multiples of twice the Nyquist frequency.(8)
These are particularly undesirable in audio applications (such as in a CD player).
Rather than using expensive and complex analog filters, the performance of the D/A converter can be dramatically improved if we are willing to perform the conversion at a higher rate than the strict minimum.
This is achieved by oversampling the signal in the digital domain and the block diagram of the operation is shown in Figure 11 point 22.
Note that this is a paradigmatic instance of cheap and easy discrete-time processing solving an otherwise difficult analog design: the lowpass filter used in discrete-time oversampling is an FIR with arbitrarily high performance, a filter which is much easier to design than an analog lowpass and has no stability problems.
The only price paid is an increase in the working frequency of the converter.
Figure 11 point 23 details an example of D/A conversion with an oversampling factor of two.
The top panel shows the spectrum of the oversampled discrete-time signal, together with the associated repetitions in the [-pi,pi] interval which are going to be filtered out by a lowpass filter with cutoff pi over 2. The discrete-time filter response is dashed in gray in the top panel and, while the displayed characteristic is that of an ideal lowpass, note that in the discrete-time domain, we can approximate a very sharp filter rather easily.
The two terms of Equation (11 point 19) (with the magnitude response of the interpolator dashed in gray) are shown in the middle panel; now the interpolation frequency is ΩN,O equal to 2ΩN, for example twice the frequency used in the previous example, in which the signal was critically sampled.
Shrinking the spectrum in the digital domain and stretching in the analog makes sure that the analog spectrum is unchanged around the baseband.
The final spectrum of the interpolated signal is shown in the bottom panel and we can notice how the first high frequency leakage occurs at twice the frequency of the previous example and is smaller in amplitude.
An oversampling of N with N is larger than 2 will push the leakage even higher up in frequency; at this point a very simple analog lowpass (with a very large transition band) will suffice to remove all undesired frequency components.
Design of a Digital Communication System
The power of digital signal processing can probably be best appreciated in the enormous progresses which have been made in the field of telecommunications.
These progresses stem from three main properties of digital processing:
The flexibility and power of discrete-time processing techniques, which allow for the low-cost deployment of sophisticated and, more importantly, adaptive equalization and filtering modules.
The ease of integration between low-level digital processing and high-level information-theoretical techniques which counteract transmission errors.
The regenerability of a digital signal: in the necessary amplification of analog signals after transmission, the noise floor is amplified as well, thereby limiting the processing gain.
Digital signals, on the other hand, can be exactly regenerated under reasonable SNR conditions (Fig. 1 point 10).
The fruits of such powerful communication systems are readily enjoyable in everyday life and it suffices here to mention the fast ADSL connections which take the power of high data rates into the home.
ADSL is actually a quantitative evolution of a humbler, yet extraordinarily useful device: the voiceband modem.
Voiceband modems, transmitting data at a rate of up to 56 Kbit/sec over standard telephone lines, are arguably the crown achievement of discrete-time signal processing in the late 90’s and are still the cornerstone of most wired telecommunication devices such as laptops and fax machines.
In this Chapter, we explore the design and implementation of a voiceband modem as a paradigmatic example of applied digital signal processing.
In principle, the development of a fully-functional device would require the use of concepts which are beyond the scope of this book, such as adaptive signal processing and information theory.
Yet we will see that, if we neglect some of the impairments that are introduced by real-world telephone lines, we are able to design a working system which will flawlessly modulates and demodulates a data sequence.
A telecommunication system works by exploiting the propagation of electromagnetic waves in a medium.
In the case of radio transmission, the medium is the electromagnetic spectrum; in the case of land-line communications such as those in voiceband or ADSL modems, the medium is a copper wire.
In all cases, the properties of the medium determine two fundamental constraints around which any communication system is designed:
Bandwith constraint: data transmission systems work best in the frequency range over which the medium behaves linearly; over this passband we can rely on the fact that a signal will be received with only phase and amplitude distortions, and these are “good” types of distortion since they amount to a linear filter.
Further limitations on the available bandwidth can be imposed by law or by technical requirements and the transmitter must limit its spectral occupancy to the prescribed frequency region.
Power constraint: the power of a transmitted signal is inherently limited by various factors, including the range over which the medium and the transmission circuitry behaves linearly.
In many other cases, such as in telephone or radio communications, the maximum power is strictly regulated by law.
Also, power could be limited by the effort to maximize the operating time of battery-powered mobile devices.
At the same time, all analog media are affected by noise, which can come in the form of interference from neighboring transmission bands (as in the case of radio channels) or of parasitic noise due to electrical interference (as in the case of AC hum over audio lines).
The noise floor is the noise level which cannot be removed and must be reckoned with in the transmission scheme.
Power constraints limit the achievable signal to noise ratio (SNR) with respect to the channel’s noise floor; in turn, the SNR determines the reliability of the data transmission scheme.
These constraints define a communication channel and the goal, in the design of a communication system, is to maximize the amount of information which can be reliably transmitted across a given channel.
In the design of a digital communication system, the additional goal is to operate entirely in the discrete-time domain up to the interface with the physical channel; this means that:
at the transmitter, the signal is synthesized, shaped and modulated in the discrete-time domain and is converted to a continuous-time signal just prior to transmission;
at the receiver, the incoming signal is sampled from the channel and demodulation, processing and decoding is performed in the digital domain.
A classic example of a regulated electromagnetic channel is commercial radio.
Bandwidth constraints in the case of the electromagnetic spectrum are rigorously put in place because the spectrum is a scarce resource which needs to be shared amongst a multitude of users (commercial radio, amateur radio, cellular telephony, emergency services, military use, etc).
Power constraints on radio emissions are imposed for human safety concerns.
The AM band, for instance, extends from 530 kHz to 1700 kHz; each radio station is allotted an 8 kHz frequency slot in this range.
Suppose that a speech signal x(t), obtained with a microphone, is to be transmitted over a slot extending from fmin equal to 650 kHz to fmax equal to 658 kHz.
Human speech can be modeled as a bandlimited signal with a frequency support of approximately 12 kHz; speech can, however, be filtered through a lowpass filter with cutoff frequency 4 kHz with little loss of intelligibility so that its bandwidth can be made to match the 8 kHz bandwidth of the AM channel.
The filtered signal now has a spectrum extending from -4 kHz to 4 kHz; multiplication by a sinusoid at frequency fc equal to (fmax plus fmin) over 2 equal to 654 KHz shifts its support according to the continuous-time version of the modulation theorem: if x(t)←FT→X(jΩ) then:
where Ωc equal to 2pifc.
This is, of course, a completely analog transmission system, which is schematically displayed in Figure 12 point 1.
The telephone channel is basically a copper wire connecting two users.
Because of the enormous number of telephone posts in the world, only a relatively small number of wires is used and the wires are switched between users when a call is made.
The telephone network (also known as POTS, an acronym for “Plain Old Telephone System”) is represented schematically in Figure 12 point 2. Each physical telephone is connected via a twisted pair (for example a pair of plain copper wires) to the nearest central office (CO); there are a lot of central offices in the network so that each telephone is usually no more than a few kilometers away.
Central offices are connected to each other via the main lines in the network and the digits dialed by a caller are interpreted by the CO as connection instruction to the CO associated to the called number.
To understand the limitations of the telephone channel we have to step back to the old analog times when COs were made of electromechanical switches and the voice signals traveling inside the network were boosted with simple operational amplifiers.
The first link of the chain, the twisted pair to the central office, actually has a bandwidth of several MHz since it is just a copper wire (this is the main technical fact behind ADSL, by the way).
Telephone companies, however, used to introduce what are called loading coils in the line to compensate for the attenuation introduced by the capacitive effects of longer wires in the network.
A side effect of these coils was to turn the first link into a lowpass filter with a cutoff frequency of approximately 4 kHz so that, in practice, the official passband of the telephone channel is limited between fmin equal to 300 Hz and fmax equal to 3000 Hz, for a total usable positive bandwidth W equal to 2700 Hz.
While today most of the network is actually digital, the official bandwidth remains in the order of 8 KHz (for example a positive bandwidth of 4 KHz); this is so that many more conversations can be multiplexed over the same cable or satellite link.
The standard sampling rate for a telephone channel is nowadays 8 KHz and the bandwidth limitations are imposed only by the antialiasing filters at the CO, for a maximum bandwidth in excess of W equal to 3400 Hz.
The upper and lower ends of the band are not usable due to possible great attenuations which may take place in the transmission.
In particular, telephone lines exhibit a sharp notch at f equal to 0 (also known as DC level) so that any transmission scheme will have to use bandpass signals exclusively.
The telephone channel is power limited as well, of course, since telephone companies are quite protective of their equipment.
Generally, the limit on signaling over a line is 0 point 2 V rms; the interesting figure however is not the maximum signaling level but the overall signal-to-noise ratio of the line (for example the amount of unavoidable noise on the line with respect to the maximum signaling level).
Nowadays, phone lines are extremely high-quality: a SNR of at least 28 dB can be assumed in all cases and one of 32-34 dB can be reasonably expected on a large percentage of individual connections.
Data transmission over a physical medium is by definition analog; modern communication systems, however, place all of the processing in the digital domain so that the only interface with the medium is the final D/A converter at the end of the processing chain, following the signal processing paradigm of Section ??.
In order to develop a digital communication system over the telephone channel, we need to re-cast the problem in the discrete-time domain.
To this end, it is helpful to consider a very abstract view of the data transmitter, as shown in Figure 12 point 3. Here, we neglect the details associated to the digital modulation process and concentrate on the digital-to-analog interface, represented in the picture by the interpolator I(t); the input to the transmitter is some generic binary data, represented as a bit stream.
The bandwidth constraints imposed by the channel can be represented graphically as in Figure 12 point 4. In order to produce a signal which “sits” in the prescribed frequency band, we need to use a D/A converter working at a frequency Fs ≥ 2fmax.
Once the interpolation frequency is chosen (and we will see momentarily the criteria to do so), the requirements for the discrete-time signal s[n] are set.
The bandwidth requirements become simply and they can be represented as in Figure 12 point 5 (in the figure, for instance, we have chosen Fs equal to 2 point 28fmax).
We can now try to understand how to build a suitable s[n] by looking more in detail into the input side of the transmitter, as shown in Figure 12 point 6.
The input bitstream is first processed by a scrambler, whose purpose is to randomize the data; clearly, it is a pseudo-randomization since this operation needs to be undone algorithmically at the receiver.
Please note how the implementation of the transmitter in the digital domain allows for a seamless integration between the transmission scheme and more abstract data manipulation algorithms such as randomizers.
The randomized bitstream could already be transmitted at this point; in this case, we would be implementing a binary modulation scheme in which the signal s[n] varies between the two levels associated to a zero and a one, much in the fashion of telegraphic communications of yore.
Digital communication devices, however, allow for a much more efficient utilization of the available bandwidth via the implementation of multilevel signaling.
With this strategy, the bitstream is segmented in consecutive groups of M bits and these bits select one of 2M possible signaling values; the set of all possible signaling values is called the alphabet of the transmission scheme and the algorithm which associates a group of M bits to an alphabet symbol is called the mapper.
We will discuss practical alphabets momentarily; however, it is important to remark that the series of symbols can be complex so that all the signals in the processing chain up to the final D/A converter are complex signals.
Spectral Properties of the Symbol Sequence.
The mapper produces a sequence of symbols a[n] which is the actual discrete-time signal which we need to transmit.
In order to appreciate the spectral properties of this sequence consider that, if the initial binary bitstream is a maximum-information sequence (for example if the distribution of zeros and ones looks random and “fifty-fifty”), and with the scrambler appropriately randomizing the input bitstream, the sequence of symbols a[n] can be modeled as a stochastic i.i.d.
process distributed over the alphabet.
Under these circumstances, the power spectral density of the random signal a[n] is simply where σA depends on the design of the alphabet and on its distribution.
Choice of Interpolation Rate.
We are now ready to determine a suitable rate Fs for the final interpolator.
The signal a[n] is a baseband, fullband signal in the sense that it is centered around zero and its power spectral density is nonzero over the entire [-pi,pi] interval.
If interpolated at Fs, such a signal gives rise to an analog signal with nonzero spectral power over the entire [-Fs over 2,Fs over 2] interval (and, in particular, nonzero power at DC level).
In order to fulfill the channel’s constraints, we need to produce a signal with a bandwidth of ωw equal to ωmax - ωmin centered around ωc equal to Â±(ωmax plus ωmin) over 2. The “trick” is to upsample (and interpolate) the sequence a[n], in order to narrow its spectral support.(1)
Assuming ideal discrete-time interpolators, an upsampling factor of 2, for instance, produces a half-band signal; an upsampling factor of 3 produces a signal with a support spanning one third of the total band, and so on.
In the general case, we need to choose an upsampling factor K so that.
Maximum efficiency occurs when the available bandwidth is entirely occupied by the signal, for example when K equal to 2pi over ωw.
In terms of the analog bandwidth requirements, this translates to where fw equal to fmax - fmin is the effective positive bandwidth of the transmitted signal; since K must be an integer, the previous condition implies that we must choose an interpolation frequency which is a multiple of the positive passband width fw.
The two criteria which must be fulfilled for optimal signaling are therefore.
The Baseband Signal.
The upsampling by K operation, used to narrow the spectral occupancy of the symbol sequence to the prescribed bandwidth, must be followed by a lowpass filter, to remove the multiple copies of the upsampled spectrum; this is achieved by a lowpass filter which, in digital communication parlance, is known as the shaper since it determines the time domain shape of the transmitted symbols.
We know from Section 11 point 2 point 1 that, ideally, we should use a sinc filter to perfectly remove all repeated copies.
Since this is clearly not possible, let us now examine the properties that a practical discrete-time interpolator should possess in the context of data communications.
The baseband signal b[n] can be expressed as where aKU[n] is the upsampled symbol sequence and g[n] is the lowpass filter’s impulse response.
Since aKU[n] equal to 0 for n not a multiple of K, we can state that:
It is reasonable to impose that, at multiples of K, the upsampled sequence b[n] takes on the exact symbol value, for example b[mK] equal to a[m]; this translates to the following requirement for the lowpass filter.
This is nothing but the classical interpolation property which we saw in Section 9 point 4 point 1.
For realizable filters, this condition implies that the minimum frequency support of G(ejω) cannot be smaller than [-pi over K,pi over K].(2)
In other words, there will always be a (controllable) amount of frequency leakage outside of a prescribed band with respect to an ideal filter.
To exactly fullfill (12 point 5), we need to use an FIR lowpass filter; FIR approximations to a sinc filter are, however, very poor, since the impulse response of the sinc decays very slowly.
A much friendlier lowpass characteristic which possesses the interpolation property and allows for a precise quantification of frequency leakage, is the raised cosine.
A raised cosine with nominal bandwidth ωw (and therefore with nominal cutoff ωb equal to ωw over 2) is defined over the positive frequency axis as and is symmetric around the origin.
The parameter β, with 0 is smaller than β is smaller than 1, exactly defines the amount of frequency leakage as a percentage of the passband.
The closer β is to one, the sharper the magnitude response; a set of frequency responses for ωb equal to pi over 2 and various values of β are shown in Figure 12 point 8.
The raised cosine is still an ideal filter but it can be shown that its impulse response decays as 1 over n3 and, therefore, good FIR approximations can be obtained with a reasonable amount of taps using a specialized version of Parks-McClellan algorithm.
The number of taps needed to achieve a good frequency response obviously increases as β approaches one; in most practical applications, however, it rarely exceeds 50.
The Bandpass Signal.
The filtered signal b[n] equal to g[n] * aKU[n] is now a baseband signal with total bandwidth ωw.
In order to shift the signal into the allotted frequency band, we need to modulate(3) it with a sinusoidal carrier to obtain a complex bandpass signal:
where the modulation frequency is the center-band frequency:
Note that the spectral support of the modulated signal is just the positive interval [ωmin,ωmax]; a complex signal with such a one-sided spectral occupancy is called an analytic signal.
The signal which is fed to the D/A converter is simply the real part of the complex bandpass signal:
If the baseband signal b[n] is real, then (12 point 7) is equivalent to a standard cosine modulation as in (12 point 1); in the case of a complex b[n] (as in our case), the bandpass signal is the combination of a cosine and a sine modulation, which we will examine in more detail later.
The spectral characteristics of the signals involved in the creation of s[n] are shown in Figure 12 point 9.
Baud Rate vs Bit Rate.
The baud rate of a communication system is the number of symbols which can be transmitted in one second.
Considering that the interpolator works at Fs samples per second and that, because of upsampling, there are exactly K samples per symbol in the signal s[n], the baud rate of the system is where we have assumed that the shaper G(z) is an ideal lowpass.
As a general rule, the baud rate is always smaller or equal to the positive passband of the channel.
Moreover, if we follow the normal processing order, we can equivalently say that a symbol sequence generated at B symbols per second gives rise to a modulated signal whose positive passband is no smaller than B Hz.
The effective bandwidth fw depends on the modulation scheme and, especially, on the frequency leakage introduced by the shaper.
The total bit rate of a transmission system, on the other hand, is at most the baud rate times the log in base 2 of the number of symbols in the alphabet; for a mapper which operates on M bits per symbol, the overall bitrate is the formula.
A Design Example.
As a practical example, consider the case of a telephone line for which fmin equal to 450 Hz and fmax equal to 3150 Hz (we will consider the power constraints later).
The baud rate can be at most 2700 symbols per second, since fw equal to fmax -fmin equal to 2700 Hz.
We choose a factor β equal to 0 point 125 for the raised cosine shaper and, to compensate for the bandwidth expansion, we deliberately reduce the actual baud rate to B equal to 2700 over (1 plus β) equal to 2400 symbols per second, which leaves the effective positive bandwidth equal to fw.
The criteria which the interpolation frequency must fulfill are therefore the following.
The first solution is for K equal to 3 and therefore Fs equal to 7200.
With this interpolation frequency, the effective bandwidth of the discrete-time signal is ωw equal to 2pi(2700 over 7200) equal to 0 point 75pi and the carrier frequency for the bandpass signal is ωc equal to 2pi(450 plus 3150) over (2Fs) equal to pi over 2. In order to determine the maximum attainable bitrate of this system, we need to address the second major constraint which affects the design of the transmitter, for example the power constraint.
The purpose of the mapper is to associate to each group of M input bits a value α from a given alphabet A. We assume that the mapper includes a multiplicative factor G0 which can be used to set the final gain of the generated signal, so that we don’t need to concern ourselves with the absolute values of the symbols in the alphabet; the symbol sequence is therefore:
and, in general, the values α are set at integer coordinates out of convenience.
Transmitted Power.
Under the above assumption of an i.i.d.
uniformly distributed binary input sequence, each group of M bits is equally probable; since we consider only memoryless mappers, for example mappers in which no dependency between symbols is introduced, the mapper acts as the source of a random process a[n] which is also i.i.d.
The power of the output sequence can be expressed as the formula.
where pa(α) is the probability assigned by the mapper to symbol α ∈A; the distribution over the alphabet A is one of the design parameters of the mapper, and is not necessarily uniform.
The variance σα2 is the intrinsic power of the alphabet and it depends on the alphabet size (it increases exponentially with M), on the alphabet structure, and on the probability distribution of the symbols in the alphabet.
Note that, in order to avoid wasting transmission energy, communication systems are designed so that the sequence generated by the mapper is balanced, for example its DC value is zero:
Using (8 point 25), the power of the transmitted signal, after upsampling and modulation, is the following formula.
The shaper is designed so that its overall energy over the passband is G2 equal to 2pi and we can express this as follows:
In order to respect the power constraint, we have to choose a value for G0 and design an alphabet A so that:
where Pmax is the maximum transmission power allowed on the channel.
The goal of a data transmission system is to maximize the reliable throughput but, unfortunately, in this respect the parameters σα2 and G0 act upon conflicting priorities.
If we use (12 point 9) and boost the transmitter’s bitrate by increasing M, then σα2 grows and we must necessarily reduce the gain G0 to fulfill the power constraint; but, in so doing, we impair the reliability of the transmission.
To understand why that is, we must leap ahead and consider both a practical alphabet and the mechanics of symbol decoding at the transmitter.
QAM.
The simplest mapping strategies are one-to-one correspondences between binary values and signal values: note that in these cases the symbol sequence is uniformly distributed with pa(α) equal to 2-M for all α ∈A.
For example, we can assign to each group of M bits (b0,…,bM-1) the signed binary number b0b1b2⋅⋅⋅bM-1 which is a value between -2M-1 and 2M-1 (b0 is the sign bit).
This signaling scheme is called pulse amplitude modulation (PAM) since the amplitude of each transmitted symbol is directly determined by the binary input value.
The PAM alphabet is clearly balanced and the inherent power of the mapper’s output is readily computed as(4).
Now, a pulse-amplitude modulated signal prior to modulation is a baseband signal with positive bandwidth of, say, ω0 (see Figure 12 point 9, middle panel); therefore, the total spectral support of the baseband PAM signal is 2ω0.
After modulation, the total spectral support of the signal actually doubles (Fig. 12 point 9, bottom panel); there is, therefore, some sort of redundancy in the modulated signal which causes an underutilization of the available bandwidth.
The original spectral efficiency can be regained with a signaling scheme called quadrature amplitude modulation (QAM); in QAM the symbols in the alphabet are complex quantities, so that two real values are transmitted simultaneously at each symbol interval.
Consider a complex symbol sequence
Since the shaper is a real-valued filter, we have that:
so that, finally, (12 point 7) becomes the following formula.
In other words, a QAM signal is simply the linear combination of two pulse-amplitude modulated signals: a cosine carrier modulated by the real part of the symbol sequence and a sine carrier modulated by the imaginary part of the symbol sequence.
The sine and cosine carriers are orthogonal signals, so that bI[n] and bQ[n] can be exactly separated at the receiver via a subspace projection operation, as we will see in detail later.
The subscripts I and Q derive from the historical names for the cosine carrier (the in-phase carrier) and the sine carrier which is the quadrature (for example the orthogonal carrier).
Using complex symbols for the description of the internal signals in the transmitter is an abstraction which simplifies the overall notation and highlights the usefulness of complex discrete-time signal models.
Constellations.
The 2M symbols in the alphabet can be represented as points in the complex plane and the geometrical arrangement of all such points is called the signaling constellation.
The simplest constellations are upright square lattices with points on the odd integer coordinates; for M even, the 2M constellation points αhk form a square shape with 2M over 2 points per side.
Such square constellations are called regular and a detailed example is shown in Figure 12 point 10 for M equal to 4; other examples for M equal to 2,6,8 are shown in Figure 12 point 11.
The nominal power associated to a regular, uniformly distributed constellation on the square lattice can be computed as the second moment of the points; exploiting the fourfold symmetry, we have the following formula.
Square-lattice constellations exist also for alphabet sizes which are not perfect squares and examples are shown in Figure 12 point 12 for M equal to 3 (8-point constellation) and M equal to 5 (32-point).
Alternatively, constellations can be defined on other types of lattices, either irregular or regular; Figure 12 point 13 shows an alternative example of an 8-point constellation defined on an irregular grid and a 19-point constellation defined over a regular hexagonal lattice.
We will see later how to exploit the constellation’s geometry to increase performance.
Transmission Reliability.
Let us assume that the receiver has eliminated all the “fixable” distortions introduced by the channel so that an “almost exact” copy of the symbol sequence is available for decoding; call this sequence Ã¢[n].
What no receiver can do, however, is eliminate all the additive noise introduced by the channel so that:
where η[n] is a complex white Gaussian noise term.
It will be clear later why the internal mechanics of the receiver make it easier to consider a complex representation for the noise; again, such complex representation is a convenient abstraction which greatly simplifies the mathematical analysis of the decoding process.
The real-valued zero-mean Gaussian noise introduced by the channel, whose variance is σ02, is transformed by the receiver into complex Gaussian noise whose real and imaginary parts are independent zero-mean Gaussian variables with variance σ02 over 2. Each complex noise sample η[n] is distributed according to
The magnitude of the noise samples introduces a shift in the complex plane for the demodulated symbols Ã¢[n] with respect to the originally transmitted symbols; if this displacement is too big, a decoding error takes place.
In order to quantify the effects of the noise we have to look more in detail at the way the transmitted sequence is retrieved at the receiver.
A bound on the probability of error can be obtained analytically if we consider a simple QAM decoding technique called hard slicing.
In hard slicing, a value Ã¢[n] is associated to the most probable symbol α ∈A by choosing the alphabet symbol at the minimum Euclidean distance (taking the gain G0 into account):
The hard slicer partitions the complex plane into decision regions centered on alphabet symbols; all the received values which fall into the decision region centered on α are mapped back onto α.
Decision regions for a 16-point constellation, together with examples of correct and incorrect hard slicing are represented in Figure 12 point 14: when the error sample η[n] moves the received symbol outside of the right decision region, we have a decoding error.
For square-lattice constellations, this happens when either the real or the imaginary part of the noise sample is larger than the minimum distance between a symbol and the closest decision region boundary.
Said distance is dmin equal to G0, as can be easily seen from Figure 12 point 10, and therefore the probability of error at the receiver is
where fη(x) is the pdf of the additive complex noise and D is a square on the complex plane centered at the origin and 2dmin wide.
We can obtain a closed-form expression for the probability of error if we approximate the decision region D by the inscribed circle of radius dmin (Fig. 12 point 15), so:
where we have used (12 point 17) and the change of variable z equal to ρejθ.
The probability of error decreases exponentially with the gain and, therefore, with the power of the transmitter.
The concept of “reliability” is quantified by the probability of error that we are willing to tolerate; note that this probability can never be zero, but it can be made arbitrarily low – values on the order of pe equal to 10-6 are usually taken as a reference.
Assume that the transmitter transmits at the maximum permissible power so that the SNR on the channel is maximized.
Under these conditions it is and from (12 point 18) we have.
For a regular square-lattice constellation we can use (12 point 15) to determine the maximum number of bits per symbol which can be transmitted at the given reliability figure:
and this is how the power constraint ultimately affects the maximum achievable bitrate.
Note that the above derivation has been carried out with very specific hypotheses on both the signaling alphabet and on the decoding algorithm (the hard slicing); the upper bound on the achievable rate on the channel is actually a classic result of information theory and is known under the name of Shannon’s capacity formula.
Shannon’s formula reads where C is the absolute maximum capacity in bits per second, B is the available bandwidth in Hertz and S over N is the signal to noise ratio.
Design Example Revisited.
Let us resume the example on page Â§ by assuming that the power constraint on the telephone line limits the maximum achievable SNR to 22 dB.
If the acceptable bit error probability is pe equal to 10-6, Equation (12 point 20) gives us a maximum integer value of M equal to 4 bits per symbol.
We can therefore use a regular 16-point square constellation; recall we had designed a system with a baud rate of 2400 symbols per second and therefore the final reliable bitrate is R equal to 9600 bits per second.
This is actually one of the operating modes of the V.32 ITU-T modem standard.(5)
The analog signal s(t) created at the transmitter is sent over the telephone channel and arrives at the receiver as a distorted and noise-corrupted signal ŝ(t).
Again, since we are designing a purely digital communication system, the receiver’s input interface is an A/D converter which, for simplicity, we assume, is operating at the same frequency Fs as the transmitter’s D/A converter.
The receiver tries to undo the impairments introduced by the channel and to demodulate the received signal; its output is a binary sequence which, in the absence of decoding errors, is identical to the sequence injected into the transmitter; an abstract view of the receiver is shown in Figure 12 point 16.
Let us assume for the time being that transmitter and receiver are connected back-to-back so that we can neglect the effects of the channel; in this case ŝ(t) equal to s(t) and, after the A/D module, ŝ[n] equal to s[n].
Demodulation of the incoming signal to a binary data stream is achieved according to the block diagram in Figure 12 point 17 where all the steps in the modulation process are undone, one by one.
The first operation is retrieving the complex bandpass signal ĉ[n] from the real signal ŝ[n].
An efficient way to perform this operation is by exploiting the fact that the original c[n] is an analytic signal and, therefore, its imaginary part is completely determined by its real part.
To see this, consider a complex analytic signal x[n], for example a complex sequence for which X(ejω) equal to 0 over the [-pi,0] interval (with the usual 2pi-periodicity, obviously).
We can split x[n] into real and imaginary parts: so that we can write.
In the frequency domain, these relations translate to (see (4 point 46)).
Since x[n] is analytic, by definition X(ejω) equal to 0 for -pi ≤ ω is smaller than 0, X*(e-jω) equal to 0 for 0 is smaller than ω ≤ pi and X(ejω) does not overlap with X*(e-jω) (Fig. 12 point 18).
We can therefore use (12 point 21) to write.
Now, xr[n] is a real sequence and therefore its Fourier transform is conjugate-symmetric, for example Xr(ejω) equal to Xr*(e-jω); as a consequence
By using (12 point 23) and (12 point 24) in (12 point 22) we finally obtain:
which is the product of Xr(ejω) with the frequency response of a Hilbert filter (Sect.
In the time domain this means that the imaginary part of an analytic signal can be retrieved from the real part only via the convolution:
At the demodulator, ŝ[n] equal to s[n] is nothing but the real part of c[n] and therefore the analytic bandpass signal is simply.
In practice, the Hilbert filter is approximated with a causal, 2L plus 1-tap type III FIR, so that the structure used in demodulation is that of Figure 12 point 19.
The delay in the bottom branch compensates for the delay introduced by the causal filter and puts the real and derived imaginary part back in sync to obtain.
Once the analytic bandpass signal is reconstructed, it can be brought back to baseband via a complex demodulation with a carrier with frequency -ωc.
Because of the interpolation property of the pulse shaper, the sequence of complex symbols can be retrieved by a simple downsampling-by-K operation.
Finally, the slicer (which we saw in Section 12 point 2 point 2) associates a group of M bits to each received symbol and the descrambler reconstructs the original binary stream.
If we now abandon the convenient back-to-back scenario, we have to deal with the impairments introduced by the channel and by the signal processing hardware.
The telephone channels affects the received signal in three fundamental ways:
it adds noise to the signal so that, even in the best case, the signal-to-noise ratio of the received signal cannot exceed a maximum limit;
it distorts the signal, acting as a linear filter;
it delays the signal, according to the propagation time from transmitter to receiver.
Distortion and delay are obviously both linear transformations and, as such, their description could be lumped together; still, the techniques which deal with distortion and delay are different, so that the two are customarily kept separate.
Furthermore, the physical implementation of the devices introduces an unavoidable lack of absolute synchronization between transmitter and receiver, since each of them runs on an independent internal clock.
Adaptive synchronization becomes a necessity in all real-world devices, and will be described in the next Section.
Noise.
The effects of noise have already been described in Section 12 point 2 point 2 and can be summed up visually by the plots in Figure 12 point 20 in each of which successive values of Ã¢[n] are superimposed on the same axes.
The analog noise is transformed into discrete-time noise by the sampler and, as such, it leaks through the demodulation chain to the reconstructed symbols sequence Ã¢[n]; as the noise level increases (or, equivalently, as the SNR decreases) the shape of the received constellation progressively loses its tightness around the nominal alphabet values.
As symbols begin to cross the boundaries of the decision regions, more and more decoding errors, take place.
Equalization.
We saw previously that the passband of a communication channel is determined by the frequency region over which the channel introduces only linear types of distortion.
The channel can therefore be modeled as a continuous-time linear filter Dc(jΩ) whose frequency response is unknown (and potentially time-varying).
The received signal (neglecting noise) is therefore Ŝ(jΩ) equal to Dc(jΩ)S(jΩ) and, after the sampler, we have where D(ejω) represents the combined effect of the original channel and of the anti-aliasing filter at the A/D converter.
To counteract the channel distortion, the receiver includes an adaptive equalizer E(z) right after the A/D converter; this is an FIR filter which is modified on the fly so that E(z) ≈ 1 over D(z).
While adaptive filter theory is beyond the scope of this book, the intuition behind adaptive equalization is shown in Figure 12 point 21.
In fact, the demodulator contains an exact copy of the modulator as well; if we assume that the symbols produced by the slicer are error-free, a perfect copy of the transmitted signal s[n] can be generated locally at the receiver.
The difference between the equalized signal and the reconstructed original signal is used to adapt the taps of the equalizer so that.
Clearly, in the absence of a good initial estimate for D(ejω), the sliced values Ã¢[n] are nothing like the original sequence; this is obviated by having the transmitter send a pre-established training sequence which is known in advance at the receiver.
The training sequence, together with other synchronization signals, is sent each time a connection is established between transmitter and receiver and is part of the modem’s handshaking protocol.
By using a training sequence, E(z) can quickly converge to an approximation of 1 over D(z) which is good enough for the receiver to start decoding symbols correctly and use them in driving further adaptation.
Delay:The continuous-time signal arriving at the receiver can be modeled as where v(t) is the continuous-time impulse response of the channel, η(t) is the continuous-time noise process and td is the propagation delay, for example the time it takes for the signal to travel from transmitter to receiver.
After the sampler, the discrete-time signal to be demodulated is ŝ[n] equal to ŝ(nTs); if we neglect the noise and distortion, we can write where we have split the delay as td equal to (nd plus τ)Ts with nd ∈ N and |τ|≤ 1 over 2. The term nd is called the bulk delay and it can be estimated easily in a full-duplex system by the following handshaking procedure:
System A sends an impulse to system B at time n equal to 0; the impulse appears on the channel after a known processing delay tp1 seconds; let the (unknown) channel propagation delay be td seconds.
System B receives the impulse and sends an impulse back to A; the processing time tp2 (decoding of the impulse and generation of response) is known by design.
The response impulse is received by system A after td seconds (propagation delay is symmetric) and detected after a processing delay of tp3 seconds.
In the end, the total round-trip delay measured by system A is since tp is known exactly in terms of the number of samples, td can be estimated to within a sample.
The bulk delay is easily dealt with at the receiver, since it translated to a simple z-nd component in the channel’s response.
The fractional delay, on the other hand, is a more delicate entity which we will need to tackle with specialized machinery.
In order for the receiver to properly decode the data, the discrete-time signals inside the receiver must be synchronous with the discrete-time signals generated by the transmitter.
In the back-to-back operation, we could neglect synchronization problems since we assumed ŝ[n] equal to s[n].
In reality, we will need to compensate for the propagation delay and for possible clock differences between the D/A at the transmitter and the A/D at the receiver, both in terms of time offsets and in terms of frequency offsets.
Carrier recovery is the modem functionality by which any phase offset between carriers is estimated and compensated for.
Phase offsets between the transmitter’s and receiver’s carriers are due to the propagation delay and to the general lack of a reference clock between the two devices.
Assume that the oscillator in the receiver has a phase offset of θ with respect to the transmitter; when we retrieve the baseband signal ˆb [n] from ĉ[n] we have where we have neglected both distortion and noise and assumed ĉ[n] equal to c[n].
Such a phase offset translates to a rotation of the constellation points in the complex plane since, after downsampling, we have Ã¢[n] equal to a[n]ejθ.
Visually, the received constellation looks like in Figure 12 point 22, where θ equal to pi over 20 equal to 9∘.
If we look at the decision regions plotted in Figure 12 point 22, it is clear that in the rotated constellation some points are shifted closer to the decision boundaries; for these, a smaller amount of noise is sufficient to cause slicing errors.
An even worse situation happens when the receiver’s carrier frequency is slightly different than the transmitter’s carrier frequency; in this case the phase offset changes over time and the points in the constellation start to rotate with an angular speed equal to the difference between frequencies.
In both cases, data transmission becomes highly unreliable: carrier recovery is then a fundamental part of modem design.
The most common technique for QAM carrier recovery over well-behaved channels is a decision directed loop; just as in the case of the adaptive equalizer, this works when the overall SNR is sufficiently high and the distortion is mild so that the slicer’s output is an almost error-free sequence of symbols.
Consider a system with a phase offset of θ; in Figure 12 point 23 the rotated symbol αˆ (indicated by a star) is sufficiently close to the transmitted value α (indicated by a dot) to be decoded correctly.
In the z plane, consider the two vectors ⃗a1 and ⃗a2, from the origin to ˆα and α respectively; the magnitude of their vector product can be expressed as.
Moreover, the angle between the vectors is θ and it can be computed as.
We can therefore obtain an estimate for the phase offset.
For small angles, we can invoke the approximation sin(θ) ≈ θ and obtain a quick estimate of the phase offset.
In digital systems, oscillators are realized using the algorithm we saw in Section 2 point 1 point 3; it is easy to modify such a routine to include a time-varying corrective term derived from the estimate of θ above so that the resulting phase offset is close to zero.
This works also in the case of a slight frequency offset, with θ converging in this case to a nonzero constant.
The carrier recovery block diagram is shown in Figure 12 point 24.
This decision-directed feedback method is almost always able to “lock” the constellation in place; due to the fourfold symmetry of regular square constellations, however, there is no guarantee that the final orientation of the locked pattern be the same as the original.
This difficulty is overcome by a mapping technique called differential encoding; in differential encoding the first two bits of each symbol actually encode the quadrant offset of the symbol with respect to the previous one, while the remaining bits indicate the actual point within the quadrant.
In so doing, the encoded symbol sequence becomes independent of the constellation’s absolute orientation.
Timing recovery is the ensemble of strategies which are put in place to recover the synchronism between transmitter and receiver at the level of discrete-time samples.
This synchronism, which was one of the assumptions of back-to-back operation, is lost in real-world situations because of propagation delays and because of slight hardware differences between devices.
The D/A and A/D, being physically separate, run on independent clocks which may exhibit small frequency differences and a slow drift.
The purpose of timing recovery is to offset such hardware discrepancies in the discrete-time domain.
A Digital PLL.
Traditionally, a Phase-Locked-Loop (PLL) is an analog circuit which, using a negative feedback loop, manages to keep an internal oscillator “locked in phase” with an external oscillatory input.
Since the internal oscillator’s parameters can be easily retrieved, PLLs are used to accurately measure the frequency and the phase of an external signal with respect to an internal reference.
In timing recovery, we use a PLL-like structure as in Figure 12 point 25 to compensate for sampling offsets.
To see how this PLL works, assume that the discrete-time samples ŝ[n] are obtained by the A/D converter as where the sequence of sampling instants tn is generated as the formula.
Normally, the sampling period is a constant and T[n] equal to Ts equal to 1 over Fs but here we will assume that we have a special A/D converter for which the sampling period can be dynamically changed at each sampling cycle.
Assume the input to the sampler is a zero-phase sinusoid of known frequency f0 equal to Fs over N for N ∈ N and N ≥ 2.
If the sampling period is constant and equal to Ts and if the A/D is synchronous to the sinusoid, the sampled signal are simply.
We can test such synchronicity by downsampling x[n] by N and we should have xND[n] equal to 0 for all n; this situation is shown at the top of Figure 12 point 26 and we can say that the A/D is locked to the reference signal x(t).
If the local clock has a time lag τ with respect to the reference time of the incoming sinusoid (or, alternatively, if the incoming sinusoid is delayed by τ), then the discrete-time, downsampled signal is the constant.
Note, the A/D is still locked to the reference signal x(t), but it exhibits a phase offset, as shown in Figure 12 point 26, middle panel.
If this offset is sufficiently small then the small angle approximation for the sine holds and xND[n] provides a direct estimate of the corrective factor which needs to be injected into the A/D block.
If the offset is estimated at time n0, it will suffice to set for the A/D to be locked to the input sinusoid.
Suppose now that the the A/D converter runs slightly slower than its nominal speed or, in other words, that the effective sampling frequency is Fs′ equal to βFs, with β is smaller than 1.
As a consequence the sampling period is T′s equal to Ts over β is larger than Ts and the discrete-time, downsampled signal becomes
We can use the downsampled signal to estimate β and we can re-establish a locked PLL by setting
The same strategy can be employed if the A/D runs faster than normal, in which case the only difference is that β is larger than 1.
A Variable Fractional Delay.
In practice, A/D converters with “tunable” sampling instants are rare and expensive because of their design complexity; furthermore, a data path from the discrete-time estimators to the analog sampler would violate the digital processing paradigm in which all of the receiver works in discrete time and the one-way interface from the analog world is the A/D converter.
In other words: the structure in Figure 12 point 25 is not a truly digital PLL loop; to implement a completely digital PLL structure, the adjustment of the sampling instants must be performed in discrete time via the use of a programmable fractional delay.
Let us start with the case of a simple time-lag compensation for a continuous-time signal x(t).
Of the total delay td, we assume that the bulk delay has been correctly estimated so that the only necessary compensation is that of a fractional delay τ, with |τ|≤ 1 over 2. From the available sampled signal x[n] equal to x(nTs) we want to obtain the signal using discrete-time processing only.
Since we will be operating in discrete time, we can assume Ts equal to 1 with no loss of generality and so we can write simply:
We know from Section ?? that the “ideal” way to obtain xτ[n] from x[n] is to use a fractional delay filter:
where Dτ(ejω) equal to ejωτ.
We have seen that the problem with this approach is that Dτ(ejω) is an ideal filter, and that its impulse response is a sinc, whose slow decay leads to very poor FIR approximations.
An alternative approach relies on the local interpolation techniques we saw in Section 9 point 4 point 2. Suppose 2N plus 1 samples of x[n] are available around the index n equal to n0; we could easily build a local continuous-time interpolation around n0 as where Lk(N)(t) is the k-th Lagrange polynomial of order 2N defined in (9 point 14).
The approximation is good, at least, over a unit-size interval centered around n0, for example for |t|≤ 1 over 2 and therefore we can obtain the fractionally delayed signal as as shown in Figure 12 point 27 for N equal to 1 (for example for a three-point local interpolation).
Equation (12 point 39) can be rewritten in general as which is the convolution of the input signal with a (2N plus 1)-tap FIR whose coefficients are the values of the 2N plus 1 Lagrange polynomials of order 2N computed in t equal to τ.
For instance, for the above three-point interpolator, we have
The resulting FIR interpolators are expressed in noncausal form purely out of convenience; in practical implementations an additional delay would make the whole processing chain causal.
The fact that the coefficients ˆd τ[n] are expressed in closed form as a polynomial function of τ makes it possible to efficiently compensate for a time-varying delay by recomputing the FIR taps on the fly.
This is actually the case when we need to compensate for a frequency drift between transmitter and receiver, for example we need to resample the input signal.
Suppose that, by using the techniques in the previous Section, we have estimated that the actual sampling frequency is either higher or lower than the nominal sampling frequency by a factor β which is very close to 1. From the available samples x[n] equal to x(nTs) we want to obtain the signal using discrete-time processing only.
With a simple algebraic manipulation we can write the formula.
Here, we are in a situation similar to that of Equation (12 point 37) but in this case the delay term is linearly increasing with n.
Again, we can assume Ts equal to 1 with no loss of generality and remark that, in general, β is very close to one so that it is
Nonetheless, regardless of how small τ is, at one point the delay term nτ will fall outside of the good approximation interval provided by the local interpolation scheme.
For this, a more elaborate strategy is put in place, which we can describe with the help of Figure 12 point 28 in which β equal to 0 point 82 and therefore τ ≈ 0 point 22:
We assume initial synchronism, so that xβ[0] equal to x(0).
For n equal to 1 and n equal to 2, 0 is smaller than nτ is smaller than 1 over 2; therefore xβ[1] equal to xτ[1] and xβ[2] equal to x2τ[2] can be computed using (12 point 40).
For n equal to 3, 3τ is larger than 1 over 2; therefore we skip x[3] and calculate xβ[3] from a local interpolation around x[4]: xβ[3] equal to xτ′[4] with τ′ equal to 1 - 3τ since |τ′| is smaller than 1 over 2.
For n equal to 4, again, the delay 4τ makes xβ[4] closer to x[5], with an offset of τ′ equal to 1 - 4τ so that |τ′| is smaller than 1 over 2; therefore xβ[4] equal to xτ′[5].
In general the resampled signal can be computed for all n using (12 point 40) as the formula.
It is evident that, τn is the quantity nτ “wrapped” over the [-1 over 2,1 over 2] interval(6) while γn is the number of samples skipped so far.
Practical algorithms compute τn and (n plus γn) incrementally.
Figure 12 point 29 shows an example in which the sampling frequency is too slow and the discrete-time signal must be resampled at a higher rate.
In the figure, β equal to 1 point 28 so that τ ≈-0 point 22; the first resampling steps are.
We assume initial synchronism, so that xβ[0] equal to x(0).
For n equal to 1 and n equal to 2, -1 over 2 is smaller than nτ; therefore xβ[1] equal to xτ[1] and xβ[2] equal to x2τ[2] can be computed using (12 point 40).
For n equal to 3, 3τ is smaller than -1 over 2; therefore we fall back on x[2] and calculate xβ[3] from a local interpolation around x[2] once again: xβ[3] equal to xτ′[2] with τ′ equal to 1 plus 3τ and |τ′| is smaller than 1 over 2.
For n equal to 4, the delay 4τ makes xβ[4] closer to x[3], with an offset of τ′ equal to 1 plus 4τ so that |τ′| is smaller than 1 over 2; therefore xβ[4] equal to xτ′[5].
In general the resampled signal can be computed for all n using (12 point 40) as where τn and γn are as in (12 point 43) and (12 point 44).
Nonlinearity.
The programmable delay is inserted in a PLL-like loop as in Figure 12 point 30 where is a processing block which extracts a suitable sinusoidal component from the baseband signal.(7)
Hypothetically, if the transmitter inserted an explicit sinusoidal component p[n] in the baseband with a frequency equal to the baud rate and with zero phase offset with respect to the symbol times, then this signal could be used for synchronism; indeed, from we would have pKD[n] equal to 0. If this component was present in the signal, then the block would be a simple resonator R with peak frequencies at ω equal to Â±2pi over K, as described in Section 7 point 3 point 1.
Now, consider more in detail the baseband signal b[n] in (12 point 4); if we always transmitted the same symbol a, then b[n] equal to a∑ ig[n-iK] would be a periodic signal with period K and, therefore, it would contain a strong spectral line at 2pi over K which we could use for synchronism.
Unfortunately, since the symbol sequence a[n] is a balanced stochastic sequence we have that:
and so, even on average, no periodic pattern emerges.(8)
The way around this impasse is to use a fantastic “trick” which dates back to the old days of analog radio receivers, for example we process the signal through a nonlinearity which acts like a diode.
We can use, for instance, the square magnitude operator; if we process b[n] with this nonlinearity, it will be the following formula.
Since we have assumed that a[n] is an uncorrelated i.i.d.
sequence, and, therefore,
The last term in the above equation is periodic with period K and this means that, on average, the squared signal contains a periodic component at the frequency we need.
By filtering the squared signal through the resonator above (for example by setting S{x[n]} equal to R{|x[n]|2}), we obtain a sinusoidal component suitable for use by the PLL.

