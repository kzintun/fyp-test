The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Over the last several lectures, we developed the Fourier representation for continuous-time signals.
What I'd now like to do is develop a similar representation for discrete-time.
And let me begin the discussion by reminding you of what our basic motivation was.
The idea is that what we wanted to do was exploit the properties of linearity and time invariance for linear time-invariant systems.
So in the case of linear time-invariant systems, the basic idea was to consider decomposing the input as a linear combination of basic inputs.
And then, because of linearity, the output could be expressed as a linear combination of corresponding outputs where psi sub i is the output due to phi sub i.
So basically, what we attempted to do was decompose the input, and then reconstruct the output through a linear combination of the outputs to those basic inputs.
We then focused on the notion of choosing the basic inputs with two criteria in mind.
One was to choose them so that a broad class of signals could be constructed out of those basic inputs.
And the second was to choose the basic inputs, so that the response to those was easy to compute.
And as you recall, one representation that we ended up with, with those basic criteria in mind, was the representation through convolution.
And then in beginning the discussion of the Fourier representation of continuous-time signals, we chose as another set of basic inputs complex exponentials.
So for continuous-time, we chose a set of basic inputs which were complex exponentials.
The motivation there was the fact that the complex exponentials have what we refer to as the eigenfunction property.
Namely, if we put complex exponentials into our continuous-time systems, then the output is a complex exponential of the same form with only a change in complex amplitude.
And that change in complex amplitude is what we referred to as the frequency response of the system.
And also, by the way, as it developed later, that frequency response, as you should now recognize from this expression, is in fact the Fourier transform, the continuous-time Fourier transform of the system impulse response.
And the notion of decomposing a signal as a linear combination of these complex exponentials is what, first the Fourier series representation, and then later the Fourier transform representation corresponded to.
And finally, to remind you of one additional point, the fact is that because of the eigenfunction property, the response-- once we have decomposed the input as a linear combination of complex exponentials, the response to that linear combination is straightforward to compute once we know the frequency response because of the eigenfunction property.
Now, basically the same strategy and many of the same ideas work in discrete-time, paralleling almost exactly what happened in continuous-time.
So the similarities between discrete-time and continuous-time are very strong.
Although as we'll see, there are a number of differences.
And it's important as we go through the discussion to illuminate not only the similarities, but obviously also the differences.
Well, let's begin with the eigenfunction property, and let me just state that just as in continuous-time, if we consider a set of basic signals, which are complex exponential rules, then discrete-time linear time-m invariant systems have the eigenfunction property.
Namely, if we put a complex exponential into the system,
the response is a complex exponential at the same complex frequency, and simply multiplied by an appropriate complex factor, or constant.
And just as we did in continuous-time, we will be referring to this complex constant, which is a function, of course, the frequency of the complex exponential input.
We'll be referring to this as the frequency response.
And although it's not particularly evident at this point, as the discussion develops through this lecture, what in fact will happen is very much paralleling continuous-time.
This particular expression, in fact, will correspond to what we'll refer to as the Fourier transform, the discrete-time Fourier transform of the system impulse response.
So there, of course, there's a very strong parallel between continuous time and discrete time.
Now, just as we did in continuous-time, let's begin the discussion by first concentrating on periodic-- the representation through complex exponentials of periodic sequences, and then we'll generalize that discussion to the representation of aperiodic signals.
So let's consider first a periodic signal, or in general, signals which are periodic.
Period denoted by capital N. And then, of course, the fundamental frequency is 2 pi divided by capital N.
Now, we can consider exponentials which have this as a fundamental frequency, or which are harmonics of that, and that would correspond to the class of complex exponentials of the form e to the jk omega 0 n.
So these complex exponentials then, as k varies, are complex exponentials that are harmonically related, all of which are periodic with the same period capital N.
Although the fundamental period is different for each of these.
Each of them being related by an integer amount.
Now, again, just as we did in continuous time, we can consider attempting to build our periodic signal out of a linear combination of these.
And so we consider a periodic signal, which is a weighted sum of these complex exponentials.
And, of course, this periodic signal-- this is a periodic signal.
This can be verified, more or less, in a straightforward way by substitution.
And, of course, one of the things that we'll want to address shortly is how broad a class of signals, again, can be represented by this sum?
And another question obviously will be, how do we determine the coefficients a sub k?
However, before we do that, let me focus on an important distinction between continuous-time and discrete-time in the context of these complex exponentials and this representation.
When we talked about complex exponentials and sinusoids early in the course, one of the differences that we saw between continuous-time and discrete-time is that in continuous-time, as we vary the frequency variable, we see different complex exponentials as omega varies.
Whereas, in discrete-time, we saw, in fact, that there was a periodicity.
Or said another way, it's straightforward to verify that if we think of this class of complex exponentials.
That, in fact, if we consider varying k by adding to it capital N, where capital N is the period of the fundamental complex exponential.
Then in fact, if we replace k by k plus capital N, we'll see exactly the same complex exponentials over again.
Now, what does that say?
What it says is that if I consider this class of complex exponentials, as k varies from 0 through capital N minus 1, we will see all of the ones that there are to see.
There aren't anymore.
And so, in fact, if we can build x of n out of this linear combination, then we better be able to do it as k varies from 0 up to N minus 1.
Because beyond that, we'll simply see the same complex exponentials over again.
So, for example, if k takes on the value capital N, that will be exactly the same complex exponential as if k is equal to 0.
So in fact, this sum ranges only over capital N of the distinct complex exponentials.
Let's say, for example, from 0 to capital N minus 1 .
Although, in fact, since these complex exponentials repeat in k, I could actually consider instead of from 0 to N minus 1, I could consider from 1 to N, or from 2 to N plus 1, or whatever.
Or said another way, in this representation, I could alternatively choose k outside this range, thinking of these coefficients simply as periodically repeating in k because of the fact that these complex exponentials periodically repeat in k.
So, in fact, in place of this expression, it will be common in writing the Fourier series expression to write it as I've indicated here, where the implication is that these Fourier coefficients periodically repeat as k continues to repeat outside the interval from 0 to N minus 1.
And so this notation, in fact, says that what we're going to use is k ranging over one period of this periodic sequence, which is the Fourier series coefficients.
So the expression that we have then for the Fourier series I've repeated here.
And the implication now is that the a sub k's are periodic.
They periodically repeat because, of course, these exponentials periodically repeat.
This indicates that we only use them over one period.
And now we can inquire as to how we determine the coefficients a sub k.
Well, we can formally go through this much as we did in the continuous-time case.
And we do, in fact, do that in the text, which involves substituting some sums and interchanging the orders of summation, et cetera.
But let me draw your attention to the fact that this, in fact, can be thought of as capital N equations and capital N unknowns.
In other words, we know x of n over a period, and so we know what the left-hand side of this is for capital N values.
And we'd like to determine these constants a sub k.
Well, it turns out that there is a nice convenient closed-form expression for that.
And, in fact, if we evaluate the closed-form expression through any of a variety of algebraic manipulations, we end up then with the analysis equation.
And the analysis equation, which tells us how to get the coefficients a sub k from x of n is what I've indicated here.
And so this tells us how from x of n to get the a sub k's.
And, of course, the first equation tells us how x of n is built up out of the a sub k.
Notice incidentally that there is a strong duality between these two equations.
And that's a duality that we'll return to, actually toward the end of the next lecture.
Now, there is a real difference between the way those equations look and the way the continuous-time Fourier series looked.
In the continuous-time case, let me remind you that it required an infinite number of coefficients to build up this continuous-time function.
And so this was not simply a matter of identifying how to invert capital N or a finite number of equations and a finite number of unknowns.
And the analysis equation was an integration as opposed to the synthesis equation, which is a summation.
So there is a real difference there between the continuous-time and discrete-time cases.
And the difference arises, to a large extent, because of this notion that in discrete-time, the complex exponentials are periodic in their frequency.
So we have then to summarize the synthesis equation and the analysis equation for the discrete-time Fourier series.
Again, x of n, our original signal is periodic.
And, of course, the complex exponentials involved are periodic.
They're periodic obviously in n.
But in contrast to continuous-time, these repeat in k.
In other words, as k omega 0 goes outside a range that covers a 2 pi interval.
And because of that, we're imposing, in a sense, the interpretation that the a sub k's are likewise a periodic sequence.
And in fact, if we look at the analysis equation, as we let k vary outside the range from 0 to N minus 1, what you can easily verify by substitution in here is that this sequence will, in fact, periodically repeat.
So to underscore the difference between the continuous-time and discrete-time cases, we have this periodicity in the time domain, and that's a periodicity that is, of course, true in discrete-time and it's also true in continuous-time if we replace the integer variable by the discrete-time time variable.
And we also, in discrete-time, have this periodicity in k, or in k omega 0.
And correspondingly, a periodicity in the Fourier coefficients.
And that is a set of properties that does not happen in continuous-time.
And it is that that essentially leads to all of the important differences between discrete-time Fourier representations and continuous-time Fourier representations.
Now, just quickly, let me draw your attention to the issue of convergence and when a sequence can and can't be represented, et cetera.
And recall that in the continuous-time case, we focused on convergence in the context either of conditions, which I referred to as square integrability, or another set of conditions, which were the Dirichlet conditions.
And there was this issue about when the signal does and doesn't converge at discontinuities, et cetera.
Let me just simply draw your attention to the fact that in the discrete-time case, what we have is the representation of the periodic signal as a sum of a finite number of terms.
This represents capital N equations and capital N unknowns.
If we consider earth the partial sum, namely taking a smaller number of terms, then simply what happens is as the number of terms increases to the finite number required to represent x of n, we simply end up with the partial sum representing the finite NOISEEVENT sequence.
What all that boils down to is the statement that in discrete-time there really are no convergence issues as there were in continuous-time.
OK, well let's look at an example of the Fourier series representation for a particular signal.
And the one that I've picked here is a simple one.
Namely, a constant, a sine term, and a cosine term.
Now, for this particular example, we can expand this out directly in terms of complex exponentials and essentially recognize this as a sum of complex exponentials.
It's examined in more detail in Example 5.2 in the text.
And if we look at the Fourier series coefficients, we can either look at it in terms of real and imaginary parts or magnitude and angle.
On the left side here, I have the real part of the Fourier coefficients.
And let me draw your attention to the fact that I've drawn this to specifically illuminate the periodicity of the Fourier series coefficients with a period of capital N.
So here are the Fourier coefficients.
And, in fact, it's this line that represents the DC, or constant term, and these two lines that represent the cosine term.
And of course, these are the three terms that are required.
Or equivalently, this one, this one, and this one.
And then because of the periodicity of the Fourier series coefficients, this simply periodically repeats.
So here is the real part and below it I show the imaginary part.
And in the imaginary part, incidentally let me draw your attention to the fact that it's this term and this term in the imaginary part that represent the sinusoid.
Whereas it's the symmetric terms in the real part the represent the cosine.
OK, let's look at another example.
This is another example from the text, and one that we'll be making frequent reference to in this particular lecture.
And what it is, is a square wave.
And I've expressed the Fourier series coefficients, which are algebraically developed in the text.
I've expressed the Fourier series coefficients as samples of an envelope function.
And so I've expressed it as samples of this particular function, which is referred to as a sin nx over sin x function.
And let me just compare it to a continuous-time example,
which is the continuous-time square wave, where with the continuous-times square wave the form of the Fourier series coefficients was as samples of what we refer to as a sin x over x function.
Now, the sin nx over sin x function, which is the envelope of the Fourier series coefficients for the discrete-time periodic square wave plays the role-- and we'll see it very often in discrete-time-- that sin x over x does in continuous-time.
And, in fact, we should understand right from the beginning that the sin x over x envelope couldn't possibly be the envelope of the discrete-time Fourier series coefficients.
And one obvious reason is that it is not periodic.
What we require, of course, from the discussion that I've just gone through is periodicity of the coefficients.
And then consequently, also periodicity of the envelope in the discrete-time case.
So once again, if we look back at the algebraic expression that I have, it's samples of the sin nx over sine x function that represent the Fourier series coefficients of this periodic square wave.
Now, in the representation in the continuous-time case, we essentially had used the concept of an envelope to represent the Fourier series coefficients, and the notion that the Fourier series coefficients were samples of an envelope.
And that is the same notion that we'll be using in discrete-time.
So again for this square wave example, then what we have is an envelope function, the sin nx over sin x envelope function for a particular value of the period.
Here indicated with a period of 10 samples.
These samples of this envelope function would then represent the Fourier series coefficients.
If we increased the period, then we would simply have a finer spacing on the samples of the envelope function to get the Fourier series coefficients.
And likewise, if we increase the period still further, what we would have is an even finer spacing.
So actually, as the period increases, and recall we used this in continuous-time also.
As the period increases, we can view the Fourier series coefficients as samples of an envelope.
And as the period increases, the sample spacing gets finer and finer.
And in fact, as the period goes off essentially to infinity, the samples of the envelope, in effect, become the envelope.
And recall also that this was essentially the trick that we used in continuous-time to allow us to develop or utilize the Fourier series to provide a representation of aperiodic signals as a linear combination of complex exponentials.
In particular, what we did in the continuous-time case when we had an aperiodic signal was to consider constructing a periodic signal for which the aperiodic signal was one period.
And then we developed the notion that since the periodic signal has a Fourier series, and since as the period of the periodic signal increases and goes to infinity, the periodic signal represents the aperiodic signal.
Then, essentially, the Fourier series provides us with a representation.
Now, we can do exactly the same thing in the discrete-time case.
The statement is exactly the same, except that in the discrete-time case, instead of t as the independent variable, we simply make exactly the same statement, but with our discrete-time variable n.
So the basic notion then in representing a discrete-time aperiodic signal is to first construct a periodic signal.
Here we have the aperiodic signal.
We construct a periodic signal by simply periodically replicating the aperiodic signal.
The periodic signal and the aperiodic signal are identical for one period.
And as the period goes off to infinity, it's the Fourier series representation of the periodic signal that provides a representation of the aperiodic signal.
Again, to return to the example that we have been kind of working through this lecture.
Namely, the periodic square wave.
If we have an aperiodic signal, which is a rectangle, and we construct a periodic signal.
And now we consider letting this period increase to infinity.
We would first have this set of samples of the envelope.
As the period increases, we would decrease the sample spacing to this set of samples.
As the period increases further, it would be this set of samples.
And as the period goes off to infinity, it's every point on the envelope.
In fact, what the representation of the aperiodic signal is, is the envelope.
OK, well, so that's the basic notion.
It's no different than what we did in the continuous-time case.
And mathematically, it develops in very much the same way as in the continuous-time case.
Specifically, here is our representation through the Fourier series of the-- here is a representation through the envelope function.
And this is the Fourier series synthesis equation where the equation below tells us how we get these Fourier coefficients or the envelope from x of n.
Now, x tilde of n is the periodic signal.
And we know that over one period, which is the only interval over which we use it, in fact, this is identical to the aperiodic signal.
And so, in fact, we can rewrite this equation simply by substituting in instead of x tilde, the original aperiodic signal.
And now we can use infinite limits on this sum.
And what we would want to examine, mathematically, is what happens to the top equation as we let the period go off to infinity?
And what happens is exactly identical, mathematically, to continuous-time.
I won't belabor the details.
Essentially it's this sum that goes to an integral.
Omega 0, which is the fundamental frequency, is going towards 0.
In fact, becomes the differential in the integral.
And in the second equation, of course, this then becomes x of omega.
And as N goes to infinity then, what the Fourier series becomes is the Fourier transform as summarized by the bottom two equations.
So although there is a little bit of mathematical trickery.
Or let's not call it trickery, but subtlety, to be tracked through in detail.
The important conceptual thing to think about is this notion that we take the aperiodic signal, form a periodic signal, let the period go off to infinity.
In which case, the Fourier series coefficients become these envelopes functions.
And incidentally, mathematically, one of the sums ends up going to an integral.
So what we have then is the discrete-time Fourier transform, which is a representation of an aperiodic signal.
And we have the synthesis equation, which I show as the top equation on this transparency.
And this is the integral that the Fourier series synthesis equation went to as the period went off to infinity.
And we have the corresponding analysis equation, which is shown below, where this tells us the Fourier transform.
In effect, the envelope or the Fourier series coefficients of that periodic signal.
And here represented in terms of the aperiodic signal.
So we have the analysis equation and synthesis equation.
There are a number of things to focus on as you look at this.
And we'll talk about some of its properties actually in the next lecture.
But some of the points that I'd like you to think about and focus on is the fact that now there is somewhat of an imbalance or lack of duality between the time domain and frequency domain. x of n, which is our aperiodic signal, is of course, discrete.
It's Fourier transform, x of omega, is a function of a continuous variable.
Omega is a continuous variable.
That is essentially what represents the envelope.
Also, in the time domain x of n is aperiodic.
It's not a periodic function.
However, in the frequency domain, remember that the Fourier series coefficients were always periodic.
Well, this envelope function then is also periodic with a period in omega of 2 pi.
Once again, the reason for the periodicity, it all stems back to the fact that when we talk about complex exponentials-- and recall back to the early lectures.
In discrete-time, as the frequency variable covers a range of 2 pi, when you proceed past that range, you simply see the same complex exponentials over and over again.
And so obviously, anything that we do with them would have to be periodic in that frequency variable.
All right, notationally, we'll, again, represent the discrete-time Fourier transform pair as I indicated here.
And since it's a complex function of frequency may, on occasion, want to either represent it in rectangular form as I indicate in this equation, or in polar form as I indicate in this equation.
Let's look at an example.
And, of course, one example that we can look at is the one that has kind of been tracking us through this lecture, which is the example of a rectangle.
Now, the rectangle, if we refer back to our argument of how we get a Fourier representation for an aperiodic signal, we would form a periodic signal where this is repeated.
And that's our square wave example.
As the period goes to infinity, the Fourier transform of this is represented by the envelope of those Fourier series coefficients, and that was our sin nx over sin x function, which in this particular case, for these particular numbers, is sin 5 omega over 2 divided by sin omega over 2.
And notice, of course, as we would expect--
notice that this is a periodic function of the frequency variable omega repeating, of course, with a period of 2 pi.
Whereas, in the time domain, the function was not a periodic function, it's aperiodic.
Now, let's look at another example.
Let's look at an example which is another signal that has kind of popped its head up from time to time as the lectures have gone along.
A signal which is another aperiodic signal, which is a decaying exponential of this form with the factor a chosen between 0 and 1.
And you can work out the algebra at your leisure.
Basically, if we substitute into the Fourier transform analysis equation, it's this sum that we evaluate.
Because we have a unit step here which shuts this off for n less than 0, we can change the limits on the sum.
This then corresponds to the sum over an infinite number of terms of a geometric series.
And that, as we've seen before, is 1 divided by 1 minus a e to the minus j omega.
So let's look at what that looks like.
Here then we have, again, the expression in the time domain and the expression in the frequency domain.
And let's, in particular, focus on what the magnitude of the Fourier transform looks like.
It's as we show here.
And for the particular values of a that I pick, namely between 0 and 1, it's larger at the origin than it is at pi.
And then, of course, it is periodic.
And the periodicity is inherent in the Fourier transform in discrete-time, so we really might only need to look at this either from minus pi to pi, or from 0 to 2 pi.
The periodicity, of course, would imply what the rest of this is for other values of omega.
Let me also draw your attention while we're on it to the fact that-- observe that if a were, in fact, negative, then this value would be less than this value.
And in fact, for a negative, the magnitude of the frequency response would look like this except shifted by an amount in omega equal to pi.
And this example will come up and play an important role in our discussion next time, so try to keep it in mind.
And in fact, work it out more carefully between now and next time.
And also, if you have a chance, focus on this issue of how it looks with a positive as compared with a negative.
Now, we developed the Fourier transform by beginning with the Fourier series.
We did that in continuous-time also.
What I'd like to do now, just as we did in continuous-time,
is now absorb the Fourier series within the broader framework of the Fourier transform.
And there are two relationships between the Fourier series and the Fourier transform, which are identical to relationships that we had in the continuous-time case.
Let me remind you that in continuous-time we had the statement that if we have a periodic signal, that in fact the Fourier series coefficients of that periodic signal is proportional to samples of the Fourier transform of one period.
Well, in fact, let me remind you flows easily from all the things that we built up so far, because of the fact that the Fourier transform essentially, by definition, of the way we developed it, is what we get as the Fourier series coefficients, as we focus on one period, and then let the period go off to infinity.
Well, looking at one period, the Fourier transform of that then is the envelope of the Fourier series coefficients.
And so in continuous-time, we have this relationship.
And in discrete-time, we have precisely the same relationship, except that here we're talking about an integer variable as opposed to the continuous variable, and a period of capital N as opposed to a period of t0.
OK, so once again, if we return to our example, or if we return to a periodic signal.
If we have a periodic signal and we consider the Fourier transform of one period, the Fourier series coefficients of this periodic signal are, in fact, samples-- as stated mathematically in the bottom equation, samples of the Fourier transform of one period.
So x of omega is the Fourier transform of one period.
a sub k's are the Fourier series coefficients of the periodic signal.
And this relationship simply says they're related except for scale factor through samples along the frequency axis.
And, of course, we saw this in the context of our square wave example.
In the square wave example, we have a periodic signal, which is a periodic square wave.
And the Fourier transform of one period, in fact, represents the envelope.
And here we have the envelope function.
Represents the envelope of the Fourier series coefficients.
And the Fourier series coefficients are samples.
So what we have then is a relationship back to the Fourier series coefficients from the Fourier transform that tells us that for a periodic signal now, the periodic signal-- the Fourier series coefficients are related, are samples of the Fourier transform of one period.
Now, finally, to kind of bring things back in a circle and exactly identical to what we did in the continuous-time case, we can finally absorb the Fourier series in discrete-time.
We can absorb it into the framework of the Fourier transform.
Now, remember or recall how we did that when we tried to do a similar sort of thing in continuous-time.
In continuous-time, what we essentially did is to develop that, more or less, by definition.
We have a periodic signal.
The periodic signal is represented through a Fourier series and Fourier series coefficients.
Essentially what I pointed out at that time was that if we define-- take it as a definition, the Fourier transform of the periodic signal as an impulse train where the amplitudes of the impulses are proportional to the Fourier series coefficients.
If we take that impulse train representation and simply plug it into the Fourier transform synthesis equation, what we end up with is the Fourier series synthesis equation.
So in continuous-time, we had used this definition of the continuous-time Fourier transform of a periodic signal.
And again, in discrete-time, it's simply a matter of using exactly the same expression.
And using, instead, the appropriate variables related to discrete-time rather than the variables related to continuous-time.
So in discrete-time, if we have a periodic signal, the Fourier transform of that periodic signal is defined as an impulse train where the amplitudes of the impulses are proportional to the Fourier series coefficients.
If this expression is substituted into the synthesis equation for the Fourier transform, that will simply then reduce to the synthesis equation for the Fourier series.
So once more returning to our example, which is the square wave example that we've carried through these lectures, or through this lecture, we can see that really what we're talking about really is a notational change.
Here is the periodic signal and below it are the Fourier series coefficients, where I've removed the envelope function and just indicate the amplitudes of the coefficients indexed, of course, on the coefficient number.
And so this represents a bar graph.
And if instead of talking about the Fourier series coefficients, what I want to talk about is the Fourier transform, the Fourier transform, in essence, corresponds to simply redrawing that using impulses and using an axis that is essentially indexed on the fundamental frequency omega 0, rather than on the Fourier series coefficient number k.
OK, so to summarize, what we've done is to pretty much parallel-- somewhat more quickly, the kind of development that we went through for continuous-time representation through complex exponentials, paralleled that for the discrete-time case.
And pretty much the conceptual underpinnings of the development are identical in discrete-time and in continuous-time.
We saw that there are some major differences, or important differences between continuous-time and discrete-time.
And the difference, essentially relates to two aspects.
One aspect is the fact that in discrete-time, we have a discrete representation in the time domain, whereas the independent variable in the frequency domain is a continuous variable.
Whereas in continuous-time for the Fourier transform, we had a duality between the time domain and frequency domain.
The other very important difference tied back to the difference between complex exponentials, continuous-time and discrete-time.
In continuous-time, complex exponentials, as you vary the frequency, generate distinct time functions.
In discrete-time, as you vary the frequency, once you've covered a frequency interval of 2 pi, then you've seen all the ones there are to see.
There are no more.
And this, in effect, imposes a periodicity on the Fourier domain representation of discrete-time signals.
And some of those differences and, of course, lots of the similarities will surface, both as we use this representation and as we develop further properties.
In the next lecture, what we'll do is to focus in,
again, on the Fourier transform, the discrete-time Fourier transform, develop or illuminate some of the properties of the Fourier transform, and then see how these properties can be used for a number of things.
For example, how the properties as they were in continuous-time can be used to efficiently generate the solution and analyze linear constant coefficient difference equations.
And then beyond that, the concepts of filtering and modulation.
And both the properties and interpretation, which will very strongly parallel the kinds of developments along those lines that we did in the last lecture.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Last time we began the development of the discrete-time Fourier transform.
And just as with the continuous-time case, we first treated the notion of periodic signals.
This led to the Fourier series.
And then we generalized that to the Fourier transform, and finally incorporated within the framework of the Fourier transform both aperiodic and periodic signals.
In today's lecture, what I'd like to do is expand on some of the properties of the Fourier transform, and indicate how those properties are used for a variety of things.
Well, let's begin by reviewing the Fourier transform as we developed it last time.
It, of course, involves a synthesis equation and an analysis equation.
The synthesis equation expressing x of n, the sequence, in terms of the Fourier transform, and the analysis equation telling us how to obtain the Fourier transform from the original sequence.
And I draw your attention again to the basic point that the synthesis equation essentially corresponds to decomposing the sequence as a linear combination of complex exponentials with amplitudes that are, in effect, proportional to the Fourier transform.
Now, the discrete-time Fourier transform, just as the continuous-time Fourier transform, has a number of important and useful properties.
Of course, as I stressed last time, it's a function of a continuous variable.
And it's also a complex-valued function, which means that when we represent it in general it requires a representation in terms of its real part and imaginary part, or in terms of magnitude and angle.
Also, as I indicated last time, the Fourier transform is a periodic function of frequency, and the periodicity is with a period of 2 pi.
And so it says, in effect, that the Fourier transform, if we replace the frequency variable by an integer multiple of 2 pi, the function repeats.
And I stress again that the underlying basis for this periodicity property is the fact that it's the set of complex exponentials that are inherently periodic in frequency.
And so, of course, any representation using them would, in effect, generate a periodicity with this period of 2 pi.
Just as in continuous time, the Fourier transform has important symmetry properties.
And in particular, if the sequence x sub n is real-valued, then the Fourier transform is conjugate symmetric.
In other words, if we replace omega by minus omega, that's equivalent to applying complex conjugation to the Fourier transform.
And as a consequence of this conjugate symmetry, this results in a symmetry in the real part that is an even symmetry, or the magnitude has an even symmetry, whereas the imaginary part or the phase angle are both odd symmetric.
And these are symmetry properties, again, that are identical to the symmetry properties that we saw in continuous time.
Well, let's see this in the context of an example that we worked last time and that we'll want to draw attention to in reference to several issues as this lecture goes along.
And that is the Fourier transform of a real damped exponential.
So the sequence that we are talking about is a to the n u of n, and let's consider a to be positive.
We saw last time that the Fourier transform for this sequence algebraically is of this form.
And if we look at its magnitude and angle, the magnitude I show here.
And the magnitude, as we see, has the properties that we indicated.
It is an even function of frequency.
Of course, it's a function of a continuous variable.
And it, in addition, is periodic with a period of two pi.
On the other hand, if we look at the phase angle below it, the phase angle has a symmetry which is odd symmetric.
And that's indicated clearly in this picture.
And of course, in addition to being odd symmetric, it naturally has to be, again, a periodic function of frequency with a period of 2 pi.
OK, so we have some symmetry properties.
We have this inherent periodicity in the Fourier transform, which I'm stressing very heavily because it forms the basic difference between continuous time and discrete time.
In addition to these properties of the Fourier transform, there are a number of other properties that are particularly useful in the manipulation of the Fourier transform, and, in fact, in using the Fourier transform to, for example, analyze systems represented by linear constant coefficient difference equations.
There in the text is a longer list of properties, but let me just draw your attention to several of them.
One is the time shifting property.
And the time shifting property tells us that if x of omega is the Fourier transform of x of n, then the Fourier transform of x of n shifted in time is that same Fourier transform multiplied by this factor, which is a linear phase factor.
So time shifting introduces a linear phase term.
And, by the way, recall that in the continuous-time case we had a similar situation, namely that a time shift corresponded to a linear phase.
There also is a dual to the time shifting property, which is referred to as the frequency shifting property, which tells us that if we multiply a time function by a complex exponential, that, in effect, generates a frequency shift.
And we'll see this frequency shifting property surface in a slightly different way shortly, when we talk about the modulation property in the discrete-time case.
Another important property that we'll want to make use of shortly is linearity, which follows in a very straightforward way from the Fourier transform definition.
And the linearity property says simply that the Fourier transform of a sum, or linear combination, is the same linear combination of the Fourier transforms.
Again, that's a property that we saw in continuous time.
And, also, among other properties there is a Parseval's relation for the discrete-time case that in effect says something similar to continuous time, specifically that the energy in the sequence is proportional to the energy in the Fourier transform, the energy over one period.
Or, said another way, in fact, or another way that it can be said, is that the energy in the time domain is proportional to the power in this periodic Fourier transform.
OK, so these are some of the properties.
And, as I indicated, parallel somewhat properties that we saw in continuous time.
Two additional properties that will play important roles in discrete time just as they did in continuous time are the convolution property and the modulation property.
The convolution property is the property that tells us how to relate the Fourier transform of the convolution of two sequences to the Fourier transforms of the individual sequences.
And, not surprisingly, what happens-- and this can be demonstrated algebraically-- not surprisingly, the Fourier transform of the convolution is simply the product of the Fourier transforms.
So, Fourier transform maps convolution in the time domain to multiplication in the frequency domain.
Now convolution, of course, arises in the context of linear time-invariant systems.
In particular, if we have a system with an impulse response h of n, input x of n, the output is the convolution.
The convolution property then tells us that in the frequency domain, the Fourier transform is the product of the Fourier transform of the impulse response and the Fourier transform of the input.
Now we also saw and have talked about a relationship between the Fourier transform, the impulse response, and what we call the frequency response in the context of the response of a system to a complex exponential.
Specifically, complex exponentials are eigenfunctions of linear time-invariant systems.
One of these into the system gives us, as an output, a complex exponential with the same complex frequency multiplied by what we refer to as the eigenvalue.
And as you saw in the video course manual, this eigenvalue, this constant, multiplier on the exponential is, in fact, the Fourier transform of the impulse response evaluated at that frequency.
Now, we saw exactly the same statement in continuous time.
And, in fact, we used that statement--
the frequency response interpretation of the Fourier transform, the impulse response-- we use that to motivate an intuitive interpretation of the convolution property.
Now, formally the convolution property can be developed by taking the convolution sum, applying the Fourier transform sum to it, doing the appropriate substitution of variables, interchanging order of summations, et cetera, and all the algebra works out to show that it's a product.
But as I stressed when we discussed this with continuous time, the interpretation-- the underlying interpretation-- is particularly important to understand.
So let me review it again in the discrete-time case, and it's exactly the same for discrete time or for continuous time.
Specifically, the argument was that the Fourier transform of a sequence or signal corresponds to decomposing it into a linear combination of complex exponentials.
What's the amplitude of those complex exponentials?
It's basically proportional to the Fourier transform.
If we think of pushing through the system that linear combination, then each of those complex exponentials gets the amplitude modified, or multiplied, by the Fourier transform of-- by the frequency response-- which we saw is the Fourier transform of the impulse response.
So the amplitudes of the output complex exponentials is then the amplitudes of the input complex exponentials multiplied by the frequency response.
And the Fourier transform of the output, in effect, is an expression expressing the summation, or integration, of the output as a linear combination of all of these exponentials with the appropriate complex amplitudes.
So, it's important, in thinking about the convolution property, to think about it in terms of nothing more than the fact that we've decomposed the input, and we're now modifying separately through multiplication, through scaling, the amplitudes of each of the complex exponential components.
Now what we saw in continuous time is that this interpretation and the convolution property led to an important concept, namely the concept of filtering.
Kind of the idea that if we decompose the input as a linear combination of complex exponentials, we can separately attenuate or amplify each of those components.
And, in fact, we could exactly pass some set of frequencies and totally eliminate other set of frequencies.
So, again, just as in continuous time, we can talk about an ideal filter.
And what I show here is the frequency response of an ideal lowpass filter.
The ideal lowpass filter, of course, passes exactly, with a gain of 1, frequencies around 0, and eliminates totally other frequencies.
However, an important distinction here between continuous time and discrete time is the fact that, whereas in continuous time when we talked about an ideal filter, we passed a band of frequencies and totally eliminated everything else out to infinity.
In the discrete time case, the frequency response is periodic.
So, obviously, the frequency response must periodically repeat for the lowpass filter.
And in fact we see that here.
If we look at the lowpass filter, then we've eliminated some frequencies.
But then we pass, of course, frequencies around 2 pi, and also frequencies around minus 2 pi, and for that matter around any multiple of 2 pi.
Although it's important to recognize that because of the inherent periodicity of the complex exponentials, these frequencies are exactly the same frequencies as these frequencies.
So it's lowpass filtering interpreted in terms of frequencies over a range from minus pi to pi.
Well, just as we talk about a lowpass filter, we can also talk about a highpass filter.
And a highpass filter, of course, would pass high frequencies.
In a continuous-time case, high frequencies meant frequencies that go out to infinity.
In the discrete-time case, of course, the highest frequencies we can generate are frequencies up to pi.
And once our complex exponentials go past pi, then, in fact, we start seeing the lower frequencies again.
Let me indicate what I mean.
If we think in the context of the lowpass filter, these are low frequencies.
As we move along the frequency axis, these become high frequencies.
And as we move further along the frequency axis, what we'll see when we get to, for example, a frequency of 2 pi are the same low frequencies that we see around 0.
In particular then, an ideal highpass filter in the discrete-time case would be a filter that eliminates these frequencies and passes frequencies around pi.
OK, so we've seen the convolution property and its interpretation in terms of filtering.
More broadly, the convolution property in combination with a number of the other properties that I introduced, in particular the time shifting and linearity property, allows us to generate or analyze systems that are described by linear constant coefficient difference equations.
And this, again, parallels very strongly the discussion we carried out in the continuous-time case.
In particular, let's think of a discrete-time system that is described by a linear constant coefficient difference equation.
And we'll restrict the initial conditions on the equation such that it corresponds to a linear time-invariant system.
And recall that, in fact, in our discussion of linear constant coefficient difference equations, it is the condition of initial rest that-- on the equation-- that guarantees for us that the system will be causal, linear, and time invariant.
OK, now let's consider a first-order difference equation, a system described by a first-order difference equation.
And we've talked about the solution of this equation before.
Essentially, we run the solution recursively.
Let's now consider generating the solution by taking advantage of the properties of the Fourier transform.
Well, just as we did in continuous time, we can consider applying the Fourier transform to both sides of this equation.
And the Fourier transform of y of n, of course, is Y of omega.
And then using the shifting property, the time shifting property, the Fourier transform of y of n minus 1 is Y of omega multiplied by e to the minus j omega.
And so we have this, using a linearity property we can carry down the scale factor, and add these two together as they're added here.
And the Fourier transform of x of n is X of omega.
Well, we can solve this equation for the Fourier transform of the output in terms of the Fourier transform of the input and an appropriate complex scale factor.
And simply solving this for Y of omega yields what we have here.
Now what we've used in going from this point to this point is both the shifting property and we've also used the linearity property.
At this point, we can recognize that here the Fourier transform of the output is the product of the Fourier transform of the input and some complex function.
And from the convolution property, then, that complex function must in fact correspond to the frequency response, or equivalently, the Fourier transform of the impulse response.
So if we want to determine the Fourier transform of the-- or the impulse response of the system, let's say for example, then it becomes a matter of having identified the Fourier transform of the impulse response, which is the frequency response.
We now want to inverse transform to get the impulse response.
Well, how do we inverse transform?
Of course, we could do it by attempting to go through the synthesis equation for the Fourier transform.
Or we can do as we did in the continuous-time case which is to take advantage of what we know.
And in particular, we know that from an example that we worked before, this is in fact the Fourier transform of a sequence which is a to the n times u of n.
And so, in essence, by inspection-- very similar to what has gone on in continuous time-- essentially by inspection, we can then solve for the impulse response to the system.
OK, so that procedure follows very much the kind of procedure that we've carried out in continuous time.
And this, of course, is discussed in more detail in the text.
Well, let's look at that example then.
Here we have the impulse response for that, associated with the system described by that particular difference equation.
And to the right of that, we have the associated frequency response.
And one of the things that we notice-- and this is drawn for a positive between 0 and 1-- what we notice, in fact, is that it is an approximation to a lowpass filter, because it tends to attenuate the high frequencies and retain and, in fact, amplify the low frequencies.
Now if instead, actually, the impulse response was such that we picked a to be negative between minus 1 and 0, then the impulse response in the time domain looks like this.
And the corresponding frequency response looks like this.
And that becomes an approximation to a highpass filter.
So, in fact, a first-order difference equation, as we see, has a frequency response, depending on the value of a, that either looks approximately like a lowpass filter for a positive or a highpass filter for a negative, very much like the first-order differential equation looked like a lowpass filter in the continuous-time case.
And, in fact, what I'd like to illustrate is the filtering characteristics-- or an example of filtering-- using a first-order difference equation.
And the example that I'll illustrate is a filtering of a sequence that in fact is filtered very often for very practical reasons, namely a sequence which represents the Dow Jones Industrial Average over a fairly long period.
And we'll process the Dow Jones Industrial Average first through a first-order difference equation, where, if we begin with a equals 0, then, referring to the frequency response that we have here, a equals 0 would simply be passing all frequencies.
As a is positive we start to retain mostly low frequencies,
and the larger a gets, but still less than 1, the more it attenuates high frequencies at the expense of low frequencies.
So let's watch the filtering, first with a positive and we'll see it behave as a lowpass filter, and then with a negative and we'll see the difference equation behaving as a highpass filter.
What we see here is the Dow Jones Industrial Average over roughly a five-year period from 1927 to 1932.
And, in fact, that big dip in the middle is the famous stock market crash of 1929.
And we can see that following that, in fact, the market continued a very long downward trend.
And what we now want to do is process this through a difference equation.
Above the Dow Jones average we show the impulse response of the difference equation.
Here we've chosen the parameter a equal to 0.
And the impulse response will be displayed on an expanded scale in relation to the scale of the input and, for that matter, the scale of the output.
Now with the impulse response shown here which is just an impulse, in fact, the output shown on the bottom trace is exactly identical to the input.
And what we'll want to do now is increase, first, the parameter a, and the impulse response will begin to look like an exponential with a duration that's longer and longer as a moves from 0 to 1.
Correspondingly we'll get more and more lowpass filtering as the coefficient a increases from 0 toward 1.
So now we are increasing the parameter a.
We see that the bottom trace in relation to the middle trace in fact is looking more and more smoothed or lowpass-filtered.
And here now we have a fair amount of smoothing, to the point where the stock market crash of 1929 is totally lost.
And in fact I'm sure there are many people who wish that through filtering we could, in fact, have avoided the stock market crash altogether.
Now, let's decrease a from 1 back towards 0.
And as we do that, we will be taking out the lowpass filtering.
And when a finally reaches 0, the impulse response of the filter will again be an impulse, and so the output will be once again identical to the input.
And that's where we are now.
All right now we want to continue to decrease a so that it becomes negative, moving from 0 toward minus 1.
And what we will see in that case is more and more highpass filtering on the output in relation to the input.
And this will be particularly evident in, again, the region of high frequencies represented by sharp transitions which, of course, the market crash of 1929 would represent.
So here, now, a is decreasing toward minus 1.
We see that the high frequencies, or rapid variations are emphasized., And finally, let's move from minus 1 back towards 0, taking out the highpass filtering and ending up with a equal to 0, corresponding to an impulse response which is an impulse, in other words, an identity system.
And let me stress once again that the time scale on which we displayed the impulse response is an expanded time scale in relation to the time scale on which we displayed the input and the output.
OK, so we see that, in fact, a first-order difference equation is a filter.
And, in fact, it's a very important class of filters,
and it's used very often to do approximate lowpass and highpass filtering.
Now, in addition to the convolution property, another important property that we had in continuous time, and that we have in discrete time, is the modulation property.
The modulation property tells us what happens in the frequency domain when you multiply signals in the time domain.
In continuous time, the modulation property corresponded to the statement that if we multiply the time domain, we convolve the Fourier transforms in the frequency domain.
And in discrete time we have very much the same kind of relationship.
The only real distinction between these is that in the discrete-time case, in carrying out the convolution, it's an integration only over a 2 pi interval.
And what that corresponds to is what's referred to as a periodic convolution, as opposed to the continuous-time case where what we have is a convolution that is an aperiodic convolution.
So, again, we have a convolution property in discrete time that is very much like the convolution property in continuous time.
The only real difference is that here we're convolving periodic functions.
And so it's a periodic convolution which involves an integration only over a 2 pi interval, rather than an integration from minus infinity to plus infinity.
Well, let's take a look at an example of the modulation property, which will then lead to one particular application, and a very useful application, of the modulation property in discrete time.
The example that I want to pick is an example in which we consider modulating a signal with-- a signal with another signal, x of n, or x1 of n as I indicated here, which is minus 1 to the n.
Essentially what that says is that any signal which I modulate with this in effect corresponds to taking the original signal and then going through that signal alternating the algebraic signs.
Now we--
in applying the modulation property, of course, what we need to do is develop the Fourier transform of this signal.
This signal which I rewrite--
I can write either as minus 1 to the n or rewrite as e to the j pi n since e to the j pi is equal to minus 1-- is a periodic signal.
And it's the periodic signal that I show here.
And recall that to get the Fourier transform of a periodic signal, one way to do it is to generate the Fourier series coefficients for the periodic signal, and then identify the Fourier transform as an impulse train where the heights of the impulses in the impulse train are proportional, with a proportionality factor of 2 pi, proportional to the Fourier series coefficients.
So let's first work out what the Fourier series is and for this example, in fact, it's fairly easy.
Here is the general synthesis equation for the Fourier series.
And if we take our particular example where, if we look back at the curve above, what we recognize is that the period is equal to 2, namely it repeats after 2 points.
Then capital N is equal to 2, and so we can just write this out with the two terms.
And the two terms involved are x1 of n is a0, the 0-th coefficient, that's with k equals 0, and a1, and this is with k equals 1, and we substituted in capital N equal to 2.
All right, well, we can do a little bit of algebra here, obviously cross off the factors of 2.
And what we recognize, if we compare this expression with the original signal which is e to the j pi n, then we can simply identify the fact that a0, the 0-th coefficient is 0, that's the DC term.
And the coefficient a1 is equal to 1.
So we've done it simply by essentially inspecting the Fourier series synthesis equation.
OK, now, if we want to get the Fourier transform for this, we take those coefficients and essentially generate an impulse train where we choose as values for the impulses 2 pi times the Fourier series coefficients.
So, the Fourier series coefficients are a0 is equal to 0 and a1 is equal to 1.
So, notice that in the plot that I've shown here of the Fourier transform of x1 of n, we have the 0-th coefficient,
which happens to be 0, and so I have it indicated, an impulse there.
We have the coefficient a1, and the coefficient a1 occurs at a frequency which is omega 0, and omega 0 in fact is equal to pi because the signal is e to the j pi n.
Well, what's this impulse over here?
Well, that impulse is a--
corresponds to the Fourier series coefficient a sub minus 1.
And, of course, if we drew this out over a longer frequency axis, we would see lots of other impulses because of the fact that the Fourier transform periodically repeats or, equivalently, the Fourier series coefficients periodically repeat.
So this is the coefficient a0, This is the coefficient a1 with a factor of 2 pi, this is 2 pi times a0 and 2 pi times a1.
And then this is simply an indication that it's periodically repeated.
All right.
Now, let's consider what happens if we take a signal and multiply it, modulate it, by minus 1 to the n.
Well in the frequency domain that corresponds to a convolution.
Let's consider a signal x2 of n which has a Fourier transform as I've indicated here.
Then the Fourier transform of the product of x1 of n and x2 of n is the convolution of these two spectra.
And recall that if you could convolve something with an impulse train, as this is, that simply corresponds to taking the something and placing it at the positions of each of the impulses.
So, in fact, the result of the convolution of this with this would then be the spectrum that I indicate here, namely this spectrum shifted up to pi and of course to minus pi.
And then of course to not only pi but 3 pi and 5 pi, et cetera, et cetera.
And so this spectrum, finally, corresponds to the Fourier transform of minus 1 to the n times x2 of n where x2 of n is the sequence whose spectrum was X2 of omega.
OK, now, this is in fact an important, useful, and interesting point.
What it says is if I have a signal with a certain spectrum and if I modulate-- multiply-- that signal by minus 1 to the n, meaning that I alternate the signs, then it takes the low frequencies-- in effect, it shifts the spectrum by pi.
So it takes the low frequencies and moves them up to high frequencies, and will incidentally take the high frequencies and move them to low frequencies.
So in fact we, in essence, saw this when we took--
or when I talked about the example of a sequence which was a to the n times u of n.
Notice--
let me draw your attention to the fact that when a is positive, we have this sequence and its Fourier transform is as I show on the right.
For a negative, the sequence is identical to a positive but with alternating sines.
And the Fourier transform of that you can now see, and verify also algebraically if you'd like, is identical to this spectrum, simply shifted by pi.
So it says in fact that multiplying that impulse response, or if we think of a positive and a negative, that is algebraically similar to multiplying the impulse response by minus 1 to the n.
And in the frequency domain, the effect of that, essentially, is shifting the spectrum by pi.
And we can interpret that in the context of the modulation property.
Now it's interesting that what that says is that if we have a system which corresponds to a lowpass filter, as I indicate here, with an impulse response h of n.
And it can be any approximation to a lowpass filter and even an ideal lowpass filter.
If we want to convert that to a highpass filter, we can do that by generating a new system whose impulse response is minus 1 to the n times the impulse response of the lowpass filter.
And this modulation by minus 1 to the n will take the frequency response of this system and shift it by pi so that what's going on here at low frequencies will now go on here at high frequencies.
This also says, incidentally, that if we look at an ideal lowpass filter and an ideal highpass filter, and we choose the cutoff frequencies for comparison, or the bandwidth of the filter to be equal.
Since this ideal highpass filter is this ideal lowpass filter with the frequency response shifted by pi, the modulation property tells us that in the time domain, what that corresponds to is an impulse response multiplied by minus 1 to the n.
So it says that the impulse response of the highpass filter, or equivalently the inverse Fourier transform of the highpass filter frequency response, is minus 1 to the n times the impulse response for the lowpass filter.
That all follows from the modulation property.
Now there's another way, an interesting and useful way,
that modulation can be used to implement or convert from lowpass filtering to highpass filtering.
The modulation property tells us about multiplying the time domain is shifting in the frequency domain.
And in the example that we happened to pick said if you multiply or modulate by minus 1 to the n, that takes low frequencies and shifts them to high frequencies.
What that tells us, as a practical and useful notion, is the following.
Suppose we have a system that we know is a lowpass filter, and it's a good lowpass filter.
How might we use it as a highpass filter?
Well, one way to do it, instead of shifting its frequency response, is to take the original signal, shift its low frequencies to high frequencies and its high frequencies to low frequencies by multiplying the input signal, the original signal, by minus 1 to the n, process that with a lowpass filter where now what's sitting at the low frequencies were the high frequencies.
And then unscramble it all at the output so that we put the frequencies back where they belong.
And I summarize that here.
Let's suppose, for example, that this system was a lowpass filter, and so it lowpass-filters whatever comes into it.
Down below, I indicate taking the input and first interchanging the high and low frequencies through modulation with minus 1 to the n.
Doing the lowpass filtering, which--
and what's sitting at the low frequencies here were the high frequencies of this signal.
And then after the lowpass filtering, moving the frequencies back where they belong by again modulating with minus 1 to the n.
And that, in fact, turns out to be a very useful notion for applying a fixed lowpass filter to do highpass filtering and vice versa.
OK, now, what we've seen and what we've talked about are the Fourier representation for discrete-time signals, and prior to that, continuous-time signals.
And we've seen some very important similarities and differences.
And what I'd like to do is conclude this lecture by summarizing those various relationships kind of all in one package, and in fact drawing your attention to both the similarities and differences and comparisons between them.
Well, let's begin this summary by first looking at the continuous-time Fourier series.
In the continuous-time Fourier series, we have a periodic time function expanded as a linear combination of harmonically-related complex exponentials.
And there are an infinite number of these that are required to do the decomposition.
And we saw an analysis equation which tells us how to get these Fourier series coefficients through an integration on the original time function.
And notice in this that what we have is a continuous periodic time function.
What we end up with in the frequency domain is a sequence of Fourier series coefficients which in fact is an infinite sequence, namely, requires all values of k in general.
We had then generalized that to the continuous-time Fourier transform, and, in effect, in doing that what happened is that the synthesis equation in the Fourier series became an integral relationship in the Fourier transform.
And we now have a continuous-time function which is no longer periodic, this was for the aperiodic case, represented as a linear combination of infinitesimally close-in-frequency complex exponentials with complex amplitudes given by X of omega d omega divided by 2 pi.
And we had of course the corresponding analysis equation that told us how to get X of omega.
Here we have a continuous-time function which is aperiodic, and a continuous function of frequency which is aperiodic.
The conceptual strategy in the discrete-time case was very similar, with some differences resulting in the relationships because of some inherent differences between continuous time and discrete time.
We began with the discrete-time Fourier series,
corresponding to representing a periodic sequence through a set of complex exponentials, where now we only required a finite number of these because of the fact that, in fact, there are only a finite number of harmonically-related complex exponentials.
That's an inherent property of discrete-time complex exponentials.
And so we have a discrete, periodic time function.
And we ended up with a set of Fourier series coefficients,
which of course are discrete, as Fourier series coefficients are, and which periodically repeat because of the fact that the associated complex exponentials periodically repeat.
We then used an argument similar to the continuous-time case for going from periodic time functions to aperiodic time functions.
And we ended up with a relationship describing a representation for aperiodic discrete-time signals in which now the synthesis equation went from a summation to an integration, since the frequencies are now infinitesimally close, involving frequencies only over a 2 pi interval, and for which the amplitude factor X of omega-- well, the amplitude factor is X of omega d omega divided by 2 pi.
And this term, X of omega, which is the Fourier transform, is given by this summation, and of course involves all of the values of x of n.
And so the important difference between the continuous-time and discrete-time case kind of arose, in part, out of the fact that discrete time is discrete time, continuous time is continuous time, and the fact that complex exponentials are periodic in discrete time.
The harmonically-related ones periodically repeat whereas they don't in continuous time.
Now this, among other things, has an important consequence for duality.
And let's go back again and look at this equation, this pair of equations.
And clearly there is no duality between these two equations.
This involves a summation, this involves an integration.
And so, in fact, if we make reference to duality, there isn't duality in the continuous-time Fourier series.
However, for the continuous-time Fourier transform, we're talking about aperiodic time functions and aperiodic frequency functions.
And, in fact, when we look at these two equations, we see very definitely a duality.
In other words, the time function effectively is the Fourier transform of the Fourier transform.
There's a little time reversal in there, but basically that's the result.
And, in fact, we had exploited that duality property when we talked about the continuous-time Fourier transform.
With the discrete-time Fourier series, we have a duality indicated by the fact that we have a periodic time function and a sequence which is periodic in the frequency domain.
And in fact, if you look at these two expressions, you see the duality very clearly.
And so it's the discrete-time Fourier series that has a duality.
And finally the discrete-time Fourier transform loses the duality because of the fact, among other things, that in the time domain things are inherently discrete whereas in the frequency domain they're inherently continuous.
So, in fact, here there is no duality.
OK, now that says that there's a difference in the duality, continuous time and discrete time.
And there's one more very important piece to the duality relationships.
And we can see that first algebraically by comparing the continuous-time Fourier series and the discrete-time Fourier transform.
The continuous-time Fourier series in the time domain is a periodic continuous function, in the frequency domain is an aperiodic sequence.
In the discrete-time case, in the time domain we have an aperiodic sequence, and in the frequency domain we have a function of a continuous variable which we know is periodic.
And so in fact we have, in the time domain here, aperiodic sequence.
In the frequency domain we have a continuous periodic function.
And in fact, if you look at the relationship between these two, then what we see in fact is a duality between the continuous-time Fourier series and the discrete-time Fourier transform.
One way of thinking of that is to kind of think, and this is a little bit of a tongue twister which you might want to get straightened out slowly, but the Fourier transform in discrete time is a periodic function of frequency.
That periodic function has a Fourier series representation.
What is this Fourier series?
What are the Fourier series coefficients of that periodic function?
Well in fact, except for an issue of time reversal, what it is the original sequence for which that's the Fourier transform.
And that is the duality that I'm trying to emphasize here.
OK, well, so what we see is that these four sets of relationships all tie together in a whole variety of ways.
And we will be exploiting as the discussion goes on the inner-connections and relationships that I've talked about.
Also, as we've talked about the Fourier transform, both continuous time and discrete time, two important properties that we focused on, among many of the properties, are the convolution property and the modulation property.
We've also shown that the convolution property leads to a very important concept, namely filtering.
The modulation property leads to an important concept, namely modulation.
We've also very briefly indicated how these properties and how these concepts have practical implications.
In the next several lectures, we'll focus in more specifically first on filtering, and then on modulation.
And as we'll see the filtering and modulation concepts form really the cornerstone of many, many signal processing ideas.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
In discussing the continuous-time and discrete-time Fourier transforms, we developed a number of important properties.
Two particularly significant ones, as I mentioned at the time, are the modulation property and the convolution property.
Starting with the next lecture, the one after this one, we'll be developing and exploiting some of the consequences of the modulation property.
In today's lecture though, I'd like to review and expand on the notion of filtering, which, as I had mentioned, flows more or less directly from the convolution property.
To begin, let me just quickly review what the convolution property is.
Both for continuous-time and for discrete-time, the convolution property tells us that the Fourier transform of the convolution of two time functions is the product of the Fourier transforms.
Now, what this means in terms of linear time-invariant filters, since we know that in the time domain the output of a linear time-invariant filter is the convolution of the input and the impulse response, it says essentially then in the frequency domain that the Fourier transform of the output is the product the Fourier transform of the impulse response, namely the frequency response, and the Fourier transform of the input.
So the output is described through that product.
Now, recall also that in developing the Fourier transform, I interpreted the Fourier transform as the complex amplitude of a decomposition of the signal in terms of a set of complex exponentials.
And the frequency response or the convolution property, in effect, tells us how to modify the amplitudes of each of those complex exponentials as they go through the system.
Now, this led to the notion of filtering, where the basic concept was that since we can modify the amplitudes of each of the complex exponential components separately, we can, for example, retain some of them and totally eliminate others.
And this is the basic notion of filtering.
So we have, as you recall, first of all the notion in continuous-time of an ideal filter, for example, I illustrate here an ideal lowpass filter where we pass exactly frequency components in one band and reject totally frequency components in another band.
The band being passed, of course, referred to as the passband, and the band rejected as the stopband.
I illustrated here a lowpass filter.
We can, of course, reject the low frequencies and retain the high frequencies.
And that then corresponds to an ideal highpass filter.
Or we can just retain frequencies within a band.
And so I show below what is referred to commonly as a bandpass filter.
Now, this is what the ideal filters looked like for continuous-time.
For discrete-time, we have exactly the same situation.
Namely, we have an ideal discrete-time lowpass filter,
which passes exactly frequencies which are the low frequencies.
Low frequencies, of course, being around 0, and because of the periodicity, also around 2pi.
We show also an ideal highpass filter.
And a highpass filter, as I indicated last time, passes frequencies around pi.
And finally, below that, I show an ideal bandpass filter passing frequencies someplace in the range between 0 and pi.
And recall also that the basic difference between continuous-time a discrete-time for these filters is that the discrete-time versions are, of course, periodic in frequency.
Now, let's look at these ideal filters, and in particular the ideal lowpass filter in the time domain.
We have the frequency response of the ideal lowpass filter.
And shown below it is the impulse response.
So here is the frequency response and below it the impulse response of the ideal lowpass filter.
And this, of course, is a sine x over x form of impulse response.
And recognize also or recall that since this frequency response is real-valued, the impulse response, in other words, the inverse transform is an even function of time.
And notice also, since I want to refer back to this, that the impulse response of an ideal lowpass filter, in fact, is non-causal.
That follows, from among other things, from the fact that it's an even function.
But keep in mind, in fact, that a sine x over x function goes off to infinity in both directions.
So the impulse response of the ideal lowpass filter is symmetric and continues to have tails off to plus and minus infinity.
Now, the situation is basically the same in the discrete-time case.
Let's look at the frequency response and associated impulse response for an ideal discrete-time lowpass filter.
So once again, here is the frequency response of the ideal lowpass filter.
And below what I show the impulse response.
Again, it's a sine x over x type of impulse response.
And again, we recognize that since in the frequency domain, this frequency response is real-valued.
That means, as a consequence of the properties of the Fourier transform and inverse Fourier transform, that the impulse response is an even function in the time domain.
And also, incidentally, the sine x over x function goes off to infinity, again, in both directions.
Now, we've talked about ideal filters in this discussion.
And ideal filters all are, in fact, ideal in a certain sense.
What they do ideally is they pass a certain band of frequencies exactly and they reject a band of frequencies exactly.
On the other hand, there are many filtering problems in which, generally, we don't have a sharp distinction between the frequencies we want to pass and the frequencies we want to reject.
One example of this that's elaborated on in the text is the design of an automotive suspension system, which, in fact, is the design of a lowpass filter.
And basically what you want to do in a case like that is filter out or attenuate very rapid road variations and keep the lower variations in, of course, elevation of the highway or road.
And what you can see intuitively is that there isn't really a very sharp distinction or sharp cut-off between what you would logically call the low frequencies and what you would call the high frequencies.
Now, also somewhat related to this is the fact that as we've seen in the time domain, these ideal filters have a very particular kind of character.
For example, let's look back at the ideal lowpass filter.
And we saw the impulse response.
The impulse response is what we had shown here.
Let's now look at the step response of the discrete-time ideal lowpass filter.
And notice the fact that it has a tail that oscillates.
And when the step hits, in fact, it has an oscillatory behavior.
Now, exactly the same situation occurs in continuous-time.
Let's look at the step response of the continuous-time ideal lowpass filter.
And what we see is that when a step hits then, in fact, we get an oscillation.
And very often, that oscillation is something that's undesirable.
For example, if you were designing an automotive suspension system and you hit a curve, which is a step input, in fact, you probably would not like to have the automobile oscillating, dying down in oscillation.
Now there's another very important point, which again,
we can see either in continuous-time or discrete-time, which is that even if we want it to have an ideal filter, the ideal filter has another problem if we want to attempt to implement it in real time.
What's the problem?
The problem is that since the impulse response is even and, in fact, has tails that go off to plus and minus infinity, it's non-causal.
So if, in fact, we want to build a filter and the filter is restricted to operate in real time, then, in fact, we can't build an ideal filter.
So what that says is that, in practice, although ideal filters are nice to think about and perhaps relate to practical problems, more typically what we consider are nonideal filters and in the discrete-time case, a nonideal filter then we would have a characteristic somewhat like I've indicated here.
Where instead of a very rapid transition from passband to stopband, there would be a more gradual transition with a passband cutoff frequency and a stopband cutoff frequency.
And perhaps also instead of having an exactly flat characteristic in the stopband in the passband, we would allow a certain amount of ripple.
We also have exactly the same situation in continuous-time,
where here we'll just simply change our frequency axis to a continuous frequency axis instead of the discrete frequency axis.
Again, we would think in terms of an allowable passband ripple, a transition from passband to stopband with a passband cutoff frequency and a stopband cutoff frequency.
So the notion here is that, again, ideal filters are ideal in some respects, not ideal in other respects.
And for many practical problems, we may not want them.
And even if we did want them, we may not be able to get them, perhaps because of this issue of causality.
Even if causality is not an issue, what happens in filter design and implementation, in fact, is that the sharper you attempt to make the cutoff, the more expensive, in some sense, the filter becomes, either in terms of components, in continuous-time, or in terms of computation in discrete-time.
And so there are these whole variety of issues that really make it important to understand the notion nonideal filters.
Now, just to illustrate as an example, let me remind you of one example of what, in fact, is a nonideal lowpass filter.
And we have looked previously at the associated differential equation.
Let me now, in fact, relate it to a circuit, and in particular an RC circuit, where the output could either be across the capacitor or the output can be across the resistor.
So in effect, we have two systems here.
We have a system, which is the system function from the voltage source input to the capacitor output, the system from the voltage source input to the resistor output.
And, in fact, just applying Kirchhoff's Voltage Law to this, we can relate those in a very straightforward way.
It's very straightforward to verify that the system from input to resistor output is simply the identity system with the capacitor output subtracted from it.
Now, we can write the differential equation for either of these systems and, as we talked about last time in the last several lectures, solve that equation using and exploiting the properties of the Fourier transform.
And in fact, if we look at the differential equation relating the capacitor output to the voltage source input, we recognize that this is an example that, in effect, we've solved previously.
And so just working our way down, applying the Fourier transform to the differential equation and generating the system function by taking the ratio of the capacitor voltage or its Fourier transform to the Fourier transform of the source, we then have the system function associated with the system for which the output is the capacitor voltage.
Or if we solve instead for the system function associated with the resistor output, we can simply subtract H1 from unity.
And the system function that we get in that case is the system function that I show here.
So we have, now, two system functions, one for the capacitor output, the other for the resistor output.
And one, the first, corresponding to the capacitor output, in fact, if we plot it on a linear amplitude scale, looks like this.
And as you can see, and as we saw last time, is an approximation to a lowpass filter.
It is, in fact, and nonideal lowpass filter, whereas the resistor output is an approximation to a highpass filter, or in effect, a nonideal highpass filter.
So in one case, just comparing the two, we have a lowpass filter as the capacitor output associated with the capacitor output, and a highpass filter associated with the resistor output.
Let's just quickly look at that example now, looking on a Bode plot, instead of on the linear scale that we showed before.
And recall incidentally, and be aware incidentally, of the fact that we can, of course, cascade several filters of this type and improve the characteristics.
So I have shown at the top a Bode plot of the system function associated with the capacitor output.
It's flat out to a frequency corresponding to 1 over the time constant, RC.
And then it falls off at 10 dB per decade, a decade being a factor of 10.
Or if instead we look at the system function associated with the resistor output, that corresponds to a 10 dB per decade increase in frequency up to approximately the reciprocal of the time constant, and then approaching a flat characteristic after that.
And if we consider either one of these, looking back again at the lowpass filter, if we were to cascade several filters with this frequency response, then because we have things plotted on a Bode plot, the Bode plot for the cascade would simply be summing these.
And so if we cascaded, for example, two stages instead of a roll-off at 10 dB per decade, it would roll off at 20 dB per decade.
Now, filters in this type, RC filters, perhaps several of them in cascade, are in fact very prevalent.
And in fact, in an environment like this, where we're, in fact, doing recording, we see there are filters of that type that show up very commonly both in the audio and the video portion of the signal processing that's associated with making this set of tapes.
In fact, let's take a look in the control room.
And what I'll be able to show you in the control room is the audio portion of the processing that's done and the kinds of filters, very much of the type we just talked about, that are associated with the signal processing that's done in preparing the audio for the tapes.
So let's just take a walk into the control room and see what we see.
This is the control room that's used for camera switching.
It's used for computer editing and also audio control.
You can see the monitors, and these are used for the camera switching.
And this is the computer editing console that's used for online and offline computer editing.
What I really want to demonstrate though, in the context of the lecture is the audio control panel, which contains, among other things, a variety of filters for high frequency, low frequencies, et cetera, basically equalization filters.
And what we have in the way of filtering is, first of all,
what's referred to as a graphic equalizer, which consists of a set of bandpass filters, which I'll describe a little more carefully in a minute.
And then also, an audio control panel, which is down here and which contains separate equalizer circuits for each of a whole set of channels and also lots of controls on them.
Well, let me begin in the demonstration by demonstrating a little bit of what the graphic equalizer does.
Well, what we have is a set of bandpass filters.
And what's indicated up here are the center frequencies of the filters, and then a slider switch for each one that lets us attenuate or amplify.
And this is a dB scale.
So essentially, if you look across this bank of filters with the total output of the equalizer just being the sum of the outputs from each of these filters, interestingly the position of the slider switches as you move across here, in effect, shows you what the frequency response of the equalizer is.
So you can change the overall shaping of the filter by moving the switches up and down.
Right now the equalizer is out.
Let's put the equalizer into the circuit.
And now I put in this filtering characteristic.
And what I'd like to demonstrate is filtering with this, when we do things that are a little more dramatic than what would normally be done in a typical audio recording setting.
And to do this, let's add to my voice some music to make it more interesting.
Not that my voice isn't interesting as it is.
But in any case, let's bring some music up.
And now what I'll do is set the low frequencies flat.
And let me take out the high frequencies above 800 cycles.
And so now what we have, effectively, is a lowpass filter.
And now with the lowpass filter, let me now bring the highs back up.
And so I'm bringing up those bandpass filters.
And now let me cut out the lows.
And you'll hear the lows disappearing and, in effect,
keeping the highs in effectively crispens the sound, either my voice or the music.
And finally, let me go back to 0 dB equalization on each of the filters.
And what I'll also do now is take the equalizer out of the circuit totally.
Now, let's take a look at the audio master control panel.
And this panel has, of course, for each channel and, for example, the channel that we're working on, of a volume control.
I can turn the volume down, and I can turn the volume up.
And it also has, for this particular equalizer circuit,
it has a set of three bandpass filters and knobs which let us either put in up to 12 dB gain or 12 dB attenuation in each of the bands, and also a selector switch that lets us select the center the band.
So let me just again demonstrate a little bit with this.
And let's get a close up of this panel.
So what we have, as I indicated, is three bandpass filters.
And these knobs that I'm pointing to here are controls that allow us for each of the filters to put in up to 12 dB gain or 12 dB attenuation.
There are also with each of the filters a selector switch that lets us adjust the center frequency of the filter.
Basically it's a two-position switch.
There also, as you can see, is a button that let's us either put the equalization in or out.
Currently the equalization is out.
Let's put the equalization in.
We won't hear any effect from that, because the gain controls are all set at 0 dB.
And I'll want to illustrate shortly the effect of these.
But before I do, let me draw your attention to one other filter, which is this white switch.
And this switch is a highpass filter that essentially cuts out frequencies below about 100 cycles.
So what it means is that if I put this switch in, everything is more or less flat above 100 cycles.
And what that's used for, basically, is to eliminate perhaps 60 cycle noise, if that's present, or some low frequency hum or whatever.
Well, we won't really demonstrate anything with that.
Let's NOISEEVENT now with the equalization in, demonstrate the effect of boosting or attenuating the low and high frequencies.
And again, I think to demonstrate this, it illustrates the point the best if we have a little background music.
So maestro, if you can bring that up.
NOISEEVENT And so now what I'm going to do is first boost the low frequencies.
And that's what this potentiometer knob will do.
So now, increasing the low frequency gain and, in fact,
all the way up to 12 dB when I have the knob over as far as I've gone here.
And so that has a very bassy sound.
And in fact, we can make it even bassier by taking the high frequencies and attenuating those by 12 dB.
OK well, let's put some of the high frequencies back in.
And now let's turn the low-frequency gain first back down to 0.
And now we're back to flat equalization.
And now I can turn the low frequency gain down so that I attenuate the low frequencies by much as 12 dB.
And that's where we are now.
And so this has, of course, a much crisper sound.
And to enhance the highs even more, I can, in addition to cutting out the lows, boost the highs by putting in, again, as much as 12 dB.
OK well, let's turn down the music now and go back to no equalization by setting these knobs to 0 dB.
And in fact, we can take the equalizer out.
Well, that's a quick look at some real-world filters.
Now let's stop having so much fun, and let's go back to the lecture.
OK well, that's a little behind-the-scenes look.
What I'd like to do now is turn our attention to discrete-time filters.
And as I've meant in previous lectures, there are basically two classes of discrete-time filters or discrete-time difference equations.
One class is referred to a non-recursive or moving average filter.
And the basic idea with a moving average filter is something that perhaps you're somewhat familiar with intuitively.
Think of the notion of taking a data sequence, and let's suppose that what we wanted to do was apply some smoothing to the data sequence.
We could, for example, think of taking adjacent points,
averaging them together, and then moving that average along the data sequence.
And what you can kind of see intuitively is that that would apply some smoothing.
So in fact, the difference equation, let's say, for three-point moving average would be the difference equation that I indicate here, just simply taking a data point and the two data points adjacent to it and forming an average of those three.
So if we thought of the processing involved, if we're forming an output sequence value, we would take three adjacent points and average them.
That would give us the output add the associated time.
And then to compute the next output point, we would just simply slide this by one point, average these together, and that would give us the next output point.
And we would continue along, just simply sliding and averaging to form the output data sequence.
Now, that's an example of what's commonly referred to a three-point moving average.
In fact, we can generalize that notion in a number of ways.
One way of generalizing the notion of a moving average from the three-point moving average, which I summarize again here, is to think of extending that to a larger number of points, and in fact applying weights to that as I indicated here, so that, in addition to just summing up the points and dividing by the number of points summed, we can, in fact, apply individual weights to the points so that it's what is often referred to as a weighting moving average.
And I show below one possible curve that might result, where these would be essentially the weights associated with this weighted moving average.
And in fact, it's easy to verify that this indeed corresponds to the impulse response of the filter.
Well, just to cement this notion, let me show you an example or two.
Here is an example of a five-point moving average.
A five-point moving average would have an impulse response that just consists of a rectangle of length five.
And if this is convolved with a data sequence, that would correspond to taking five adjacent points and, in effect, averaging them.
We've looked previously at the Fourier transform of this rectangular sequence.
And the Fourier transform of that, in fact, is of the form of a sine n x over sine x curve.
And as you can see, that is some approximation to a lowpass filter.
And so this, again, is the impulse response and frequency response of a nonideal lowpass filter.
Now, there are a variety of algorithms that, in fact, tell you how to choose the weights associated with a weighted moving average to, in some sense, design better approximations and without going into the details of any of those algorithms.
Let me just show the result of choosing the weights for the design of a 251-point moving average filter, where the weights are chosen using an optimum algorithm to generate as sharp a cutoff as can possibly be generated.
And so what I show here is the frequency response of the resulting filter on a logarithmic amplitude scale and a linear frequency scale.
Notice that on this scale, the passband is very flat.
Although here is an expanded view of it.
And in fact, it has what's referred to as an equal-ripple characteristic.
And then here is the transition band.
And here we have to stopband, which in fact is down somewhat more than 80 dB and, again, has what's referred to as an equal-ripple characteristic.
Now, the notion of a moving average for filtering is something that is very commonly used.
I had shown last time actually the result of some filtering on a particular data sequence, the Dow Jones Industrial Average.
And very often, in looking at various kinds of stock market publications, what you will see is the Dow Jones average shown in its raw form as a data sequence.
And then very typically, you'll see also the result of a moving average, where the moving average might be on the order of day, or it might be on the order of months.
The whole notion being to take some of the random high frequency fluctuations out of the average and show the low frequency, or trends, over some period of time.
So let's, in fact, go back to the Dow Jones average.
And let me now show you what the result of filtering with a moving average filter would look like on the same Dow Jones industrial average sequence that I showed last time.
So once again, we have the Dow Jones average from 1927 to roughly 1932.
At the top, we see the impulse response for the moving average.
Again, I remind you on an expanded time scale, and what's shown here is the moving average with just one point.
So the output on the bottom trace is just simply identical to the input.
Now, let's increase the length of the moving average to two points.
And we see that there is a small amount of smoothing,
three points and just a little more smoothing, that gets inserted.
Now a four-point moving average, and next the five-point moving average, and a six-point moving average next.
And we see that the smoothing increases.
Now, let's increase the length of the moving average filter much more rapidly and watch how the output is more and more smooth in relation to the input.
Again, I emphasize that the time scale for the impulse response is significantly expanded in relationship to the time scale for both the input and the output.
And once again, through the magic of filtering, we've been able to eliminate the 1929 Stock Market Crash.
All right, so we've seen moving average filters, or what are sometimes referred to as non-recursive filters.
And they are, as I stressed, a very important class of discrete-time filters.
Another very important class of discrete-time filters are what are referred to as recursive filters.
Recursive filters are filters for which the difference equation has feedback from the output back into the input.
In other words, the output depends not only on the input, but also on previous values of the output.
So for example, as I've stressed previously, a recursive difference equation has the general form that I indicate here, a linear combination of weighted outputs on the left-hand side and linear combination of weighted inputs on the right-hand side.
And as we've talked about, we can solve this equation for the current output y of n in terms of current and past inputs and past outputs.
For example, just to interpret this, focus on the interpretation of this as a filter, let's look at a first order difference equation, which we've talked about and generated the solution to previously.
So the first order difference equation would be as I indicated here.
And imposing causality on this, so that we assume that we are running this as a recursive forward in time, we can solve this for y of n in terms of x of n and y of n minus 1 weighted by the factor a.
And I simply indicate the block diagram for this.
But what we want to examine now for this first order recursion is the frequency response and see its interpretation as a filter.
Well in fact, again, the mathematics for this we've gone through in the last lecture.
And so interpreting the first order difference equation as a system, what we're attempting to generate is the frequency response, which is the Fourier transform of the impulse response.
And from the difference equation, we can, of course, solve for either one of those by using the properties, exploiting the properties, of Fourier transform.
Applying the Fourier transform to the difference equation, we will end up with the Fourier transform of the output equal to the Fourier transform of the input times this factor, which we know from the convolution property, in fact, is the frequency response of the system.
So this is the frequency response.
And of course, the inverse Fourier transform of that, which I indicate below, is the system impulse response.
So we have the frequency response obtained by applying the Fourier transform to the difference equation, the impulse response.
And, as we did last time, we can look at that in terms of a frequency response characteristic.
And recall that, depending on whether the factor a is positive or negative, we either get a lowpass filter or a highpass filter.
And if, in fact, we look at the frequency response for the factor a being positive, then we see that this is an approximation to a lowpass filter, whereas below it I show the frequency response for a negative.
And there this corresponds to a highpass filter, because we're attenuating low frequencies and retaining the high frequencies.
And recall also that we illustrated this characteristic as a lowpass or highpass filter for the first order recursion by looking at how it worked as a filter in both cases when the input was the Dow Jones average.
And indeed, we saw that it generated both lowpass and highpass filtering in the appropriate cases.
So for discrete-time, we have the two classes, moving average and recursive filters.
And there are a variety of issues discussed in the text about why, in certain contexts, one might want to use one of the other.
Basically, what happens is that for the moving average filter, for a given set a filter specifications, there are many more multiplications required than for a recursive filter.
But there are, in certain contexts, some very important compensating benefits for the moving average filter.
Now, this concludes, pretty much, what I want to say in detail about filtering, the concept of filtering, in the set of lectures.
This is only a very quick glimpse into a very important and very rich topic, and one, of course, that can be studied on its own in an considerable amount of detail.
As the lectures go on, what we'll find is that the basic concept of filtering, both ideal and nonideal filtering, will be a very important part of what we do.
And in particular, beginning with the next lecture, we'll turn to a discussion of modulation, exploiting the property of modulation as it relates to some practical problems.
And what we'll find when we do that is that a very important part of that discussion and, in fact, a very important part of the use of modulation also just naturally incorporates the concept and properties of filtering.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
In this lecture we begin a discussion of the topic of modulation, which is, among other things, a very important topic in practical terms.
For example, it forms the cornerstone for many communication systems.
And also, as we'll see as these lectures go along, a particular form of modulation referred to as pulse amplitude modulation, and eventually impulse modulation or impulse train modulation, forms a very important bridge between continuous time signals and discrete time signals.
Now in general terms what we mean when we refer to modulation is the notion of using one signal to vary a parameter of another signal.
For example, a sinusoidal signal has three parameters, amplitude, frequency, and phase.
And we could think, for example, of using one signal to vary, let's say, the amplitude of a sinusoidal signal.
And what that leads to is a notion, which we'll develop in some detail, referred to as sinusoidal amplitude modulation, and would correspond to a sinusoidal signal, referred to as the carrier, and it's amplitude being varied on the basis of another signal.
Now alternatively we could think of varying either the frequency or the phase of a sinusoidal signal, again with another signal.
And what that leads to is another very important notion,
which is referred to sinusoidal frequency modulation, where essentially it's the frequency of a sinusoid that's changing depending on the signal that's we're using to modulate the sinusoid.
Now sinusoidal amplitude, frequency, and phase modulation are extremely important topics and ideas in the context of communication systems.
One of the reasons is that if you want to transmit a signal,
let's say for example a voice signal, the voice signal that you're listening to now.
If you try to transmit that over long distances, because of the frequencies involved the medium that you use to transmit it won't carry it long distances.
The idea then is to essentially take that signal,
like a voice signal, use it to modulate a much higher-frequency signal, and then transmit that higher-frequency signal over a medium that essentially can support long-distance transmission at those frequencies.
Then at the other end of course, the voice information, or whatever else the information is, is taken off.
Now also, a notion that that leads to, and we'll be developing in some detail, is the idea that you can simultaneously transmit more than one signal by in essence taking several voice signals or other signals, using them to modulate either the frequency or amplitude of sinusoidal signals at different frequencies, adding all those together-- that's a process called multiplexing-- and then at the other end of the transmission system, taking those sinusoidal signals apart.
And then extracting the envelope or frequency modulation information to get back to the voice signal or other information-carrying signal.
So that's one of the very important ways in which sinusoidal modulation is used in communication systems.
And what we'll see, in particular as we go through today's lecture, is that sinusoidal amplitude modulation, follows in a fairly straightforward way from the properties of the Fourier transform that we've developed in some of the earlier lectures.
So our focus in today's lecture will be on sinusoidal amplitude modulation in continuous time.
In the next lecture we'll consider the same set of notions related to discrete time, and also a concept referred to as pulse amplitude modulation.
And all of these follow, in a very straightforward way, from the modulation property for the Fourier transform.
Issues of frequency and phase modulation are a little more difficult to analyze.
But many of the techniques that we've developed in the previous lectures also provide important insights into frequency and phase modulation.
And some of this is developed in more detail in the book.
So what I'd like to do is focus, for now, on the concept of amplitude modulation.
And as I indicated, there are several kinds of carrier signals on which the modulation can be superimposed.
The basic structure for an amplitude modulation system is one in which there is the modulating signal, let's say for example, voice, and a carrier signal-- what's referred to as the carrier.
And then of course the resulting output is the modulated output.
Now to analyze this, since we have multiplication in the time domain, we know from the property of the Fourier transform that we've developed previously-- the modulation property-- that multiplication in the time domain corresponds to convolution in the frequency domain.
And it's this basic property or equation that lets us analyze, in some detail in fact, the notions of amplitude modulation.
As we go through this lecture and the next lecture, we'll be talking, as I indicated, about several different types of carrier signals.
One is what's referred to as pulse carriers.
And that leads to, among other things, the concept of pulse amplitude modulation.
That will be deferred until the next lecture.
In today's lecture what I'll focus on is first, the case of a complex exponential carrier, second, the case of sinusoidal carrier.
And in fact the complex exponential carrier and sinusoidal carrier are obviously very closely related, since the complex exponential carrier is, in effect, two sinusoidal carriers.
One for the real part and one for the imaginary part.
So let's first begin the discussion of amplitude modulation by considering a complex exponential carrier, and then moving on to a discussion of a sinusoidal carrier.
So the issue then is that we have a signal, x of t.
It's multiplied by a carrier.
And the carrier that we're considering is a carrier signal, c of t, of the form e to the j omega c t plus theta sub c.
That's the form of our carrier signal.
And what we can first analyze is what the resulting signal or spectrum is at the output of the modulator.
Well, we can do that by concentrating on the modulation property.
And let's consider, just as a general form for a spectrum,
what I've indicated here for the Fourier transform of the input signal or modulating signal, X of omega.
And so this is intended to represent the spectrum of x of t.
And then the carrier signal, since it's a single complex exponential, has a Fourier transform which is an impulse in the frequency domain.
And the amplitude of the impulse is 2 pi e to the j theta sub c, where we notice that the complex amplitude incorporates the phase information.
So now if we multiply in the time domain, we convolve in the frequency domain.
And as you know, convolving a signal with an impulse just shifts that signal to the location of the impulse.
And so as a consequence of taking care of various factors, what we end up with is a spectrum that is centered at the carrier frequency omega sub c.
So what this says is that if we have a signal, x of t, and we use it to modulate a complex exponential carrier in the frequency domain, what we've simply done is to take the original spectrum and shift it in frequency.
So that what was originally at zero frequency is now centered around the carrier frequency.
We've now modulated, in effect, to a higher frequency.
Things are happening in a higher-frequency band.
And the next question is, how do we demodulate, or in other words, how do we get the original signal back?
Of course one way that we can think of doing it,
particularly in the context of this specific carrier, if we look back at the top equation we have, as the result of the modulation, x of t times c of t, where c of t is this.
And we could consider, for example, just simply dividing the modulator output by this.
Or equivalently, taking the modulated output and multiplying by e to the minus j omega c t plus theta c.
Let's track that through in terms of the spectra.
We have, again, the spectrum of the output of the modulator, which is the original spectrum shifted up to the carrier frequency.
We have, below that, the spectrum of e to the minus j omega c t plus theta c.
And if we now convolve this with this, that results in simply shifting this spectrum-- except for an issue of a scale factor-- shifting this spectrum back down to the origin.
So convolving these two together, the spectrum that we end up with is that.
So we can track this through in the frequency domain.
In the frequency domain it says shift the spectrum up.
When you want to demodulate, shift the spectrum back down.
And alternatively, we can look at it algebraically in the time domain.
And what it says is, if you multiply by e to the plus j omega c t, then when you want to get back, multiply by e to the minus j omega c t.
Now one question that you could conceivably be asking is, if we're talking about practical systems and not simply mathematics, does it make sense in the real world to consider using a complex exponential carrier?
And the answer to that, in fact, is yes.
That very often in practical systems one considers using a carrier which in fact is a complex exponential.
Well, a complex exponential is complex.
There's a square root of minus one in there.
And you could ask well, how do we get a square root of minus one?
And the answer is fairly simple.
Let's look again at the modulator, which we have here.
And in effect, what that says is we want to multiply a real-valued signal by e to the j omega c t plus theta c.
Now, we can equivalently use Euler's relationship to break this down into a cosine and sine term.
And so what that means in terms of an implementation, equivalently, is modulating x of t onto a cosine carrier.
And that then gives us the real part of the complex output.
And modulating it onto a sinusoidal carrier-- these two being 90 degrees out of phase-- and that gives us the imaginary part.
And so in effect, this is the complex signal.
If we just simply think of hanging a tag on here that says square root of minus 1, or j, and we appropriately combine complex signals following the rules of complex arithmetic.
And indeed, that's exactly the way things are done in the real world.
A complex signal is simply a set of two real signals.
And of course, if we look at the spectra involved, we have here the real part and the imaginary part of the complex output.
If we again refer back to the original spectrum, X of omega,
and the modulated spectrum which I show down here, the original spectrum shifted up to the carrier frequency.
In effect we're building this out of two lines.
One line representing the real part of that.
And the real part in the time domain corresponds to the even part in the frequency domain.
And so with the output of the cosine modulator, we have a spectrum that looks like this.
And the output along the imaginary branch has a spectrum that looks like this.
Recall in the top branch that this, for positive frequencies was positive, and was positive here.
And so in effect when you add them, this portion of the spectrum will cancel out.
So in effect, what we're doing is building the complex signal out of two real signals.
Or we're building the spectrum of the complex signal out of separate lines that represent the even and the odd parts.
Now, there are lots of applications of amplitude modulation.
And we'll be seeing a number of these as we go through the discussion.
What I'd like to do is just indicate briefly one now,
which is an application that in fact surfaces fairly often in the context of a complex exponential carrier.
And that is the notion of using modulation to permit the application of a very well designed and implemented low-pass filter to be used as a band-pass filter and in fact, as a set of band-pass filters.
And here's the idea.
The idea is if we have a fixed filter-- let's say we have a signal.
And we want to think of a filter, which we want to move along the signal, one way to do it is to somehow have filters that move along the signal.
The other possibility is to keep the filter fixed and let the signal move in frequency in front of the filter.
Let me be a little more specific.
Suppose that we have a signal, x of t.
And we modulate it with a complex exponential carrier with a carrier frequency, omega c.
And the output of that is then processed with a low-pass filter and then we demodulate the result.
Then what we've done is to take the spectrum of the input signal, shift it, pull out what is now around low frequencies, and then shift that part of the spectrum back to where it belongs.
So if we look at that in terms of actually tracking through the spectra, we would have initially a spectrum for the original signal, which I show at the top as X of omega.
After modulating or shifting that spectrum up to a center frequency of omega c, we then have what I indicate here.
And the dotted line corresponds to the pass band of the low-pass filter.
Well, the result of low-pass filtering rejects all the spectrum except the part around low frequencies.
And the next step is then to demodulate this.
And so in effect, demodulating will shift this spectrum back to where it originally came from.
And so that result will be what I show in the final result, which is here.
And what we can see is that this is equivalent.
If we can look back at the top spectrum, this is equivalent to having extracted, with a band-pass filter, a section out of this part of the spectrum.
So in terms of tracking through the spectrum and looking at the equivalent filtering operation, then what we accomplished was to pull out this part of the spectrum using a low-pass filter and modulation.
But equivalently what we implemented was a band-pass filter as I indicated here.
Now of course, a signal with this spectrum, since the spectrum is not conjugate symmetric, we know that this signal does not correspond to a real-valued signal.
Equivalently this filter doesn't correspond to a filter whose impulse response is real.
If we add another step to this, which is to take the real part of the output, then by taking the real part of the output we would be taking the even part of the spectrum associated with that complex signal.
And the equivalent filter that we would end up with then is the filter that I indicate at the bottom, which is a band-pass filter.
Now just to reiterate a point that I made earlier.
A question, of course, is why would you go to this trouble?
Why not just build a band-pass filter?
And one of the reasons is that it's often much easier to build a fixed filter, a filter with a fixed-center frequency, for example a low-pass filter, than it is to build a filter that has variable components in it so that when you vary them the filter's center frequency shifts around.
Now, if you want to look at the energy in a signal in different frequency bands, then you'd like to look at it through different filters.
And so the idea here, which is really the basis for many spectrum analyzers, is to build a really good quality low-pass filter and then use modulation, which is often easier to implement.
Use modulation to shift the signal essentially in front of the filter.
So we've worked our way through modulation with a complex exponential carrier.
And what we saw, among other things with a complex exponential carrier, is that what it corresponds to is two branches.
One being modulation with a cosine, and the other, modulation with a sine.
And so in the real world, or in a practical system,
modulation of the complex exponential carrier really would be accomplished with modulation with a sinusoidal carrier, and in particular with sinusoidal carriers that are in quadrature, as it's referred to, or equivalently 90 degrees out of phase.
Well, in fact sinusoidal modulation, in other words,
modulation using only a sinusoidal carrier, very often is used in its own right not only for generating a complex exponential carrier, but as a carrier by itself.
Let's look at what the consequences of modulation with a sinusoidal carrier are.
And in particular work through, again, what the spectra are and how we get the original signal back again.
So we are talking about a carrier signal which is simply a sinusoidal signal with some phase.
And of course we can write that as the sum of two complex exponential signals.
And so now, when we apply the modulation property we have the original spectrum, which I show here, X of omega.
And that's convolved with the spectrum of the carrier.
And the spectrum of the carrier, in this case, is two impulses.
One at plus omega c, and one at minus omega c.
And the amplitudes of these incorporate the phase.
And later on in the lecture, and in subsequent lectures,
I'll have a tendency to drop the theta sub c, just to keep the notation and algebra a little cleaner, but for now I've incorporated it.
And so now when we apply the modulation property, what we will do is convolve this spectrum with this spectrum, and the result is that the spectrum of the original signal gets replicated at both omega sub c and at minus omega sub c.
And the resulting spectrum at the output of the modulator, then, is the spectrum that I show here.
Now the question, of course, is-- so now what's happened is that with a sinusoidal carrier, we've moved the spectrum to both plus omega c and minus omega c.
And now if we want to get the original signal back again,
what we would like to do somehow is move that spectrum back down to the origin.
Now in the case of a complex exponential, that was easy to do.
We'd shifted one up, we'd just shift it back down.
Let's see what happens if we attempt to demodulate by again multiplying by the same sinusoidal carrier.
So let's examine what happens if we now take our modulated signal and, again, modulate it onto the same sinusoidal carrier to generate the output w of t.
If we look at the spectra, we have the modulated spectrum which we had initially.
And we now want to convolve that, again, with the spectrum of the carrier signal.
The spectrum of the carrier signal, I indicate here.
And if you track through the convolution, which is fairly straightforward, then what happens as you convolve this with this is you end up with a composite spectrum, which is what I've indicated on the bottom curve, and has the spectrum of the original signal, x of t, replicated in three places.
One is at minus 2 omega sub c.
One is around the origin.
And one is shifted up to twice the carrier frequency.
Well it's this piece that we want.
If we could eliminate everything else and keep this,
then that would correspond to the spectrum of the original signal, x of t.
How do we do that?
Well, we know how to eliminate part of the spectrum and keep another part of the spectrum.
That's called filtering.
So what we would do is put the result of this through a low-pass filter.
The low-pass filter route would retain the part of the spectrum around DC and eliminate the remaining part of the spectrum.
So we would keep this part and eliminate the part of the spectrum that we have over here.
And let me just draw your attention to the fact that,
because of the way the algebra works out, the amplitude of this replication of the spectrum is half what the original spectrum was.
And that means that ideally, to keep scale factors correct,
we would choose the amplitude of this to be 2, to scale this back up to 1.
So what we have is the modulator and demodulator.
And just to summarize, for the case of a sinusoidal carrier as opposed to a complex exponential carrier, the modulator is just as it is in the complex exponential case.
It's multiplication with the sinusoidal carrier, with frequency, omega c, and phase, theta sub c.
In the demodulator we would take the modulated signal, modulate it again with the same carrier signal-- and as we'll see later, it's important to keep the same phase relationship.
This result is not yet quite the demodulated signal.
We need to process that with a low-pass filter that extracts the part of the spectrum around DC and throws away the upper part of the spectrum that gets generated in the second modulation process.
And the resulting output is the original signal, x of t.
What we've done then is we've taken x of t.
We've modulated it onto a carrier.
And then we've taken that modulated signal and we've figured out how to get back x of t.
And of course one could ask, well, if you start with x of t and you want to get x of t back again, why bother going through all that?
Why not just use x of t at the beginning and at the end?
And obviously there are lots of reasons as I indicated before.
And just to reiterate what they are.
The notion, often, is that what you'd like to do is shift the signal into a different frequency band for transmission over some medium that is more matched to that frequency band than the frequency range of the original signal.
Also, as I alluded to, is the notion that you can take lots of signals and transmit them simultaneously over one channel-- whether the channel is a wire, a microwave link, a satellite link, or whatever-- again, using the idea of modulation.
And what that process is referred to as is multiplexing.
And let me just quickly indicate what that multiplexing process corresponds to.
We could think, for example, of taking one signal and modulating in it onto one carrier with one carrier frequency, taking a second signal, modulating it onto a different carrier frequency, taking a third signal and modulating it onto a third carrier frequency, et cetera.
And if we choose these carrier frequencies appropriately, then we can add all those together-- and do it in such a way that the spectra don't overlap-- and end up with one broader band signal that incorporates the information simultaneously in all of those signals.
So just to illustrate that in the frequency domain.
What we have are our three spectra, Xa, Xb, and Xc.
And we would, for example, take this spectrum and modulate it to a carrier frequency, omega sub a.
We can take this spectrum and modulate it to a carrier frequency, omega sub b, where omega sub b is chosen so that when we add these two together they don't overlap, so that they can eventually be separated out.
And then we can do the same thing with the third signal,
and put that in a frequency range over here, being careful that none of those overlap.
And when we add all those together, the composite spectrum is what I show here.
And as you can see, essentially, by doing appropriate band-pass filtering we can pull out whatever part of the spectrum we choose to, and then demodulate that in the appropriate way.
And of course we can do this, not just with three signals, but perhaps with tens or hundreds of signals.
So that's a process that is typically referred to as multiplexing.
And as I've described it here, it's referred to as frequency-division multiplexing.
That is, dividing the frequency band into cells and plunking different signals into each one of those.
And so if we want now to recover one of those channels in a frequency-division multiplex system, as I indicated, we would first demultiplex.
Demultiplexing corresponding to pulling out the appropriate channel with a band-pass filter.
And after demultiplexing, we would then demodulate.
And we would demodulate with the carrier appropriate to the channel that we've pulled out.
And the demodulation, of course, involves multiplying by the carrier and doing appropriate low-pass filtering to finally get the signal back.
And frequency-division multiplexing is the type of multiplexing that's used, for example, in typical broadcast AM radio systems, where all the channels are superimposed together.
And it's your home radio receiver that does the appropriate demultiplexing and demodulating.
And of course, you can see that not only is modulation an important part of that, but as I alluded to in the last lecture, filtering also becomes important part of these practical systems.
Now, the kind of amplitude modulation that I've talked about so far is what's referred to as synchronous modulation.
And the reason for the term synchronous is that what's implied in these systems is a synchronization between the transmitter and receiver.
In particular, in the system as we've talked about it, the modulator and the demodulator have a synchronization in both frequency and phase.
The phase here is indicated as theta sub c.
And if we take a look at the demodulator, the demodulator has phase of theta sub c.
And in general, there's the issue of whether we can maintain that synchronization between the modulator and demodulator.
And so what we want to examine now, more generally, is what the consequence might be, and the solution to the resulting problems, if we don't have synchronization between the modulator and demodulator.
Synchronization in terms of phase.
And there also is another problem, which is the issue of synchronization in frequency.
That's examined more in the text.
And what I'll focus on here is just the issue of synchronization in phase, to give you some sense of what the issue is.
So now what we want to look at is what happens if we have a modulator with phase, theta sub c, and a demodulator where the phase, instead of being theta sub c, is some other phase, phi sub c.
And if you track through the details and the algebra, then what you'll find is that the output of the low-pass filter, rather than being x of t, the signal that we want, is x of t multiplied by a scale factor.
And the scale factor is the cosine of the phase difference.
Now one could ask, OK well, what's the big deal about scale factor?
If it's too small we'll make it big, it it's too big we'll make it small.
But there are several points.
One is, notice, for example, that if the phase difference between the modulator and demodulator is 90 degrees, then the output of the demodulator is zero.
Or if it isn't quite 90 degrees, the amplitude might be small.
And the implication would be that if there's other noise it gets injected in the system, the signal-to-noise ratio is very low.
Now even worse is the issue that if there's a phase difference, but the exact phase difference isn't maintained, so that the modulator and demodulator kind of fade in and out of phase, then the output of the demodulator is x of t multiplied by a time-varying fading term, which is the cosine of the phase difference.
Well what that means, essentially, is that if you use this kind of system to do the demodulation, then what you need to be careful about is maintaining synchronization in phase, and also in frequency, between the modulator and the demodulator.
Now there are alternatives to this.
And the alternative is what's referred to as asynchronous demodulation.
And let me indicate what the idea behind asynchronous demodulation is.
Now, recall that what we've done in amplitude modulation is to take the carrier signal and vary its amplitude with the signal that eventually we want to get back.
So if we look at the amplitude-modulated waveform, it might typically look as I indicate here.
And we're trying to get back the envelope.
Well, one could imagine building a circuit, or designing a device, which in some sense will track the envelope.
And a common circuit to do that is a fairly simple circuit consisting of a diode and a resistor and capacitor in parallel.
The idea being that the capacitor charges up as this waveform moves up to its peak.
And then as the waveform drops down, the capacitor discharges through the resistor.
And it kind of tracks the envelope.
In fact, the kind of output that we would get is the type of behavior that I've indicated here.
And then that is a type of demodulation.
It's a demodulation that doesn't require synchronization between the modulator and demodulator.
And it's fairly inexpensive to build.
But it has, obviously, some tradeoffs associated with it.
Well, to indicate where the tradeoff comes from, or where the issue surfaces, notice that what we're doing is tracking the envelope of the sinusoidal signal.
And we're calling that, or we're assuming that that is our original signal, x of t.
Well, suppose that x of t, the original signal, is sometimes positive and sometimes negative.
What might we see as we look at the output of the demodulator?
Well, the output of the demodulator would follow the envelope down, and then it would follow the envelope back up again.
In other words, what it would tend to generate is a full-wave rectified version of the signal that you were really trying to get back.
Now, there's a simple solution to this.
The simple solution is to make sure that the signal that is the modulating signal, x of t, never goes negative.
So if it happens to-- a voice signal tends to go negative.
If it happens to, we can simply add a constant to it,
add a large enough constant, so that it always stays positive.
Well let's look at that.
What we want to do then, if we're considering asynchronous demodulation, is to take our original signal, x of t, and add to it a constant, where the constant is made large enough so that we're sure that this is a positive signal.
And incidentally, let me just draw your attention to the fact that I'm now suppressing the phase on the carrier signal, since the phase is not important to the argument and it's just some additional notation to carry around.
So the idea then, is add a constant to x of t.
Notice that if we just take this term and expand it out into two terms, x of t cosine omega c t plus a times cosine omega c t, then in block diagram terms we can represent that as I've shown here.
And so it would correspond to modulating the signal, x of t,
onto the carrier, omega sub c t, and also injecting some carrier with an amplitude, A. And the output of the modulator is then the sum of those two.
And depending on exactly what this value A is will influence what the envelope will look like.
And I indicate below, two possibilities.
One is where I've made A fairly large, and one is where I've made A significantly smaller.
And there are both positive and negative issues associated with whether A is too large or A is too small.
For example, if A is large in relation to the amplitude of the signal, then this envelope tends to be very flat.
And it tends to be easy to track it with that simple diode RC circuit, as compared with the case down here.
On the other hand, there is a price that you pay for this kind of envelope.
And the price that you pay is perhaps best seen in the frequency domain.
If we look in the frequency domain, here is our original spectrum.
Here is the spectrum at the output of the modulator.
And the impulse that occurs here corresponds to the carrier that's injected.
The larger A is, the more carrier is injected.
The more carrier that's injected, the easier it is for the envelope detector to demodulate.
So one can ask, why not just put a lot in?
Well, the obvious answer is that it's not an information-carrying part of the signal.
And so in some sense it represents an inefficiency in transmission, because what you're transmitting is power, energy, that doesn't have any information associated with it.
It's simply the injection of a carrier to make the demodulation for an asynchronous demodulator-- to make the demodulation easier.
And so there's this tradeoff.
And in fact, one represents the tradeoff and the associated parameters very often in terms of percent modulation, where the percent modulation is essentially the ratio of the maximum signal level to the amplitude of the injected carrier.
And depending on whether the modulation's very high or very low, the tradeoff is that the transmission is more inefficient and it takes more energy, but the demodulator is simpler.
Or the demodulator is more complicated but the transmission is simpler.
Now, there are situations where you might very well want to use one or the other.
For example, in home radio you're often willing to transmit a lot of power so that you can have inexpensive consumer-oriented receivers.
On the other hand, in satellite communication you're willing to pay a very high price for the modulators and demodulators, but it's the amount of power that's transmitted that's at a premium.
And so in one case, satellite communication, you would use synchronous modulation and demodulation.
Whereas in typical consumer-oriented broadcasting, you would use an asynchronous system and transmit more power, even if it's inefficient, so that the demodulator can be simpler.
Now, in the asynchronous system, as we've indicated,
there's one source of inefficiency, which is this injection of the carrier.
There also is a somewhat different issue, related to inefficiency in sinusoidal amplitude modulation.
And it's an inefficiency that is separate from the issue of synchronous versus asynchronous systems.
In other words, it's not associated with the injection of the carrier, it's a very different issue.
Let me indicate what that is.
Let's look again at the spectrum of x of t, which I've indicated here.
And in a sinusoidal amplitude modulation system, we would center it around plus and minus the carrier frequency.
Now, notice that in the original system we occupy a frequency spectrum that's 2 times omega sub M. By the time we've shifted it, thinking of positive and negative frequencies, we've used up twice as much of the frequency spectrum.
Well you could say, OK, let's just shift this up this way and get rid of this part.
That's of course what the complex exponential carrier did.
And the issue there is that now you've got to transmit both a real part and an imaginary part.
So what you can think about, and ask, is if you still want to transmit a real-valued signal, how can you somehow remove the inefficiency or redundancy in the spectrum?
Well, notice that what we have is this spectrum moved here, and moved here.
And we could imagine building real-valued signal by eliminating what I refer to here as the lower sideband out of the positive frequencies, and the lower sideband out of the negative frequencies.
And in effect, what we've done is taken just the positive frequencies here, shifted them there, the negative frequencies here, and shifted them here.
And the resulting spectrum is what I indicate below.
Well, this is what is often done.
And what it's referred to as is single sideband.
What we've done is kept the upper sideband, in this particular case.
We could alternatively think of putting this system together where we retain the lower sideband instead of the upper sideband.
And in either case, what we've removed is an inefficiency in transmission of the signal.
Namely we have a real-valued signal, but it only requires as much total bandwidth, in terms of the frequencies in which there's energy present, as the original signal.
Well how do we do this?
There are a variety of ways.
And there's one procedure that is discussed in more detail in the text, which uses what's referred to as a 90 degree phase splitter.
The simplest way, at least conceptually, is to think about doing it with filtering.
And the idea simply is that if we have our modulated signal-- here's the spectrum of the modulated signal.
And if that modulated signal is simply put through a high-pass filter, then the result will be to eliminate the lower sideband, if we choose the high-pass filter to have a characteristic as I indicate here.
So this, conceptually, is a very sharp cutoff filter.
And what it eliminates are the lower sidebands.
And the resulting spectrum is what we have below.
And this in fact is really the basic idea behind single-sideband transmission.
Again, there's a tradeoff.
It's clearly more efficient than double-sideband transmission, but also has the complication, or additional issue, that the modulator becomes a little more complicated because you need this filtering operation, or some equivalent operation, to get rid of the unwanted sideband.
Well, this is a fairly quick tour through a variety of issues related to modulation.
And it really is just the tip of the iceberg, obviously.
Modulation in the context of sinusoidal modulation, as we've talked about, has a lot of detailed issues associated with it.
It's important to recognize, and to be somewhat pleased by the fact, that not only with the mathematical foundations that we've developed can we understand the basics of sinusoidal amplitude modulation.
But what you'll find if you dig into this somewhat deeper that the basic background that we built up so far-- the mathematical tools-- are really pretty much what you need for a much deeper understanding of all of the issues involved.
So from what might have seemed like a fairly abstract mathematical property associated with the Fourier transform, we've begun to develop what should give you the sense of some important practical considerations.
And as we'll see the next lecture, very much the same kinds of notions apply for discrete time, sinusoidal, and complex exponential amplitude modulation.
And also as I indicated at the beginning of the lecture, in the next lecture we'll also talk about what's referred to as pulse amplitude modulation.
It's a different kind of carrier.
And what that will lead to, among other things, is a very important bridge between the notions of continuous time and the notions of discrete time.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MITOpenCourseWare at ocw.mit.edu.
Well, today, we have a chance to put away the equations and have some fun.
With me is Professor Sandy Hill from the University of Massachusetts at Amherst.
And Sandy, maybe you could just give us a quick tour of what we have here.
OK.
We'll be dealing with about four instruments today.
Two of them generate signals, and two of them display and analyze signals.
The first, and perhaps simplest, is simply an RC audio generator, that will put out a variety wave shapes.
And we'll look at that in a moment.
Over to the right is a device that will allow us to study amplitude modulation and the various flavors of it.
Down below is a standard oscilloscope to look at the signals in the time domain so we can see their shape.
And then, finally the spectrum analyzer will analyze the signal into its Fourier components.
And we'll display those, so you get to see the spectral content of a signal.
Now, we'll be seeing all of these in a fair amount of detail as we go through the demonstrations.
But maybe we can begin just with the signal generator.
OK, let's look quickly at the various buttons involved.
There's a lot of flexibility with this device.
The top three buttons here allow us to change from one simple wave form to another one, a sine wave, a triangle wave, a square wave.
And right below it is a button that will allow us to change the size of the signal.
Moving over towards the left, we can change the DC offset which varies the signal, the level upon which the signal rides.
And we'll be using that actually when we demonstrate amplitude modulation.
That's right And then finally, there's a way of changing the frequency of the signal, either changing it by a factor of 10 or a vernier adjustment, if you want very specific adjustments, very delicate changes.
OK.
So maybe we can just vary some of this and see what a little bit of it looks like.
All right.
What we have displayed at this point is the effect of pushing the sine wave button.
And so displayed down here is a 500 Hertz sine wave.
In fact, let's listen to that.
Great.
And the scope is said to have one millisecond for each division here.
By varying the amplitude knob can make the sine wave smaller or larger.
The ear isn't too sensitive to that.
By changing the DC offset, it simply rides on a different level.
It's like putting a battery in series.
And changing the frequency, you get a very perceptible change, a lower frequency.
It takes longer to sweep through a period.
Higher frequency, it takes a shorter amount of time.
And also, we can change the wave shape itself from a sine wave to a triangle wave-- and you notice that has a richer sound to it-- and then finally to a square wave, which is an even brighter sound.
And actually, as you realize from the previous lectures, with the triangle and square wave with the same fundamental as the sine wave, the richness comes in because of the higher harmonics in the Fourier series representation.
OK, well let's go back to the sine wave.
And we'll use that to take a closer look at the spectrum analyzer.
OK.
We'll look at a block diagram of the spectrum analyzer shortly, but first let's just look at a few quick things to get a feel for it.
We have, of course, the time domain display of the sine wave.
And the spectrum analyzer gives us a frequency domain analysis and display.
And the vertical axis is the amplitude.
The horizontal axis is frequency.
And as it's set currently, the frequency axis goes from DC-- zero-- up to 2 kilohertz.
And that, of course, can be varied on the spectrum analyzer.
And since we have a sine wave input, we get, as we would expect, a line spectrum, corresponding to the frequency of the sine wave.
Now, in fact, we can measure that frequency because the spectrum analyzer has a cursor associated with it, which I've just enabled.
And this line is the cursor line.
There is a read out for the frequency and a read out for the amplitude.
And let's position the cursor on the frequency of the sine wave.
And we're getting close.
And there we are at the frequency of the sine wave.
And we can read out, in fact, that it's 500 cycles.
And this is the amplitude.
And, of course, you could verify the frequency also in the time domain display, simply by measuring the frequency or the period.
This is set at still one millisecond per centimeter.
So that is 500 Hertz.
OK.
Well now maybe what we can do is vary the frequency.
Let's vary the frequency of the sine wave generator.
Maybe also listen to it?
I'll turn on the tone.
Yep.
OK, I'll make the frequency higher.
And you'll see a correspondence in the time domain.
The frequency clearly goes up, and that line of the spectrum also migrates up away from the cursor.
OK, and let me just point out again that this is the cursor line.
This isn't the spectra line.
And this is the frequency content of the sine wave.
We can also change the amplitude of that line by making the sine wave itself smaller.
And you notice the intensity of it goes--
While you do that, let me just position the cursor if I can on the sine wave.
And now as you vary the amplitude, we should, in fact, see this vary.
Well, that's right.
OK.
Now as Sandy varied the frequency, he did it slowly and we saw the line move as a single line.
Maybe now, Sandy, you could vary the frequency more rapidly.
And what will happen is that, in fact, what we'll get if I--
There'll be a spreading as we go--
Let me turn the cursor off.
And what we'll see is a spreading of the line, so that the frequency content is richer.
And, in fact, the analysis of this is considerably more complicated.
It corresponds to frequency modulation, where Sandy is now the modulating signal.
That's right.
And we won't really be going into issues of frequency modulation in any more detail in this demonstration.
Right.
OK, well now let's go to the overhead projector and take a look at a block diagram of the spectrum analyzer.
The specific spectrum analyzer that we're using in the demonstration is made by Rockland Systems, referred to as the Rockland Systems Model FFT 512.
And basically, the idea is to sample the incoming wave form, convert that into digital form, and then the spectrum, in fact, is computed digitally using a microprocessor.
So the overall system block diagram first consists of a system which is a low pass filter.
And this low pass filter is used in advance of the sampling process to basically reduce the artifacts that are introduced due to sampling.
And although we haven't talked yet about sampling and the associated artifacts, basically, as we'll see in the upcoming lectures, in order to sample a wave form and convert it into digital form, it requires that the wave form first be low pass filtered.
So this low pass filter is referred to-- and will be in later lectures-- as an anti-aliasing filter.
And this low pass filtered wave form is then converted to a sequence.
And so a sequence is generated for which the sequence values are simply samples of the low pass filtered input.
This low pass filtered input is then put into digital memory.
Basically, a time block of it is put into digital memory, and so that's what we have down here.
Here is the sequence.
The sequence is put into digital memory.
And then an arithmetic processor computes for the samples in this memory.
It computes the Fourier transform or the spectrum.
And after the spectrum is computed and put either into the same or a different memory, that is then put through a conversion process back to a continuous time signal, and finally, put out on the display that we've been seeing in the demonstration.
And so this is just indicative of the display.
So basically, the idea then is that the input wave form comes in.
It's filtered and sampled and captured on a block basis, put into a digital memory, and then, a digital computer or microprocessor computes the Fourier transform.
And then that Fourier transform is what we see on the display.
So what we're computing, of course, are samples of the Fourier transform.
And so, for example, if the input-- let's say--
was a rectangular pulse whose Fourier transform is of the form of a sine(x) over x function, what we would, in fact, see on the display are samples of that at discrete frequencies.
Or if as we have an input which is a square wave, what will generate through the spectrum analyzer are the Fourier series coefficients or equivalently, the harmonics associated with the square wave.
OK, well, let's now go back to the equipment and look at the spectrum analyzer.
We'll look at the square wave through the spectrum analyzer shortly, but first what we have is what we saw before, which is the sine wave.
And just to point out, again, the fact that the sine wave spectrum, of course, is just a single line corresponding to the fundamental frequency.
And here we have a frequency scale now that goes from 0 to 5 kilohertz.
And this is then the 500 cycle sine wave.
And, in fact, we can flip the cursor on and I happen to just magically have it positioned correctly.
And we see that it's a 500 cycle sine wave.
Might be interesting, Al, to go and look at a richer set of signals, such as the triangle wave and the square wave-- things that are conveniently on the signal generator itself.
I'll switch over to a square wave now.
And what you see is all the harmonics coming up, being displayed.
OK, it's actually interesting to point out, I think, that the square wave, as we know, is an odd harmonic function.
And so, in fact, the even harmonics are missing in the square wave.
So this is the fundamental.
This is the third harmonic, fifth harmonic, et cetera.
And then the amplitude of the square wave decays proportional to 1 over f, which is the kind of analysis that we've gone through in looking at Fourier series.
Right, and if we switch to the triangle wave,
instead of their decaying as 1 over f, the harmonics decay as 1 over f squared.
And you see they drop off much more quickly.
And again, of course, it's an odd harmonic signal.
And so the even numbered harmonics are missing.
And maybe just to kind of emphasize the point, we can show the sine wave again and the square wave again.
We'll go through from the most bland to the richer to the richest.
OK, and it's really kind of interesting and dramatic--
It's fun to do that.
--to see the harmonics pop in.
While we're at the square wave, let me fiddle with the frequency of the square wave, and we can see the duality of the time and frequency domains.
That is, as you compress things in the time domain, such as this, going to a higher frequency square wave, the harmonics wonder further away from each other.
So this is the fundamental.
The second harmonic is missing.
This is the third harmonic.
And of course, the fifth.
And as we take this to an extreme by going to very low repetition rate square waves, all those harmonics come scurrying in and cluster together near DC.
Kind of fun with Fourier transforms.
Well, speaking of time and frequency scaling, recall that we had demonstrated time and frequency scaling previously with the glockenspiel.
And what we had done there was to record a particular glockenspiel note, and then we played that back at half speed.
And what we had done in that case is expanded things in time, consequently compress them in frequency.
And so, comparing that with a note an octave lower than we saw that, in fact, the time scaling had led to a frequency scaling.
And then we also played the same note back at twice speed.
And in that case, the frequencies were all scaled up by a factor of 2.
And again, we illustrated that by comparing with the glockenspiel note a full octave up.
Now when we did that, we didn't actually look at the time wave forms or spectra.
And having the equipment that we have here gives us kind of a nice opportunity to do that.
So what I have is the tape that we had originally made of the glockenspiel, the original note that we recorded.
And what we'll do is look at the spectrum of that, and then compare that spectrum when we play the tape at half speed and also play it at twice speed.
So let's play the tape.
And what we have is the glockenspiel right now displayed on a frequency scale from 0 to 5 kilohertz.
Let's just change that to zero to 10 kilohertz.
So here is this spectrum.
Over here we have the time wave form.
And here is then the first spectral line, and we can see where that is by setting up the cursor.
And magically, once again, I have the cursor positioned at just the right spot.
The first spectral line is at 1.775 kilohertz.
And so this is the spectrum then of the original glockenspiel note.
All right.
Let's stop the tape and rewind it.
And now what we want to do is play that back at half speed.
Played at half speed, the frequencies should be scaled down.
And, in particular then the first spectra line should be at a lower frequency.
So let's play that now.
There are really very complicated signals, aren't they?
They really are.
Here we have the first spectral line.
And we can compare that with the first spectral line that we had before which was at 1.775 kilohertz.
And, once again, you see that the time wave form over here has been scaled by a factor of 2.
So, once again, we see that time and frequency scaling really works.
Incidentally as Sandy pointed out, and rightfully so, the glockenspiel really is a pretty complicated signal, as a graduate student and I found out when we were preparing the original glockenspiel demo.
Speaking of complicated signals, one of my favorites is to look at speech.
I set this up so that what's coming into my microphone is indeed what you're going to see on the two screens.
The telephone company thinks of speech, basically in terms of bandwidth, that it extends from about 300 Hertz to 3,300 Hertz.
But, as we'll see in the spectrum analyzer, there's a lot of leakage outside of that.
The telephone company just filters out everything outside of that and things of that as a speech signal.
So it doesn't have the high fidelity that you might have on high fire equipment.
As we look at the scope, again, the time wave form is extremely complicated, seems to have some periodicities in it, although they're short-lived, and then it goes on to some other periodic chunk.
And over in the frequency domain, you can see as we extend from 0 to 10 kilohertz, that as I speak, there are trenchants that have spectral content in them, covering that entire band.
I can try some simpler signals.
A whistle is almost a sinusoid, but as you'll see isn't terribly sinusoidal.
NOISEEVENT That's the best I can do.
I can sing, a little bit embarrassedly, a B,
NOISEEVENT Boo, and things like that.
They don't ask me to, Sandy.
And then different vowel sounds have a lot of energy in, like the letter A. (SUNG) A. Whereas some of the others are very impulsive, like--
NOISEEVENT Or puh and tuh.
And it's a little hard to grab them at the right time.
But what's fascinating is just to stare at equipment like this and try different speech sounds, and you begin to get sort of a sense of the complicated nature of them.
OK, well that's a look at some spectra signals.
And now what we'd like to focus on is the modulator and talk a little bit about modulation and demodulation and demonstrate it.
And let's begin that by first taking a look at a block diagram of the modulator system.
Well, as we've discussed in a previous lecture, amplitude modulation basically consists of multiplying the modulating signal by an appropriate carrier, illustrated here, as we've seen previously, for the case of a sinusoidal carrier.
And then, specifically for sinusoidal amplitude modulation, we may or may not inject some carrier signal--
A times the carrier.
Or equivalently, if we look at the modulated output, the injection of the carrier is equivalent, mathematically, to simply adding a DC offset or a constant to the modulating signal.
And, as you recall when we talked about this, the idea of injecting a carrier or not is related to the issue of whether or not we want to do synchronous or asynchronous demodulation.
The asynchronous demodulation corresponding to the simple use of an envelope detector.
And to remind you of the wave forms that are involved, again, I show two that we saw previously.
And for the case, this is for one value of the amount of carrier that's injected.
And this is for an amount of carrier injected that's less.
And this, in fact, corresponds to what we refer to as 50% modulation, and this is the case of 100% modulation.
Well, the modulating system that we're using in this demonstration is basically of the form that we're indicating here.
And a simple block diagram for it is more or less identical to what we just saw.
Specifically, the modulating signal is multiplied by the carrier.
And there also is the capability of injecting some additional carrier, meaning adding it to the output of this product.
And so the modulated output can have a variable percent modulation-- the percent modulation being changed, depending on how we set this variable gain.
Now, in addition to sinusoidal modulation--
in fact, for the particular system that we're using, we have somewhat more flexibility.
We can use, in addition to a sinusoidal carrier at this point, we can alternatively choose a square wave carrier or a triangular carrier.
And, as we'll indicate in a moment when we illustrate this, there are some specific advantages to using, for example, a square wave carrier.
So this is a somewhat simplified version of--
or rather block diagram of the modulating system that we're demonstrating.
The external modulating input here, a choice of carrier with also the capability for injecting some additional carrier into the output.
OK, now let's go back to the equipment and take a look at this.
Well, Sandy, maybe to begin, you can just point out what some of the controls are on the modulator box.
OK, there are some interesting points to look at here.
This is an input for an external signal.
We'll be taking a signal right out of the signal generator and putting it in here.
And that will be the signal that will be modulated, according to the carrier.
The carrier is generated internally in this device.
And there are several ways of controlling it.
One is the amount of carrier injection.
One is the wave form of the carrier itself-- and this is typically sinusoidal in the broadcast industry, but others are interesting to look at as well.
The output that will be displaying on both the spectrum analyzer and scope comes out here.
And then farther over to the left, there are some knobs for changing the frequency of the carrier signal.
OK, let me also just point out again for emphasis that changing the carrier level, as we've talked about, is mathematically equivalent to changing the DC level of the modulating signal.
And that's also what affects the percent modulation as we had just discussed.
Now what we have set up is a sinusoidal modulating signal and a sinusoidal little carrier.
And, as usual, we have the time wave form displayed here.
And so this is the modulated signal.
And then on the spectrum analyzer, we have the spectral display.
And this is the carrier signal.
That's the carrier frequency.
And these side bands then correspond to the side bands associated with the modulating signal.
So this is the spectrum of the total modulated output, right?
Right.
It's interesting to vary some of these parameters.
You can see the sinusoidal modulating shape at this point.
Let me switch that to a triangular shape.
And, again, it's a little hard to sink in both the modulating signal and the carrier, so you see the carrier kind of wondering by, but there it is.
Another thing that can be varied is the amplitude of the modulating signal.
I'll make it smaller-- you'll see the side bands go away-- until finally we have a pure sinusoidal carrier-- as I bring them back in.
Again, this is triangular.
There would be harmonics there that maybe they're a little hard to see in the spectrum analyzer.
I'll go back first to a square wave modulating signal, which is, again, you can see the square wave on top and one on the bottom.
And so all of this, then, represents side bands.
Is that--
That's right.
Those are the side bands due to the square wave modulation.
Going back to the simplest, the sinusoidal carrier.
Another thing we can do is to vary the frequency of the sinusoidal signal that's modulating it.
I'll make it higher.
And you'll notice that the side bands wander away from the carrier signal in the spectrum.
The spectrum, the carrier signal of that, doesn't have a frequency that's changing as I do this.
It's just the width of the band, due to the modulating signal.
So this is the carrier.
And these are the side bands.
That's right.
Now as we go out very far, those side bands get gobbled up by the carrier itself.
I'll come back to a nice reasonable point there.
OK, now, we can change also the carrier frequency and, as Sandy's indicated, various parameters.
Now let me just point out, since I didn't previously,
that the frequency scale that we're looking at here is a frequency scale out to 20 kilohertz.
And we had talked about-- or Sandy had indicated-- that we can change the carrier signal shape.
And let me change the carrier signal from a sine wave to a square wave.
Now you haven't seen any change on this particular display , but let me change the frequency scale.
And what you see now is the modulating signal showing up around harmonics associated with the carrier.
And those harmonics will go away when we go back to a sinusoidal carrier.
This is actually called a ring modulator.
It's very simple to multiply a signal by a square wave.
It's just a chopping process.
And so a square wave carrier signal is very convenient to generate.
And then you simply filter out the higher order harmonics.
Right, I'll go back to the scale that we had before.
And Sandy had also commented that we can change the carrier level.
And let's do that on the modulator box.
We could do that either by changing a DC offset on the modulating signal or by changing the amount of carrier that's injected.
And as we decrease the amount of carrier, in fact, going down to no carrier at all or almost no carrier.
That's suppressed carrier.
We have only the two side bands and the signal is now highly over-modulated.
We bring the carrier back up and when we do that, then we are reducing the percent modulation and simultaneously obviously related to that is changing the carrier level.
Now I had indicated in previous lectures that reducing the carrier is efficient in terms of power transmission, but requires a synchronous demodulator, whereas if there's carrier injected, as we have here, so that we're not over-modulating-- the percent modulation is less than 100-- then because of the shape of the time wave form, as you can see here, you can do the demodulation with a more or less simple envelope detector.
And in fact, a envelope detector of the type that we've talked about before is exactly what is used in AM radios, because of the fact that it's so inexpensive.
And so here the power transmitted required is higher, but the demodulator, based on using an envelope detector, is considerably simpler.
Well, speaking of AM radios, what we'd like to now demonstrate is modulation and demodulation with an AM radio, which Sandy happens to have here.
And let's see, I guess, what you're going to do, Sandy, is take it apart for us, right?
Right.
I've taken off the back.
And I'm just now going to slip out the guts of it all, and try and set it up so you can see it conveniently, such as right there.
Now, this is your daughter's radio that you promised to give back.?
I would say was my daughter's radio.
And now let's start attaching clip leads, because what we're going to want to do is to be able to see the signals coming out of the radio.
We want to see both the audio and then the modulated signal and try and see them on the oscilloscope.
So I'll start setting that up now.
OK.
Let me--
while you're doing that-- also just comment that the kind of AM radio that we're looking at here is called a superheterodyne receiver.
And the way that it does the demodulation is not exactly the way we've talked about it in the lectures.
It's very close.
The idea is this the RF signal, the radio frequency signal, first gets modulated down to what's called an IF stage-- the intermediate frequency stage.
And then it's that signal that goes to an envelope detector of the type we've talked about to generate the demodulated signal.
So, I guess what Sandy has some probes on are the RF input and a ground--
That's right.
The red lead here is a ground.
It's common for both the signals we'll be looking at.
This is at the audio.
We're actually looking at the signal going directly into the speaker across these two leads.
And then this probe down here, which I found by hunting and pecking, is the IF signal, the intermediate frequency signal, with a station on it that I'll now try and get something that sounds reasonable.
NOISEEVENT OK.
A little sports.
Never quite know what you're going to get when you turn this on.
That's right, you don't.
Make some adjustments there.
And what you can see right here is the audio signal.
I'll give it a little bit more game.
And this is the actual intermediate frequency, modulated by the audio.
And there's a correspondence between them that goes by awfully fast, but I think it's pretty simple to see.
So basically the top trace is the envelope of the bottom right.
That's right.
This is what comes out of the envelope detector.
And this is the signal into the envelope detector, in this particular radio.
What I'd like to do for a last experiment is what we've been doing at this point--
I'll turn this off for a moment--
is we're taking something that's coming in over the airwaves, as they used to say, and we're simply viewing it.
What I'd like to do now is generate our own radio frequency signal, tune it up so this radio is going to hear it-- whatever that means-- and display it here.
What we're going to do then is use the audio oscillator, along with the amplitude modulator.
And we're going to take from the output now the lead here.
And this will act as the antenna.
This device was not designed to put out a lot of power.
But fortunately we don't have a lot of distance to go over.
What I'm going to do is turn myself into an antenna--
I'm basically 150 pounds of salt water--
and when I touch this, the signal coming in here, which is radiating a little from the wire, is going to suddenly radiate from my whole body and hopefully will be enough to be picked up.
So what I'll do is I'm going to turn this on-- excuse the noise.
Now I've set this up so it's tuned to a particular radio station, and I hope I can find it now.
It takes a little fussing.
That's not it.
That sounds like it now.
OK.
So this gets billed as Sandy Hill, human antenna.
That's right.
What you're looking at here is I've got the antenna at this point, and what we're going to do is you're seeing the audio signal.
This is coming from the signal generator up here.
And this is the intermediate frequency signal again.
You can hear it's much stronger when my body becomes the antenna.
And I'll leave it on for a second.
And I can switch to square wave-- doesn't look much like a square wave.
Various things.
There's significant distortion, because of all the transformations that the signal is going through.
But it does indeed work, and we're picking up a frequency that the radio is tuned to.
And that's it.
So Sandy has now been modulated and demodulated.
That's right.
Well, hopefully, all the things that we've gone through in the process of this tape and this set of demonstrations gives you a feel for at least some of the things that we've talked about so far in the course.
And Sandy, I'd really like to thank you for joining us, for sharing your insights with us, as well as sharing your equipment with us.
Thanks a lot.
It was a treat.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu
In the last lecture, we began the discussion of modulation.
And in particular, what we focused on, was the continuous time case.
And in talking about continuous time modulation, we've covered a number of topics.
We talked about the properties and analysis of modulation when we had a complex exponential carrier signal.
We talked about the properties and analysis in the case of a sinusoidal carrier.
And in that context and related to the application associated with communications, we talked about synchronous modulation, asynchronous modulation and also the notion of single side band modulation.
In the lecture today, there are two issues that I'd like to address, broad topics.
One is a parallel discussion, particularly, as associated with complex exponential and sinusoidal modulation for discrete time signals.
And the second is the introduction and analysis of another kind of carriers, specifically a pulse kind of carrier in continuous time leading to the notions of pulse amplitude modulation and, eventually, a very powerful theorem and result called the sampling theorem.
Well, let me begin the lecture, though, focusing on the discrete time modulation to essentially draw your attention to the fact that the analysis in discrete time very much parallels the analysis in continuous time.
Well, let's consider, in the discrete time case, just as we had in continuous time, a signal modulating a carrier signal and the resulting modulated signal is y of n.
And it was in continuous time the modulation property associated with the Fourier Transform that provided the basis for the analysis.
And exactly the same thing is true in the discrete time case.
In particular, what we have in discrete time, is the modulation property as it relates to the Fourier Transform, which tells us that the Fourier Transform of the modulated signal is the convolution of the Fourier Transform of the carrier and the Fourier Transform of the modulated signal.
And the only real difference at issue here is that, in the discrete time case, what we're talking about is a periodic convolution because the specter, of course is periodic.
Whereas, in the continuous time case, it was an aperiodic convolution.
So let's parallel the discussion, and in particular,
what we'll focus on is, first, a complex exponential carrier and second a sinusoidal carrier.
And we'll see how this parallels our discussion in continuous time, and we'll make fairly brief reference as we introduce the pulse carrier for continuous time.
We'll make very brief reference to the pulse carrier for discrete time indicating that, again, the analysis and discrete time and continuous time is very parallel.
So let's, first, consider complex exponential and sinusoidal carriers for the discrete time case, emphasizing the very strong parallel and similarity between discrete time and continuous time.
Well we have, once, again the modulation property.
And the modulation property tells us that the spectrum of the modulated signal is the periodic convolution of the two spectra.
And let's consider, for example, an input, or modulating spectrum, as I've indicated here.
And since we want to consider, first of all, a complex exponential carrier, we'll consider the case of c of n equal to e to the j omega sub cn.
And let me stress, by the way, as I did in the continuous time case, that I'll tend to suppress the phase angle which, of course, can be associated with the carrier also.
All right, so we have, then, the spectrum of the modulated signal.
The spectra, the carrier signal, if this is the carrier, then it's spectrum is an impulse train, and that impulse train, I've indicated here.
And let me stress, also, that in the discrete time case, of course, these spectra and all of the spectra involved, are periodic with a period of 2 pi.
So this then is the spectrum of the carrier signal.
This is the spectrum of the input signal.
The periodic convolution of these two is the spectrum the modulated signal.
And the result is, then, this spectrum shifted to a center frequency, which is the carrier frequency omega sub c.
So the result of modulation with a complex exponential is a straightforward shift of the spectrum so that what occurred around zero frequency now occurs around the frequency omega sub c.
Now, in the continuous time case, we demodulated, when we had a complex exponential carrier, we demodulated by, essentially, just shifting the spectrum back.
And in fact, in the discrete time case, were able to do exactly the same thing.
So if we were to replace c of n which is either j omega sub cn by c of n equals e to the minus j omega sub cn, the resulting spectra would be an impulse train, as I indicate here, and the result of multiply y of n by that new carrier, in the frequency domain as a convolution of these two, and it's relatively straightforward to verify that if you convolve these with a periodic convolution, then that will get us back to the original spectrum that we started with.
So what's happened in the discrete time case, with the complex exponential, is exactly the same as in continuous time.
Namely, we modulate that corresponds to shifting the spectrum.
We demodulate by multiplying by the complex conjugate of the original modulated carrier and that shifts the spectrum back to where it was originally.
OK.
Now let's consider the case of a sinusoidal carrier in discrete time.
And again, things very much parallel what we saw in continuous time.
And again, as we look at the spectra, I will choose a phase angle of zero, mainly for notational and analytical convenience.
So in this case, now, rather than a carrier signal, which is a single complex exponential, it's now a sinusoidal carrier and the sinusoidal carrier is the sum of two complex exponential.
And so if we consider a modulated spectrum, that is the spectrum of x of n, something of the type that I indicate here, and the spectrum of the carrier, now, since the carrier is sinusoidal rather than a complex exponential consists of two impulses, one at plus omega sub c and one at minus omega sub c, convolving this spectrum with this spectrum gives us a replication of x of omega around plus and minus omega sub c.
And incidentally, with an amplitude change of a half.
So again, things have worked as they did in continuous time.
In continuous time or in discrete time, modulating with a sinusoidal carrier would correspond to a replication of the spectrum around, plus the carrier frequency and a replication of the spectrum around minus the carrier frequency, in both cases, as long as the carrier frequency is large enough compared with the bandwidth of the signal so that those two replication don't overlap, then it's reasonable to suppose that we should be able to recover the original signal.
Well, in fact, to demodulate in the discrete time case, we would again follow very much the strategy that we did in continuous time.
In particular, let's consider demodulating by taking the modulated signal and, again, putting that through a modulator, again, with the carrier which is cosine omega sub cn.
If we do that, we have a demodulator or what will turn out to be part of a demodulator, as I indicate here, the spectrum of the input signal is, as I had just developed, a replication of the original spectrum around plus and minus omega sub c with an amplitude of a half.
When this is, again, convolved with the spectrum of the carrier, then we get a replication of the original spectrum, first around zero frequency, as I indicate here, and then around twice the carrier frequency and minus twice the carrier frequency.
And as long as the carrier frequency is large enough compared with the width of the original signal, then, as you can see, by extracting this part of the spectrum with a low pass filter, we can, in principle, recover the spectrum associated with the original signal.
And again, just as in continuous time, because this amplitude is a half, we would want to choose, for scaling purposes, a low pass filter amplitude which is 2 to compensate for this factor of a half.
So once again things work out basically the same way as they had in continuous time.
We have sinusoidal modulation which consists of using a sinusoidal carrier.
And we have the demodulator which consists of taking a modulated signal, multiplying by the carrier, and then processing that with a low pass filter to extract the portion of the spectrum, which is around zero frequency, as I indicate in the spectrum below and the result, then, that this low pass filter having a gain of 2 is that we've recovered the original spectrum, x of omega, which is the spectrum that we started with.
Now several other things to stress.
This is a fairly quick tour through sinusoidal modulation for discrete time.
There are very similar issues that arise in the discrete time case in terms of having phase synchronization and frequency synchronization between the modulator and demodulator.
And we had discussed that in a fair amount of detail for the continuous time case.
In some sense, in practical terms, that becomes much more of an issue in continuous time than it does in discrete time, in part, because synchronization between a modulator and demodulator is often much harder in a continuous time system, which is essentially an analog system as compared with a digital system.
Another very important reason and it's important to stress this at the outset is that, whereas the theory involving the use of complex exponential and sinusoidal modulation parallels very strongly in the continuous time and the discrete time case.
In practical terms, it has much more significance in continuous time than it does in discrete time.
That is, the notion called sinusidal modulation, in the context of communication systems, is extremely important for continuous time systems, and less so in discrete time systems.
Now as a preview of a point to be raised later on, I should modify that slightly with the statement that one very important place in which sinusoidal modulation in a discrete time context arises, is in a class of systems called transmultiplexers or transmodulation systems.
And this surface is basically because so many communication systems are now becoming digital and, specifically, discrete time, although the actual transmission is continuous time, the signal processing manipulation and switching is discrete time.
And so, in fact, it turns out to be very important and useful to take a discrete time representation of the analog signals or continuous time signals and, in a discrete time, or digital representation, to convert them from one modulation scheme or one multiplexing scheme to another.
And although I said a lot there that really requires much more detail to develop in any sense at all, you should get the notion that discrete time modulation systems become very important, in part, because of implementational issues.
OK, now, there is another application that we have discussed for both continuous time and actually, previously, for discrete time, amplitude modulation with sinusoidal complex exponential carriers.
And let me just remind you of that, because, in fact, it becomes a very important one in the case of discrete time systems.
And that is the notion of using modulation together with fixed filtering to implement a filter, which either has a variable cut off or converts, let's say, a low pass filter to a high pass filter.
We had originally talked about this when we introduce the modulation property in the context of converting a low pass filter to a high pass filter.
And the notion was that, if we modulate the signal with a carrier which is minus 1 to the n, and that's just simply a complex exponential or sinusoidal carrier with a carrier frequency of pi, then that, in effect, interchanges the low frequencies and the high frequencies.
And if, after modulation, that is processed with a low pass filter, and then the result is demodulated, using exactly the same carrier, namely a carrier which is minus 1 to the n, then the effect of that is equivalent to high pass filtering on the original signal.
And a generalization of that would involve, instead of this specific choice of minus 1 to the n, would involve a choice, in general, of e to the j omega sub cn, that is a more general carrier frequency, and a demodulator which is e to the minus j omega sub cn.
And as I've represented it here, and as we had talked about it when we talked about the modulation property for discrete time signals, we had specifically chosen the conversion of a low pass to a high pass filter.
Well, let me continue the review of that just by reminding you of the details of what happens with the spectra, and, specifically, the notion, if we take this particular case of omega sub c is equal to pi, or equivalently, a carrier signal which is minus 1 to the n, then if we have the original spectra and the spectrum of the carrier signal, the spectrum of the carrier signal convolved with this spectrum will then, in effect, shift this by pi.
And so, after modulating, the result that we have is a shift of that spectrum so that what happened in low frequencies now happens at high frequencies, namely around pi, and what happened at high frequencies now happens at low frequencies.
Well, if that's processed now, with a low pass filter, and this dashed line indicates the low pass filter, then the result that we get is shown here, where we've extracted the low frequency portion of the modulated signal.
And now when we modulate or demodulate back, then this spectrum is shifted back to where it belongs.
Namely, it's shifted back to be centered around minus pi and around plus pi.
So if we just compare this spectrum with the original spectrum at the top, what we can see is that, in effect, what we've done is to extract a portion of the spectrum equivalent to processing with a high pass filter.
And, again, this is very similar to what we did in continuous time and all of the analytical processes and convolution involved are very much the same.
Really, the biggest difference between continuous time discrete time has to do, not so much with the details of the analysis, but perhaps has more to do with issues of practical applications.
OK, well, so what we've done, so far, for continuous time and discrete time, is to talk about modulation, amplitude modulation with complex exponential and sinusoidal carriers.
We saw that the analysis is very similar, although applications are slightly different.
And now what I'd like to turn to is a different choice of carrier signal.
And the carrier signal, in this particular case, is a pulse train rather than a sinusoidal signal.
Now the idea is the following.
In general, of course, the modulator consists of all of multiplying x of t by whatever the carrier signal is.
And previously, we've talked about a carrier signal which is sinusoidal signal.
The carrier signal that we want to consider now is a carrier signal which, in fact, is a pulse train.
And so, in fact, what we want to do is multiply the input signal by a pulse train and, in effect, then, the modulated signal consists of the original signal, simply with time slices pulled out of it, as I've indicated in the bottom curve.
So what we have now is a modulated signal that is a chopped or sliced version of the original waveform and that is what's referred to as pulse amplitude modulation.
Now it seems like it's kind of a crazy idea.
The idea is to chop out slices of the wave form and hope that you could put things back together again.
And the amazing thing about it, as we'll see, is that, in fact, under fairly broad general and applicable conditions, you really can put the waveform back together again if you just have these time slices.
Not only that, but that basic notion, as we'll see, is independent, in fact, of what the width of those time slices are.
In fact the width can go to zero.
And, in fact, we're going to make it go to zero, and really only dependent on what the frequency of the pulse train is.
So let's explore that in some detail.
And what we want to look at is the analysis, but let me,
first, just comment, very briefly, that all of the analysis we go through, as has been true in the case of sinusoidal modulation, all of the analysis then we go through holds just as well with, essentially minor analytical modifications, to discrete time pulse amplitude modulation as it does to continuous time pulse amplitude modulation.
And so we'll really only go through this in terms of tracking the wave forms and spectra for the continuous time case.
But bear in mind that the results are basically similar for discrete time.
OK, well, let's see how so we get the basic result that we want to get.
What we have is modulated signal which is a pulse train,
basically a square wave, and as we've seen in previous lectures, the spectra or Fourier transform associated with that is an impulse train.
And the envelope of that impulse train is on the form of a sine x over x function.
The Fourier transform is impulses.
And the spacing of the impulses is associated with the fundamental frequency of the pulse train and that's omega sub p.
So omega sub p is pi divided by the period of the pulse train.
And the amplitude and shape of this envelope is dictated by the parameter delta, which has to do with how wide the pulses are.
OK, so we have a time function.
It's multiplied by this pulse train.
Now we're talking continuous time.
So, in the frequency domain, we have the Fourier transform of the time function convolved with this Fourier transform for the pulse train.
And let's see what that looks like.
If we were to consider, let's say, a Fourier transform,
which I have chosen as more or less a general one, then in fact, when we convert all of this with this impulse train, what we end up with is a replication of this spectrum at the places in the frequency domain where the individual impulses occurred.
So we can see that this spectrum is replicated at each of these locations.
And as long as the frequency of the pulse train is large enough, compared with the maximum frequency in the original signal, x of t, so that there's no overlap between these triangles, then what you can see, in fact, somewhat amazingly is that, simply by low pass filtering the result, we can get back, except for amplitude factor, we can get back to the original signal.
Now it's amazing.
It really is amazing that all that this depends on is the original signal being band limited and the frequency of the pulse train being high enough so that when you replicate the spectrum the frequency domain, there's no overlap between these individual replications.
And we'll have address that a little more in a few minutes.
But let me, first of all, point out that this has a whole variety of very important implications.
One is, in the context of communications, it leads to another very important multiplexing scheme for communications.
We had talked last time about frequency division multiplexing, where individual signals were put into individual frequencies slots by choosing different carrier frequencies for a sinusoidal modulating signal.
What this suggests is that what we can put different signals into, non-overlapping time slots and, in fact, be able to recover the original signals back again.
So in particular, suppose that I had a signal which I modulated with a pulse train and I chose another signal, modulated with another pulse train, where the time slot was different, and I continued this process.
And after I'd done this with some number of channels, simply added all those together as I indicate here.
Then as long as I knew what time slots to associate with what signal, I could get the original modulated signals back again.
And then as long as the frequency of the impulse train was such that I was able to do this reconstruction by simply low pass filtering, then I would be able to demodulate.
So it's a very different very important modulation scheme called time division multiplexing in contrast to frequency division multiplexing as we had talked about last time.
I had made reference earlier to the concept of trans-multiplexing.
And in fact, what happens in many communication systems is that the signals are represented, in fact, in discrete time.
The analog and continuous time signals are represented in discrete time.
And very often the conversion from frequency division multiplexing to time division multiplexing and back is done, in fact, in the discrete time domain.
OK, so what we have then, is the notion that we can multiply a time function by a pulse train, as I indicate here.
And from the output I can, if the frequency of this pulse train is high enough in relation to this bandwidth, from the output, which consists of time slices, from those time slices I can recover the original signal.
Stressing again the reason it relates to the spectra, and the reason is that the original spectra is simply replicated at multiples of the fundamental frequency of the pulse train.
Now there's a very important thing to observe here, which is that the ability to do the reconstruction is associated with the notion of whether we can extract that central triangle.
I happened to choose a triangular shape but obviously I could be talking about any shape, as long as it's band limited, the ability to extract that.
And notice that, in this modulated output spectrum, the ability to recover this is totally independent called what the value of delta is.
In other words, if we look back at the modulator, then, in fact, we can make delta, the width of these pulses, arbitrarily small.
And, in theory, that doesn't affect our ability to do the reconstruction.
Now in practical terms it might.
Looking back once more at the spectrum of the output, notice that this amplitude is proportional to delta.
And what that suggests is that, as we make delta smaller and smaller, which we might, in fact, want to do, if you want to time division multiplex lots of channels, in principle, in theory, you could make it an infinite number of channels just by making that infinitesimally small.
The smaller it is, in some sense, the less energy there is.
And again, in practical terms, this one of those things if you push down here pops up there, namely, you eventually run into issues such as noise problems.
So, more typically what's done is to, in fact, eliminate this scale factor of delta.
And the way that that's done is very simply.
It's done by choosing the width of the pulses, and the height of the pulses, in such a way that the area is constant, even as we make delta get arbitrarily small.
So we can just modify our argument so that what we're referring to is a modulated pulse train, which is a pulse train with pulses of width delta and height, 1 over delta.
In that case, as delta gets arbitrarily small, then, in fact, what these rectangles become are impulses, in which case, what we're talking about is a carrier signal which, in fact, is an impulse train.
And the resulting modulated signal is an impulse train for which the amplitudes of the impulses are proportional to the original input waveform at the times at which these impulses occur.
OK well, let's look at the analysis of that.
And so now, what we're talking about, is a spectrum that consists of the result of the spectrum we talked about before with the sine x over x envelope, except that, now, as delta goes to zero that becomes flat.
In other words, the modulated signal is an impulse train.
And so as we look at the spectrum of the modulated signal, that is, then, an impulse train in the frequency domain.
The height is proportional to the frequency of the impulse train and omega sub s now denotes the frequency of the impulse train.
And the resulting output of the modulator has a spectrum which is this original spectrum, again, replicated around each of these impulses, in other words, replicated in multiples of the sampling frequency Now this is very much identical to the more general case.
We have this replication of the spectra.
And as long as the frequency of the impulse train is large enough, compared with the bandwidth of the signal so that these triangles don't overlap, I can extract this portion of the spectrum by low pass filtering, in fact, would then give us back the original signal.
Now if, instead, this frequency omega sub m is greater than omega sub s minus omega sub m, we would have a spectrum that looked something more like this.
And what's happened, in this case, is that, because we have an overlap here, we've destroyed the ability to recover the original signal from the impulse train.
And that would be true, also in a more general case, of pulse amplitude modulation with pulses of non-zero width.
This effect by the way, is one that we'll be exploring in considerably more detail in the next lecture.
And it's a phenomenon or distortion refer to as aliasing which, in fact, is an important and interesting topic.
But going back to the case in which we've chosen the frequency of the impulse train high enough, then we would recover the original signal by processing it through a low pass filter.
And in that case, what this says is, that if we have a signal, and we modulate it with an impulse train, if we then process that impulse train through an idea low pass filter, given the right conditions on the frequency of impulse train and the bandwidth of the signal, we can recover the original signal back again.
Now let me stress, just going back to the picture in which we had done this modulation, that this process, where the modulation, where the carrier signal involves an impulse train, is often referred to as sampling.
And what that means, specifically, is that, if we notice, this resulting impulse train is, in fact, a sequence of samples of the original continuous time signal.
In other words, what we've done, in effect, is taken instantaneous sample of this wave form.
And the implication is that, if we do that at a rapid enough rate in relation to the bandwidth of the signal, then we can, in fact, recover the original signal back again.
And, finally to remind you of the argument once more, we have an original signal and we have its spectrum.
When we've sampled it, and this is now the sampled signal, it's an impulse train whose instantaneous values are samples of the original waveform, the spectrum of that is the original one replicated.
And when that is processed, through a low pass filter, to extract this part of the spectrum, then, after the low pass filter, we can recover the original signal back again.
OK well, in fact, although if you follow through the spectra and the wave forms, this all seems fairly straightforward and, perhaps or perhaps not, obvious, it's really worth reflecting on how amazing the result really is.
We began this discussion by talking about modulation.
And in fact modulation and sinusoidal of modulation is important in its own right.
We ended the discussion by talking about first pulse amplitude modulation, and then showing how, under the right set of conditions, you can, in fact, take a wave form and sample it with a set of instantaneous samples.
And that set of instantaneous samples, in fact, are sufficient to totally represent and reconstruct the signal.
What in fact, the formal statement that is, is refer to as the sampling theorem, a very powerful theorem that says, if we're given equally spaced samples of a time function, and if that time function is band limited, and if the bandwidth and if the sampling frequency is chosen in the right way, in relation to the bandwidth, then, in fact, the original time function is uniquely recoverable with a low pass filter.
Now the sampling theorem is, I would say, a watershed or cornerstone of a lot of the discussion that we've been having for a whole variety of reasons.
It, first of all, drops out almost as a straightforward obvious statement.
But more importantly what it says is, if I have a continuous time signal which satisfies the right set of conditions, I could represent it by what it does at sampling instance or, equivalently, at discrete instance of time.
Now what that leads to is a whole host of things.
One of which is this statement that says, if we have a continuous time signal, I could in fact, represent it as a discrete time signal.
And I could even think of processing a continuous time signal using discrete time concepts.
And when I'm all done converting back, through the power of the sampling theorem, converting back to a continuous time signal.
So the sampling theorem provides us with a very major important bridge between continuous time and discrete time implementations and ideas.
In the next several lectures, we will be exploring some of this in considerable detail.
First, to focus in more, next time, on some of the specific issues and distortions associated with sampling.
And following that, a discussion of what is referred to discrete time processing of continuous time signals.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
We concluded the last lecture with the statement of the sampling theorem.
And just as a quick reminder, the sampling theorem said that if we have a continuous-time signal and we have equally spaced samples of that signal, sampled at a sampling period, which I indicate is capital T and if x of t is band-limited-- in other words, the Fourier transform is zero outside some band where omega sub m is the highest frequency-- then under the condition that the sampling frequency, which is 2 pi divided by the period, is greater than twice the highest frequency.
The original signal is uniquely recoverable from the set of samples.
And the sampling theorem essentially was derived by observing or using the notion that sampling could be done by multiplication or modulation with an impulse train.
And the sampling theorem developed by examining the consequence of the modulation property in the context of the Fourier transform.
In particular, if we have our signal x of t and if multiplied by an impulse train to give us a sampled signal-- another impulse train whose values or areas are samples of the original time function, as I indicate here-- then in fact, if we examine this equation or equivalently, bringing x of t inside this sum, if we examine either of these equations in the frequency domain, the Fourier transform of x of p of t is the convolution of the Fourier transform of the original signal and the Fourier transform of the impulse train.
Now the impulse train is a periodic signal.
It's Fourier transform.
Therefore, as we talked about with Fourier transforms is itself an impulse train.
And when we do this convolution, then using the fact that the Fourier transform, the impulse train is an impulse train.
The result of this convolution, then tells us that the Fourier transform of the sample signal or the impulse train, which represents the samples, is a sum of frequency-shifted replications of the Fourier transform of the original signal.
So mathematically, that's the relationship.
It essentially says that after sampling or modulation with an impulse train, the resulting spectrum is the original spectrum added to itself, shifted by integer multiples of the sampling frequency.
Well, let's see that as we did last time in terms of pictures.
And again, to remind you of the basic picture involved, if we have an original signal with a spectrum as I indicated here-- where it's band-limited with the highest frequency omega sub m-- and if the time function is sampled so that in the frequency domain we convolve this spectrum with the spectrum shown below, which is the spectrum of the impulse train, the convolution of these two is then the Fourier transform or spectrum of the sample time function.
And so that's what we end up with here.
And then as you recall, to recover the original time function from this-- as long as these individual triangles don't overlap--to recover it just simply involves passing the impulse train through a low-pass filter, in effect extracting just one of these replications of the original spectrum.
So the overall system then for doing the sampling and then the reconstruction of the original signal from the samples, consists of multiplying the original time function by an impulse train.
And that gives us then the sampled signal.
The Fourier transform I show here of the original signal and after modulation with the impulse train, the resulting spectrum that we have is that replicated around integer multiples of the sampling frequency.
And then finally, to recover the original signal or to generate a reconstructed signal, we then multiply this in the frequency domain by the frequency response of an ideal low-pass filter.
And what that accomplishes for us then is recovering the original signal.
Now in this picture, an important point that I raised last time, relates to the fact that in doing the reconstruction--well we've assumed-- is that in replicating these individual versions of the original signal, those replications don't overlap and so by passing this through a low-pass filter in fact, we can recover the original signal.
Well, what that requires is that this frequency, omega sub m, be less than this frequency.
And this frequency is omega sub s minus omega sub m.
And so what we require is that the frequency omega sub m be less than omega sub s minus omega sub m.
Or equivalently, what we require is that the sampling frequency be greater than twice the highest frequency in the original signal.
Now, if in fact that condition is violated, then we end up with a very important effect.
And that effect is referred to as aliasing.
In particular, if we look back at our original example--we are here-- we were able to recover our original spectrum by low-pass filtering.
If in fact the sampling frequency is not high enough to avoid aliasing, then what happens in that case is that the individual replications of the Fourier transform of the original signal overlap and what we end up with is some distortion.
As you can see, if we try to pass this through a low-pass filter to recover the original signal, in fact we won't recover the original signal since these individual replications have overlapped.
And this is the case where omega sub s minus omega sub m is less than omega sub s.
In other words, the sampling frequency is not greater in this case than twice the highest frequency.
So what happens here then is that in effect, higher frequencies get folded down into lower frequencies.
What would come out of the low-pass filter is the reflection of some higher frequencies into lower frequencies.
As I suggested a minute ago, that effect is referred to as aliasing.
And in order to both understand that term better and to understand in fact the effect better, it's useful to examine this a little more closely for the specific example of a sinusoidal signal.
So let's concentrate on that.
And what we want to look at is the effect of aliasing when our input signal is a sinusoidal signal.
Now to do that, what I want to show shortly is a computer-generated movie that we've made.
And let's first walk through a few frames of it to give you--
first of all, to set up our notation and to suggest what it is that we're trying to demonstrate.
Well, what we have is an input signal-- is a sinusoidal signal.
And the spectrum or Fourier transform of that is an impulse in the frequency domain at the frequency of the sinusoid.
We then have samples of that and when we sample that-- and for this particular example, it's sampled at 10 kilohertz-- this spectrum is then replicated at multiples of the sampling frequency.
And I haven't shown negative frequencies here, but the contribution due to the negative frequency is at 10 kilohertz minus the input sinusoid.
We then carry out a reconstruction with an ideal low-pass filter.
And the ideal low-pass filter is set at half the sampling frequency or 5 kilohertz.
So what we have then is the input signal x of t and the impulse train x of p of t.
And then the reconstructed signal is the output from the low-pass filter which I denote as x of r of t.
Now as the input frequency x of t increases, this impulse moves up in frequency, but this impulse moves down in frequency.
And so let's just look at a few frames as the input frequency increases.
So we have here a case where the input frequency has moved up close to 5 kilohertz.
As we continue further, these two impulses will cross and what we'll end up with, as I indicated, is aliasing.
So here now is a case where we have aliasing.
The replication of the negative frequency has crossed into the passband of the filter and the reconstructed sinusoid will now be the frequency associated with this impulse rather than the frequency associated with the original sinusoid.
And to dramatize that even further, here is the example where now the input frequency has moved up close to 10 kilohertz, but what comes out of the low-pass filter is a much lower frequency.
And in fact, you can see that here is the reconstructed sinusoid, whereas here we have the input sinusoid.
Well, now what I'm going to want to do is demonstrate this as I indicated with a computer-generated movie.
And what we'll see is the effect of reconstructing from the samples using a low-pass filter for an input which changes in frequency and with a sampling rate of 10 kilohertz.
And what we'll see in the first part of this movie is the input x of t and the reconstructed signal x of r of t without explicitly showing the samples.
And then, at a later point, we'll also show this and indicate that in fact the samples of those two are equal, even though they themselves are not.
So at the top, we'll have the input sinusoid without showing the samples.
And its Fourier transform is an impulse in the frequency domain as we've indicated.
And if we sample it, that impulse then gets replicated.
And so its samples, in particular, will have a Fourier transform not only with an impulse at the input sinusoidal frequency, but also at 10 kilohertz minus that frequency.
Now for the reconstruction, we passed the samples through an ideal low-pass filter.
I picked the cutoff frequency of the low-pass filter at half the sampling frequency, namely 5 kilohertz.
And here, what we see is that the output reconstructed signal in fact matches in frequency the input signal.
Now as we change the input frequency, the reconstructed sinusoid is identical until we get to an input frequency, which exceeds half the sampling frequency.
At that point we have aliasing and while the input frequency is increasing, the output frequency in fact is decreasing because that's what's inside the passband of the filter.
Now let's sweep it back.
And as the input frequency decreases, the output frequency increases until there's no aliasing and now the output reconstructed signal is equal to the input.
So we've sampled a signal and then reconstructed the signal from the samples.
And keep in mind, that given a set of samples, there are lots of continuous curves that we can thread through the set of samples.
The one that we picked, of course, is the one consistent with the assumption about the signal bandwidth.
In particular, we've reconstructed the signal whose spectrum falls within the passband of the filter.
Now what I'd like to show is the same reconstruction and input as I showed before, but now let's look at the samples and what we'll see is that when there's aliasing, even though the output-- the reconstructed signal-- is not identical to the input.
In fact it's consistent with the input samples that is sampling the reconstructed signal.
It gives a set of samples identical to the samples of the input and it's just that the interpolation in between those samples is an interpolation consistent with the assumed bandwidth of the input based on the sampling theorem.
So let's now look at that with the samples also shown along with the sinusoid.
So at the top, we have the input sinusoid together with its samples.
The bottom trace is the Fourier transform of the sampled waveform.
The middle trace is the reconstructed sinusoid together with its samples.
And notice, of course, that the samples of the input or reconstructed signal are identical.
And also the input sinusoidal frequency and the output sinusoidal frequency are identical.
And we now increase the frequency at the input.
The reconstructed sinusoid tracks the input in frequency and, of course, the samples of the two are identical.
The interpolation in between the samples is identical because of the fact that the input frequency is still less than half the sampling frequency.
And so, as long as the input is frequency is less than half the sampling frequency, not only will the samples be identical, but also the reconstructed continuous waveform will match the input waveform.
Now when we get to half the sampling frequency, we're just on the verge of aliasing.
This isn't aliasing quite yet, but any increase in the input frequency will now generate aliasing.
We now have aliasing, the output frequency is lower than the input frequency, but notice that the samples are identical.
Now the low-pass filter is interpolating in between those samples with a sinusoid that falls within the passband of the low-pass filter, which no longer matches the frequency of the input sinusoid.
But the important point is that even when we have aliasing, the samples of the reconstructed waveform are identical to the samples of the original waveform.
And notice that as the input frequency increases, in fact the interpolated output, the reconstructed output has decreased in frequency.
Now as the input frequency begins to get closer to 10 kilohertz-- in fact your eye tends to also interpolate between the samples with a frequency that is lower than the input frequency.
And that's particularly evident here.
Notice that the input samples in fact look like they would be associated with a much lower frequency sinusoid, than in fact was the sinusoid that generated them.
The lower-frequency sinusoid in fact corresponds to the reconstructed one.
Now as we sweep back down, the aliasing eventually disappears and the output sinusoid tracks the input sinusoid in frequency.
So we've seen the effect of aliasing for sinusoidal signals in terms of waveforms.
Now let's hear how it sounds.
Now what we have for this demonstration is an oscillator and a sampler.
And the output of the sampler goes into a low-pass filter.
So the input from the oscillator goes into the sampler and the output of the sampler goes into the low-pass filter.
The sampler frequency is 10 kilohertz.
And so the low-pass filter has a cutoff frequency as I indicate here, of 5 kilohertz.
And what we'll listen to is the reconstructed output as the oscillator input frequency varies.
And recall that what should happen is that when the oscillator input frequency gets past half the sampling frequency, we should hear aliasing.
So we'll start the oscillator at 2 kilohertz.
NOISEEVENT And keep in mind that what you see on the dial is the input frequency, what you hear is the output frequency.
As long as the input frequency is less than half the sampling frequency-- in other words, 5 kilohertz -- the reconstructed signal sounds identical to the input.
Now at 5 kilohertz, we're right on the verge of aliasing, and when we increase the input frequency past 5 kilohertz, the reconstructed frequency in fact will decrease.
So as we move, for example, from 5 kilohertz up to let's say, 6 kilohertz. 6 kilohertz in fact gets aliased down to, what?
It gets aliased down to 4 kilohertz.
So 6 kilohertz at the input is 4 kilohertz at the output.
Now, if we move up even further, 7 kilohertz at the input gets aliased down to 3 kilohertz at the output.
So that, then is an audio demonstration of aliasing.
So to summarize, if we sample a signal and then reconstruct from the samples using a low-pass filter, as long as the sampling frequency is greater than twice the highest frequency in the signal we reconstruct exactly.
If on the other hand, the sampling frequency is too low,
less than twice the highest frequency, then we get aliasing.
In other words, higher frequencies get folded or reflected down into lower frequencies as they come through the low-pass filter.
Now, one of the common applications of the whole concept of sampling is the use of sampling to convert a continuous-time signal into a discrete-time signal to carry out what's often referred to as discrete-time processing of continuous-time signals.
And this in fact is something that we'll be talking about in a fair amount of detail, beginning with the next lecture.
But let me indicate that for that kind of processing,
essentially what happens, is that we begin with the continuous-time signal and convert it to a discrete-time signal, carry out the discrete-time processing, and then convert back to continuous-time.
And the conversion from a continuous-time signal to a discrete-time signal in fact, is done by exploiting sampling, specifically by sampling the continuous-time signal with an impulse train and then converting the impulse train into a sequence in a matter that I'll talk about in more detail next time.
Now in doing that-- of course, as you can imagine-- it's important since we want an accurate representation of the original continuous-time signal, to choose the sampling frequency, to very carefully avoid aliasing.
And so in fact, in that context and in many other contexts, aliasing is something that we're very eager to avoid.
However, it's also important to understand that aliasing isn't all bad.
And there are some very specific contexts in which aliasing is very useful and very heavily exploited.
One example of a very useful context of aliasing is when you want to look at things that happen at frequencies that you can't look at, for one reason or another.
And sampling and aliasing is used to map those into lower frequencies.
One very common example of that is the use of the stroboscope which was invented by Doctor Harold Edgerton at MIT.
And sometime earlier, in fact we had the opportunity to visit Doctor Edgerton's laboratory at MIT and see some examples of this.
So I'd like to-- as a conclusion to this lecture-- take you on a visit to the strobe lab at MIT.
In the lecture-- in discussing aliasing-- we've stressed the fact that in most situations, it's something that we'd like to avoid.
However, right now we're at MIT, in Strobe Alley as it's called, on the way to visit the laboratory of my MIT colleague, Professor Harold Edgerton, where in fact aliasing is an everyday occurrence.
Basically, the idea is the following-- that if in fact you want to make measurements at frequencies that, for one reason or another, you can't measure, then sampling and, consequently, aliasing can be used to bring those frequencies down into a frequency range that you can measure.
Well, Professor Edgerton alias Doc Edgerton invented the stroboscope for exactly that reason.
And, kind of, the idea is the following.
The eye, essentially, is a low-pass filter and so there are things that happen at frequencies above which your eye can track.
And by sampling with light pulses, sampling in time, what in effect you're able to do is sample in such a way that higher frequencies get aliased down to lower frequencies so that, in fact, your eye can track them.
So let's take a look inside the lab and in fact see an illustration of this strobe and some of its effects.
Let me introduce you to my MIT colleague, Doc Edgerton.
Also by the way, this is a great place for kids of all ages and so my daughter, Justine, insisted on coming along to also help out.
Doc, maybe we could begin with you just saying a little bit about what the strobe is and what some of the history is?
Sure, it's a very simple application of intermittent light.
And this is a xenon lamp that flashes in a controlled rate depending on this knob which Justine's going to turn.
And we're going to look at a motor that's driving an unbalanced weight to set up some NOISEEVENT oscillations in the spring.
I'll turn on the motor.
I'll turn on the strobe.
NOISEEVENT Just get the right range.
All right, Justine, turn that now, until it stops.
See that, Justine, the frequency is that the light, which corresponds to the frequency of the motor.
And it's a little less or a little more, when you lean to go forward to backwards.
Doc, maybe we could turn this strobe off for minute.
And let me point out, by the way, the fact that when we're looking at this without the strobe on, what we're seeing essentially are frequencies that our eye can't track.
So we can't see the motor turning and we can't really see other than with a blur.
We can't see the movement of the spring.
And so I guess, your point is that when we put the strobe on, we're essentially sampling this.
And now we brought this down to a frequency that our eye is able to track.
In fact, I guess if we turn the incandescent light off,
what we'll be able to really bring out are the alias frequencies.
So now, what we're looking at in fact are the alias frequencies.
The spring, of course, is moving a lot faster than we see it, isn't that right?
Yes, it's going approximately 30 times a second.
The motor is going far from 30 times a second.
I will speed this up while I hit the next mode, where I get a figure 8 out of this thing.
You want to see that now?
Yeah, great.
NOISEEVENT NOISEEVENT
So what we'll be seeing now is essentially a second harmonic, is it?
Yes, that's the second harmonic.
Justine, you think you could make that spring dance around a little bit by changing the strobe frequency?
Yeah, you need to go around that way.
You go around this way.
Hey, that's really neat.
Let's turn the lights back on if we can.
Tomorrow NOISEEVENT it's periodic, it has to be periodic.
And what's interesting now, if we look at this in a-- let's see, can you flip this strobe off again?
Sure.
Notice, Justine, when we look at the spring now, all that we can see is a blur.
And you really can't see-- because your eye can't track it, you can't see things happening spatially in frequency.
You said, by the way, that this was originally demonstrated at the World's Fair.
This particular instrument was made the World's Fair in Chicago-- not the last one, but the one before that.
Wow.
It was a--you see it all scratched up because it's a-- the NOISEEVENT use this thing is to break the springs.
Because of the uses, you try to find the parts that fail.
I see.
You put them under stress and fatigue and--
If I run this for half an hour and so, the spring will break.
And they work on automobiles, they run them until something vibrates.
Then they find out what the part is and what frequency it is.
Well let's--by the way, I bet you run this for a lot more than half an hour in this state.
Oh, yeah, we've broken many, many springs in this thing-- and it's continuous.
We experiment, try new things on it.
Maybe we could look at a couple of other things.
How about the fan?
Maybe--
Sure, I'll plug this fan and this is a classic experiment for the strobe.
NOISEEVENT That's a good idea.
Get that thing off.
Makes too much noise.
Guess, we move that over here.
This is just an ordinary electric fan,
but it has a mark on one blade, so that you can identify it.
We'll plug it in, get it up to speed.
This looks like a fan that was also demonstrated in the World's Fair, a few years ago.
Yeah, could've been.
There was a movie Quicker'n a Wink had this thing in there and--
With this very fan?
Well, one like it.
It was loaned to MGM.
And Pete Smith, he said he wanted me to throw out a custard pie into it.
I said, no, I'm a serious scientist.
So he says, let's compromise on the egg.
So we dropped an egg into it and you would see a high-speed movie of the egg dropping.
No, not with the strobe, but with this NOISEEVENT
That was with the high-speed photography.
High-speed movies, yeah.
So again, I guess, without the strobe, when we look at it, what we're looking at are frequencies that are much higher than the eye can follow.
And now, with the strobe on, you can see both the alias frequency and you can also see the original frequency because we had the incandescent light on.
Let's turn down the background light again.
And then, really all that we're able to see are the aliasing frequencies.
And I guess when we see more than one mark, that means that we're actually running it at--
Four times the speed of the fan.
--four times the speed the fan, yeah.
You see a little variation in the--
Oh, yeah.
Right.
It's because the blades aren't exactly the same.
Actually, this gives me a chance to illustrate another important point related to the lecture.
Let's see, can we bring it down to a frequency so that we only get one mark?
Sure.
You may miss this because it's too lowered to just one blade there now.
So the way we have it now, we've essentially aliased the fan's speed down so that it's just a little higher than DC.
And now, I'm right at DC.
And now, if I go down just a little further, in fact it looks like the fan is turning backwards.
And if you think of this in the context of aliasing, it's like the two impulses in the frequency domain have crossed over.
And what you get in effect, if you analyze it mathematically, is you get a phase reversal.
And it wasn't until I first understood about aliasing, by the way, Doc, that I understood why when I went to Western movies, every once in a while you'd see the wagon wheels turning backwards.
Then there's the wagon wheels of the Western movie going backwards, I guess.
And, Justine, why don't you see if you can--
Too much flicker there.
Why don't you bring it up so you get two marks.
See if you can bring the frequency up so that you get two marks.
You turn that, Justine.
Grab right ahold of that and give it a big twist.
You went past it.
They're not regular there now.
Here we are.
Now hold it right there.
Put your finger on there, hold the dial.
It's flashing twice per revolution now, Al.
I guess another thing that this demonstrates is something that I've heard a long time ago, which is that you should never use a power saw with a fluorescent light because the fluorescent light gives you a little bit of a strobe effect and you could actually convince yourself that that's standing still and make the mistake of trying to put your finger between the blades.
You want to stick your finger in there?
No, I don't think I want to try it.
How about you, Justine?
What do you think?
Is that standing still or is that moving?
She knows it's going.
We won't let her get close to that fan.
Actually if we turn the lights back on again, what that will let us see once again is that we can see both the alias frequencies when we do that and we can also see the higher frequencies because of the incandescent lighting.
Maybe what we can do now is take a look at some other fun things.
And one I guess I'm curious about is the disk that you have over there.
Doc, maybe you can tell us what we have here?
Sure, Al.
This is a disk to show how you can get motion pictures out of a series of still pictures.
This circle is repeated 12 times.
The white dot goes from the outer part of it on this side to the inner part on the other.
If I flash one time per revolution on this, you'll see it exactly as it is.
But if I skip one picture each time, then you get the relative motion of this ball.
Well, the object is to show the ball rotating either this way or that way depending on whether the strobe was going faster or slower than the other.
This way motion pictures were developed hundred years ago, long before photography.
They drew pictures of people in different poses, animated pictures.
Like to see it run?
Yeah, great.
It's actually, the title, kind of, is "Aliasing Can Be Fun."
That's right.
Let me get it up to speed.
On the way up, you get a lot of different, sort of, patterns as it goes through.
When it eventually reaches its speed, which is about 1,100 per minute, you'll see it stop.
And the background blur, basically at the high frequencies that the eye can't follow and then, kind of, superimposed on that again, we can see the frequencies that are aliased down.
And that's what the eye can follow.
Right now, we have one flash per revolution, so you can see the part of the disk that's illuminated with the strobe exactly as if it was standing still.
Now if I increase the frequency, so they skip one circle, then you get the illusion that, that dot is moving.
In fact, let's really enhance the revolution, let's turn the incandescent lights off again.
And now, now what we see really are the alias frequencies.
What do you think of this Justine?
Neat.
It looks like magic.
I still have great joy in watching this thing, though it's so simple.
Now, while we're watching this, something also I might point out for the lecture--for the course-- is that actually there really are two sampling frequencies that we're seeing.
One is the strobe, which is the strobe that you're running.
The other is the inherent frame rate for the TV, that's running at 30 frames a second.
And that's one of the reasons, by the way, that people watching this on the video course are in fact seeing a flicker or a beating or modulation between the two unsynchronized frame rates.
I'll run the frequencies of the strobe up, so we get two of them in there.
You keep watching, we had all these other interesting patterns.
There's two now.
And I'll make the two bounce on each other.
You get all these patterns for free.
You design a disk to show one thing and then when you run it, you find all the other patterns.
I think it would be a terrific homework problem for the video course, to have them all sit down and analyze all the frequencies that they're seeing and what they're being aliased to.
What do you think of that?
That's a good idea.
As a teacher, I love to give quizzes.
Find out whether the students are listening.
I think that'll chase a few people away from the course, that's what I like--
No, it attracts them because you get involved in these optical things, there's no limit on what you can do.
Let's bring the incandescent lights back up again, just to remind everybody that in back of all these are some frequencies that are a lot higher than the ones that we begin to get the impression that we're watching.
It's just a motor running at constant speed with a pattern on it.
Doc, I have to say that there aren't many people I know that have as much fun in their work as you do.
Well, I'm a lucky man.
Well, what I'd like to do now, maybe, is take a look at one last experiment, if you could.
Sure.
And what I'd like to do is go take a look at, I guess, what sometimes is called the--well, not the water drop experiment-- what's the name of the--
You mean the Double Piddler Hydraulic Happening Machine?
That's the one I was thinking of.
Let's take a look over there.
Come on, Justine, let's go and turn on the water.
So, Doc, this is the--what did you call it DPHHM for Double Piddler Hydraulic Happening Machine?
Got it.
It looks like a continuous stream, but it's not.
It's a pump over there.
It's pumping 60 pulses a second.
The water is coming out in spurts.
So actually, again it's the 60 pulses a second your eye can't follow.
Your eye's no good at 60 a second.
Basically looks like a blur.
It is a blur, a nice juicy blur.
Now we put the strobe on.
So again, I guess we have this essentially aliased down.
And again with the incandescent light, you can see both the high frequency and the alias frequency.
And let's see, I guess that's what the frequency close to DC and we can adjust it so that it's stopped.
All right, make the water go up.
And then we can actually make it go up.
Of course, nobody believes that.
Yeah, in fact, let me just again, to stress this point to the class.
The idea here of the phase reversal-- of course, you can see it in the time domain-- you just think about when the flashes of light come.
But if you think of these impulses that we have in the frequency domain and we're aliasing as we change the sampling frequency, what happens is that these impulses cross over and what that means is that we get a phase reversal depending on which phases are associated with which side of DC so that's kind of the idea of the phase reversal.
Let's turn the--
Well, we tried to have Justine put her finger in between those two drops.
Yeah, let's turn the incandescent light off first.
And--
Take one finger out now.
Put it right in between those two drops.
Justine,you think you can do that?
Better get on the other side.
Use your other hand, so they can see with it.
You can--
Think you can get your finger in there?
Whoop, there's water there all the time.
Well I don't know, Doc.
It seems to me if we-- can't we just adjust this so that the dots just go through each other?
Sure.
Now if the dots can do it, Justine, how come you can't get your finger in there?
I don't know.
Why don't you try that once more?
I guess not.
No, that's one thing you can't do with it.
Well let's bring the lights back up and again,
just to stress the point, here we are at DC, here we are at a frequency that's just a little above DC, and we can go back down to DC and we can actually get a phase reversal.
And I guess, if we do this long enough, we can empty out the whole ocean and put it back in wherever it comes from.
Isn't that right?
And we caution the students when they run this, not to run it too long--
That's right.
We've got the bucket here.
You have to be careful--
--and it's been a while since somebody believes me.
Well, I don't know about them, but I guess I believe you, Doc.
I'll put a little more pressure on so we get little more interesting patterns.
Little patterns or surface tension that's pulling in things together.
We have these machines, they're all over the place.
They're a lot of fun.
Well, Doc, this is really terrific.
I think that this whole idea of using aliasing and strobes and the kinds of things that you do with them are just fantastic.
And we really appreciate the chance to come in here and see the demonstration.
Well, that's the whole game.
We've NOISEEVENT them for years and probably will for many years to come.
So as I emphasized at the beginning, in lots of situations aliasing can, in fact, be very useful.
Also what this demonstrates is that particularly when you have a colleague, like Doc Edgerton, aliasing and for that matter science, in general, can be an awful lot of fun.
Thanks for coming in.
Thanks a lot, Doc.
See you again.
And thank you, Justine.
You're welcome.
Well, I have to say that visit was an awful lot of fun for me and for Justine and in fact, for the whole camera crew that was there.
And hopefully, all of you at some point will also have a chance to visit at Strobe Alley.
Well, hopefully what we've gone through today gives you a good feeling for the concepts of sampling and aliasing and both, why it might be useful and why we might want to avoid it.
In the next lecture, we'll continue on the discussion of sampling.
And in particular, what I'll be talking about is the interpretation of the reconstruction process not in the frequency domain, but in a time domain and interpretation specifically associated with the concept of interpolating between the samples.
We'll then proceed from there to a discussion as I've alluded to in several lectures of what I've referred to as discrete-time processing of continuous-time signals, very heavily exploiting the concept and issues associated with sampling.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
In discussing the sampling theorem, we saw that for a band limited signal, which is sampled at a frequency that is at least twice the highest frequency, we can implement exact reconstruction of the original signal by low pass filtering an impulse train, whose areas are identical to the sample values.
Well essentially, this low pass filtering operation provides for us an interpolation in between the sampled values.
In other words, the output of a low pass filter, in fact, is a continuous curve, which fits between the sampled values some continuous function.
Now, I'm sure that many of you are familiar with other kinds of interpolation that we could potentially provide in between sampled values.
And in fact, in today's lecture what I would like to do is first of all developed the interpretation of the reconstruction as an interpolation process and then also see how this exact interpolation, using a low pass filter, relates to other kinds of interpolation, such as linear interpolation that you may already be familiar with.
Well to begin, let's again review what the overall system is for exact sampling and reconstruction.
And so let me remind you that the overall system for sampling and desampling, or reconstruction, is as I indicate here.
The sampling process consists of multiplying by an impulse train.
And then the reconstruction process corresponds to processing that impulse train with a low pass filter.
So if the spectrum of the original signal is what I indicate in this diagram, then after sampling with an impulse train, that spectrum is replicated.
And this replicated spectrum for reconstruction is then processed through a low pass filter.
And so, in fact, if this frequency response is an ideal low pass filter, as I indicate on the diagram below, then multiplying the spectrum of the sample signal by this extracts for us just the portion of the spectrum centered around the origin.
And what we're left with, then, is the spectrum,
finally, of the reconstructed signal, which for the case of an ideal low pass filter is exactly equal to the spectrum of the original signal.
Now, that is the frequency domain picture of the sampling and reconstruction.
Let's also look at, basically, the same process.
But let's examine it now in the time domain.
Well in the time domain, what we have is our original signal multiplied by an impulse train.
And this then is the sample signal, or the impulse train whose areas are equal to the sample values.
And because of the fact that this is an impulse train, in fact, we can take this term inside the summation.
And of course, what counts about x of t in this expression is just as values at the sampling instance, which are displaced in time by capital T. And so what we can equivalently write is the expression for the impulse train samples, or impulse train of samples, as I've indicated here.
Simply an impulse train, whose areas are the sampled values.
Now, in the reconstruction we process that impulse train with a low pass filter.
That's the basic notion of the reconstruction.
And so in the time domain, the reconstructed signal is related to the impulse train of samples through a convolution with the filter impulse response.
And carrying out this convolution, since this is just a train of pulses, in effect, what happens in this convolution is that this impulse response gets reproduced at each of the locations of the impulses in x of p of t with the appropriate area.
And finally, then, in the time domain, the reconstructed signal is simply a linear combination of shifted versions of the impulse response with amplitudes, which are the sample values.
And so this expression, in fact then, is our basic reconstruction expression in the time domain.
Well in terms of a diagram, we can think of the original waveform as I've shown here.
And the red arrows denote the sampled wave form, or the train of impulses, whose amplitudes are the sampled values of the original continuous time signal.
And then, I've shown here what might be a typical impulse response, particularly typical in the case where we're talking about reconstruction with an ideal low pass filter.
Now, what happens in the reconstruction is that the convolution of these impulses with this impulse response means that in the reconstruction, we superimpose one of these impulse responses-- whatever the filter impulse response happens to be-- at each of these time instance.
And in doing that, then those are added up.
And that gives us the total reconstructed signal.
Of course, for the case in which the filter is an ideal low pass filter, then what we know is that in that case, the impulse response is of the form of a sync function.
But generally, we may want to consider other kinds of impulse responses.
And so in fact, the interpolating impulse response may have and will have, as this discussion goes along, some different shapes.
Now what I'd like to do is illustrate, or demonstrate,
this process of effectively doing the interpolation by replacing each of the impulses by an appropriate interpolating impulse response and adding these up.
And I'd like to do this with a computer movie that we generated.
And what you'll see in the computer movie is,
essentially, an original wave form, which is a continuous curve.
And then below that in the movie is a train of samples.
And then below that will be the reconstructed signal.
And the reconstruction will be carried out by showing the location of the impulse response as it moves along in the wave form.
And then the reconstructed curve is simply the summation of those as that impulse response moves along.
So what you'll see then is an impulse response like this--
for the particular case of an ideal low pass filter for the reconstruction-- placed successively at the locations of these impulses.
And that is the convolution process.
And below that then will be the summation of these.
And the summation of those will then be the reconstructed signal.
So let's take a look at, first of all that reconstruction where the impulse response corresponds to the impulse response of an ideal low pass filter.
Shown here, first, is the continuous time signal, which we want to sample and then reconstruct using band limited interpolation, or equivalently, ideal low pass filtering on the set of samples.
So the first step then is to sample this continuous time signal.
And we see here now the set of samples.
And superimposed on the samples are the original continuous time signal to focus on the fact that those are samples of the top curve.
Let's now remove the continuous time envelope of the samples.
And it's this set of samples that we then want to use for the reconstruction.
The reconstruction process, interpreted as interpolation,
consists of replacing each sample with a sine x over x function.
And so let's first consider the sample at t equals 0.
And here is the interpolating sine x over x function associated with that sample.
Now, the more general process then is to place a sine x over x function at the time location of each sample and superimpose those.
Let's begin that process at the left-hand set of samples.
And in the bottom curve, we'll build up the reconstruction as those sine x over x functions are added together.
So we begin with the left-hand sample.
And we see there the sine x over x function on the bottom curve is the first step in the reconstruction.
We now have the sine x over x function associated with the second sample.
Let's add that in.
Now we move on to the third sample.
And that sine x over x function is added in.
Continuing on, the next sample generates a sine x over x function, which is superimposed on the result that we've accumulated so far.
And now let's just speed up the process.
We'll move on to the fifth sample.
Add that in.
The sixth sample, add that in.
And continue on through the set of samples.
And keep in mind the fact that, basically, what we're doing explicitly here is the convolution of the impulse train with a sine x over x function.
And because the set of samples that we started with were samples of an exactly band limited function, what we are reconstructing exactly is the original continuous time signal that we have on the top trace.
OK, so that then kind of gives you the picture of doing interpolation by replacing the impulses by a continuous curve.
And that's the way we're fitting a continuous curve to the original impulse train.
And let me stress that this reconstruction process-- by putting the impulses through a filter-- follows this relationship whether or not this impulse response, in fact, corresponds to an ideal low pass filter.
What this expression always says is that reconstructing this way corresponds to replacing the impulses by a shifted impulse response with an amplitude that is an amplitude corresponding to the sample value.
Now the kind of reconstruction that we've just talked about,
and the ideal reconstruction, is often referred to as band limited interpolation because we're interpolating in between the samples by making the assumption that the signal is band limited and using the impulse response for an ideal low pass filter, which has a cut off frequency consistent with the assumed bandwidth for the signal.
So if we look here, for example, at the impulse train,
then in the demonstration that you just saw, we built up the reconstructed curve by replacing each of these impulses with the sync function.
And the sum of those built up the reconstructed curve.
Well, there are lots of other kinds of interpolation that are perhaps maybe not as exact but often easier to implement.
And what I'd like to do is focus our attention on two of these.
The first that I want to mention is what's referred to as the zero order hold, where in effect, we do the interpolation in between these sample values by simply holding the sample value until the next sampling instant.
And the reconstruction that we end up, in that case, will look something like this.
It's a staircase, or box car, kind of function where we've simply held the sample value until the next sampling instant and then replaced by that value, held it until the next sampling instant, et cetera.
Now that's one kind of interpolation.
Another kind of very common interpolation is what's referred to as linear interpolation, where we simply fit a straight line between the sampled values.
And in that case, the type of reconstruction that we would get would look something like I indicate here, where we take a sample value, and the following sample value, and simply fit an interpolated curve between them, which is a straight line.
Now interestingly, in fact, both the zero order hold and the linear interpolation, which is often referred to as a first order hold, can also be either implemented or interpreted, both implemented and interpreted, in the context of the equation that we just developed.
In particular, the processing of the impulse train of samples by a linear time invariant filter.
Specifically, if we consider a system where the impulse response is a rectangular function, then in fact, if we processed the train of samples through a filter with this impulse response, exactly the reconstruction that we would get is what I've shown here.
Alternatively, if we chose an impulse response which was a triangular impulse response, then what in effect happens is that each of these impulses activates this triangle.
And when we add up those triangles at successive locations, in fact, what we generate is this linear interpolation.
So what this says, in fact, is that either a zero order hold,
which holds the value, or linear interpolation can likewise be interpreted as a process of convulving the impulse train of samples with an appropriate filter impulse response.
Well, what I'd like to do is demonstrate, as we did with the band limited interpolation or the sync interpolation as it's sometimes called-- interpolating with a sine x over x-- let me now show the process.
First of all, where we have a zero order hold as corresponding to this impulse response.
In which case, we'll see basically the same process as we saw in the computer generated movie previously.
But now, rather than a sync function replacing each of these impulses, we'll have a rectangular function.
That will generate then our approximation, which is a zero order hold.
And following that, we'll do exactly the same thing with the same wave form, using a first order hold or a triangular impulse response.
In which case, what we'll see again is that as the triangle moves along here, and we build up the running sum or the convolution, then we'll, in fact, fit the original curve with a linear curve.
So now let's again look at that, remembering that at the top we'll see the original continuous curve, exactly the one that we had before.
Below it, the set of samples together with the impulse response moving along.
And then finally below that, the accumulation of those impulse responses, or equivalently the convolution, or equivalently the reconstruction.
So we have the same continuous time signal that we use previously with band limited interpolation.
And in this case now, we want to sample and then interpolate first with a zero order hold and then with a first order hold.
So the first step then is to sample the continuous time signal.
And we show here the set of samples, once again, superimposed on which we have the continuous time signal, which of course is exactly the same curve as we have in the top.
Well, let's remove that envelope so that we focus attention on the samples that we're using to interpolate.
And the interpolation process consists of replacing each sample by a rectangular signal, whose amplitude is equal to the sample size.
So let's put one, first of all, at t equals 0 associated with that sample.
And that then would be the interpolating rectangle associated with the sample at t equals 0.
Now to build up the interpolation, what we'll have is one of those at each sample time, and those are added together.
We'll start that process, as we did before, at the left-hand end of the set of samples and build the interpolating signal on the bottom.
So with the left-hand sample, we have first the rectangle associated with that.
That's shown now on the bottom curve.
We now have an interpolating rectangle with a second sample that gets added into the bottom curve.
Similarly, an interpolating rectangle with the zero order hold with the third sample.
We add that into the bottom curve.
And as we proceed, we're building a staircase approximation.
On to the next sample, that gets added in as we see there.
And now let's speed up the process.
And we'll see the staircase approximation building up.
And notice in this case, as in the previous case, that what we're basically watching dynamically is the convolution of the impulse train of samples with the impulse response of the interpolating filter, which in this particular case is just a rectangular pulse.
And so this staircase approximation that we're generating is the zero order hold interpolation between the samples of the band limited signal, which is at the top.
Now let's do the same thing with a first order hold.
So in this case, we want to interpolate using a triangular impulse response rather then the sine x over x, or rectangular impulse responses that we showed previously.
So first, let's say with the sample at t equals 0, we would replace that with a triangular interpolating function.
And more generally, each impulse or sample is replaced with a triangular interpolating function of a height equal to the sample type.
And these are superimposed to generate the linear interpolation.
We'll begin this process with the leftmost sample.
And we'll build the superposition below in the bottom curve.
So here is the interpolating triangle for the leftmost sample.
And now it's reproduced below.
With the second sample, we have an interpolating triangle, which is added into the bottom curve.
And now on to the third sample.
And again, that interpolating triangle will be added on to the curve that we've developed so far.
And now onto the next sample.
We add that in.
Then we'll speed up the process.
And as we proceed through, we are building, basically, a linear interpolation in between the sample points, essentially corresponding to-- if one wants to think of it this way-- connecting the dots.
And what you're watching, once again, is essentially the convolution process convulving the impulse train with the impulse response of the interpolating filter.
And what we're generating, then, is a linear approximation to the band limited continuous time curve at the top.
OK, so what we have then is several other kinds of interpolation, which fit within the same context as exact band limited interpolation.
One being interpolation in the time domain with an impulse response, which is a rectangle.
The second being interpolation in the time domain with an impulse response, which is a triangle.
And in fact, it's interesting to also look at the relationship between that and band limited interpolation.
Look at it, specifically, in the frequency domain.
Well, in the frequency domain, what we know, of course, is that for exact interpolation, what we want as our interpolating filter is an ideal low pass filter.
Now keep in mind, by the way, that an ideal low pass filter is an abstraction, as I've stressed several times in the past.
An ideal low pass filter is a non-causal filter and, in fact, infinite extent, which is one of the reasons why in any case we would use some approximation to it.
But here, what we have is the exact interpolating filter.
And that corresponds to an ideal low pass filter.
If, instead, we carried out the interpolating using the zero order hold, the zero order hold has a rectangular impulse response.
And that means in the frequency domain, its frequency response is of the form of a sync function, or sine x over x.
And so this, in fact, when we're doing the reconstruction with a zero order hold, is the associated frequency response.
Now notice that it does some approximate low pass filtering.
But of course, it permits significant energy outside the past band of the filter.
Well, instead of the zero order hold, if we used the first order hold corresponding to the triangular impulse response, in that case then in the frequency domain, the associated frequency response would be the Fourier transform of the triangle.
And the Fourier transform of a triangle is a sine squared x over x squared kind of function.
And so in that case, what we would have for the frequency response, associated with the first order hold, is a frequency response as I show here.
And the fact that there's somewhat more attenuation outside the past band of the ideal filter is what suggests, in fact, that the first order hold, or linear interpolation, gives us a somewhat smoother approximation to the original signal than the zero order hold does.
And so, in fact, just to compare these two, we can see that here is the ideal filter.
Here is the zero order hold, corresponding to generating a box car kind of reconstruction.
And here is the first order hold, corresponding to a linear interpolation.
Now in fact, in many sampling systems, in any sampling system really, we need to use some approximation to the low pass filter.
And very often, in fact, what is done in many sampling systems, is to first use just the zero order hold, and then follow the zero order hold with some additional low pass filtering.
Well, to illustrate some of these ideas and the notion of doing a reconstruction with a zero order hold or first order hold and then in fact adding to that some additional low pass filtering, what I'd like to do is demonstrate, or illustrate, sampling and interpolation in the context of some images.
An image, of course, is a two-dimensional signal.
The independent variables are spatial variables not time variables.
And of course, we can sample in both of the spatial dimensions, both in x and y.
And what I've chosen as a possibly appropriate choice for an image is, again, our friend and colleague J.B.J. Fourier.
So let's begin with the original image, which we then want to sample and reconstruct.
And the sampling is done by effectively multiplying by a pulse both horizontally and vertically.
The sample picture is then the next one that I show.
And as you can see, this corresponds, in effect, to extracting small brightness elements out of the original image.
In fact, let's look in a little closer.
And what you can see, essentially, is that what we have, of course, are not impulses spatially but small spatial pillars that implement the sampling for us.
OK, now going back to the original sample picture, we know that a picture can be reconstructed by low pass filtering from the samples.
And in fact, we can do that optically in this case by simply defocusing the camera.
And when we do that, what happens is that we smear out the picture, or effectively convulve the impulses with the point spread function of the optical system.
And this then is not too bad a reconstruction.
So that's an approximate reconstruction.
And focusing back now what we have again is the sample picture.
Now these images are, in fact, taken off a computer display.
And a common procedure in computer generated or displayed images is in fact the use of a zero order hold.
And if the sampling rate is high enough, then that actually works reasonably well.
So now let's look at the result of applying a zero order hold to the sample image that I just showed.
The zero order hold corresponds to replacing the impulses by rectangles.
And you can see that what that generates is a mosaic effect, as you would expect.
And in fact, let's go in a little closer and emphasize the mosaic effect.
You can see that, essentially, where there were impulses previously, there are now rectangles with those brightness values.
A very common procedure with computer generated images is to first do a zero order hold, as we've done here, and then follow that with some additional low pass filtering.
And fact, we can do that low pass filtering now again by defocusing the camera.
And you can begin to see that with the zero order hold plus the low pass filtering, the reconstruction is not that bad.
Well, let's go back to the full image with the zero order hold.
And again, now the effect of low pass filtering will be somewhat better.
And let's defocus again here.
And you can begin to see that this is a reasonable reconstruction.
With the mosaic, in fact, with this back in focus, you can apply your own low pass filtering to it either by squinting, or if you have the right or wrong kind of eyeglasses, either taking them off or putting them on.
Now, in addition to the zero order hold, we can, of course, apply a first order hold.
And that would correspond to replacing the impulses,
instead of with rectangles as we have here, replacing them with triangles.
And so now let's take a look at the result of a first order hold applied to the original samples.
And you can see now that the reconstruction is somewhat smoother because of the fact that we're using an impulse response that's somewhat smoother or a corresponding frequency response that has a sharper cut off.
I emphasize again that this is a somewhat low pass filtered version of the original because we have under sampled somewhat spatially to bring out the point that I want to illustrate.
OK, to emphasize these effects even more, what I'd like to do is go through, basically, the same sequence again.
But in this case, what we'll do is double the sample spacing both horizontal and vertically.
This of course, means that we'll be even more highly under sampled than in the ones I previously showed.
And so the result of the reconstructions with some low pass filtering will be a much more low pass filtered image.
So we now have the sampled picture.
But I've now under sampled considerably more.
And you can see the effect of the sampling.
And if we now apply a zero order hold to this picture, we will again get a mosaic.
And let's look at that.
And that mosaic, of course, looks even blockier than the original.
And again, it emphasizes the fact that the zero order hold simply corresponds to filling in squares, or replacing the impulses, by squares, with the corresponding brightness values.
Finally, if we, instead of a zero order hold, use a first order hold, corresponding to two dimensional triangles in place of these original blocks.
What we get is the next image.
And that, again, is a smoother reconstruction consistent with the fact that the triangles are smoother than the rectangles.
Again, I emphasize that this looks so highly low pass filtered because of the fact that we've under sampled so severely to essentially emphasize the effect.
As I mentioned, the images that we just looked at were taken from a computer, although of course the original images were continuous time images or more specifically, continuous space.
That is the independent variable is a spatial variable.
Now, computer processing of signals, pictures, speech, or whatever the signals are is very important and useful because it offers a lot of flexibility.
And in fact, the kinds of things that I showed with these pictures would have been very hard to do without, in fact, doing computer processing.
Well, in computer processing of any kind of signal,
basically what's required is that we do the processing in the context of discrete time signals and discrete time processing because of the fact that a computer is run off a clock.
And essentially, things happen in the computer as a sequence of numbers and as a sequence of events.
Well, it turns out that the sampling theorem, in fact, as I've indicated previously, provides us with a very nice mechanism for converting our continuous time signals into discrete time signals.
For example, for computer processing or, in fact, if it's not a computer for some other kind of discrete time or perhaps digital processing.
Well, the basic idea, as I've indicated previously, is to carry out discrete time processing of continuous time signals by first converting the continuous time signal to a discrete time signal, carry out the appropriate discrete time processing of the discrete time signal, and then after we're done with that processing, converting from the discrete time sequence back to a continuous time signal, corresponding to the output that we have here.
Well in the remainder of this lecture, what I'd like to analyze is the first step in that process, namely the conversion from a continuous time signal to a discrete time signal and understand how the two relate both in the time domain and in the frequency domain.
And in the next lecture, we'll be analyzing and demonstrating the overall system, including some intermediate processing.
So the first step in the process is the conversion from a continuous time signal to a discrete time signal.
And that can be thought of as a process that involves two steps, although in practical terms it may not be implemented specifically as these two steps.
The two steps are to first convert from the continuous time, or continuous time continuous signal, to an impulse train through a sampling process and then to convert that impulse train to a discrete time sequence.
And the discrete time sequence x of n is simply then a sequence of values which are the samples of the continuous time signal.
And as we'll see as we walk through this, basically the step of going from the impulse train to the sequence corresponds principally to a relabeling step where we pick off the impulse values and use those as the sequence values for the discrete time signal.
So what I'd like to do as a first step in understanding this process is to analyze it in particular with our attention focused on trying to understand what the relationship is in the frequency domain between the discrete time Fourier transform of the sequence, discrete time signal, and the continuous time Fourier transform of the original unsampled, and then the sampled signal.
So let's go through that.
And in particular, what we have is a process where the continuous time signal is, of course, modulated or multiplied by an impulse train.
And that gives us, then, another continuous time signal.
We're still in the continuous time domain.
It gives us another continuous time signal, which is an impulse train.
And in fact, we've gone through this analysis previously.
And what we have is this multiplication or taking this term inside the summation and recognizing that the impulse train is simply an impulse train with areas of the impulses, which are the samples of the continuous time function.
We can then carry out the analysis in the frequency domain.
Now in the time domain, we have a multiplication process.
So in the frequency domain, we have a convolution of the Fourier transform of the continuous time signal, the original signal, and the Fourier transform of the impulse train, which is itself an impulse train.
So in the frequency domain then, the Fourier transform of the sampled signal, which is an impulse train, is the convolution of the Fourier transform of the sampling function P of t and the Fourier transform of the sampled signal.
Since the sampling signal is a periodic impulse train, its Fourier transform is an impulse train.
And consequently, carrying out this convolution in effect says that this Fourier transform simply gets replicated at each of the locations of these impulses.
And finally, what we end up with then is a Fourier transform after the sampling process, which is the original Fourier transform of the continuous signal but added to itself shifted by integer multiples of the sampling frequency.
And so this is the basic equation then that tells us in the frequency domain what happens through the first part of this two step process.
Now I emphasize that it's a two step process.
The first process is sampling, where we're still essentially in the continuous time world.
The next step is essentially a relabeling process, where we convert that impulse train simply to a sequence.
So let's look at the next step.
The next step is to take the impulse train and convert it through a process to a sequence.
And the sequence values are simply then samples of the original continuous signal.
And so now we can analyze this.
And what we want to relate is the discrete time Fourier transform of this and the continuous time Fourier transform of this, or in fact, the continuous time Fourier transform of x of C of T.
OK, we have the impulse train.
And it's Fourier transform we can get by simply evaluating the Fourier transform.
And since the Fourier transform of this-- since this corresponds to an impulse train-- the Fourier transform, by the time we change some sums and integrals, will then have this impulse replaced by the Fourier transform of the shifted impulse, which is this exponential factor.
So this expression is the Fourier transform of the impulse train, the continuous time Fourier transform.
And alternatively, we can look at the Fourier transform of the sequence.
And this, of course, is a discrete time Fourier transform.
So we have the continuous time Fourier transform of the impulse train, we have the discrete Fourier transform of the sequence.
And now we want to look at how those two relate.
Well, it pretty much falls out of just comparing these two summations.
In particular, this term and this term are identical.
That's just a relabeling of what the sequence values are.
And notice that when we compare these exponential factors, they're identical as long as we associate capital omega with little omega times capital T. In other words, if we were to replace here capital omega by little omega times capital T, and replace x of n by x of c of nt, then this expression would be identical to this expression.
So in fact, these two are equal with a relabeling, or with a transformation, between small omega and capital omega.
And so in fact, the relationship that we have is that the discrete time Fourier transform of the sequence of samples is equal to the continuous time Fourier transform of the impulse train of samples where we associate the continuous time frequency variable and the discrete time frequency variable through a frequency scaling as I indicate here.
Or said another way, the discrete time spectrum is the continuous time spectrum of the samples with small omega replaced by capital omega divided by capital T. All right.
So we have then this two step process.
The first step is taking the continuous time signal, sampling it with an impulse train.
In the frequency domain, that corresponds to replicating the Fourier transform of the original continuous time signal.
The second step is relabeling that, in effect turning it into a sequence.
And what that does in the frequency domain is provide us with a rescaling of the frequency axis, or as we'll see a frequency normalization, which is associated with the corresponding time normalization in the time domain.
Well, let's look at those statements a little more specifically.
What I show here is the original continuous time signal.
And then below it is the sampled signal.
And these two are signals in the continuous time domain.
Now, what is the conversion from this impulse train to a sequence?
Well, it's simply taking these impulse areas, or these sample values, and relabeling them, in effect as I show below, as sequence values.
And essentially, I'm now replacing the impulse by the designation of a sequence value.
That's one step.
But the other important step to focus on is that whereas in the impulse train, these impulses are spaced by integer multiples of the sampling period capital T. In the sequence, of course, because of the way that we label sequences, these are always spaced by simply integer multiples of one.
So in effect, you could say that the step in going from here to here corresponds to normalizing out in the time domain the sampling period capital T.
To stress that another way, if the sampling period were doubled so that in this picture, the spacing stretched out by a factor of two.
Nevertheless, for the discrete time signal, the spacing would remain as one.
And essentially, it's the envelope of those sequence values that would then get compressed in time.
So you can think of the step in going from the impulse train to the samples as, essentially, a time normalization.
Now let's look at this in the frequency domain.
In the frequency domain, what we have is the Fourier transform of our original continuous signal.
After sampling with an impulse train, this spectrum retains its shape but is replicated at integer multiples of the sampling frequency 2 pi over capital T, as I indicate here.
Now, we know that a discrete time spectrum must be periodic in frequency with a period of 2 pi.
Here, we have the periodicity.
But it's not periodic with a period of 2 pi.
It's periodic with a period, which is equal to the sampling frequency.
However, in converting from the samples to the sequence values, we go through another step.
What's the other step?
The other step is a time normalization, where we take the impulses, which are spaced by the sampling period.
And we rescale that, essentially in the time domain, to a spacing which is unity.
So we're dividing out in the time domain by a factor, which is equal to the sampling period.
Well, dividing out in the time domain by capital T would correspond to multiplying in the frequency domain the frequency axis by capital T. And indeed, what happens is that in going from the impulse train to the sequence values, we now rescale this axis so that, in fact, the axis gets stretched by capital T. And the frequency, which corresponded to 2 pi over capital T, now gets renormalized to 2 pi.
So just looking at this again, and perhaps with the overall picture, in the time domain, we've gone from a continuous curve to samples, relabeled those, and in effect implemented a time normalization.
Corresponding in the frequency domain, we have replicated the spectrum through the initial sampling process and then rescaled the frequency axis so that, in fact, now this periodicity corresponds to a periodicity here, which is 2 pi, and here, which is the sampling frequency.
So very often, in fact-- and we'll be doing this next time-- when you think of continuous time signals, which have been converted to discrete time signals, when you look at the discrete time frequency axis, the frequency 2 pi is associated with the sampling frequency as it was applied to the original continuous time signal.
Now as I indicated, what we'll want to go on to from here is an understanding of what happens when we take a continuous time signal, convert it to a discrete time signal as I've just gone through, do some discrete time processing with a linear time invariant system, and then carry that back into the continuous time world.
That is a procedure that we'll go through, and analyze, and in fact, illustrate in some detail next time.
In preparation for that, what I would be eager to encourage you to do using the study guide and in reviewing this lecture, is to begin the next lecture with a careful and thorough understanding of the arguments that I've just gone through.
In particular, understanding the process that's involved in going from a continuous time signal through sampling to a discrete time signal.
And what that means in the frequency domain in terms of taking the original spectrum, replicating it because of the sampling process, and then rescaling that so that the periodicity gets rescaled so that it's periodic with a period of 2 pi.
So we'll continue with that next time, focusing now on the subsequent steps in the processing.
Thank you.
NOISEEVENT
Last time, we began the discussion of discreet-time processing of continuous-time signals.
And, as a reminder, let me review the basic notion.
The idea was that we convert from a continuous-time signal to a sequence through an operation which I represent as a continuous to discrete time converter.
And then that sequence is used as the input to an appropriate discreet-time system.
And after appropriate discreet-time processing, that sequence is converted back to a continuous-time signal through an operation which I label as a discrete to continuous time converter.
Now, in the lecture last time, we carried out some analysis which related for us the spectra in the first step of this operation.
Namely in the transformation from a continuous-time time signal to a sequence.
And let me, by the way, draw your attention to the fact that, in the real world, this operation is essentially implemented by what you would typically label as an analog to digital converter, if in fact the discreet-time processing is being done digitally.
Now, it's important to emphasize that it's not exactly what an analog to digital converter does, but, in some sense at least, you should think of this mapping from continuous-time to discreet-time in very much the same way that one would think of an analog to digital converter.
And the mapping back then corresponds, in some sense, to what would happen with a digital to analog converter.
Well, let me review what is involved in the mapping from the continuous-time signal to the sequence.
And, let me stress again that, this operation is basically-- in the continuous to discreet-time conversion-- a two-step process.
In the first part of the process, the continuous-time signal is modulated with impulse train, where the period of the impulse train is capital T. And so we have a continuous-time time impulse train signal which captures the samples of the original continuous-time signal.
That impulse train is then put through an operation which,
essentially, re-labels the samples so that the sample values, the impulse areas, are re-labeled as sequence values.
And the result of that conversion is then the sequence x of n.
So the overall process, then, is a sampling process, followed by what is simply, in this box, a re-labeling process.
And, although as I indicated just a minute ago, that is essentially what an analog to digital converter does.
An analog to digital converter doesn't necessarily carry it out in those two steps, but particularly, in terms of carrying through an analysis, thinking of it as a two-step process is particularly convenient.
Now we talked last time about what this mapping from continuous-time to discreet-time means, both in the time domain, and terms of the spectra.
And in particular, in the time domain we begin with the continuous-time signal, which is then sampled with an impulse train and converted to a sequence by simply generating a sequence whose values are the areas of the impulses.
And I stress the fact that what this corresponds to,
essentially, is a normalization of the time axis, essentially, by dividing the time axis by capital T.
In the frequency domain, then, we had the spectrum of the original signal, which, because of the sampling process, is replicated at integer multiples of the sampling frequency omega sub s, or 2 pi over capital T. And then, in converting the impulses to a sequence, we are essentially normalizing the frequency axis, so that the frequency 2 pi over capital T gets re-labeled as 2 pi.
And the resulting discreet-time spectrum looks like I indicate here.
Which really is nothing more than a frequency scaling corresponding to the associated time scaling.
So the mapping from the impulse train spectrum to the discreet-time spectrum corresponds to a mapping specified by capital omega equal to small omega times capital T.
And equivalently, it's the frequency 2 pi over capital T,
which is, of course, the sampling frequency which gets normalized to the frequency 2 pi.
And so, in the frequency domain, there is a frequency normalization associated with the fact that corresponding to this spectrum is a time sequence or discreet-time sequence, as I showed previously, and the discreet-time sequence is related to the original continuous-time signal through a time normalization.
In other words, these sequence values are simply samples of the continuous-time signal with the time axis renormalized.
Now, what we want to consider--
this is the conversion from continuous-time to discreet-time-- what we want to consider now is the overall system which implements not just the conversion, but filtering, and then coming back out of the conversion back to continuous-time.
So let's look at the overall system.
And the overall system, of course, as I've stressed several times in the past, consists of first, the sampling process, conversion to an impulse train, and the impulse train converted to a sequence.
That sequence is then processed through our discreet-time filter.
And after the discreet-time time processing, the result of that is converted back to an impulse train.
So this resulting process sequence is then converted back to an impulse train.
And then, finally, we carry out the desampling process by simply using a low-pass filter with a cutoff associated with the sampling frequency that we used.
Now, typically in a system like that-- which implements discreet-time processing of continuous-time time signals-- we need to ensure in one way or another that the bandwidth of the input is sufficiently limited, so that we avoid aliasing.
One way to do that is to force it one way or another, or simply know that our signal satisfies the bandwidth constraint.
Although, a fairly typical thing to do in addition to this sampling process is to include what is referred to an anti-aliasing filter.
In other words, this is a filter that would band-limit the input at at least half the sampling frequency, so that we are guaranteed, then, that there is no aliasing that's carried out in this process.
And it's important to stress that, in this kind of processing-- discreet-time processing of continuous-time signals-- except in certain special situations, it's very important to avoid aliasing because we're going to want to do a reconstruction after we do the sampling and processing.
OK, now, this is the sequence of steps in the time domain.
Let's examine what happens as a consequence of this in the frequency domain.
Well, let's choose some type of simple representative spectrum.
And, of course, what's important about it is that the spectrum we choose is band-limited, or that there's an anti-aliasing filter.
And it's not the shape, of course, that is critical.
And as we work our way through the system, this is the continuous-time spectrum.
After sampling, that spectrum is replicated at multiples of the sampling frequency-- integer multiples of the sampling frequency-- and so there would be another one over here, and another one over here, et cetera.
And then, in converting to a discreet-time sequence, there is the associated frequency normalization, so that the sampling frequency gets normalized to a frequency of 2 pi.
OK, now, at that point, where we are in the system is at this point, so that we've converted to a sequence.
We now want to carry out some filtering, and then, after that filtering, convert back to a continuous-time signal.
All right, so, here we are at the spectrum associated with the sequence.
And now, the processing that we're carrying out is linear time and variant filtering in the discreet-time domain.
And what that corresponds to, then, is multiplying this spectrum by the filter frequency response.
And I've chosen a particular shape.
And again, it's not the shape that's important to the discussion, but the fact, for example, that it has a particular cutoff frequency, which we will track as we work through this.
And so now, the spectrum of y of n, the output of the digital filter, is the product of this spectrum, and the Fourier transform, or frequency response, of the digital filter.
Now, in working our way through, we're going to take the output of the filter and undo the two-step process.
So we now want to take that sequence, convert it to an impulse train, and then take that impulse train and desample through a low-pass filter.
So, here we are now at the output of the digital filter.
We then convert that to an impulse train.
Well that's really undoing the original time normalization.
And so, what that means, is that we are undoing the frequency normalization.
In particular, we're dividing the frequency axis by capital T. Whereas, this point in y of omega was 2 pi, now it's 2 pi over capital T. What that means is that, equivalently, we're multiplying this spectrum by the frequency response of the digital filter, but now linearly-scaled in frequency, so that what was a cutoff frequency of omega sub c is now cutoff frequency of omega sub c, divided by capital T.
So now, the next step in the process is the reconstructing low-pass filter.
And what that extracts is simply the portion of this periodic spectrum around the origin.
And so finally, then, the spectrum of the output of the overall system will be the spectrum of the input multiplied by a frequency response, which is the digital filter frequency response frequency scaled by dividing that digital filter frequency axis by capital T.
OK, now, what we can ask is, we've got this processing--
we've converted to discreet-time, and we've gone back to continuous-time, and one can ask now what equivalent, overall continuous-time system does that correspond to?
In other words, if we--
that, of course, is a continuous-time system, it's a continuous-time input and continuous-time time output-- and the overall system, then, would be one that would give us exactly the same output spectrum as we're getting.
Well, what is that?
What we have is an output spectrum, which is the product of the input spectrum and the digital filter frequency characteristic frequency-scaled.
And so, in fact, the resulting continuous-time time filter is simply the digital filter with an appropriate frequency scaling.
In other words, with the frequency axis divided by capital T. So said another way, if we show here the frequency response of the original digital filter, then the corresponding continuous-time filter would be this, frequency-scaled.
And then, because of the associated low-pass filtering and the reconstruction, we would select out just one of these periods-- in particular, the portion around the origin.
And the essential consequence of that is that the corresponding continuous-time filter, then, is given by this.
And these two are related simply by a linear scaling of the frequency axis.
And note that, where the digital filter has a cutoff frequency of omega sub c, the continuous-time filter has a cutoff frequency of omega sub c divided by capital T.
So that's the linear frequency scaling.
And, by the way, plant away for now-- and we'll return to this point later-- the observation that even if the digital filter frequency response is fixed, which we would assume it is, by changing the sampling frequency, in fact, what we're able to do is affect a linear scaling all of the equivalent continuous-time filter.
OK, well, this is pretty much the process and the analysis,
but to highlight a number of the issues and emphasize these points, what I'd like to do is illustrate some of this with a videotape demonstration that, in fact, was made originally as part of another course-- a course devoted entirely to digital signal processing, which essentially is discreet-time time processing, whether or not it's related to continuous-time signals.
And what I'd like to now focus on are some of the details of that demonstration.
In the demonstration, the specific impulse response that is used for the digital filter, or discreet-time filter, is the one that I show here.
And the associated frequency response is the frequency response of a discreet-time, low-pass filter, as I indicate below.
And the cutoff frequency of that filter--
as I indicate, the filter was designed as a discreet-time filter with a cutoff frequency of pi over 5.
And let me just draw your attention to the fact that pi over 5 is also a 10th of 2 pi.
And so in fact the digital or discreet-time filter cutoff frequency is a 10th of 2 pi.
And as I'll stress again shortly, remember that, in the frequency normalization or unnormalization, 2 pi represents, in effect, the sampling frequency.
And so the consequence of that is that the cutoff frequency really, is going to be associated with a 10th of the sampling frequency.
But, for now, keep in mind that it's just simply a 10th of 2 pi.
Now, the equivalent continuous-time system, in terms of the impulse response, is, of course, a band-limited interpolation of the impulse response associated with the discreet-time filter.
And in the frequency domain, the frequency response is correspondingly a time-scaled, frequency scaled version of the frequency response.
So, in fact, in the frequency domain and in the time domain related to the continuous-time signal, the associated impulse response is what I indicate here-- a band-limited interpolation of the discreet-time impulse response and time scale, in fact.
And the frequency response--
following the discussion that we've previously gone through-- is a frequency-scaled version of the one associated with the digital filter.
Well, the first thing that I'll want to look at is the impulse response.
And when we do, let me just indicate that in the actual implementation things are slightly different than they are associated with the ideal analysis.
In particular, in converting from a discreet-time sequence to the continuous-time signal, whereas this way of looking at it is convenient in the context of the analysis, in fact, the way it's done is using a more or less standard digital to analog converter.
And what a digital to analog converter does, as I indicated in the previous lecture, is to convert the sequence not to an impulse train but, in fact, to go directly through a zero-order hold.
And so, usually what comes out of a digital to analog converter is a staircase type of signal associated with a zero-order hold.
And then, the result of that is low-pass filtered to do the reconstruction.
So what we'll want to look at, then, is that reconstruction, first with just an impulse input.
And so, what we'll see after the low-pass filter, for the impulse response, is a smooth curve like this.
But also, as part of the demonstration, what I'll do,
just to show the zero-order hold, is to take the low-pass filter out temporarily and then put it back in.
So first, let's just look at the filter impulse response.
What we see here is the impulse response of the overall system.
And we observe, for one thing, that it's a symmetrical impulse response.
In other words, corresponds to a linear phase filter.
We could also look at the impulse response before the desampling low-pass filter-- let's take out the desampling low-pass filter slowly-- and what we observe is, basically, the output of the digital to analog converter.
Which, of course, is a staircase, or boxcar, function, not an impulse train.
In the real world, the output of a D to A converter, generally, is a boxcar type of function.
We can put the desampling filter back in now and notice that the effect of the desampling filter is, basically, to smooth out the rough edges in the boxcar output from the D to A converter.
OK, so, that's the impulse response of the system.
Now, what I'd like to show is the frequency response of the system.
And to measure the frequency response, of course, what we can do is put a sine wave into the system and look at the sinusoidal output.
So, in particular now, what will happen is that, with the system, we will put in a continuous-time sinusoid, which is sampled, converted to a sequence.
The sampled continuous-time sinusoid is a discreet-time sinusoid.
That goes through the digital filter and gets attenuated, or amplified appropriately.
And then the output of that is converted back-- and that's, again, a sinusoidal output-- that gets converted back to a continuous-time sinusoid.
Theoretically, as I indicate here-- but again, as we just saw, really, represented by a zero-order hold, followed by a low-pass filter.
So, that's the overall operation, with one modification from the diagram that we have here.
In this particular diagram I've included and anti-aliasing filter.
In fact, in the demonstration there is no anti-aliasing filter.
And so, in fact, the input is a sinusoidal input which is not band-limited by virtue of an anti-aliasing filter.
It's only, of course, band-limited appropriately if we choose the sinusoidal frequency that way.
So, there is no anti-aliasing filter, and this is the system.
And one consequence of that is that, in fact, if we sweep the input sinusoid only up to half the sampling frequency, we'll see no aliasing.
But if we let it sweep past that, we're going to get aliasing.
Now, in the demonstration, the sampling rate that's picked for this part of the demonstration is a 20 kilohertz sampling rate.
That means, based on the sampling theorem, that as long as the input frequency is below 10 kilohertz we get no aliasing.
When the input frequency goes beyond 10 kilohertz , that higher frequency is going to get aliased down into a lower frequency.
A consequence of that, then, is that as we go through the processing, and we demonstrate the frequency response of the system, what we'll see in the output is no aliasing when the input is below 10 kilohertz.
As the input sweeps past 10 kilohertz--
when we let it, which we will eventually in the demonstration-- then, in fact, that frequency, as it finally shows up here, will begin to be aliased down into a lower frequency.
Another way of thinking about that is that, when we watch the frequency response of the system, as we look at the digital filter frequency response, what we're sweeping as we go from 0 up to 10 kilohertz in the input frequency is this portion of the frequency response.
As we sweep from 10 kilohertz out to 20 kilohertz, what we'll see is this portion of the frequency response.
In other words, we'll see it periodically replicated.
Or, if we look at the corresponding continuous-time frequency response, what it means, really, is that sweeping from 0 to 10 kilohertz is moving up this way.
And then sweeping from 10 kilohertz to 20 kilohertz on the input, really because of the aliasing, reflects itself in the digital filter by looking back toward lower frequencies.
And so the continuous-time filter sweeps back down from 10 kilohertz back to 0.
OK, so, that's what we'll see, and we'll see it in several different ways as explained in the demonstration.
So now let's look at the frequency response of the filter.
Now what we'd like to illustrate is the frequency response of the equivalent continuous-time filter.
And we can do that by sweeping the filter with sinusoidal input.
So, what we'll see in this demonstration is, on the upper trace, the input sinusoid, on the lower trace, the output sinusoid, using a 20 kilohertz sampling rate, and a sweep from 0 to 10 kilohertz.
In other words, a sweep from 0 to, effectively, pi, in terms of the digital filter.
So what we'll observe as the input frequency increases, is that the output sinusoid will have, essentially, constant amplitude up to the cutoff frequency of the filter, and then approximately zero amplitude past.
So let's now sweep the filter frequency response.
And there is the filter cutoff frequency.
Now, we can also observe the filter frequency response in several other ways.
One way in which we can observe it is by looking,
also, at the amplitude of the output sinusoid as a function of frequency, rather than as a function of time.
And so we'll observe that on the left-hand scope.
While on the right-hand scope, we'll have the same trace the we just saw, namely two traces-- the upper trace is the inputs sinusoid, the lower trace is the output sinusoid.
And, in addition to observing the frequency response, let's also listen to the output sinusoid and observe the attenuation in the output as we go from the filter passband to the filter stopband.
Again, a 20 kilohertz sampling rate and a sweep range from 0 to 10 kilohertz.
Now, of course, we're in the filter stopband.
Now, if we increase the sweep range from 10 kilohertz the 20 kilohertz, so that the sweep range is equal to the sampling frequency, in essence, that corresponds to sweeping out the digital filter from 0 to 2 pi.
And, in that case, we'll begin to see some of the periodicity in the digital filter frequency response.
So let's do that now with a 20 kilohertz sampling rate and a sweep range of 0 to 20 kilohertz.
Now as we come near 2 pi, we get back the past-band.
And, finally, back to a 0 to 10 kilohertz sweep, so that we're again sweeping only from 0 to pi with regard to the digital filter.
Now, an important observation is that, with the digital or discreet-time filter cutoff frequency fixed as I've indicated here-- and I remind you that what the cutoff frequency is, is a 10th of 2 pi-- with that cutoff frequency fixed, because of the normalization that we get as we come back to a continuous-time filter, in fact, what we have is a cutoff frequency that is dependent on the sampling frequency or on the sampling period.
And, more specifically, since the discreet-time, or digital,
filter or has a cutoff frequency which is a 10th of 2 pi, the normalization, as you recall, is that 2 pi, in discreet-time frequency, corresponds to omega sub s, the sampling frequency, in terms of continuous-time frequency.
The consequence is that this cutoff frequency, in fact, is 1/10 of-- not 2 pi now because of the normalization-- it's 1/10 of the sampling frequency.
So, consequently, as we change the sampling frequency, what will happen is that, even with the discreet-time filter cutoff fixed, the cutoff frequency of the equivalent continuous-time filter will change.
Now, that's what I want to demonstrate.
But let me again stress and ask you to keep in mind that this demonstration is done without an anti-aliasing filter in.
And we are going to be changing the sampling frequency and, so keep in mind that, as we look at this, as the input frequency sweeps past half the sampling frequency-- whatever sampling frequency we happen to be looking at-- then, because of the fact that there's no anti-aliasing filter we'll get aliasing.
In other words, the frequency and the digital filter, or discreet-time filter, as we sweep the input frequency up, will move up in frequency until we get past half the sampling frequency and then essentially will move back down in frequency.
Consequently, what we'll get, then--
or what we'll see-- are periodic replications of the frequency response when we swept past half the sampling frequency.
All right, so now, let's look at the same digital filter,
but the frequency response, as we change the sampling frequency.
Now, what we would like to demonstrate is the effect of changing the sampling frequency.
And we know that the effective filter cutoff frequency is tied to the sampling frequency and, for this particular filter, corresponds to a 10th of the sampling frequency.
Consequently, if we double the sampling frequency, we should double the effective filter passband width, or double the filter cutoff frequency.
And, so, let's do that now.
Again a 0 to 10 kilohertz sweep range, but a 40 kilohertz sampling frequency.
And we should observe that the filter cutoff frequency has now doubled out to four kilohertz.
Now, let's begin to decrease the filter sampling frequency.
So from 40, let's change the sampling frequency to 20 kilohertz.
We should see the cutoff frequency cut in half.
Now, we can go even further.
We can cut the sampling frequency down to 10 kilohertz.
And remember that the sweep range is 0 to 10 kilohertz.
So now we'll be sweeping from 0 to 2 pi.
So as we get close to 2 pi, we'll see the passband again.
And, now, let's cut down the sampling frequency even further, to 5 kilohertz.
Here we are at 2 pi.
And then at 4pi.
All right, so, that illustrates the effect of changing the sampling frequency.
Now let's conclude this demonstration of the effect of the sampling frequency on the filter cutoff frequency by carrying out some filtering on some live audio.
What we'll watch, in this case, is the output audio waveform as a function of time on the single tray scope, and also we'll listen to the output.
We'll begin it with a 40 kilohertz sampling rate, then reduce that to 20 kilohertz, 10 kilohertz, and then 5 kilohertz.
And in each of those cases, the effective filter cutoff frequency, then, is cut in half from 4 kilohertz, to 2 kilohertz, to 1 kilohertz, and then to 500 cycles.
So let's begin with a 40 kilohertz sampling frequency, or an effective filter cutoff frequency of 4 kilohertz.
Now, let's reduce that a 20 kilohertz sampling frequency, or a 2 kilohertz filter.
Then a 10 kilohertz sampling frequency.
And, finally, a 5 kilohertz sampling frequency corresponding to a 500 cycle equivalent analog filter.
Alright, now, let's finally conclude by returning to a little higher quality ragtime by changing the sampling frequency back to 40 kilohertz.
Alright, well, hopefully what you've seen in the demonstration and in this lecture gives you a sense and a feeling for the analysis and the use of discreet-time filters for processing continuous-time signals.
And as you may be aware, and as I've tried to indicate previously in the past, this, in fact, is one very important-- but not the only-- but one very important context in which discreet-time filtering is used.
And this, in fact, is an area that is developing rapidly because of the fact that microprocessors, digital technology, computers, et cetera afford considerable flexibility in carrying out digital processing of signals.
And when digital processing is used, that naturally corresponds to implementing the processing and analyzing it in discreet-time.
Now, in the next lecture we'll be continuing on another aspect-- developing another aspect-- of sampling.
And, in particular, what we'll be talking about is sampling of discreet-time signals.
As I'll indicate there, one of the contexts in which discreet-time sampling, in fact, plays an important role is in the context in which we are processing continuous-time signals using discreet-time processing.
Where, in fact, one step that we might want to take-- in addition to the steps so we've talked about here-- is an additional sampling process following whatever kinds of filtering that we do.
Well, that's a discussion and a topic that we'll be going into in the next lecture.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
In the last lecture, we discussed discrete time processing of continuous time signals.
And, as you know, the basis for that arises essentially out of a sampling theorem.
Now in that context, and also in its own right, another important sampling issue is the sampling of discrete time signals, in other words, the sampling of a sequence.
One common context in which this arises, for example, is,
if we've converted from a continuous time signal to a sequence, and we then carry out some additional filtering, then there's the possibility that we can resample that sequence, and as we'll see as we go through the discussion, save something in the way of storage or whatever.
So discrete time sampling, as I indicated, has important application in a context referred to here, namely resampling after discrete time filtering.
And closely related to that, as we'll indicate in this lecture, is the concept of using discrete time sampling for what's referred to as sampling rate conversion.
And also closely associated with both of those ideas is a set of ideas that I'll bring up in today's lecture, referred to as decimation and interpolation of discrete time signals or sequences.
Now the basic process for discrete time sampling is the same as it is for continuous time sampling.
Namely, we can analyze it and set it up on the basis of multiplying or modulating a discrete time signal by an impulse train, the impulse train essentially, or pulse train, pulling out sequence values at the times that we want to sample.
So the basic block diagram for the sampling process is to modulate or multiply the sequence that we want to sample by an impulse train.
And here, the impulse train has impulses spaced by integer multiples of capital N. This then becomes the sampling period.
And the result of that modulation is then the sample sequence x of p of n.
So if we just look at what a sequence and a sampled version of that sequence might look like, what we have here is an original sequence x of n.
And then we have the sampling impulse train, or sampling sequence, and it's the modulation or product of these two that gives us the sample sequence x of p of n.
And so, as you can see, multiplying this by this essentially pulls out of the original sequence sample values at the times that this pulse train is on.
And of course here, I've drawn this for the case where capital N, the sampling period, is equal to 3.
Now the analysis of discrete time sampling is very similar to the analysis of continuous time sampling.
And let's just quickly look through the steps that are involved.
We're modulating or multiplying in the time domain.
And what that corresponds to in the frequency domain is a convolution.
And so the spectrum of the sampled sequence is the periodic convolution of the spectrum of the sampling sequence and the spectrum of the sequence that we're sampling.
And since the sampling sequence is an impulse train,
as we know, the Fourier transform of an impulse train is itself an impulse train.
And so this is the Fourier transform of the sampling sequence.
And now, finally, the Fourier transform of the resulting sample sequence, being the convolution of this with the Fourier transform of the sequence that we're sampling,
gives us then a spectrum which consists of a sum of replicated versions of the Fourier transform of the sequence that we're sampling.
In other words, what we're doing, very much as we did in continuous time, is, through the sampling process when we look at it in the frequency domain, taking the spectrum of the sequence there were sampling and shifting it and then adding it in-- shifting it by integer multiples of the sampling frequency.
In particular, looking back at this equation, what we recognize is that this term, k times 2 pi over capital N, is in fact an integer multiple of the sampling frequency.
And the same thing is true here.
This is k times omega sub s, where omega sub s, the sampling frequency, is 2 pi divided by capital N.
All right, so now let's look at what this means pictorially or graphically in the frequency domain.
And as you can imagine, since the analysis and algebra is similar to what happens in continuous time, we would expect the pictures to more or less be identical to what we've seen previously for continuous time.
And indeed that's the case.
So here we have the spectrum of the signal that's we're sampling.
This is its Fourier transform, with an assumed highest frequency omega sub m, highest frequency over a 2pi range, or over a range of pi, rather.
And now the spectrum of the sampling signal is what I show below, which is an impulse train with impulses occurring at integer multiples of the sampling frequency.
And then finally, the convolution of these two is simply this one replicated at the locations of these impulses.
And so that's finally what I show below.
And here I made one particular choice for the sampling period.
This in particular corresponds to a sampling period which is capital N equal to 3.
And so the sampling frequency, omega sub s, is 2pi divided by 3.
Now when we look at this, what we recognize is that we have basically the same issue here as we had in continuous time, in the sense that when these individual replications of the Fourier transform, when the sampling frequency is chosen high enough so that they don't overlap, then we see the potential for being able to get one of them back.
On the other hand, when they do overlap then what we'll have is aliasing, in particular, discrete time aliasing, much as we had continuous time aliasing in the continuous time case.
Well notice in this picture that what we have is we've chosen this picture so that omega sub s minus omega sub m is greater than omega sub m, or equivalently, so that omega sub s is greater than 2 omega sub m.
And so with omega sub s greater than 2 omega sum m, that corresponds to this picture.
Whereas, if that condition is violated then, in fact, the picture that we would have is a picture that looks like.
And in this picture, the individual replications of the Fourier transform of the original signal overlap.
And we can no longer recover the Fourier transform of the original signal.
And this, just as it was in continuous time, is referred to as aliasing.
Now let's look more closely at the situation in which there is no aliasing.
So in that case, what we have is a Fourier transform for the sampled signal, which is as I indicated here, and the Fourier transform for the original signal, as I indicate at the top.
And the question now is how do we recover this one from this one.
Well, the way that we do that, just as we did in a continuous time case, is by a low pass filtering.
In particular, processing in the time domain or in the frequency domain, this with an ideal low pass filter has the effect of extracting that part of the spectrum that in fact we identify with the original signal that we began with.
So what we see, again, is that the process is very much the same.
As long as there's no aliasing, we can recover the original signal by ideal low pass filtering.
So the overall system is, just to reiterate, a system which consists of modulating the original sequence with a pulse train or impulse train.
And then that is going to be processed with a low pass filter.
The spectrum of the original signal x of n is what I show here.
The spectrum of the sampled signal, where I'm drawing the picture on the assumption that the sampling period is 3, is now what's indicated, where these are replicated, where the original spectrum is replicated.
This is now processed through a filter which, for exact reconstruction, is an ideal low pass filter.
And so we would multiply this spectrum by this one.
And the result, after doing that, will generate a reconstructed spectrum which, in fact, is identical to the original.
So the frequency domain picture is the same.
And what we would expect then is that the time domain picture would be the same.
Well, let's in fact look at the time domain.
And in the time domain, what we have is an analysis more or less identical to what we had in continuous time.
We of course have the same system.
And in the time domain, we are multiplying by an impulse train.
Consequently, the sample sequence is an impulse train whose values are samples of x of at integer multiples of capital N. For the reconstruction, this is now processed through an ideal low pass filter.
And that implements a convolution in the time domain.
And so the reconstructed signal is the convolution of the sample sequence and the filter impulse response.
And expressed another way, namely writing out the convolution as a sum, we have this expression.
And so it says then that the reconstruction is carried out by replacing the impulses here, these impulses, by versions of the filter impulse response.
Well, if the filter is an ideal low pass filter, then that corresponds in the time domain to sine nx over sine x kind of function.
And that is the interpolation in between the samples to do the reconstruction.
Also, as is discussed somewhat in the text, we can consider other kinds of interpolation, for example discrete time zero order hold or discrete time first order hold, just as we had in continuous time.
And the issues and analysis for the discrete times zero order hold and first order hold are very similar to what they were in continuous time-- the zero order hold just simply holding the value until the next sampling instant, and the first order hold carrying out linear interpolation in between the samples.
Now in this sampling process, if we look again at the wave forms involved, or sequences involved, the process consisted of taking a sequence and extracting from it individual values.
And in between those values, we have sequence values equal to 0.
So what we're doing in this case is retaining the same number of sequence values and simply setting some number of them equal to 0.
Well, let's say, for example, that we want to carry out sampling.
And what we're talking about is a sequence.
And let's say this sequence is stored in a computer memory.
As you can imagine, the notion of sampling it and actually replacing some of the values by zero is somewhat inefficient.
Namely, it doesn't make sense to think of storing in the memory a lot of zeros, when in fact those are zeros that we can always put back in.
We know exactly what the values are.
And if we know what the sampling rate was in discrete time, then we would know when and how to put the zeros back in.
So actually, in discrete time sampling, what we've talked about so far is really only one part or one step in the process.
Basically, the other step is to take those zeros and just throw them away because we could put them in any time we want to and really only retain, for example in our computer memory or list of sequence values or whatever, only retain the non-zero values.
So that process and the resulting sequence that we end up with is associated with a concept called decimation.
What I mean by decimation is very simple.
What we're doing is, instead of working with this sequence, we're going to work with this sequence.
Namely, we'll toss out the zeros in between here and collapse the sequence down only to the sequence values that are associated with the original x of n.
Now, in talking about a decimated sequence, we could of course do that directly from this step down to here, although again in the analysis it will be somewhat more convenient to carry that out by thinking, at least analytically, in terms of a 2-step process-- one being a sampling process, then the other being a decimation.
But basically, this is a decimated version of that.
Now for the grammatical purists out there, the word decimation of course means taking every tenth one.
The implication is not that we're always sampling with a period of 10.
The idea of decimating is to pick out every nth sample and end up with a collapsed sequence.
Let's now look at a little bit of the analysis and understand what the consequence is in the frequency domain.
In particular what we want to develop is how the Fourier transform of the decimated sequence is related to the Fourier transform of the original sequence or the sample sequence.
So let's look at this in the frequency domain.
So what we have is a decimated sequence, which consists of pulling out every capital Nth value of x of n.
And of course that's the same as we can either decimate x of n or we can decimate the sample signal.
Now in going through this analysis, I'll kind of go through it quickly because again there's the issue of some slight mental gymnastics.
And if you're anything like I am, it's usually best to kind of try to absorb that by yourself quietly, rather than having somebody throw it at you.
Let me say, though, that the steps that I'm following here are slightly different than the steps that I use in the text.
It's a slightly different way of going through the analysis.
I guess you could say for one thing that if we've gone through it twice, and it comes out the same, well of course it has to be right.
Well anyway, here we have then the relationship between the decimated sequence, the original sequence, and the sampled sequence.
And we know of course that the Fourier transform of the sample sequence is just simply this summation.
And now kind of the idea in the analysis is that we can collapse this summation by recognizing that this term is only non-zero at every nth value.
And so if we do that, essentially making a substitution of variables with n equal to small m times capital N, we can turn this into a summation on m.
And that's what I've done here.
And we've just simply used the fact that we can collapse the sum because of the fact that all but every nth value is equal to zero.
So this then is the Fourier transform all of the sampled signal.
And now if we look at the Fourier transform of the decimated signal, that Fourier transform, of course, is this summation on the decimated sequence.
Well, what we want to look at is the correspondence between this equation and the one above it.
So we want to compare this equation to this one.
And recognizing that this decimated sequence is just simply related to the sample sequence this way, these two become equal under a substitution of variables.
In particular, notice that if we replace in this equation omega by omega times capital N, then these two equations become equal.
So the consequence of that, then, what it all boils down to and says, is that the relationship between the Fourier transform of the decimated sequence and the Fourier transform of the sampled sequence is simply a frequency scaling corresponding to dividing the frequency axis by capital N.
So that's essentially what happens.
That's really all that's involved in the decimation process.
And now, again, let's look at that pictorially and see what it means.
So what we want to look at, now that we've looked in the time domain in this particular view graph, we now want to look in the frequency domain.
And in the frequency domain, we have, again, the Fourier transform of the original sequence and we have the Fourier transform of the sampled sequence.
And now the Fourier transform of the decimated sequence is simply this spectrum with a linear frequency scaling.
And in particular, it simply corresponds to multiplying this frequency axis by capital N. And notice that this frequency now, 2 pi over capital N, that frequency ends up getting rescaled to a frequency of 2 pi.
So in fact now, in the rescaling, it's that this point in the decimation gets rescaled to this point.
And correspondingly, of course, this whole spectrum broadens out.
Now we can also look at that in the context of the original spectrum.
And you can see that the relationship between the original spectrum and the spectrum of the decimated signal corresponds to simply linearly scaling this.
But it's important also to keep in mind that that analysis, that particular relationship, assumes that we've avoided aliasing.
The relationship between the spectrum of the decimated signal and the spectrum of the sample signal is true whether or not we have aliasing.
But being able to clearly associate it with just simply scaling of this spectrum of the original signal assumes that the spectrum of the original signal, the shape of it, is preserved when we generate the sample signal.
Well, when might discrete time sampling, and for that matter, decimation, be used?
Well, I indicated one context in which it might be useful at the beginning of this lecture.
And let me now focus in on that a little more specifically.
In particular, suppose that we have gone through a process in which the continuous time signal has been converted to a discrete time signal.
And we then carry out some additional discrete time filtering.
So we have a situation where we've gone through a continuous to discrete time conversion.
And after that conversion, we carry out some discrete time filtering.
And in particular, in going through this part of the process, we choose the sampling rate for going from the continuous time signal to the sequence so that we don't violate the sampling theorem.
Well let's suppose, then, that this is the spectrum of the continuous time signal.
Below it, we have the spectrum of the output of the continuous to discrete time conversion.
And I've chosen the sampling frequency to be just high enough so that I avoid aliasing.
Well that then is the lowest sampling frequency I can pick.
But now, if we go through some additional low pass filtering, then let's see what happens.
If I now low pass filter the sequence x of n, then in effect, I'm multiplying the sequence spectrum by this filter.
And so the result of that, the product of the filter frequency response and the Fourier transform of x of n would have a shape somewhat like I indicate below.
Now notice that in this spectrum, although in the input to the filter this entire band was filled up, in the output of the filter, there is a band that in fact has zero energy in it.
So what I can consider doing is taking the output sequence from the filter and in fact resampling it, in other words sampling it, which would be more or less associated with a different sampling rate for the continuous time signals involved.
So I could now go through a process which is commonly referred to as down sampling that is lowering the sampling rate.
When we do that, of course, what's going to happen is that in fact this spectral energy will now fill out more of the band.
And for example, if this was a third, then in fact if I down sampled by a factor of three, then I would fill up the entire band with this energy.
But since I've done some additional low pass filtering, as I indicate here, there's no problem with aliasing.
If I had, let's say, down sampled by a factor of three and I'm now taking that signal and converting it back to a continuous time signal, then of course the way I can do that is by simply running my output clock for the discrete to continuous time converter.
I can run my output clock at a third the rate of the input clock.
And that, in effect, takes care of the bookkeeping for me.
So here we have now the notion of sampling a sequence, and very closely tied in with that, the notion of decimating a sequence, and related to both of those, the notion of down sampling, that is changing the sampling rates so that, if we were trying this in with continuous time signals, we've essentially changed our clock rate.
And we might also want to, and it's important to, consider the opposite of that.
So now a question is what's the opposite of decimation.
Suppose that we had a sequence and we decimate it.
Thinking about it as a 2-step process, that would correspond to first multiplying by an impulse train, where there are bunch of zeros in there, and then choosing, throwing away the zeros and keeping only the values that are non-zero, because the zeros we can always recreate.
Well, in fact, the inverse process is very specifically a process of recreating the zeros and then doing the desampling.
So in the opposite operation, what we would do is undo the decimation step.
And that would consist of converting the decimated sequence back to an impulse train and then processing that impulse train by an ideal low pass filter to do the interpolation or reconstruction, filling in the values which, in this impulse train, are equal to zero.
So we now have the two steps.
We take the decimated sequence and we expand it out, putting in zeros.
And then we desample that by processing it through a low pass filter.
So just kind of looking at sequences again, what we have is an original sequence, the sequence x of n.
And then the sample sequence is simply a sequence which alternates, in this particular case, those sequence values was zero.
Here what we're assuming is that the sampling period is 2.
And so every other value here is equal to zero.
The decimated sequence then is this sequence, collapsed as I show in the sequence above.
And so it's, in effect, time compressing the sample sequence or the original sequence so that we throw out the sequence values which were equal to zero in the sample sequence.
Now in recovering the original sequence from the decimated sequence, we can think of a 2-step process.
Namely, we spread this out alternating with zeros, and again, keeping in mind that this is drawn for the case where capital N is 2.
And then finally, we interpolate between the non-zero values here by going through a low pass filter to reconstruct the original sequence.
And that's what we show finally on the bottom curve.
So that's what we would see in the time domain.
Let's look at what we would see in the frequency domain.
In the frequency domain, we have to begin with the sequence on the bottom, or the spectrum on the bottom, which would correspond to the original spectrum.
Then, through the sampling process, that is periodically replicated.
Again, this is drawn on the assumption that the sampling frequency is pi or the sampling period is equal to 2.
And so this is now replicated.
And then, in going from this to the spectrum of the decimated sequence, we would rescale the frequency axis so that the frequency pi now gets rescaled in the spectrum for the decimated sequence to a frequency which is 2 pi.
And so this now is the spectrum of the decimated sequence.
If we now want to reconvert to the original sequence we would first intersperse in the time domain with zeros, corresponding to compressing in the frequency domain.
This would then be low pass filtered.
And the low pass filtering would consist of throwing away this replication, accounting for a factor which is the factor capital N, and extracting the portion of the spectrum which is associated with the spectrum of the original signal which we began with.
So once again, we have decimation and interpolation.
And the decimation can be thought of as a time compression that corresponds to a frequency expansion then.
And the interpolation process is then just the reverse.
Now there are lots of situations in which decimation and interpolation and discrete time sampling are useful.
And one context that I just want to quickly draw your attention to is the use of decimation and interpolation in what is commonly referred to as sampling rate conversion.
What the basic issue and sampling rate conversion is is that, in some situations, and a very common one is digital audio, a continuous time signal is sampled.
And those sampled values are stored or whatever.
And kind of the notion is that, perhaps when that is played back, it's played back through a different system.
And the different system has a different assumed sampling frequency or sampling period.
So that's kind of the issue and the idea.
We have, let's say, a continuous time signal which we've converted to a sequence through a sampling process using an assumed sampling period of T1.
And these sequence values may then, for example, be put into digital storage.
In the case of a digital audio system, it may, for example, go onto a digital record.
And it might be the output of this that we want to recreate.
Or we might in fact follow that with some additional processing, whatever that additional processing is.
And I'll kind of put a question mark in there because we don't know exactly what that might be.
And then, in any case, the result of that is going to be converted back to a continuous time signal.
But it might be converted through a system that has a different assumed sampling period.
And so a very common issue, and it comes up as I indicated particularly in digital audio, a very common issue is to be able to convert from one assumed sampling period, T1, our sampling frequency, to another assumed sampling period.
Now how do we do that?
Well in fact, we do that by using the ideas of decimation and interpolation.
In particular, if we had, for example, a situation where we wanted to convert from a sampling period, T1, to a sampling period which was twice as long as that, then essentially, we're going to take the sequence and process it in a way that would, in effect, correspond to assuming that we had sampled at half the original frequency.
Well how do we do that?
The way we do it is we take the sequence we have and we just throw away every other value.
So in that case, we would then, for this sampling rate conversion, down sample and decimate.
Or actually, we might not go through this step formally.
We might just simply decimate.
Now we might have an alternative situation where in fact the new sampling period, or the sampling period of the output, is half the sampling period of the input, corresponding to an assumed sampling frequency, which is twice as high.
And in that case, then., we would go through a process of interpolation.
And in particular, we would up sample and interpolate by a factor of 2 to one.
So in one case, we're simply throwing away every other value.
In the other case, what we're going to do is take our sequence, put in zeros, put it through a low pass filter to interpolate.
Now life would be simple if everything happened in simple integer amounts like that.
A more common situation is that we may have an assumed output sampling period which is 3/2 of the input sampling period.
And now the question is what are we going to do to convert from this sampling period to this sampling period.
Well, in fact, the answer to that is to use a combination of down sampling and up sampling, or up sampling and down sampling, equivalently interpolation and decimation.
And for this particular case, in fact, what we would do is to first take the data, up sample by a factor of 2, and then down sample the result of that by a factor of 3.
And what that would give us is a sampling rate conversion, overall, of 3/2, or a sampling period conversion of 3/2.
And more generally, what you could think of is how you might do this if, in general, the relationship between the input and output sampling periods was some rational number p/q.
And so in fact, in many systems, in hardware systems related to digital audio, very often the sampling rate conversion, most typically the sampling rate conversion, is done through a process of up sampling or interpolating and then down sampling by some other amount.
Now what we've seen, what we talked about in a set of lectures, is the concepts of sampling a signal.
And what we've seen is that the signal can be represented by samples under certain conditions.
And the sampling that we've been talking about is sampling in the time domain.
And we've done that for continuous time and we've done it for discrete time.
Now we know that there is some type of duality both continuous time and discrete time, some type of duality, between the time domain and frequency domain.
And so, as you can imagine, we can also talk about sampling in the frequency domain and expect that, more or less, the kinds of properties and analysis will be similar to those related to sampling in the time domain.
Well I want to talk just briefly about that and leave the more detailed discussion to the text and video course manual.
But let me indicate, for example, one context in which frequency domain sampling is important.
Suppose that you have a signal and what you'd like to measure is its Fourier transform, its spectrum.
Well of course, if you want to measure it or calculate it, you can never do that exactly at every single frequency.
There are too many frequencies, namely, an infinite number of them.
And so, in fact, all that you can really calculate or measure is the Fourier transform at a set of sample frequencies.
So essentially, if you are going to look at a spectrum,
continuous time or discrete time, you can only really look at samples.
And a reasonable question to ask, then, is when does a set of samples in fact tell you everything that there is to know about the Fourier transform.
That, and the answer to that, is very closely related to the concept of frequency domain sampling.
Well, frequency domain sampling, just to kind of introduce the topic, corresponds and can be analyzed in terms doing modulation in the frequency domain, very much like the modulation that we carried out in the time domain for time domain sampling.
And so we would multiply the Fourier transform of the signal whose spectrum is to be sampled by an impulse train in frequency.
And so shown below is what might be a representative spectrum for the input signal.
And the spectrum, then for the signal associated with the frequency domain sampling, consists of multiplying the frequency domain by this impulse train.
Or correspondingly, the Fourier transform of the resulting signal is an impulse train in frequency with an envelope which is the original spectrum that we were sampling.
Well, this of course is what we would do in the frequency domain.
It's modulation by an impulse train.
What does this mean in the time domain?
Well, let's see.
Multiplication in the time domain is convolution in the frequency domain.
Convolution in the frequency domain is multiplication--
I'm sorry.
Multiplication in the frequency domain, then, is convolution in the time domain.
And in fact, the process in the time domain is a convolution process.
Namely, the time domain signal is replicated at integer amounts of a particular time associated with the spacing in frequency under which we're doing the frequency domain sampling.
So in fact, if we look at this in the time domain, the resulting picture corresponds to an original signal whose spectrum or Fourier transform we've sampled.
And a consequence of the sampling is that the associated time domain signal is just like the original signal, but periodically replicated, in time now, not frequency, but in time, at integer multiples of 2 pi divided by the spectral sampling interval omega 0.
And so this then is the time function associated with the sample frequency function.
Now, that's not surprising because what we've done is generated an impulse train and frequency with a certain envelope.
We know that an impulse train in frequency is the Fourier transform of a periodic time function.
And so in fact, we have a periodic time function.
We also know that the envelope of those impulses--
we know this from way back when we talked about Fourier transforms-- the envelope, in fact, is the Fourier transform of one period.
And so all of this, of course, fits together as it should in a consistent way.
Now given that we have this periodic time function whose Fourier transform is the samples in the frequency domain, how do we get back the original time function?
Well, with time domain sampling, what we did was to multiply in the frequency domain by a gate, or window, to extract that part of the spectrum.
What we do here is exactly the same thing, namely multiply in the time domain by a time window which extracts just one period of this periodic signal, which would then give us back the original signal that we started with.
Now also let's keep in mind, going back to this time function and the relationship between them, then again, there is the potential, if this time function is too long in relation to 2 pi divided by omega 0, there's the potential for these to overlap.
And so what this means is that, in fact, what we can end up with, if the sample spacing and the frequency is not small enough, what we can end up with is an overlap in the replication in the time domain.
And what that corresponds to and what it's called is, in fact, time aliasing.
So we can have time aliasing with frequency domain sampling just as we can have frequency aliasing with time domain sampling.
Finally, let me just indicate very quickly that, although we're not going through this in any detail, the same basic idea applies in discrete time.
Namely, if we have a discrete time signal and if the discrete time signal is a finite length, if we sample its Fourier transform, the time function associated with those samples is a periodic replication.
And we can now extract, from this periodic signal, the original signal by multiplying by an appropriate time window, the product of that giving us the reconstructed time function as I indicate below.
So we've now seen a little bit of the notion of frequency domain sampling, as well as time domain sampling.
And let me stress that, although I haven't gone into this in a lot of detail, it's important.
It's used very often.
It's naturally important to understand it.
But, in fact, there is so much duality between the time domain and frequency domain, that a thorough understanding of time domain sampling just naturally leads to a thorough understanding of frequency domain sampling.
Now we've talked a lot about sampling.
And this now concludes our discussion of sampling.
I've stressed many times in the lectures associated with this that sampling is a very important topic in the context of our whole discussion, in part because it forms such an important bridge between continuous time and discrete time ideas.
And your picture now should kind of be a global one that sees how continuous time and discrete time fit together, not just analytically, but also practically.
Beginning in the next lecture, what I will introduce is the Laplace transform and, beyond that, the Z transform.
And what those will correspond to are generalizations of the Fourier transform.
So we now want to turn our attention back to some analytical tools, in particular developing some generalizations of the Fourier transform in both continuous time and discrete time.
And what we'll see is that those generalizations provide us with considerably enhanced flexibility in dealing with and analyzing both signals and linear time invariant systems.
Thank you.
NOISEEVENT
I'm Al Oppenheim, and I'd like to welcome you to this videotape course on signals and systems.
Signals, at least as an informal definition, are functions of one or more independent variables that typically carry some type of information.
Systems, in our setting, would typically be used to process signals.
One very common example of a signal might be, let's say, a speech signal.
And you might think of the air pressure as a function of time, or perhaps the electrical signal after it goes through the microphone transducer as a function of time, as representing the speech signal.
And we might see a typical speech signal looking something like I've indicated here.
It's a function of time, in this particular case.
And the independent variable, being time, is, in fact, continuous.
And so a signal like this, we will typically be referring to as a continuous time signal.
Now, it also, for this particular example, is a function of one independent variable.
And that will be referred to as a one-dimensional signal corresponding to the fact that there's only one independent variable instead of several independent variables.
So the speech signal is an example of a continuous time, one-dimensional signal.
Now, signals can, of course, be multi-dimensional.
And they may not have, as their independent variables, time variables.
One very common example are the examples represented by images.
Images, as signals, we might think of as representing brightness, as it varies in a horizontal and vertical direction.
And so the brightness as a function of these two spatial variables is then a two-dimensional signal.
And the independent variables would typically be continuous, but of course they're not time variables.
And incidentally, it's worth just commenting that very often, simply for convenience, we'll have a tendency to refer to the independent variables when we talk about signals as time variables, whether or not they really do represent time.
Well, let me illustrate one example of an image.
And this is a picture of J. B. J. Fourier, who, perhaps, more than anyone else, is responsible for the elegance and beauty of a lot of the concepts that we'll be talking about throughout this course.
And when you look at this, in addition to seeing Fourier himself, you should recognize that what you're looking at is basically a signal which is brightness as a function of the horizontal and vertical spatial variables.
As another example of an image as a signal, let's look at an aerial photograph.
This is an aerial photograph taken over a set of roads, which you can more or less recognize in the picture.
And one of the difficulties with this signal is that the road system is somewhat obscured by cloud cover.
And what I'll want to show later as an example of what a system might do to such a signal, in terms of processing it, is an attempt to at least compensate somewhat for the cloud cover that's represented in the photograph.
Although in terms of the detailed analysis that we go through during the course, our focus of attention is pretty much restricted to one-dimensional signals.
In fact, we will be using two-dimensional signals, more specifically images, very often to illustrate a variety of concepts.
Now, speech and images are examples of what we've referred to as continuous-time signals in that they are functions of continuous variables.
An equally important class of signals that we will be concentrating on in the course are signals that are discrete-time signals, where by discrete-time, what we mean is that the signal is a function of an integer variable, and so specifically only takes on values at integer values of the argument.
So here is a graphical illustration of a discrete-time signal.
And discrete-time signals arise in a variety of ways.
One very common example that is seen fairly often is discrete-time signals in the context of economic time series, for example, stock market analysis.
So what I show here is one very commonly occurring example of a discrete-time signal.
It represents the weekly stock market index.
The independent variable in this case is the week number.
And we see what the stock market is doing over this particular period as a function of the number of the week.
And, of course, along the vertical axis is the weekly index.
Incidentally, this particular period was not chosen at random.
It In fact captures a very interesting aspect of stock market history, namely the stock market crash in 1929, which, in fact, is represented by the behavior of this discrete-time signal, or sequence, in this particular area.
So this dramatic dip, in fact, is the stock market crash of 1929.
Well, the Dow Jones weekly average is an example of a one-dimensional discrete-time signal.
And just as with continuous time, we had not just one-dimensional but multi-dimensional signals, likewise we have multi-dimensional signals in the discrete-time case where, in that case, then, the discrete-time signal that we're talking about, or sequence, is a function of two integer variables.
And as one example, this might, let's say, represent a spatial antenna array where this is array number in, let's say, the horizontal direction, and this is array number in the vertical direction.
Both classes of signals, continuous-time and discrete-time, as I've indicated, are very important.
And it should be emphasized that the importance of discrete-time signals and associated processing continues to grow in large part because of the current and emerging technologies that permit, basically, the processing of continuous-time signals by first converting them to discrete-time signals and processing them with discrete-time systems.
And that, in fact, is a topic that we will discuss in a fair amount of detail later on in the course.
Let's now our attention to systems.
And as I indicated, a system basically processes signals.
And they have, of course, inputs and outputs.
And depending on whether we're talking about continuous time or discrete time, the system may be a continuous-time system or a discrete-time system.
So in the continuous-time case, I indicate here an input x(t) and an output y(t) If we were talking about a discrete-time system, I would represent the input in terms of a discrete-time variable, and, of course, the output in terms of a discrete-time variable also.
Now, in very general terms, systems are hard to deal with because they are defined very broadly and very generally.
And in dealing with systems and analyzing them, what we will do is attempt to exploit some very specific, and as we'll see, very useful system properties.
To indicate what I mean and how things might be split up, we could talk about systems, and will talk about systems, that are linear.
And we could divide systems, basically, into systems that are either linear or nonlinear, and we will, and also divide systems into systems that are what we'll refer to as time-invariant or time-varying systems.
And these aren't terms that we've defined yet, of course, but we will be defining in the course very shortly.
And while, in some sense, this division represents all systems, and this does, too, the focus of the course is really going to be principally on linear, time-invariant systems.
So it's basically these systems that we will be focusing on.
And we'll be referring to those systems as linear, time-invariant systems.
Well, as a brief glimpse at some of the kinds of things that systems can do, let me illustrate, first in a one-dimensional continuous-time context, and then later with a discrete-time example, one example of some processing of signals with an appropriate system.
The particular example that I want to illustrate relates to the restoration of old recordings.
And this is some work that was done by Professor Thomas Stockham, who is at the University of Utah, and work that he had done a number of years ago relating to the fact that in old recordings, for example in Caruso recordings, the recording was done through a mechanical horn, and the characteristics of the horn tended to vary from day to day.
And because of the characteristics of the horn,
the recording tended to have a muffled quality, something like this, sort of the sense that you would get if you were speaking through a megaphone.
What Professor Stockham did was develop a system to process these old recordings in such a way that a lot of the characteristics and distortion due to that recording system was removed.
So I'd like to illustrate that as one example of some signal processing with an appropriate continuous-time system.
And what you'll hear is a two-track recording.
On the first track is the original, unrestored Caruso recording, and on the second track is the result of the restoration.
And so as I switch back and forth from channel one to channel two, we'll be switching from the original to the restored.
We'll begin the tape by playing the original.
And then, as it proceeds, we'll switch.
So we'll begin on channel one.
NOISEEVENT That's the original recording.
And switch now to the processed.
NOISEEVENT Now let's switch back, back to the original.
Back to the restoration.
And once again, back to the original.
And presumably and hopefully, what you heard was that in the restoration, in fact, a lot of the muffled characteristics of the original recording were compensated for or removed.
Now one of the interesting things that happened, in fact,
in the work that Professor Stockham did is that in the process of the restoration-- and perhaps you heard this-- in the process of the restoration, in fact, some of the background noise on the recording was emphasized.
And so he processed the signal further in an attempt to remove that background noise.
And with that particular processing, the processing was very highly nonlinear.
A very interesting thing happened, which was that not only in that processing was the background noise removed, but somewhat surprisingly, also the orchestra was removed.
And let me just play that now as an example of some very sophisticated processing with a nonlinear system.
What you'll hear on channel one is the restoration as we had just played it.
When I switch to channel two, it will be after the processing with an attempt to remove the orchestra and the background noise.
Channel one now.
And now the noise and orchestra removed.
Back to channel one.
And finally, once again, with the orchestra removed.
So that's an example of processing of a continuous-time signal with a corresponding continuous-time system.
Now I'd like to illustrate an example of some processing on a discrete-time signal.
And I'd like to do that in the context of the example that I showed before of a discrete-time signal, which was the Dow Jones Industrial weekly stock market index.
I had shown it before, as I've shown it here again, over a period of slightly more than a year, where this is the number of weeks and this is the weekly index.
And to illustrate some of the processing, what I'd like to do is show the stock market index, the weekly index, over a much longer time period, in particular, the weekly index over a 10 year period.
And that's what I show here.
So what this covers is roughly 1927 to 1937.
And in this case, although this is still a discrete-time signal, just simply for the purposes of display, what we've done is to essentially connect the dots and draw a continuous curve through the points so that this picture isn't filled up with a lot of vertical lines.
So this is the discrete-time sequence that represents the weekly Dow Jones Index over a 10 year period.
And here, by the way, again, is the crash of 1929.
It's interesting to note, by the way, that actually the disaster in the stock market wasn't so much the 1929 crash but the long downward trend that followed that.
And you can see that here by filtering through, by eye, the rapid variations in the index.
And what you see is this smooth downward trend followed, eventually, by an upward trend.
Now, this issue of looking at something like this, looking at a sequence, and following the smoother parts of it, namely the long term trends, is, in fact, something that is done quite typically with economic time series like this.
And in particular, what's done is to smooth it, or average over some time period, to emphasize the slow variations and de-emphasize the rapid variations.
And that, in fact, is processing that is done with a discrete-time system.
So when you hear referred to, let's say, in stock market reports, a 51-day moving average, that, in fact, is processing the stock market index with a particular discrete-time system.
The result of doing that on this particular example generates a smooth version of the curve, which I overlay here.
And the overlay, then, is really attempting to track the smoother variations and de-emphasize the more rapid variations.
Let me just slightly offset that so that the difference stands out a little more.
And so here you see what is the original weekly index.
And this is the result of processing that sequence with an appropriate system to apply smoothing.
And in fact, what it is is a moving average.
And so here again, you can see, in the smoother curve, this general downward trend up until this time period, followed by, eventually, a recovery.
Well, we've seen an example with a continuous-time signal,
the Caruso recording, an example of the discrete-time signal, this stock market index.
And what I'd also like to show is a third example, which is the result of some processing on an image, in particular the image that we talked about before, which was the aerial photograph that had the problem of some cloud cover.
So once again, what we see here is the original aerial photograph with the cloud cover.
And some processing was applied to this using a system which, in fact, was both nonlinear and quote "time-varying," or, in the case of these independent variables, we would refer to it as spatially-varying.
And the result of applying that processing is shown in the adjoining picture.
And what we see there is hopefully a reasonable attempt to compensate for the cloud cover.
And this, by the way, was some work that was done by Professor Lim at MIT, and has been very successful type of processing for aerial photographs.
I should say, also, that this particular example is one where, although the original signal was a signal that is continuous-time, that is, the independent variables are continuous, as they are in a spatial, aerial photograph, in fact, for the processing, that picture was first converted to a sequence through a process called sampling, which we'll be talking about later.
And then the processing, in fact, was done on a digital computer.
Well, these then are some examples of the use of some systems to process some signals, both in continuous time and discrete time, for one-dimensional signals and for multi-dimensional signals.
And as I've referred to systems, we've thought of them as one big block with an appropriate, or associated, input and output.
And as we'll be getting into in the first part of the course, very often, systems are interconnected together for a variety of reasons.
Some of the kinds of interconnections that we'll talk about are connecting systems in what are called series, or cascade interconnections, parallel interconnections, feedback interconnections.
And feedback interconnections, in fact, are very interesting,
very important, and very useful, and will be a major topic toward the end of the course.
Feedback, as you may or may not know, comes into play in a variety of situations, for example, in amplifier design, as we'll talk about, feedback plays an important role.
In a situation where you have a basically unstable system, feedback is often used to stabilize the system.
And feedback interconnections of systems in that sense are very often used in high performance aircraft, which are inherently unstable, and are stabilized through this kind of interconnection.
Just to give you a little sense of this without going into any of the details, what I'd like to show you is an excerpt from a lecture that we'll be seeing toward the end of the course relating to the analysis of feedback systems and the uses of feedback.
And this is in the context of what's referred to as the inverted pendulum, which is a system that's basically unstable, and feedback interconnections are used to stabilize it.
The idea, as you'll see in this brief clip, is that there is a cart that's moving on a track with a rod that's pivoted at the base.
And so that system, in the absence of anything, is unstable in that the rod would tend to fall.
And as we go into in detail in the lecture later, we use feedback to position the cart under the pendulum to balance it.
And in fact, that balancing can be done even when we modify the system in a variety of ways, as you'll see in this clip.
So let's take a look at that, remembering that this is just a brief excerpt from a longer discussion.
NOISEEVENT I can change the overall system even further by, let's say, for example, pouring a liquid in.
And now, let me also comment that I've changed the physics of it a little bit because the liquid can slosh around a little bit.
It becomes a little more complicated a system, but as you can see, it still remains balanced.
NOISEEVENT As you'll see when we get to it, by the way, that demonstration was a lot of fun to do.
Now, in talking about signals and systems as we go through the course, there are several domains, two in particular, that we will find convenient for the analysis and representation of signals and systems.
One is the time domain, which is what we tend to think of,
and which we have kind of been focusing on in the discussion so far in this lecture.
But equally important is what's referred to as the frequency domain as a representation for signals, and as a means for analysis for systems.
And in the context of the frequency domain representation, some of the kinds of ideas and topics that we'll be exploring are the Fourier Transform, and the Laplace Transform, and a discrete-time counterpart of the Laplace Transform, which is the z-Transform.
The Fourier Transform discussion we'll get into fairly early in the course.
And the Laplace Transform and z-Transform represent extensions of the Fourier transform, and we'll be getting into that later in the course.
Just initially to think about the time domain and frequency domain, you might think, for example, of a note being played.
And the time-domain representation would be how the sound pressure, as a function of time, would change.
And the frequency-domain representation would correspond to a representation of the frequency content of the note.
And, in fact, what I'd like to do is illustrate that and those two domains simultaneously by playing a glockenspiel note.
What you'll hear is the note repeated several times over.
And at the same time, you'll see two displays, one on the left representing the time domain display a representation of the signal, and the one on the right representing the frequency domain.
So let's look at that.
And you'll hear the note and simultaneously see these two displays.
So there's the note on the left, the time waveform.
And on the right, what we see is the frequency content, in particular, indicating the fact that there are several harmonic lines in the tone.
Well, what I've gone through in this lecture represents a brief overview of signals and systems.
And beginning with the next lecture, we will be much more specific and precise, first discussing some basic signals, and then talking about systems, and system properties, and how to exploit them.
As one final comment that I'd like to make in this lecture,
I'd like to emphasize at the outset that the taped lectures represent only one component of the course.
And equally important will be both the textbook and the video course manual.
In particular, it's important not only to be viewing the tapes, but simultaneously, or in conjunction with that, doing the appropriate reading in the textbook and also working through the problems carefully in the video course manual.
In a course like this, you basically only get out of it as much as you put into it.
The hope is that if you put the right amount of time and effort into it, you'll find the course to be educational and interesting.
And I certainly hope that that will be the case.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
Over the last series of lectures, in discussing filtering, modulation, and sampling, we've seen how powerful and useful the Fourier transform is.
Beginning with this lecture, and over the next several lectures, I'd like to develop and exploit a generalization of the Fourier transform, which will not only lead to some important new insights about signals and systems, but also will remove some of the restrictions that we've had with the Fourier transform.
The generalization that we'll be talking about in the continuous time case is referred to as the Laplace transform, and in the discrete time case, is referred to as the z transform.
What I'd like to do in today's lecture is begin on the continuous time case, namely a discussion of the Laplace transform.
Continue that into the next lecture, and following that develop the z transform for discrete time.
And also, as we go along, exploit the two notions together.
Now, to introduce the notion of the Laplace transform, let me remind you again of what led us into the Fourier transform.
We developed the Fourier transform by considering the idea of representing signals as linear combinations of basic signals.
And in the Fourier transform, in the continuous time case,
the basic signals that we picked in the representation were complex exponentials.
And in what we had referred to as the synthesis equation, the synthesis equation corresponded to, in effect, a decomposition as a linear combination, a decomposition of x of t as a linear combination of complex exponentials.
And of course, associated with this was the corresponding analysis equation that, in effect, gave us the amplitudes associated with the complex exponentials.
Now, why did we pick complex exponentials?
Well, recall that the reason was that complex exponentials are eigenfunctions of linear time-invariant systems, and that was very convenient.
Specifically, if we have a linear time-invariant system with an impulse response h of t, what we had shown is that that class of systems has the property that if we put in a complex exponential, we get out a complex exponential at the same frequency and with a change in amplitude.
And this change in amplitude, in fact, corresponded as we showed as the discussion went along, to the Fourier transform of the system impulse response.
So the notion of decomposing signals into complex exponentials was very intimately connected, and the Fourier transform was very intimately connected, with the eigenfunction property of complex exponentials for linear time-invariant systems.
Well, complex exponentials of that type are not the only eigenfunctions for linear time-invariant systems.
In fact, what you've seen previously is that if we took a more general exponential, e to the st, where s is a more general complex number.
Not just j omega, but in fact sigma plus j omega.
For any value of s, the complex exponential is an eigenfunction.
And we can justify that simply by substitution into the convolution integral.
In other words, the response to this complex exponential is the convolution of the impulse response with the excitation.
And notice that we can break this term into a product, e to the st e to the minus s tau.
And the e to the st term can come outside the integration.
And consequently, just carrying through that algebra,
would reduce this integral to an integral with an e to the st factor outside.
So just simply carrying through the algebra, what we would conclude is that a complex exponential with any complex number s would generate, as an output, a complex exponential of the same form multiplied by whatever this integral is.
And this integral, of course, will depend on what the value of s is.
But that's all that it will depend on.
Or said another way, what this all can be denoted as is some function h of s that depends on the value of s.
So finally then, e to the st as an excitation to a linear time-invariant system generates a response, which is a complex constant depending on s, multiplying the same function that excited the system.
So what we have then is the eigenfunction property, more generally, in terms of a more general complex exponential where the complex factor is given by this integral.
Well, in fact, what that integral corresponds to is what we will define as the Laplace transform of the impulse response.
And in fact, we can apply this transformation to a more general time function that may or may not be the impulse response of a linear time-invariant system.
And so, in general, it is this transformation on a time function which is the Laplace transform of that time function, and it's a function of s.
So the definition of the Laplace transform is that the Laplace transform of a time function x of t is the result of this transformation on x of t.
It's denoted as x of s, and as a shorthand notation as we had with the Fourier transform, then we have in the time domain, the time function x of t, and in the Laplace transform domain, the function x of s.
And these then represent a transform pair.
Now, let me remind you that the development of that mapping is exactly the process the we went through initially in developing a mapping that ended up giving us the Fourier transform.
Essentially, what we've done is just broadened our horizon somewhat, or our notation somewhat.
And rather than pushing just a complex exponential through the system, we've pushed a more general time function e to the st, where s is a complex number with both a real part and an imaginary part.
Well, the discussion that we've gone through so far, of course, is very closely related to what we went through for the Fourier transform.
The mapping that we've ended up with is called the Laplace transform.
And as you can well imagine and perhaps, may have recognized already, there's a very close connection between the Laplace transform and the Fourier transform.
Well, to see one of the connections, what we can observe is that if we look at the Fourier transform expression and if we look at the Laplace transform expression, where s is now a general complex number sigma plus j omega, these two expressions, in fact, are identical if, in fact, sigma is equal to 0.
If sigma is equal to 0 so that s is just j omega, then all that this transformation is, is the same as that.
Substitute in s equals j omega and this is what we get.
What this then tells us is that if we have the Laplace transform, and if we look at the Laplace transform at s equals j omega, then that, in fact, corresponds to the Fourier transform of x of t.
Now, there is a slight notational issue that this raises, and it's very straightforward to clean it up.
But it's something that it's--
you have to just kind of focus on for a second to understand what the issue is.
Notice that on the left-hand side of this equation, x of s representing the Laplace transform.
When we look at that with sigma equal to 0 or s equal to j omega, our natural inclination is to write that as x of j omega, of course.
On the other hand, the right-hand side of the equation, namely the Fourier transform of x of t, we've typically written as x of omega.
Focusing on the fact that it's a function of this variable omega.
Well, there's a slight awkwardness here because here we're talking about an argument j omega, here we're talking about an argument omega.
And a very straightforward way of dealing with that is to simply change our notation for the Fourier transform, recognizing that the Fourier transform, of course, is a function of omega, but it's also, in fact, a function of j omega.
And if we write it that way, then the two notations come together.
In other words, the Laplace transform at s equals j omega just simply reduces both mathematically and notationally to the Fourier transform.
So the notation that we'll now be adopting for the Fourier transform is the notation whereby we express the Fourier transform no longer simply as x of omega, but choosing as the argument j omega.
Simple notational change.
Now, here we see one relationship between the Fourier transform and the Laplace transform.
Namely that the Laplace transform for s equals j omega reduces to the Fourier transform.
We also have another important relationship.
In particular, the fact that the Laplace transform can be interpreted as the Fourier transform of a modified version of x of t.
Let me show you what I mean.
Here, of course, we have the relationship that we just developed.
Namely that s equals j omega.
The Laplace transform reduces to the Fourier transform.
But now let's look at the more general Laplace transform expression.
And if we substitute in s equals sigma plus j omega,
which is the general form for this complex variable s, and we carry through some of the algebra, breaking this into the product of two exponentials, z to the minus sigma t times z to the minus j omega t.
We now have this expression where, of course, in both of these there is a dt.
And now when we look at this, what we observe is that this, in fact, is the Fourier transform of something.
What's the something?
It's not x of t anymore, it's the Fourier transform of x of t multiplied by e to the minus sigma t.
So if we think of these two terms together, this integral is just the Fourier transform.
It's the Fourier transform of x of t multiplied by an exponential.
If sigma is greater than 0, it's an exponential that decays with time.
If sigma is less than 0, it's an exponential that grows with time.
So we have then this additional relationship, which tells us that the Laplace transform is the Fourier transform of an exponentially weighted time function.
Now, this exponential weighting has some important significance.
In particular, recall that there were issues of convergence with the Fourier transform.
In particular, the Fourier transform may or may not converge.
And for convergence, in fact, what's required is that the time function that we're transforming be absolutely integrable.
Now, we can have a time function that isn't absolutely integrable because, let's say, it grows exponentially as time increases.
But when we multiply it by this exponential factor that's embodied in the Laplace transform, in fact that brings the function back down for positive time.
And we'll impose absolute integrability on the product of x of t times e to the minus sigma t.
And so the conclusion, an important point is that the Laplace transform, the Fourier transform of this product may converge, even though the Fourier transform of x of t doesn't.
In other words, the Laplace transform may converge even when the Fourier transform doesn't converge.
And we'll see that and we'll see examples of it as the discussion goes along.
Now let me also draw your attention to the fact, although we won't be working through this in detail.
To the fact that this equation, in effect, provides the basis for us to figure out how to express x of t in terms of the Laplace transform.
In effect, we can apply the inverse Fourier transform to this, thereby to this, account for the exponential factor by bringing it over to the other side.
And if you go through this, and in fact, you'll have an opportunity to go through this both in the video course manual and also it's carried through in the text, what you end up with is a synthesis equation, an expression for x of t in terms of x of s which corresponds to a synthesis equation.
And which now builds x of t out of a linear combination of not necessarily functions of the form e to the j omega t, but in terms of functions or basic signals which are more general exponentials e to the st.
OK, well, let's just look at some examples of the Laplace transform of some time functions.
And these examples that I'll go through are all examples that are worked out in the text.
And so I don't want to focus on the algebra.
What I'd like to focus on are some of the issues and the interpretation.
Let's first of all, look at the example in the text, which is Example 9.1.
If we take the Fourier transform of this exponential,
then, as you well know, the result we have is 1 over j omega plus a.
And that can't converge for any a.
In particular, it's only for a greater than 0.
What that really means is that for convergence of the Fourier transform, this has to be a decaying exponential.
It can't be an increasing exponential.
If instead we apply the Laplace transform to this,
applying the Laplace transform is the same as taking the Fourier transform of x of t times an exponential, and the exponent that we would multiply by is e to the minus sigma t.
So in effect, taking the Laplace transform of this is like taking the Fourier transform of e to the minus at e to the minus sigma t.
And if we carry that through, just working through the integral, we end up with a Laplace transform, which is 1 over s plus a.
But just as in the Fourier transform, the Fourier transform won't converge for any a.
Now what happens is that the Laplace transform will only converge when the Fourier transform of this converges.
Said another way, it's when the combination of a plus sigma is greater than 0.
So we would require that, if I write it over here, a plus sigma is greater than 0.
Or that sigma is greater than minus a.
So in fact, in the Laplace transform of this, we have an expression 1 over s plus a.
But we also require, in interpreting that, that the real part of s be greater than minus a.
So that, essentially, the Fourier transform of x of t times e to the minus sigma t converges.
So it's important to recognize that the algebraic expression that we get is only valid for certain values of the real part of s.
And so, for this example, we can summarize it as this exponential has a Laplace transform, which is 1 over s plus a, where s is restricted to the range the real part of s greater than minus a.
Now, we haven't had this issue before of restrictions on what the value of s is.
With the Fourier transform, either it converged or it didn't converge.
With the Laplace transform, there are certain values of s.
We now have more flexibility, and so there's certain values of the real part of s for which it converges and certain values for which it doesn't.
The values of s for which the Laplace transform converges is-- the values are referred to as the region of convergence of the Laplace transform.
And it's important to recognize that in specifying the Laplace transform, what's required is not only the algebraic expression, but also the domain or set of values of s for which that algebraic expression is valid.
Just to underscore that point, let me draw your attention to another example in the text, which is Example 9.2.
In Example 9.2, we have an exponential for negative time, 0 for positive time.
And if you carry through the algebra there, you end up with a Laplace transform expression, which is again 1 over s plus a.
Exactly the same algebraic expression as we had for the previous example.
The important distinction is that now the real part of s is restricted to be less than minus a.
And so, in fact, if you compare this example with the one above it, and let's just look back at the answer that we had there.
If you compare those two examples, here the algebraic expression is 1 over s plus a with a certain region of convergence.
Here the algebraic expression is 1 over s plus a.
And the only difference between those two is the domain or region of convergence.
So there is another complication, or twist, now.
Not only do we need to generate the algebraic expression, but we also have to be careful to specify the region of convergence over which that algebraic expression is valid.
Now, later on in this lecture, and actually also as the discussion of the Laplace transform goes on, we'll begin to see and understand more about how the region of convergence relates to various properties of the time function.
Well, let's finally look at one additional example from the text, And this is Example 9.3.
And what it consists of is the time function, which is the sum of two exponentials.
And although we haven't formally talked about properties of the Laplace transform yet, one of the properties that we'll see-- and it's relatively easy to develop-- is the fact that the Laplace transform of a sum is the sum of the Laplace transform.
So, in fact, we can get the Laplace transform of the sum of these two terms as the sum of the Laplace transforms.
So for this one, we know from the example that we looked at previously, Example 9.1, that this is of the form 1 over s plus 1 with a region of convergence, which is the real part of s greater than minus 1.
For this one, we have a Laplace transform which is 1 over s plus 2 with a region of convergence which is the real part of s greater than minus 2.
So for the two of them together, we have to take the overlap of those two regions.
In other words, we have to take the region that encompasses both the real part of s greater than minus 1 and the real part of s greater than minus 2.
And if we put those together, then we have a combined region of convergence, which is the real part of s greater than minus 1.
So this is the expression.
And for this particular example, what we have is a ratio of polynomials.
The ratio of polynomials, there's a numerator polynomial and a denominator polynomial.
And it's convenient to summarize these by plotting the roots of the numerator polynomial and the roots of the denominator polynomial in the complex plane.
And the complex plane which they're plotted is referred to the s-plane.
So we can, for example, take the denominator polynomial and summarize it by specifying the fact, or by representing the fact that it has roots at s equals minus 1 and at s equals minus 2.
And I've done that in this picture by putting an x where the roots of the denominator polynomial are.
The numerator polynomial has a root at s equals minus 3/2, and I've represented that by a circle.
So these are the roots of the denominator polynomial and this is the root of the numerator polynomial for this example.
And also, for this example, we can represent the region of convergence, which is the real part of s greater than minus 1.
And so that's, in fact, the region over here.
There is also, if I draw these, just the roots of the numerator and denominator of polynomials, I would need an additional piece of information to specify the algebraic expression completely.
Namely, a multiplying constant out in front of the whole thing.
Well, this particular example, has the Laplace transform as a rational function.
Namely, it's one polynomial in the numerator and another polynomial in the denominator.
And in fact, as we'll see, Laplace transforms, which are ratios of polynomials, form a very important class.
They, in fact, represent systems that are describable by linear constant coefficient differential equations.
You shouldn't necessarily--
in fact, for sure you shouldn't see why that's true now.
We'll see that later.
But that means that Laplace transforms that are rational functions, namely, the ratio of a numerator polynomial divided by the denominator polynomial, become very important in the discussion that follows.
And in fact, we have some terminology for this.
The roots of the numerator polynomial are referred to as the zeroes of the Laplace transform.
Because, of course, those are the values of s at which x of s becomes 0.
And the roots of the denominator polynomial are referred to as the poles of the Laplace transform.
And those are the values of s at which the Laplace transform blows up.
Namely, becomes infinite.
If you think of setting s equal to a value where this denominator polynomial goes to 0, of course, x of s becomes infinite.
And what we would expect and, of course, we'll see that this is true.
What we would expect is that wherever that happens, there must be some problem with convergence of the Laplace transform.
And indeed, the Laplace transform doesn't converge at the poles.
Namely, at the roots of the denominator polynomial.
So, in fact, let's focus in on that a little further.
Let's examine and talk about the region of convergence of the Laplace transform, and how it's associated both with properties of the time function, and also with the location of the poles of the Laplace transform.
And as we'll see, there are some very specific and important relationships and conclusions that we can draw about how the region of convergence is constrained and associated with the locations of the poles in the s-plane.
Well, to begin with, we can, of course, make the statement as I've just made that the region of convergence contains no poles.
In particular, if I think of this general rational function, the poles of x of s are the values of s at which the denominator is 0.
Or equivalently, x of s blows up.
And of course then, that implies that the expression has no longer converged.
Well, that's one statement that we can make.
Now, there are some others.
And one, for example, is the statement that if I have a point in the s-plane that corresponds to convergence, then in fact any line in the s-plane with that same real part will also be a set of values for which the Laplace transform converges.
And what's the reason for that?
The reason for that is that s is sigma plus j omega and convergence of the Laplace transform is associated with convergence of the Fourier transform of e to the minus sigma t times x of t.
And so the convergence only depends on sigma.
If it only depends on sigma, then if it converges for one value of sigma--
I'm sorry, for a value of sigma for some value of omega,
then it will converge for that same sigma for any value of omega.
The conclusion then is that the region of convergence, if I have a point, then I also have a line.
And so what that suggests is that as we look at the region of convergence, it in fact corresponds to strips in the complex plane.
Now, finally we can tie together the region of convergence to the convergence of the Fourier transform.
In particular, since we know that the Laplace transform reduces to the Fourier transform when the complex variable s is equal to j omega, the implication is that if we have the Laplace transform and if the Laplace transform reduces to the Fourier transform when sigma equals 0.
In other words, when s is equal to j omega, then the Fourier transform of x of t converging is equivalent to the statement that the Laplace transform converges for sigma equal to 0.
In other words, that the region of convergence includes what?
The j omega axis in the s-plane.
So we have then some statements that kind of tie together the location of the poles and the region of convergence.
Let me make one other statement, which is a much harder statement to justify.
And I won't try to, I'll just simply state it.
And that is that the region of convergence of the Laplace transform is a connected region.
In other words, if the entire region consists of a single strip in the s-plane, it can't consist of a strip over here, for example, and a strip over there.
Well, let me emphasize some of those points a little further.
Let's suppose that I have a Laplace transform, and the Laplace transform that I'm talking about is a rational function, which is 1 over s plus 1 times s plus 2.
Then the pole-zero pattern, as it's referred to, in the s-plane, the location of the roots of the numerator and denominator polynomials.
Of course, there is no numerator polynomial.
The denominator polynomial roots, which I've represented by these x's, are shown here.
And so this is the pole-zero pattern.
And from what I've said, the region of convergence can't include any poles and it must correspond to strips in the s-plane.
And furthermore, it must be just one connected region rather than multiple regions.
And so with this algebraic expression then, the possible choices for the region of convergence consistent with those properties are the following.
One of them would be a region of convergence to the right of this pole.
A second would be a region of convergence which lies between the two poles as I show here.
And a third is a region of convergence which is to the left of this pole.
And because of the fact that I said without proof that the region of convergence must be a single strip, it can't be multiple strips.
In fact, we could not consider, as a possible region of convergence, what I show here.
So, in fact, this is not a valid region of convergence.
There are only three possibilities associated with this pole-zero pattern.
Namely, to the right of this pole, between the two poles, and to the left of this pole.
Now, to carry the discussion further, we can, in fact,
associate the region of convergence of the Laplace transform with some very specific characteristics of the time function.
And what this will do is to help us understand how for various choices of the region of convergence, the interpretation that we can impose on the related time function.
Let me show you what I mean.
Suppose that we start with a time function as I indicate here, which is a finite duration time function.
In other words, it's 0 except in some time interval.
Now, recall that the Fourier transform converges if the time function has the property that it's absolutely integrable.
And as long as everything's stays finite in terms of amplitudes in a finite duration signal, there's no difficulty that we're going to run into here.
Now, here the Fourier transform will converge.
And now the question is, what can we say about the region of convergence of the Laplace transform?
Well, the Laplace transform is the Fourier transform of the time function multiplied by an exponential.
And so we can ask about whether we can destroy the absolute integrability of this by multiplying by an exponential that grows to fast or decays too fast, or whatever.
And let's take a look at that.
Suppose that this time function is absolutely integrable.
And let's multiply it by a decaying exponential.
So this is now x of t times z to the minus sigma t if I think of multiplying these two together.
And what you can see is that for positive time, sort of thinking informally, I'm helping the integrability of the product because I'm pushing this part down.
For negative time, unfortunately, I'm making things grow.
But I don't let them grow indefinitely because there's some time before which this is equal to 0.
Likewise, if I had a growing exponential, then for a growing exponential for negative time, or for this part, I'm making things smaller.
For positive time, eventually this exponential is growing without bound.
But the time function stops at some point.
So the idea then kind of is that for a finite duration time function, no matter what kind of exponential I multiply by, whether it's going this way or going this way, because of the fact that essentially the limits on the integral are finite, I'm guaranteed that I'll always maintain absolute integrability.
And so, in fact then, for a finite duration time function, the region of convergence is the entire s-plane.
Now, we can also make statements about other kinds of time functions.
And let's look at a time function which I define as a right-sided time function.
And a right-sided time function is one which is 0 up until some time, and then it goes on after that, presumably off to infinity.
Now, let me remind you that the whole issue here with the region of convergence has to do with exponentials that we can multiply a time function by and have the product end up being absolutely integrable.
Well, suppose that when I multiply this time function by an exponential which, let's say decays.
But an exponential e to the minus sigma 0 t, what you can see sort of intuitively is that if this product is absolutely integrable, if I were to increase sigma 0, then I'm making things even better for positive time because I'm pushing them down.
And whereas they might be worse for negative time, that doesn't matter because before some time the product is equal to 0.
So if this product is absolutely integrable, then if I chose an exponential e to the minus sigma 1t where sigma 1 is greater than sigma 0, then that product will also be absolutely integrable.
And we can draw an important conclusion about that, about the region of convergence from that.
In particular, we can make the statement that if the time function is right-sided and if convergence occurs for some value sigma 0, then in fact, we will have convergence of the Laplace transform for all values of the real part of s greater than sigma 0.
The reason, of course, being that if sigma 0 increases, then the exponential decays even faster for positive time.
Now what that says then thinking another way, in terms of the region of convergence as we might draw it in the s-plane, is that if we have a point that's in the region of convergence corresponding to some value sigma 0, then all values of s to the right of that in the s-plane will also be in the region of convergence.
We can also combine that with the statement that for rational functions we know that there can't be any poles in the region of convergence.
If you put those two statements together, then we end up with a statement that if x of t is right-sided and if its Laplace transform is rational, then the region of convergence is to the right of the rightmost pole.
So we have here a very important insight, which tells us that we can infer some property about the time function from the region of convergence.
Or conversely, if we know something about the time function, namely being right-sided, then we can infer something about the region of convergence.
Well, in addition to right-sided signals, we can also have left-sided signals.
And a left-sided signal is essentially a right-sided signal turned around.
In other words, a left-sided signal is one that is 0 after some time.
Well, we can carry out exactly the same kind of argument there.
Namely, if the signal goes off to infinity in the negative time direction and stops some place for positive time, if I have an exponential that I can multiply it by and have that product be absolutely integrable.
And if I choose an exponential that decays even faster for negative time so that I'm pushing the stuff way out there down even further, then I enhance the integrability even more.
And you might have to think through that a little bit, but it's exactly the flip side of the argument for right-sided signals.
And the conclusion then is that if we have a left-sided signal and we have a point, a value of the real part of s which is in the region of convergence, then in fact, all values to the left of that point in the s-plane will also be in the region of convergence.
Now, similar to the statement that we made for right-sided signals, if x of t is left-sided and, in fact, we're talking about a rational Laplace transform, which we most typically will.
Then, in fact, we can make the statement that the region of convergence is to the left of the leftmost pole because we know if we find a point that's in the region of convergence, everything to the left of that has to be in the region of convergence.
We can't have any poles in the region of convergence.
You put those two statements together and it says it's to the left of the leftmost pole.
Now the final situation is the situation where we have a signal which is neither right-sided nor left-sided.
It goes off to infinity for positive time and it goes off to infinity for negative time.
And there the thing to kind of recognize is that if you multiply by an exponential, and it's decaying very fast for positive time, it's going to be growing very fast for negative time.
Conversely, if it's decaying very fast for negative time, it's growing very fast for positive time.
And there's this notion of trying to balance the value of sigma.
And in effect, what that says is that the region of convergence can't extent too far to the left or too far to the right.
Said another way for a two-sided signal, if we have a point which is in the region of convergence, then that point defines a strip in the s-plane that takes that point and extends it to the left until you bump into a pole, and extends it to the right until you bump it into a pole.
So you begin to then see that we can tie together some properties of the region of convergence and the right-sidedness, or left-sidedness, or two-sidedness of the time function.
And you'll have a chance to examine that in more detail in the video course manual.
Let's conclude this lecture by talking about how we might get the time function given the appliance transform.
Well, if we have a Laplace transform, we can, in principle, get the time function back again by recognizing this relationship between the Laplace transform and the Fourier transform, and using the formal Fourier transform expression.
Or equivalently, the formal inverse Laplace transform expression, which is in the text.
But more typically what we would do is what we've done also with the Fourier transform, which is to use simple Laplace transform pairs together with the notion of the partial fraction expansion.
And let's just go through that with an example.
Let's suppose that I have a Laplace transform as I indicated here in its pole-zero plot and a region of convergence which is to the right of this pole.
And what we can identify from the region of convergence, in fact, is that we're talking about a right-sided time function.
So the region of convergence is the real part of s greater than minus 1.
And now looking down at the algebraic expression, we have the algebraic expression for this, as I indicated here, equivalently expanded in a partial fraction expansion, as I show below.
So if you just simply combine these together, that's the same as this.
And the region of convergence is the real part of s greater than minus 1.
Now, the region of convergence of--
this is the sum of two terms, so the time function is the sum of two time functions.
And the region of convergence of the combination must be the intersection of the region of convergence associated with each one.
Recognizing that this is to the right of the poles, that tells us immediately that each of these two then would correspond to the Laplace transform of a right-sided time function.
Well, let's look at it term by term.
The first term is the factor 1 over s plus 1 with a region of convergence to the right of this pole.
And this algebraically corresponds to what I've indicated.
And this, in fact, is similar to, or a special case of the example that we pointed to at the beginning of the lecture.
Namely, Example 9.1.
And so we can just simply use that result.
If you think back to that example or refer to your notes, we know that time function of the form e to the minus a t gives us the Laplace transform, which is 1 over s plus a with the real part of s greater than minus a.
And so this is the Laplace transform of the first.
Or, I'm sorry, this is the inverse Laplace transform of the first term.
If we now consider the pole at s equals minus 2, and here is the region of convergence that we originally began with.
In fact, we can having removed the pole at minus 1, extend this region of convergence to this pole.
And we now have an algebraic expression, which is minus 1 over s plus 2, the real part of s greater than minus 1.
Although, in fact, we can extend the region of convergence up to the pole.
And the inverse transform of this is now, again, referring to the same example, minus e to the minus 2t times the unit step.
And if we simply put the two terms together then, adding the one that we have here to what we had before, we have a total inverse Laplace transform, which is that.
So essentially, what's happened is that each of the poles has contributed an exponential factor.
And because of the region of convergence being to the right of all those poles, that is consistent with the notion that both of those terms correspond to right-sided time functions.
Well, let's just focus for a second or two on the same pole-zero pattern.
But instead of a region of convergence which is to the right of the poles as we had before, we'll now take a region of convergence which is between the two poles.
And I'll let you work through this more leisurely in the video course manual.
But when we carry out the partial fraction expansion, as I've done below, we would now associate with this pole a region of convergence to the right.
With this pole, a region of convergence to the left.
And so what we would have is the sum of a right-sided time function due to this pole.
And in fact it's of the form e to the minus t for t positive.
And a left-sided time function due to this pole.
And in fact, that's of the form e to the minus 2t for t negative.
And so, in fact, the answer that we will get when we decompose this, use the partial fraction expansion, being very careful about associating the region of convergence of this pole to the right and of this pole to the left, we'll have then, when we're all done, a time function which will be of the form e to the minus t times the unit step for t positive.
And then we'll have a term of the form e to the--
I'm sorry, this would be e to the minus 2t since this is at minus 2 and this is at minus 1.
This would be a plus sign and this would be minus e to the minus t for t negative.
And you'll look at that a little more carefully when you sit down with the video course manual.
OK, well, what we've gone through, rather quickly, is an introduction to the Laplace transform.
And a couple of points to underscore again, is the fact that the Laplace transform is very closely associated with the Fourier transform.
And in fact, the Laplace transform for s equals j omega reduces to the Fourier transform.
But more generally, the Laplace transform is the Fourier transform of x of t with an exponential weighting.
And there are some exponentials for which that product converges.
There are other exponentials for which that product has a Fourier transform that doesn't converge.
That then imposes on the discussion of the Laplace transform what we refer to as the region of convergence.
And it's very important to understand that in specifying a Laplace transform, it's important to identify not only the algebraic expression, but also the values of s for which it's valid.
Namely, the region of convergence of the Laplace transform.
Finally what we did was to tie together some properties of a time function with things that we can say about the region of convergence of its Laplace transform.
Now, just as with the Fourier transform, the Laplace transform has some very important properties.
And out of these properties, both are some mechanisms for using the Laplace transform for such systems as those described by linear constant coefficient differential equations.
But more importantly, the properties will help us.
As we understand them further, will help us in using and exploiting the Laplace transform to study and understand linear time-invariant systems.
And that's what we'll go on to next time.
In particular, talking about properties, and then associating with linear time-invariant systems much of the discussion that we've had today relating to the Laplace transform.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
Last time, we introduced the Laplace transform as a generalization of the Fourier transform, and, just as a reminder, the Laplace transform expression as we developed it is this integral, very much similar to the Fourier transform integral, except with a more general complex variable.
And, in fact, we developed and talked about the relationship between the Laplace transform and the Fourier transform.
In particular, the Laplace transform with the Laplace transform variable s, purely imaginary, in fact, reduces to the Fourier transform.
Or, more generally, with the Laplace transform variable as a complex number, the Laplace transform is the Fourier transform of the corresponding time function with an exponential weighting.
And, also, as you should recall, the exponential waiting introduced the notion that the Laplace transform may converge for some values of sigma and perhaps not for other values of sigma.
So associated with the Laplace transform was what we refer to as the region of convergence.
Now just as with the Fourier transform, there are a number of properties of the Laplace transform that are extremely useful in describing and analyzing signals and systems.
For example, one of the properties that we, in fact,
took advantage of in our discussion last time was the linearly the linearity property, which says, in essence, that the Laplace transform of the linear combination of two time functions is the same linear combination of the associated Laplace transforms.
Also, there is a very important and useful property,
which tells us how the derivative of a time function-- rather, the Laplace transform of the derivative-- is related to the Laplace transform.
In particular, the Laplace transform of the derivative is the Laplace transform x of t multiplied by s.
And, as you can see by just setting s equal to j omega, in fact, this reduces to the corresponding Fourier transform property.
And a third property that we'll make frequent use of is referred to as the convolution property.
Again, a generalization of the convolution property for Fourier transforms.
Here the convolution property says that the Laplace transform of the convolution of two time functions is the product of the associated Laplace transforms.
Now it's important at some point to think carefully about the region of convergence as we discuss these properties.
And let me just draw your attention to the fact that in discussing properties fully and in detail, one has to pay attention not just to how the algebraic expression changes, but also what the consequences are for the region of convergence, and that's discussed in somewhat more detail in the text and I won't do that here.
Now the convolution property leads to, of course, a very important and useful mechanism for dealing with linear time invariant systems, very much as the Fourier transform did.
In particular, the convolution property tells us that if we have a linear time invariant system, the output in the time domain is the convolution of the input and the impulse response.
In the Laplace transform domain, the Laplace transform of the output is the Laplace transform of the impulse response times the Laplace transform of the input.
And again, this is a generalization of the corresponding property for Fourier transforms.
In the case of the Fourier transform, the Fourier transform NOISEEVENT the impulse response we refer to as the frequency response.
In the more general case with Laplace transforms, it's typical to refer to the Laplace transform of the impulse response as the system function.
Now in talking about the system function, some issues of the region of convergence-- and for that matter, location of poles of the system function-- are closely tied in and related to issues of whether the system is stable and causal.
And in fact, there's some useful statements that can be made that play an important role throughout the further discussion.
For example, we know from previous discussions that there's a condition for stability of a system, which is absolute integrability of the impulse response.
And that, in fact, is the same condition for convergence of the Fourier transform of the impulse response.
What that says, really, is that if a system is stable,
then the region of convergence of the system function must include the j omega axis.
Which, of course, is where the Laplace transform reduces to the Fourier transform.
So that relates the region of convergence and stability.
Also, you recall from last time that we talked about the region of convergence associated with right sided time functions.
In particular for a right sided time function, the region of convergence must be to the right of the rightmost pole.
Well, if, in fact, we have a system that's causal, then that causality imposes the condition that the impulse response be right sided.
And so, in fact, for causality, we would have a region of convergence associated with the system function, which is to the right of the rightmost pole.
Now interestingly and very important is the consequence,
if you put those two statements together, in particular, you're led to the conclusion that for stable causal systems, all the poles must be in the left half of the s-plane.
What's the reason?
The reason, of course, is that if the system is stable and causal, the region of convergence must be to the right of the rightmost pole.
It must include the j omega axis.
Obviously then, all the poles must be in the left half of the s-plane.
And again, that's an issue that is discussed somewhat more carefully and in more detail in the text.
Now, the properties that we're talking about here are not the only properties, there are many others.
But these properties, in particular, provide the mechanism-- as they did with Fourier transforms-- for turning linear constant coefficient differential equations into algebraic equations and, corresponding, lead to a mechanism for dealing with and solving linear constant coefficient differential equations.
And I'd like to illustrate that by looking at both first order and second order differential equations.
Let's begin, first of all, with a first order differential equation.
So what we're talking about is a first order system.
What I mean by that is a system that's characterized by a first order differential equation.
And if we apply to this equation the differentiation property, then the derivative-- the Laplace transform of the derivative is s times the Laplace transform of the time function.
The linearity property allows us to combine these together.
And so, consequently, applying the Laplace transform to this equation leads us to this algebraic equation, and following that through, leads us to the statement that the Laplace transform of the output is one over s plus a times the Laplace transform of the input.
We know from the convolution property that this Laplace transform is the system function times x of s.
And so, one over s plus a is the system function or equivalently, the Laplace transform of the impulse response.
So, we can determine the impulse response by taking the inverse Laplace transform of h of s given by one over s plus a.
Well, we can do that using the inspection method, which is one way that we have of doing inverse Laplace transforms.
The question is then, what time function has a Laplace transform which is one over s plus a?
The problem that we run into is that there are two answers to that. one over s plus a is the Laplace transform of an exponential for positive time, but one over s plus a is also the Laplace transform of an exponential for negative time.
Which one of these do we end up picking?
Well, recall that the difference between these was in their region of convergence.
And in fact, in this case, this corresponded to a region of convergence, which was the real part of s greater than minus a.
In this case, this was the corresponding Laplace transform, provided that the real part of s is less than minus a.
So we have to decide which region of convergence that we pick and it's not the differential equation that will tell us that, it's something else that has to give us that information.
What could it be?
Well, what it might be is the additional information that the system is either stable or causal.
So for example, if the system was causal, we would know that the region of convergence is to the right of the pole and that would correspond, then, to this being the impulse response.
Whereas, with a negative--
I'm sorry with a positive-- if we knew that the system, let's say, was non-causal, then we would associate with this region of convergence and we would know then that this is the impulse response.
So a very important point is that what we see is that the linear constant coefficient differential equation gives us the algebraic expression for the system function, but does not tell us about the region of convergence.
We get the reach of convergence from some auxiliary information.
What is that information?
Well, it might, for example, be knowledge that the system is perhaps stable, which tells us that the region of convergence includes the j omega axis, or perhaps causal, which tells us that the region of convergence is to the right of the rightmost pole.
So it's the auxiliary information that specifies for us the region of convergence.
Very important point.
The differential equation by itself does not completely specify the system, it only essentially tells us what the algebraic expression is for the system function.
Alright that's a first order example.
Let's now look at a second order system and the differential equation that I picked in this case.
I've parameterized in a certain way, which we'll see will be useful.
In particular, it's a second order differential equation and I chosen, just for simplicity, to not include any derivatives on the right hand side, although we could have.
In fact, if we did, that would insert zeros into the system function, as well as the poles inserted by the left hand side.
We can determine the system function in exactly the same way, namely, apply the Laplace transform to this equation.
That would convert this differential equation to an algebraic equation.
And now when we solve this algebraic equation for y of s,
in terms of x of s, it will come out in the form of y of s, equal to h of s, times x of s.
And h of s, in that case, we would get simply by dividing out by this polynomial NOISEEVENT s, and so the system function then is the expression that I have here.
So this is the form for a second order system where there are two poles.
Since this is a second order polynomial, there are no zeros associated with the fact that I had no derivatives of the input on the right hand side of the equation.
Well, let's look at this example-- namely the second order system-- in a little more detail.
And what we'll want to look at is the location of the poles and some issues such as, for example, the frequency response.
So here again I have the algebraic expression for the system function.
And as I indicated, this is a second order polynomial, which means that we can factor it into two roots.
So c1 and c2 represent the poles of the system function.
And in particular, in relation to the two parameters zeta and omega sub n-- if we look at what these roots are, then what we get are the two expressions that I have below.
And notice, incidentally, that if zeta is less than one, then what's under the square root is negative.
And so this, in fact, corresponds to an imaginary part-- an imaginary term for zeta less than one.
And so the two roots, then, have a real part which is given by minus zeta omega sub n, and an imaginary part-- if I were to rewrite this and then express it in terms of j or the square root of minus one.
Looking below, we'll have a real part which is minus zeta omega sub n-- an imaginary part which is omega sub n times this square root.
So that's for zeta less than one and for zeta greater than one, the two roots, of course, will be real.
Alright, so let's examine this for the case where zeta is less than one.
And what that corresponds to, then, are two poles in the complex plane.
And they have a real part and an imaginary part.
And you can explore this in somewhat more detail on your own, but, essentially what happens is that as you keep the parameter omega sub n fixed and vary zeta, these poles trace out a circle.
And, for example, where zeta equal to zero, the poles are on the j omega axis at omega sub n.
As zeta increases and gets closer to one, the poles converge toward the real axis and then, in particular, for zeta greater than one, what we end up with are two poles on the real axis.
Well, actually, the case that we want to look at a little more carefully is when the poles are complex.
And what this becomes is a second order system, which as we'll see as the discussion goes on, has an impulse response which oscillates with time and correspondingly a frequency response that has a resonance.
Well let's examine the frequency response a little more carefully.
And what I'm assuming in the discussion is that, first of all, the poles are in the left half plane corresponding to zeta omega sub n being positive-- and so this is-- minus that is negative.
And furthermore, I'm assuming that the poles are complex.
And in that case, the algebraic expression for the system function is omega sub n squared in the numerator and two poles in the denominator, which are complex conjugates.
Now, what we want to look at is the frequency response of the system.
And that corresponds to looking at the Fourier transform of the impulse response, which is the Laplace transform on the j omega axis.
So we want to examine what h of s is as we move along the j omega axis.
And notice, that to do that, in this algebraic expression, we want to set s equal to j omega and then evaluate-- for example, if we want to look at the magnitude of the frequency response-- evaluate the magnitude of the complex number.
Well, there's a very convenient way of doing that geometrically by recognizing that in the complex plane, this complex number minus that complex number represents a vector.
And essentially, to look at the magnitude of this complex number corresponds to taking omega sub n squared and dividing it by the product of the lengths of these vectors.
So let's look, for example, at the vector s minus c1, where s is on the j omega axis.
And doing that, here is the vector c1, and here is the vector s-- which is j omega if we're looking, let's say, at this value of frequency-- and this vector, then, is the vector which is j omega minus c1.
So in fact, it's the length of this vector that we want to observe as we change omega-- namely as we move along the j omega axis.
We want to take this vector and this vector, take the lengths of those vectors, multiply them together, divide that into omega sub n squared, and that will give us the frequency response.
Now that's a little hard to see how the frequency response will work out just looking at one point.
Although notice that as we move along the j omega axis,
as we get closer to this pole, this vector, in fact, gets shorter, and so we might expect , that the frequency response-- as we're moving along the j omega axis in the vicinity of that pole-- would start to peak.
Well, I think that all of this is much better seen dynamically on the computer display, so let's go to the computer display and what we'll look at is a second order system-- the frequency response of it-- as we move along the j omega axis.
So here we see the pole pair in the complex plane and to generate the frequency response, we want to look at the behavior of the pole vectors as we move vertically along the j omega axis.
So we'll show the pole vectors and let's begin at omega equals zero.
So here we have the pole vectors from the poles to the point omega equal to zero.
And, as we move vertically along the j omega axis, we'll see how those pole vectors change in length.
The magnitude of the frequency response is the reciprocal of the product of the lengths of those vectors.
Shown below is the frequency response where we've begun just at omega equal to zero.
And as we move vertically along the j omega axis and the pole vector lengths change, that will, then, influence what the frequency response looks like.
We've started here to move a little bit away from omega equal to zero and notice that in the upper half plane the pole vector has gotten shorter.
The pole vector for the pole in the lower half plane has gotten longer.
And now, as omega increases further, that process will continue.
And in particular, the pole vector associated with the pole in the upper half plane will be its shortest in the vicinity-- at a frequency in the vicinity of that pole-- and so, for that frequency, then, the frequency response will peak and we see that here.
From this point as the frequency increases,
corresponding to moving further vertically along the j omega axis, both pole vectors will increase in length.
And that means, then, that the magnitude of the frequency response will decrease.
For this specific example, the magnitude of the frequency response will asymptotically go to zero.
So what we see here is that the frequency response has a resonance and as we see geometrically from the way the vectors behaved, that resonance in frequency is very clearly associated with the position of the poles.
And so, in fact, to illustrate that further and dramatize it as long as we're focused on it, let's now look at the frequency response for the second order example as we change the pole positions.
And first, what we'll do is let the polls move vertically parallel to the j omega axis and see how the frequency response changes, and then we'll have the polls move horizontally parallel to the real axis and see how the frequency response changes.
To display the behavior of the frequency response as the poles move, we've changed the vertical scale on the frequency response somewhat.
And now what we want to do is move the poles, first,
parallel to the j omega axis, and then parallel to the real axis.
Here we see the effect of moving the poles parallel to the j omega axis.
And what we observe is that, in fact, the frequency location of the resonance shifts, basically tracking the location of the pole.
If we now move the pole back down closer to the real axis,
then this resonance will shift back toward its original location and so let's now see that.
And here we are back at the frequency that we started at.
Now we'll move the poles even closer to the real axis.
The frequency location of the resonance will continue to shift toward lower frequencies.
And also in the process, incidentally, the height over the resonant peak will increase because, of course, the lengths of the pole vectors are getting shorter.
And so, we see now the resonance shifting down toward lower and lower frequency.
And, finally, what we'll now do is move the poles back to their original position and the resonant peak will, of course, shift back up.
And correspondingly the height or amplitude of the resonance will decrease.
And now we're back at the frequency response that we had generated previously.
Next we'd like to look at the behavior as the polls move parallel to the real axis.
First closer to the j omega axis and then further away.
As they move closer to the j omega axis, the resonance sharpens because of the fact that the pole vector gets shorter and responds-- or changes in length more quickly as we move past it moving along the j omega axis.
So here we see the effect of moving the poles closer to the j omega axis.
The resonance has gotten narrower in frequency and higher in amplitude, associated with the fact that the pole vector gets shorter.
Next as we move back to the original location, the resonance will broaden once again and the amplitude will decrease.
And then, if we continue to move the poles even further away from the real axis, the resonance will broaden even further and the amplitude of the peak will become even smaller.
And finally, let's now look just move the poles back to their original position and we'll see the resonance narrow again and become higher.
And so what we see then is that for a second order system, the behavior of the resonance basically is associated with the pole locations, the frequency of the resonance associated with the vertical position of the poles, and the sharpness of the resonance associated with the real part of the poles-- in other words, their position closer or further away from the j omega axis.
OK, so for complex poles, then, for the second order system, what we see is that we get a resonant kind of behavior, and, in particular, then that resonate behavior tends to peak, or get peakier, as the value of zeta gets smaller.
And here, just to remind you of what you saw, here is the frequency response with one particular choice of values-- well, this is normalized so that omega sub n is one-- one particular choice for zeta, namely 0.4.
Here is what we have with zeta smaller, and, finally, here is an example where zeta has gotten even smaller than that.
And what that corresponds to is the poles moving closer to the j omega axis, the corresponding frequency response getting peakier.
Now in the time domain what happens is that we have, of course, these complex roots, which I indicated previously, where this represents the imaginary part because zeta is less than one.
And in the time domain, we will have a form for the behavior, which is a e to the c one t, plus a conjugate, e to the c one conjugate t.
And so, in fact, as the poles get closer to the j omega axis-- corresponding to zeta getting smaller-- as the polls get closer to the j omega axis, in the frequency domain the resonances get sharper.
In the time domain, the real part of the poles has gotten smaller, and that means, in fact, that in the time domain, the behavior will be more oscillatory and less damped.
And so just looking at that again.
Here is, in the time domain, what happens.
First of all, with the parameter zeta equal to 0.4, and it oscillates and exponentially dies out.
Here is the second order system where zeta is now 0.2 instead of 0.4.
And, finally, the second order system where zeta is 0.1.
And what we see as zeta gets smaller and smaller is that the oscillations are basically the same, but the exponential damping becomes less and less.
Alright, now, this is a somewhat more detailed look at second order systems.
And second order systems-- and for that matter, first order systems-- are systems that are important in their own right, but they also are important as basic building blocks for more general, in particular, for higher order systems.
And the way in which that's done typically is by combining first and second order systems together in such a way that they implement higher order systems.
And two very common connections are connections which are cascade connections, and connections which are parallel connections.
In a cascade connection, we would think of combining the individual systems together as I indicate here in series.
And, of course, from the convolution property, the overall system function is the product of the individual system functions.
So, for example, if these were all second order systems, and I combine n of them together in cascade, the overall system would be a system that would have to n poles-- in other words, it would be a two n order system.
That's one very common kind of connection.
Another very common kind of connection for first and second order systems is a parallel connection, where, in that case, we connect the systems together as I indicate here.
The overall system function is just simply the sum of these, and that follows from the linearity property.
And so the overall system function would be as I indicate algebraically here.
And notice that if each of these are second order systems, and I had capital N of them in parallel, when you think of putting the overall system function over one common denominator, that common denominator, in general, is going to be of order two N. So either the parallel connection or the cascade connection could be used to implement higher order systems.
One very common context in which second order systems are combined together, either in parallel or in cascade, to form a more interesting system is, in fact, in speech synthesis.
And what I'd like to do is demonstrate a speech synthesizer, which I have here, which in fact is a parallel combination of four second order systems, very much of the type that we've just talked about.
I'll return to the synthesizer in a minute.
Let me first just indicate what the basic idea is.
In speech synthesis, what we're trying to represent or implement is something that corresponds to the vocal tract.
The vocal tract is characterized by a set of resonances.
And we can think of representing each of those resonances by a second order system.
And then the higher order system corresponding to the vocal tract is built by, in this case, a parallel combination of those second order systems.
So for the synthesizer, what we have connected together in parallel is four second order systems.
And a control on each one of them that controls the center frequency or the resonant frequency of each of the second order systems.
The excitation is an excitation that would represent the air flow through the vocal cords.
The vocal cords vibrate and there are puffs of air through the vocal cords as they open and close.
And so the excitation for the synthesizer corresponds to a pulse train representing the air flow through the vocal cords.
The fundamental frequency of this representing the fundamental frequency of the synthesized voice.
So that's the basic structure of the synthesizer And what we have in this analog synthesizer are separate controls on the individual center frequencies.
There is a control representing the center frequency of the third resonator and the fourth resonator, and those are represented by these two knobs.
And then the first and second resonators are controlled by moving this joystick.
The first resonator by moving the joystick along this axis and the second resonator by moving the joystick along this axis.
And then, in addition to controls on the four resonators, we can control the fundamental frequency of the excitation, and we do that with this knob.
So let's, first of all, just listen to one of the resonators, and the resonator that I'll play is the fourth resonator.
And what you'll hear first is the output as I vary the center frequency of that resonator.
NOISEEVENT So I'm lowering the center frequency.
And then, bringing the center frequency back up.
And then, as I indicated, I can also control the fundamental frequency of the excitation by turning this knob.
NOISEEVENT Lowering the fundamental frequency.
And then, increasing the fundamental frequency.
Alright, now, if the four resonators in parallel are an implementation of the vocal cavity, then, presumably, what we can synthesize when we put them all in are vowel sounds and let's do that.
I'll now switch in the other resonators.
When we do that, then, depending on what choice we have for the individual resonant frequencies, we should be able to synthesize vowel sounds.
So here, for example, is the vowel e.
Here is NOISEEVENT --ah.
A.
NOISEEVENT And, of course, we can--
NOISEEVENT --generate NOISEEVENT --lots of other vowel sounds.
NOISEEVENT --and change the fundamental frequency at the same time.
NOISEEVENT Now, if we want to synthesize speech it's not enough to just synthesize steady state vowels-- that gets boring after a while.
Of course what happens with the vocal cavity is that it moves as a function of time and that's what generates the speech that we want to generate.
And so, presumably then, if we change these resonant frequencies as a function of time appropriately, then we should be able to synthesize speech.
And so by moving these resonances around, we can generate synthesized speech.
And let's try it with some phrase.
And I'll do that by simply adjusting the center frequencies appropriately.
NOISEEVENT Well, hopefully you understood that.
As you could imagine, I spent at least a few minutes before the lecture trying to practice that so that it would come out to be more or less intelligible.
Now the system as I've just demonstrated it is, of course, a continuous time system or an analog speech synthesizer.
There are many versions of digital or discrete time synthesizers.
One of the first, in fact, being a device that many of you are very likely familiar with, which is the Texas Instruments Speak and Spell, which I show here.
And what's very interesting and rather dramatic about this device is the fact that it implements the speech synthesis in very much the same way as I've demonstrated with the analog synthesizer.
In this case, it's five second order filters in a configuration that's slightly different than a parallel configuration but conceptually very closely related.
And let's take a look inside the box.
And what we see there, with a slide that was kindly supplied by Texas Instruments, is the fact that there really are only four chips in there-- a controller chip, some storage.
And the important point is the chip that's labeled as the speech synthesis chip, in fact, is what embodies or implements the five second order filters and, in addition, incorporates some other things-- some memory and also the NOISEEVENT converters.
So, in fact, the implementation of the synthesizer is pretty much done on a single chip.
Well that's a discrete time system.
We've been talking for the last several lectures about continuous time systems and the Laplace transform.
Hopefully what you've seen in this lecture and the previous lecture is the powerful tool that the Laplace transform affords us in analyzing and understanding system behavior.
In the next lecture what I'd like to do is parallel the discussion for discrete time, turn our attention to the z transform, and, as you can imagine simply by virtue of the fact that I have shown you a digital and analog version of very much the same kind of system, the discussions parallel themselves very strongly and the z transform will play very much the same role in discrete time that the Laplace transform does in continuous time.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
In the last several lectures, we've talked about a generalization of the continuous-time Fourier transform and a very similar strategy also applies to discrete-time, and that's what we want to begin to deal with in today's lecture.
So what we want to talk about is generalizing the Fourier transform, and what this will lead to in discrete-time is a notion referred to as the z-transform.
Now, just as in continuous-time, in discrete-time the Fourier transform corresponded to a representation of a sequence as a linear combination of complex exponentials.
So this was the synthesis equation.
And, of course, there is the corresponding analysis equation.
And as you recall, and as is the same for continuous-time,
the reason that we picked complex exponentials was because of the fact that they are eigenfunctions of linear time-invariant systems.
In other words, if you have a complex exponential into a linear time-invariant system, the output is a complex exponential.
And the change in complex amplitude, which corresponds to the frequency response, in fact is what led to the definition of the Fourier transform.
In particular, it is the Fourier transform of the impulse response.
Well, that set of notions is, more or less, identical to the way we motivated the Laplace transform in the continuous-time case, in the Fourier transform in the continuous-time case.
And just as in continuous-time, there are a set of signals more general than the complex exponentials, which are also eigenfunctions of linear time-invariant systems.
In particular, in discrete-time, if we had instead of an exponential e to the j omega n, we had a more general complex number z, that the signal z to the n is also an eigenfunction of a linear time-invariant system for any particular z.
So we can see that by substituting that into the convolution sum and recognizing, again, very strongly paralleling the continuous-time argument, that we can rewrite this factor as z to the n z to the minus k.
And because of the fact that it's a sum on k and this term doesn't depend on k, we can take that term out.
And the conclusion is that if we have z to the n as an input, that the output is of the same form times a factor which depends on z.
But of course, doesn't depend on k because that is summed out as we form the summation.
So, in fact, this summation corresponds to a complex number, which we'll denote as H of z, where z will represent a more general complex number.
Namely, z is r e to the j omega, r being the magnitude of this complex number and omega, of course, being the angle.
So for a linear time-invariant system, a more general complex exponential sequence of this form generates as an output a complex exponential sequence of the same form with a change in amplitude which we're representing as H of z, recognizing the fact that it's going to be a function of what that complex number is.
And this amplitude factor is given by this summation.
And it is this summation which is defined as the z-transform of the sequence H of n.
Now, let me stress that-- and I'll continue to stress this as the lecture goes on.
That much of what we've said is directly parallel to what we said in the continuous-time case.
And what we've simply done is to expand from complex exponentials with a purely imaginary exponent, complex exponential time functions or sequences of that form, to ones that have more general complex exponential factors.
Now, we have a mapping here from the impulse response to the amplitude or the eigenvalue associated with that input, and this mapping is what is referred to as the z-transform.
So H of z is, in fact, the z-transform of the impulse response.
And if we consider applying this mapping as we did in continuous-time in a similar argument, applying this mapping to a sequence whether or not it corresponds to the impulse response or a linear time-invariant system, that leads then to the z-transform of a general sequence x of n.
The z-transform being defined by this relationship.
And again, notationally, we'll often represent a time function and a z-transform through a shorthand notation, just indicating that x of z is the z-transform of x of n.
So we've kind of motivated the development in a manner exactly identical to what we had done with the Laplace transform.
Kind of the idea that if you look at the eigenvalue associated with a linear time-invariant system, that essentially generates a mapping between the sequence-- system impulse response and a function of z.
And that corresponds to the z-transform here, it corresponded to the Laplace transform in the continuous-time case.
That same argument is also the kind of argument that we use to lead us into the Fourier transform originally.
And once again, what you would expect is that the z-transform has a very close and important relationship to the Fourier transform.
And indeed, that relationship turns out to be, more or less,
identical to the relationship between the Laplace transform and the Fourier transform in continuous-time.
Well, let's look at the relationship.
First of all, what we recognize is that if we compare the Fourier transform expression for a sequence and the z-transform expression for the same sequence that they involve, essentially, the same operations.
And in fact, since z is of the form r e to the j omega, if we want this sum to look like this sum, then that would mean that we would choose z equal to e to the j omega.
Said another way, the z-transform, when z is e to the j omega, is going to reduce to the Fourier transform.
So we have a relationship like the one, again, that we had between the Laplace transform and the Fourier transform in continuous-time.
Namely that for a certain set of values of the complex variable, the transform, the z-transform, reduces to the Fourier transform.
So if we have x of z, the z-transform, and we look at that for z equal to e to the j omega, and z equal to e to the j omega is similar to saying that we're looking at that for the magnitude of z equal to 1.
We're specifically choosing r equal to 1, which is the magnitude of z.
Then this is equal to the Fourier transform of the sequence.
So the z-transform for z equal to e to the j omega is the Fourier transform, and so this then corresponds to x of omega.
Namely, the Fourier transform.
Well, we now have ourselves in a similar situation, again, to what we had when we talked about the Laplace transform.
Namely, a notational awkwardness, or inconvenience,
which we can resolve by simply redefining some of our notation.
In particular, the awkwardness relates to the fact that whereas we've been writing our Fourier transforms this way, as x of omega, if we were to express x of z and look at it, it's equal to e to the j omega.
We end up with the independent variable being e to the j omega rather than omega.
Well, in fact, the Fourier transform is a function of omega.
It's also a function of e to the j omega.
And now what we can see is that given the fact that we want to generalize the Fourier transform to the z-transform, it's convenient now to use as notation for the Fourier transform x of z with z equal to e to the j omega.
Namely, our Fourier transforms will now be written as I've indicated here.
So just summarizing that, our new notation is that the independent variable on the Fourier transform is now going to be expressed as e to the j omega rather than as omega.
It's a minor notational change, but I recognize the fact that it's somewhat confusing initially, and takes a few minutes to sit down and just get it straightened out.
It's very similar to what we did with the Laplace transform.
But let me draw your attention to the fact that in the Laplace transform, the independent variable that we ended up with in talking about the Fourier transform is different than what we're ending up with here.
In particular, before we had j omega, now we have e to the j omega.
And the reason for that is simply that whereas in continuous time we were talking about functions of the form e to the st, now we're talking about sequences of the form z to the n.
So we have one relationship between the Fourier transform and the z-transform.
Namely, the fact that for the magnitude of z equal to 1, the z-transform reduces to the Fourier transform.
Now, in the Laplace transform, we also had another important relationship and observation, which was the fact that the Laplace transform was the Fourier transform of x of t modified.
And how was it modified?
It was modified by multiplying by a decaying or growing exponential, depending on what the real part of s is.
Well, we have a very similar situation with the z-transform.
In particular, in addition to the fact that the z-transform for z equal to e to the j omega reduces to the Fourier transform, we'll see that the z-transform for other values of z is the Fourier transform of the sequence with an exponential weighting.
And let's see where that comes from.
Here we have the general expression for the z-transform.
And recognizing that z is a complex number which we're expressing in polar form as r e to the j omega, substituting that in, this summation now becomes x of n, r e to the j omega to the minus n.
We can factor out these two terms, r to the minus n and e to the minus j omega n.
And combining the r to the minus n with x of n and the e to the minus j omega n being treated separately, what we end up with is the summation that I have here.
Well, what this says is that the z-transform, which is this, at z equal to r e to the j omega, is in fact the Fourier transform of what?
It's the Fourier transform of x of n multiplied by r to the minus n.
So that is the expression that we have here.
And in continuous-time, we had the Laplace transform as the Fourier transform of x of t e to the minus sigma t.
Here we have the Fourier transform of x of n r to the minus n.
Now, something to just reflect on for a minute is--
because it tends to cause a little bit of problem with the algebra later on if you're attention isn't drawn to it, is that we're talking about multiplying x of n times r to the minus n.
The question is, for r greater than 1, does r to the minus n increase exponentially as n increases or does it decrease?
We're talking about r to the minus n.
If r is greater than 1, if the magnitude of r is greater than 1.
For example, if it's equal to 2, r to the minus n is 1/2 to the n.
And so, in fact, that decreases exponentially.
Or more generally, the larger r is, the faster r to the minus n decays with increasing n.
Well, let's just look at some examples of the z-transform.
And examples that I've picked, again, are examples directly out of the text.
And so the details of the algebra you can look at more leisurely as you sit with the textbook.
Let's consider, first of all, an exponential sequence x of n equals a to the n times the unit step.
So 0 for negative time and an exponential for positive time.
And the Fourier transform, as we've seen in earlier lectures, is 1 over 1 minus a e to the minus j omega.
But this doesn't always converge.
In particular, for convergence of the Fourier transform, we would require absolute summability of the original sequence.
And that, in turn, requires that the magnitude of a be less than 1.
So the Fourier transform is this, provided that the magnitude of a is less than 1.
And what is the Fourier transform if the magnitude of a is not less than 1?
Well, the answer is that, in that case, it doesn't converge.
Now, let's look at the z-transform.
The z-transform is the sum from minus infinity to plus infinity of a to the n z to the minus n times the unit step.
The unit step will change the lower limit to 0.
So it's the sum from 0 to infinity.
And this is of the form a times z to the minus 1 to the n.
So we're summing from 0 to infinity a times z to the minus 1 to the n.
That sum is 1 over 1 minus a z to the minus 1.
But in order for that sum to converge, we require that the magnitude of a times z to the minus 1 be less than 1.
Now, the z-transform is the Fourier transform of the sequence a to the n times r to the minus n.
And this statement about the z-transform converging is exactly identical to the statement that what we're requiring is that the magnitude of a times r to the minus 1 be less than 1, where this represents the exponential factor that we have that in effect is applied to the sequence, so that the Fourier transform becomes the z-transform.
And so, if we put this condition, we can interpret this condition in exactly the same way that we interpret the condition on convergence of the Fourier transform.
So from what we've worked out here then, what we have is the z-transform of a to the n times u of n is 1 over 1 minus a z to the minus 1.
That works for any value of a provided that we pick the value of z correctly.
In particular, we have to pick the set of values of z, so that what?
So that the magnitude of a times z to the minus 1 is less than 1.
Or equivalently, so that the magnitude of z is greater than the magnitude of a.
So associated with the z-transform of this sequence is this algebraic expression, and this set of values on z for which that algebraic expression is valid.
And just as with the Laplace transform, this range of values is referred to as the region of convergence of the z-transform.
Now, again, as we saw with the Laplace transform, it's important to recognize that in specifying or having worked out the z-transform of a sequence, it's not just the algebraic expression, but also the region of convergence that's required to uniquely specify it.
To emphasize that further, here is Example 10.2 from the text.
And if you work that one through, what you find is that, algebraically, the z-transform of this sequence is 1 over 1 minus a z to the minus 1.
Identical algebraically to what we had up here.
But now with a region of convergence, which is the magnitude of z less than the magnitude of a.
In contrast to this example, where the region of convergence was the magnitude of z greater than the magnitude of a.
So again, it requires not just the algebraic expression, but also requires a specification of the region of convergence.
And also, as with the Laplace transform, it's convenient in looking at the z-transform to represent it in the complex plane.
In this case, the complex plane referred to as the z-plane, whereas in continuous-time when we talked about the Laplace transform, it was the s-plane. z, of course, because z is the complex variable in terms of which we're representing the z-transform.
So we will be representing the z-transform in terms of representations in the complex plane, real part and imaginary part.
But I've also identified a circle here.
And you could wonder, well, what's the significance of the circle?
Recall that in the discussion that we just came from, when we talked about the relationship between the z-transform and the Fourier transform, the z-transform reduces to the Fourier transform when the magnitude of z is equal to 1.
The magnitude of z equal to 1 in the complex plane is a circle.
And that circle, in fact, is a circle of radius 1.
And so it's on this contour in the z-plane that the z-transform reduces to the Fourier transform.
And we'll see some additional significance of that as we go along.
Just again to emphasize the relationships and differences with continuous-time, with the Laplace transform it's the behavior in the s-plane on the j omega axis that corresponds to the Fourier transform.
Here it's the behavior on the unit circle where the z-transform corresponds to the Fourier transform.
Now, we'll be talking--
as we did with the Laplace transform, we'll be talking very often about transforms which are rational, and rational transforms as we'll see, represent systems which are characterized by linear constant coefficient difference equations.
And so for the rational z-transforms, we'll again find it convenient to use a representation in terms of poles and zeroes in the z-plane.
So let's look at our example as we've worked it out previously, Example 10.1.
And with this sequence, the z-transform is 1 divided by 1 minus a z to the minus 1.
And we happen to have written it as a function of z to the minus 1.
Clearly, we can rewrite this by multiplying numerator and denominator by z, and this would equivalently then be z divided by z minus a.
And so if we were to represent this through a pole-zero plot,
we would have a 0 at the origin corresponding to this factor and a pole at z equals a corresponding to the denominator factor.
And so the pole-zero pattern for this is then a pole at z equal to a and a 0 at the origin.
Now, let me just comment quickly about the fact that we had written this as 1 over 1 minus a z to the minus 1, and that seems kind of strange because perhaps we should have multiplied through by z.
Let me just indicate that as you'll see as you work examples, it's very typical for the z-transform to come out as a function of z to the minus 1.
And so very typically, you'll get to recognize that things will be expressed in terms of factors involving terms like 1 minus a z to the minus 1, rather than factors of the form z minus a.
Well, here is the one example that we had referred to.
And if we consider another example, the other example,
which was example 10.2 consists of an algebraic expression as I indicate here.
But its region of validity is the magnitude of z less than the magnitude of a.
And that corresponds to the same pole-zero plot, but a region of convergence which is inside this circle.
Whereas, in the previous case, with the pole-zero plot, we had a region of convergence which was for the magnitude of z greater than the magnitude of a.
So these two examples, this one and the other one, have exactly the same pole-zero pattern and they're distinguished by their region of convergence.
Now, notice incidentally that in this particular case, the region of convergence includes the unit circle provided that the magnitude of a is less than 1.
And so, in fact, that would say that the sequence has a Fourier transform that converges.
Namely, with the magnitude of z equal to 1.
Whereas, in this example, the region of convergence does not include the unit circle.
And so, in fact, we cannot look at x of z for the magnitude of z equal to 1.
And so this example, with the magnitude of a less than 1, does not have a Fourier transform that converges.
Well, assuming that the magnitude of a is less than 1 and the Fourier transform converges, we can, in fact, look at the Fourier transform by observing what happens as we go around the unit circle.
We had seen this with the Laplace transform in terms of observing what happened as we move along the j omega axis.
And here again, we can use the vectors as we trace out the unit circle.
And in particular, what we would be looking at in this case is the ratio of the zero vector to the pole vector.
For example, if we were looking at the magnitude of the z-transform, the magnitude of the z-transform would be the ratio of the length of this vector to the length of this vector.
And to observe the Fourier transform, we would observe how those vectors change in length as we move around the unit circle.
And as we move around the unit circle, what we would trace out in terms of the ratio of the lengths of those vectors is the Fourier transform.
Well, let's focus on that also in the context of a slightly different z-transform.
In the z-transform here as we'll see in a later lecture,
is the z-transform associated with a second order difference equation.
It has a denominator factor which has two poles associated with it.
And so here, if we assumed that the Fourier transform of the associated sequence converged, then again we would look at the behavior of this as we moved around the unit circle.
And the ratio of the lengths of the appropriate vectors would describe for us the frequency response.
I'm sorry, the Fourier transform.
So the Fourier transform magnitude would consist of the ratio of the lengths of the zero vectors divided by the lengths of the pole vectors.
And one thing that we observe is that as we move in frequency omega in the vicinity of this pole, this pole vector, in fact, reaches a minimum length.
That would mean that it's reciprocal would be maximum.
And then, as we sweep past, the lengths of these two vectors would increase.
The zero vectors, of course, would retain the same length no matter where we were on the unit circle.
So, in fact, if we looked at the Fourier transform associated with this pole-zero pattern, if this was, for example, represented the z-transform of the impulse response or a linear time-invariant system, the corresponding frequency response would be what I plotted out below.
And so it would peak.
And in fact, where it would peak is in the vicinity of the frequency location of the pole as I indicate up here.
So as we sweep past this pole then, in fact, this Fourier transform .
Peaks.
Well, this notion of looking at the frequency response as we move around the unit circle is a very important notion.
And it's important to recognize it's the unit circle we're talking about here, whereas before we were talking about the j omega axis.
And to emphasize this further, let me just show this example.
And in fact, the previous example with the computer displays, so that we can see the frequency response as it sweeps out as we go around the unit circle.
So here we have the pole-zero pattern for the second order example.
And to generate the Fourier transform, we want to look at the behavior of the pole and zero vectors as we move around the unit circle.
So first, let's display the vectors.
And here we have them displayed to the point corresponding to zero frequency.
And the magnitude of the Fourier transform will be, as we discussed, the magnitude of the length of the zero vector is divided by the magnitude of the length of the pole vectors.
Shown below will be the Fourier transform.
And we have the Fourier transform displayed here from 0 to 2 pi, rather than from minus pi to pi as it was displayed in the transparency.
Because of the periodicity of the Fourier transform, both of those are equivalent.
Now we're sweeping away from omega equals 0 and the lengths of the pole vectors have changed.
And that, of course, generates a change in the Fourier transform.
And as we continued the process further, if we increase frequency, as we sweep closer to the location of the pole, the pole vector decreases in length dramatically.
And that generates a residence in the Fourier transform, very similar to what we saw in continuous-time.
Now as we continue to sweep further, what will happen is that that pole vector will begin to increase in length again.
And so, in fact, the magnitude of the Fourier transform will decrease.
And we see that here as we sweep toward omega equal to pi.
Now, notice in this process that the length of the zero vectors has stayed the same because of the fact that the zeroes are at the origin, and no matter where we are in the unit circle, the length of those vectors is unity.
So they don't influence in this example the magnitude, but they would, of course, influence the phase.
Now we want to continue sweeping from omega equal to pi around to 2 pi.
And because of the symmetry in the Fourier transform, what we will see in the magnitude is identical to what we would see if we swept from omega equal to pi back clockwise to omega equals 0.
In particular now, as we're increasing frequency, notice that the length of the pole vector associated with the lower half plane pole is decreasing.
And so, in fact, that corresponds to generating a resonance as we sweep past that pole location as we are here.
And then finally, that pole vector increases in length as we begin to approach omega equal to 2 pi.
Or equivalently, as we approach omega equal to 0.
Now finally, let's also look at the Fourier transform associated with the first order example that we discussed earlier in the lecture.
And so what we'll want to look at is the Fourier transform as the pole and zero vectors change.
Once again, the Fourier transform will be displayed on a scale from 0 to 2 pi, a frequency scale from 0 to 2 pi, rather than minus pi to pi.
And we want to observe the pole and zero vectors as we sweep around the unit circle.
We display first the pole and zero vectors at omega equal to 0.
And as the frequency increases, the pole vector increases in length.
The zero vector, since the zero is at the origin, will have constant length no matter where we are on the unit circle.
Although it would affect the phase, which we are not displaying here.
And so the principle effect, the only effect really on the magnitude, is due to the pole vector.
As the frequency continues to increase, the pole vector increases in length, monotonically in fact.
And so that means that the magnitude of the Fourier transform will decrease monotonically until we get past omega equal to pi.
Here we are now at omega equal to pi.
And when we continue sweeping past this frequency around to 2 pi, then we will see basically the same curve swept out in reverse.
Since because of the symmetry, again, of the Fourier transform magnitude, sweeping from pi to 2 pi is going to be equivalent with regard to the magnitude to sweeping from pi back to 0.
And so now the pole vector begins to decrease in length and correspondingly, the magnitude of the Fourier transform will increase.
And that will continue until we get around to omega equal to 2 pi, which is equivalent, of course, to omega equal to 0.
And obviously, if we continue to sweep around again, we would simply trace out other periods associated with the Fourier transform.
Well, that hopefully gives you kind of some feel for the notion of sweeping around the unit circle.
And of course, you can see that because the circle is periodic as we go around and around, of course, what we'll get is a periodic Fourier transform, which is the way Fourier transforms are supposed to be.
Now, just as with the Laplace transform, the region of convergence of the z-transform, as we've seen in this example, is a very important part of the specification of the z-transform.
And we can, in talking about sequences and their transforms, either specify the region of convergence implicitly, or we can specify it explicitly.
We can, for example, say what it is, as let's say the magnitude of z being greater than the magnitude of a.
Or we can recognize that the region of convergence has certain constraints associated with certain properties of the time function.
And in particular, there are some important properties of the region of convergence which allow us, given that we know certain characteristics of the time function, to then identify the region of convergence by looking at the pole-zero pattern.
For example, we recognize that the region of convergence does not contain any poles because of the fact that at poles, the z-transform, in fact, blows up and, of course, can't converge at that point.
Furthermore, the region of convergence consists of a ring in the z-plane centered about the origin.
Recall that with the Laplace transform, the region of convergence consisted of strips in the s-plane.
With the z-transform, the region of convergence consists of a ring, basically because of the fact that the region of convergence is dependent on the magnitude of z.
Whereas, with the Laplace transform, the region of convergence was dependent on the real part of s.
The fact that it's the magnitude of z says, in effect, that all values of z that have the same magnitude lie on a circle.
And so the region of convergence you would expect to be a concentric ring in the z-plane.
Furthermore, as we've already talked about and exploited actually, convergence of the Fourier transform is equivalent to the statement that the region of convergence includes the unit circle in the z-plane.
Now, we can also associate the region of convergence with issues about whether the sequence is of finite duration or right-sided or left-sided.
And let me sort of quickly indicate again what the style of the argument is.
If we have a finite duration sequence, so that the sequence is absolutely summable, and therefore has a Fourier transform that converges, then because of the fact that it's 0 outside some interval, I can multiply it by an exponentially decaying sequence or by an exponentially growing sequence.
And since I'm only doing this over a finite interval, no matter how I choose that exponential, we'll end up with an absolutely summable product.
So if x of n is a finite duration, then in fact the region of convergence is the entire z-plane, possibly with the exception of the origin or infinity.
On the other hand, if the sequence is a right-sided sequence, then we have to be careful that we don't multiply by an exponential that grows too fast for positive time.
Or equivalently, we might have to choose the exponential so that it decays sufficiently fast for positive time.
As a consequence of that, for a right-sided sequence, if we have a value which is in the region of convergence, as long as I multiply by exponentials that decay faster than that for positive time, then I'll also have convergence.
In other words, all finite values of z for which the magnitude of z is greater than this, so that the exponentials die off even faster will also be in the region of convergence.
If we combine that statement with the fact that there are no poles in the region of convergence, then we end up with a statement similar to what we had with the Laplace transform.
Here, the statement is that if the sequence is right-sided,
then the region of convergence has to be outside the outermost pole.
Essentially, because it has to be outside someplace and can't include any poles.
Finally, if we have a left-sided sequence, then if we have a value which is in the region of convergence, all values for which the magnitude of z is less than that will also be in the region of convergence.
Or if x of z is rational, then the region of convergence must be inside the innermost pole.
And finally, if we have a two-sided sequence, then there's kind of a balance between the exponential factor that we use.
And so in that case, then the region of convergence will be a ring in the z-plane, and essentially will extend outward to a pole and inward to a pole.
So if we had an algebraic expression, let's say as we have here, then we could associate with that a region of convergence outside this pole.
And that would correspond to a right-sided sequence.
Or we can associate with it a region of convergence, which is inside the innermost pole.
And that would correspond to a left-sided sequence.
And the third and only other possibility is a region of convergence which lies between these two poles.
And that would then correspond to a two-sided sequence.
And notice incidentally because of where I've placed these poles, that this is the only one for which the region of convergence includes the unit circle.
In other words, it's the only one for which the Fourier transform converges.
OK, now we've moved through that fairly quickly.
And I've emphasized the fact that it parallels very closely what we did with the Laplace transform.
What I'd like to do is just conclude with a discussion of how we get the time function back again when we have the z-transform including its region of convergence.
Well, we can, first of all, develop a more or less formal expression.
And the algebra for this is gone through in the text, and you went through something similar to this with the Laplace transform in the video course manual.
So I won't carry through the details.
But basically, what we can use to develop a formal expression is the fact that the z-transform is the Fourier transform of the sequence exponentially weighted.
So we can apply the inverse transform to that, and that gives us not x of n, but x of n exponentially weighted.
And if we track that through, then what we'll end up with is an expression.
After we've taken care of a few of the epsilons and deltas, we'll end up with an expression that expresses formally the sequence x of n in terms of the z-transform, where this, in fact, is a contour integral in the complex plane.
And so there's a formal expression, just as there's a formal expression for the Laplace transform.
But in fact, the more typical procedure is to use essentially transformed pairs that we know together with the idea of using a partial fraction expansion.
So if we had a z-transform as I indicate here, and if we expand it out in a partial fraction expansion, then we can recognize, as we did in a similar style with the Laplace transform-- we can recognize that each term, together with the identified region of convergence corresponds to an exponential factor.
And so this term, together with the fact we know that the magnitude of z must be greater than 2, allows us to recognize this as similar to the Example 10.1.
And in particular then, the sequence associated with that is what I indicate here.
And for the second term, the sequence is what I indicate here.
So what we're simply doing is using the fact that we've worked out the example going one direction before, and now we use that together with the partial fraction expansion to get the individual sequences back again, and then add them together.
There's one other method which I'll just point to, which is also elaborated on a little more in the text.
But it's kind of the idea of developing the inverse z-transform by recognizing that this z-transform formula, in fact, is a power series.
So if we take x of z and expand it in a power series,
then we can pick off the values of x of n by identifying the individual coefficients in this expansion.
And so by simply doing long division, for example, we can also get the inverse transform.
And that, by the way, is very useful.
Particularly if we want to get the inverse z-transform for a z-transform expression, which is not rational.
Now we've moved through this fairly quickly.
On the other hand, I've stressed that it's very similar to what we went through for the Laplace transform, except for a very important difference.
The principal difference really being that with the Laplace transform, it was the j omega axis in the s-plane that we focused attention on when we were thinking about the Fourier transform.
Here, the unit circle in the z-plane plays an important role.
What we'll see when we continue this in the next lecture is that there are properties of the z-transform, just as there were properties of the Laplace transform.
And those properties allow us to develop and exploit the z-transform in the context of systems describable by linear constant coefficient difference equations.
So in the next lecture, we'll focus on some properties of the z-transform, and then we'll see how to use those properties to help us in getting further insight and working with systems describable by difference equations.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
Last time, we began the discussion of the z-transform.
As with the Laplace transform in continuous time, we developed it as a generalization of the Fourier transform.
The expression that we got for the z-transform is the sum that I indicate here.
We also briefly talked about an inverse z-transform integral and some other informal methods of computing the inverse z-transform.
But we focused in particular on the relationship between the z-transform and the Fourier transform, pointing out first of all that the z-transform, when we choose the magnitude of z equal to 1-- so the magnitude of z of the form e to the j omega-- just simply reduces to the Fourier transform of the sequence.
Then, in addition, we explored the z-transform for z, a more general complex number.
In the discrete time z-transform case, we expressed that complex number in polar form as r e to the j omega, and recognize that the z-transform expression in fact corresponds to the Fourier transform of the sequence exponentially weighted.
Because of the exponential weighting, the z-transform converges for some values of r corresponding to some exponential waiting, and perhaps not for others, and that led to a notion which corresponded to the region of convergence associated with the z-transform.
We talked some about properties of the region of convergence, particularly in relation to the pole-zero pattern.
Now, they z-transform has a number of important and useful properties, just as the Laplace transform does.
As one part of this lecture, what we'll want to do is exploit some of these properties in the context of systems described by linear constant coefficient difference equations.
These particular properties that play an important role in that context are the properties that I indicate here.
In particular, there is-- as with continuous time-- a linearity property that tells us that the z-transform of a linear combination of sequences is the same linear combination of the z-transforms, a shifting property that indicates that the z-transform of x of n shifted is the z transform of x of n multiplied by a factor z to the minus n 0.
Then the convolution property for which the z-transform of a convolution of sequences is the product of the associated z-transforms.
With all of these properties, of course, there is again the issue of what the associated region of convergence is in comparison with the region of convergence of the original sequences.
That is an issue that's addressed somewhat more in the text and let's not go into it here.
With the convolution property, the convolution property as in continuous time, of course, provides a mechanism for dealing with linear time invariant systems.
In particular, in the time domain a linear time invariant system is described through convolution-- namely, the output is the convolution of the input and the impulse response.
Because of the convolution property associated with the z-transform, the z-transform of the output is the z-transform of the input times the z-transform of the impulse response.
Again, very much the same as what we had in continuous time and also what we had in the context of the discussion with the Fourier transform.
In fact, because of the relationship between the z-transform and the Fourier transform, the z-transform of the impulse response evaluated on the unit circle-- in other words, for the magnitude of z equal to 1-- in fact corresponds to the frequency response of the system.
More generally, when we talk about the z-transform of the impulse response, we will refer to it as the system function associated with the system.
Now, the convolution property and these other properties, as I indicated, we will find useful in talking about systems which are described by linear constant coefficient difference equations, and in fact, we'll do that shortly.
But first what I'd like to do is to continue to focus on the system function for linear time invariant systems, and make a couple of comments that tie back to some things that we said in the last lecture relating to the relationship between the region of convergence of a system, or--
I'm sorry, the region of convergence of a z-transform--
and the issue of where that is in relation to the poles of the z-transform.
In particular, we can draw some conclusions tying back to that discussion about the pole locations of the system function in relation to whether the system is stable and whether the system is causal.
In particular, recall from one of the early lectures way back when that stability for a system corresponded to the statement that the impulse response is absolutely summable.
Furthermore, when we talked about the Fourier transform,
the Fourier transform of a sequence converges if the sequence is absolutely summable.
So in fact, the condition for stability of a system and the condition for convergence of the Fourier transform of its impulse response are the same condition, namely absolute summability.
Now what does this mean?
What it means is that if the Fourier transform converges,
that means that the z-transform converges on the unit circle.
Consequently, if the system is stable, then the system function, the z-transform or the impulse response, must also converge on the unit circle.
In other words, the impulse response must have a Fourier transform that converges.
So for a stable system, then, the region of convergence of the system function must include the unit circle in the z-plane.
So we see how stability relates to the location of the region of convergence, and we can also relate causality to the region of convergence.
In particular, we know that if a system is causal, then the impulse response is right-sided.
For a sequence that's right-sided, the region of convergence of its z-transform must be outside the outermost pole.
So for causality, the region of convergence of the system function must be outside the outermost pole.
For stability, the region of convergence must include the unit circle, and we can also then draw from that the conclusion that if we have a system that's causal unstable, then all poles must be inside the unit circle because of the fact that the poles must-- because of the fact that the region of convergence must be outside the outermost pole and has to also include the unit circle.
So for example, if we had let's say a system with a system function as I indicate here with the algebraic expression for the system function being the expression that I indicate here with the pole at z equals a third and another pole at z equals 2 and a zero at the origin.
If, in fact, the system was causal, corresponding to an impulse response that's right-sided, this then would be the region of convergence of the system function.
Alternatively, if I knew that the system was stable, then I know that the region of convergence must include the unit circle, and so this would be then the region of convergence.
And what you might now want to ask yourselves is if instead the region of convergence for the system function is this, then is the system causal?
That's the first question.
The second question is is the system stable?
Remembering that for causality, the region of convergence must be outside the outermost pole, and for stability it must include the unit circle.
Now what I'd like to do is look at the properties of the z-transform, and in particular exploit these properties in the context of systems that are described by linear constant coefficient difference equations.
The three basic properties that play a key role in that discussion are the linearity property, the shifting property, and the convolution property.
These, then, are the properties that I want to exploit.
So let's do that by first looking at a first order difference equation.
In the case of a first order difference equation, which I've written as I indicate here--
no terms on the right hand side, but we could have terms of course, in general-- y of n minus ay of n minus 1 equals x of n.
We can use the linearity property, so that if we take the z-transform of both sides of this expression, that will then be the z-transform of this term plus the z-transform of this term.
So using those properties and together with the shifting property, the property that tells us that the z-transform of y of n minus 1 is z to the minus 1 times the z-transform of y of n, we then convert the difference equation to an algebraic expression.
And we can solve this algebraic expression for the z-transform of the output in terms of the z-transform of the input.
Now what we know from the convolution property is that for a system, the z-transform of the output is the system function times the z-transform of the input.
So this factor that we have, then, must correspond to the system function, or equivalently the z-transform of the impulse response of the system.
In fact, then, if we have this z-transform, we could figure out what the impulse response of the system is by computing or determining what the inverse z-transform, except for the fact that expression is an algebraic expression and doesn't yet totally specify the z-transform because we don't yet know what the region of convergence is.
How do we get the region of convergence?
Well, we have the same issue here as we had with the Laplace transform-- namely, the point that the difference equation tells us, in essence, what the algebraic expression is for the system function, but doesn't specify the region of convergence.
That is specified by either explicitly, because one way or another we know what the impulse response is, or implicitly, because we know certain properties of the system, such as causality and or stability.
If I, let's say, imposed on this system, in addition to the difference equation, the condition of causality, then what that requires is that the impulse response be right-sided or the region of convergence be outside the outermost pole.
So, for this example that would require, then, that the region of convergence correspond to the magnitude of z greater than the magnitude of a.
What you might think about is whether I would get the same condition if I required instead that the system be stable.
Furthermore, you could think about the question of whether I could specify or impose on this system that it be both stable and causal.
The real issue-- and let me just kind of point to it--
is that the answer to those questions relate to whether the magnitude of a is less than 1 or the magnitude of a is greater than 1.
If the magnitude of a is less than 1 and I specify causality, that will also mean that the system is stable.
In any case, given this region of convergence, then the impulse response is the inverse transform of that z-transform, which is a to the n times u of n.
Now let's look at a second order equation, and there's a very similar strategy.
For the second order equation, the one that I've picked is of this particular form, and I've written the coefficients parametrically as I indicate here for a specific reason, which we'll see shortly.
Again, I can apply the z-transform to this expression, and I've skipped an algebraic step or two here.
When I do this, then, again I use the linearity property and the shifting property, and I end up with this algebraic expression.
If I now solve that to express y of z in terms of x of z and a function of z, then this is what I get for the system function.
So this is now a second order system function, and we'll have two zeroes at the origin and it will have two poles.
Again, there's the question of what we assume about the region of convergence--
I haven't specified that yet.
But if we, let's say, assume that the system is causal,
which I will tend to do, then that means that the region of convergence is outside the outermost pole.
Now, where are the poles?
Well, let me just kind of indicate that if--
and you can verify this algebraically at your leisure-- that if the cosine theta term is less than 1, then the roots of this polynomial will be complex.
And in fact, the poles are at r e to the plus or minus j theta.
So for cosine theta less than 1, then the poles are complex,
and the complex poles at an angle theta and with a distance from the origin equal to the parameter r.
In fact, let's just look at that.
What I show here is the pole zero pattern.
Assuming that r is less than 1, and that cosine theta is less than 1, and we have a complex pole pair shown here-- now if we assume that the system was causal, that means that the region of convergence is outside these poles.
That would then include the unit circle, which means that the system is also stable.
In fact, as long as the reach of convergence includes the unit circle, we can also talk about the frequency response of the system-- namely, we can evaluate the system function on the unit circle.
We, in fact, evaluated the Fourier transform associated with this pole zero pattern last time.
Recall that the frequency response, then, is one that has a resonant character with the resonant peak being roughly in the vicinity of the angle of the pole location.
As the parameter r varies-- let's say, as r gets smaller-- this peak tends to broaden.
As r gets closer to 1, the resonance tends to get sharper.
This is now a look at the z-transform, and we see very strong parallels to the Laplace transform.
In fact, throughout the course, I've tried to emphasize-- and it just naturally happens-- that there are very strong relationships and parallels between continuous time and discrete time.
In fact, at one point we specifically mapped from continuous time to discrete time when we talked about discrete time processing of continuous time signals.
What I'd like to do now is turn our attention to another very important reason for mapping from continuous time to discrete time, and in the process of doing this, what we'll need to do is exploit fairly heavily the insight, intuition, and procedures that we've developed for the Laplace transform and the z-transform.
Specifically, what I would like to begin is a discussion relating to mapping continuous time filters to discrete time filters, or continuous time system functions to discrete time system functions.
Now, why would we want to do that?
Well, there are at least several reasons for wanting to map continuous time filters to discrete time filters.
One, of course, is the fact that in some situations what we're interested in doing is processing continuous time signals with discrete time systems-- or, said another way, simulate continuous time systems with discrete time systems.
So it would be natural in a setting like that to think of mapping the desired continuous time filter to a discrete time filter.
So that's one very important context.
There's another very important context in which this is done,
and that is in the context or for the purpose of exploiting established design procedures for continuous time filters.
The point is the following.
We may or may not be processing a sample continuous time signal with our discrete time filter-- it may just be discrete time signals that we're working with.
But in that situation, still, we need to design the appropriate discrete time filter.
Historically, there is a very rich history associated with design of continuous time filters.
In many cases, it's possible and very worthwhile and efficient to take those designs and map them to discrete time designs to use them as discrete time filters.
So, another very important reason for talking about the kinds of mappings that we will be going into is to simply take advantage of what has been done historically in the continuous time case.
Now, if we want to map continuous time filters to discrete time filters, then in continuous time, we're talking about a system function and an associated differential equation.
In discrete time, there is the corresponding system function and the corresponding difference equation.
Basically what we want to do is generate from a continuous time system in some way a discrete time system that meets an associated set of desired specifications.
Now, there are certain constraints that it's reasonable and important to impose on whatever kinds of mappings we use.
Obviously, we want a mapping that will take our continuous time system function and map it to a discrete time system function.
Correspondingly in the time domain, there is a continuous time impulse response that maps to the associated discrete time impulse response.
These are more or less natural.
The two that are important and sometimes easy to lose sight of are the two that I indicate here.
In particular, if we are mapping a continuous time filter with, let's say, a desired or desirable frequency response to a discrete time filter and we would like to preserve the good qualities of that frequency response as we look at the discrete time frequency response, then it's important what happens in the s-plane for the continuous time filter along the j omega axis relate in a nice way to what happens in the z-plane around the unit circle, because it's this over here that represents the frequency response in continuous time and this contour over here that represents the frequency response in discrete time.
So that's an important property.
We want to kind of the j omega axis to map to the unit circle.
Another more or less natural condition to impose is a condition that if we are assured in some way that our continuous time filter is stable, then we would like to concentrate on design procedures that more or less preserve that and will give us stable digital filters.
So these are kind of reasonable conditions to impose on the procedure.
What I'd like to do in the remainder of this lecture is look at two common procedures for mapping continuous time filters to discrete time filters.
The first one that I want to talk about is one that, in fact, is very frequently used, and also one that, as we'll see for a variety of reasons, is highly undesirable.
The second is one that is also frequently used, and as we'll see is, in certain situations, very desirable.
The first one that I want to talk about is the more or less intuitive simple procedure of mapping a differential equation to a difference equation by simply replacing derivatives by differences.
The idea is that a derivative is more or less a difference,
and there's some dummy parameter capital T that I've thrown in here, which I won't focus too much on.
But in any case, this seems to have some plausibility.
If we take the differential equation and do this with all the derivatives, both in terms of y of t and x of t, what we'll end up with is a difference equation.
Now what we can use are the properties of the Laplace transform and the z-transform to see what this means in terms of a mapping-- in particular, using the differentiation property for Laplace transforms.
In the Laplace transform domain, we would have this.
Using the properties for the z-transform, the z-transform of this expression would be this.
So, in effect, what it says is that every place in the system function or in the differential equation that we would be multiplying by s when Laplace transformed.
In the difference equation, we would be multiplying by this factor.
In fact, what this means is that the mapping from continuous time to discrete time corresponds to taking the system function and replacing s wherever we see it by 1 minus z to the minus 1 over capital T. So if we have a system function in continuous time and we map it to a discrete time system function this way by replacing derivatives by differences, then that corresponds to replacing s by 1 minus z to the minus 1 over capital T.
Now we'll see shortly what this mapping actually means more specifically in relating the s-plane to the z-plane.
Let me just quickly, because I want to refer to this, also point to another procedure very much like backward differences which corresponds to replacing derivatives not by the backward differences that I just showed, but by forward differences.
In that case, then, the mapping corresponds to replacing s by z to the minus 1 over capital T. It looks very similar to the previous case.
So there the relationship between these system functions is what I indicate here.
Let's just take a look at what those mappings correspond to when we look at this specifically in the s-plane and in the z-plane.
What I show here is the s-plane, and of course it's things on the left half of the s-plane, poles on the left half of the s-plane that would guarantee stability.
It's the j omega axis the tells us about the frequency response, and in the z-plane it's the unit circle that tells us about the frequency response.
Things inside the unit circle, or poles inside the unit circle, that guarantee stability.
Now the mapping from s-plane to the z-plane corresponding to replacing derivatives by backward differences in fact can be shown to correspond to mapping the j omega axis not to the unit circle, but to the little circle that I show here, which is inside the unit circle.
The left half of the s-plane maps to the inside of that circle.
What does that mean?
That means that if we have a really good frequency response characteristic along this contour in the s-plane, we'll see that same frequency response along this little circle.
That's not the one that we want, though--
we would like to see that same frequency response around the unit circle.
To emphasize this point even more-- suppose, for example,
that we had a pair of poles in our continuous time system function that looked like this.
Then, where they're likely to end up in the z-plane is inside the unit circle, of course.
But if the poles here are close to the j omega axis, that means that these poles will be close to this circle, but in fact might be very far away from the unit circle.
What would happen, then, is that if we saw in the continuous time filter a very sharp resonance, the discrete time filter in fact might very well have that resonance broadened considerably because the poles are so far away from the unit circle.
Now, one plus with this method, and it's about the only one, is the fact that the left half of the s-plane maps inside the unit circle-- in fact, inside a circle inside the unit circle, and so stability is always guaranteed.
Let me just quickly mention, and you'll have a chance to look at this a little more carefully in the video course manual, that for forward differences instead of backward differences, this contour in the s-plane maps to a line in the z-plane, which is a line tangent to the unit circle, and in fact is the line that I showed there.
So not only are forward differences equally bad in terms of the issue of whether they map from the j omega axis to the unit circle, but they have a further difficulty associated with them, which is the difficulty they may not and generally don't guarantee stability.
Now, that's one method, and one, as I indicated, that's often used partly because it seems so intuitively plausible.
What you can see is that by understanding carefully the issues and the techniques of Laplace and z-transforms, you can begin to see what some of the difficulties with those methods are.
The next method that I'd like to talk about is a method that, in fact, is very commonly used.
It's a very important, useful method, which kind of can be motivated by thinking along the lines of mapping the continuous time filter to a discrete time filter in such a way that the shape of the impulse response is preserved-- and, in fact, more specifically so that the discrete time impulse response corresponds to samples of the continuous time impulse response.
And this is a method that's referred to as impulse invariance.
So what impulse invariance corresponds to is designing the filter in such a way that the discrete time filter impulse response is simply a sample version of the continuous time filter impulse response with a sampling period which I denote here as capital T. That will turn into a slightly confusing parameter shortly, and perhaps carried over into the next lecture.
Hopefully, we'll get that straighted out, though, within those two lectures.
Remembering the issues of sampling, the discrete time frequency response, then since the frequency responses the Fourier transform of the impulse response is related to the continuous time impulse response as I indicate here, what this says is that it is the superposition of replications of the continuous time frequency response, linearly scaled in frequency and shifted and added to each other.
It's the same old sort of shifting, adding, or aliasing issue-- the same sampling issues that we've addressed before.
This equation will help us understand what the frequency response looks like.
But in terms of an analytical procedure for mapping the continuous time system function to a discrete time system function, we can see that and develop it in the following way.
Let's consider the continuous time system function expanded in a partial fraction expansion.
And just for convenience, I'm picking first order poles--
this can be generalized multiple order poles, and we won't do that here.
The same basic strategy applies.
If we expand this in a partial fraction expansion, and we look at the impulse response associated with this-- we know how to take the inverse of Laplace transform of this, where I'm just naturally assuming causality throughout the discussion-- the continuous time impulse response, then, is the sum of exponentials with these amplitudes and at these complex exponential frequencies.
Now, impulse invariance corresponds to sampling this,
and so the discrete time impulse response is simply a sampled version of this.
The A sub k, of course, carries down.
We have the exponential--
we're sampling at t equals n capital T, and so we've replaced that here, and then the unit step to truncate things for negative time.
Let's manipulate this further, and eventually what we want to get is a relationship-- a mapping-- from the continuous time to the discrete time filter.
We have this step, and we can rewrite that now as I show here, just simply taking this n outside, and we have e to the s sub k capital T. Now this is of the form the sum of terms like A sub k times is beta to the n.
We can compute the z-transform of this, and the z-transform that we get I show here-- it's simply A sub k over 1 minus e to the s sub k capital T z to the minus 1, simply carrying this term or this parameter down.
So we started with a continuous time system function, which was a sum of terms like A sub k over s minus s sub k-- the poles were at s sub k.
We now have the discrete time filter in this form.
Consequently, then, this procedure of impulse invariance corresponds to mapping the continuous time filter to a discrete time filter by mapping the poles in the continuous time filter.
According to this mapping, the continuous time filter pole at s sub k gets mapped to a pole e to the s sub k capital T, and the coefficients A sub k are preserved.
That, then, algebraically, is what the procedure of impulse invariance corresponds to.
Let's look at how we interpret some of this in the frequency domain.
In particular, we have the expression that tells us how the discrete time frequency response is related to the continuous time frequency response.
This is the expression that we had previously when we had talked about issues of sampling.
So that means that we would form the discrete time frequency response by taking the continuous time 1, scaling it in frequency according to this parameter capital T, and then adding replications of that together.
So if this is the continuous time frequency response, just simply an ideal low-pass filter with a cutoff frequency of omega sub c, then the frequency scaling operation would keep the same basic shape but linearly scale the frequency axis so that we now have omega sub c times T. Then the discrete time frequency response would be a superposition of these added together at multiples of 2 pi in discrete time frequency.
So that's what we have here-- so this is the discrete time frequency response.
This looks very nice-- it looks like impulse invariance will take the continuous time frequency response, just simply linearly scale the frequency axis, and incidentally periodically repeat it.
We know that for an ideal low-pass filter, that looks just fine.
In fact, for a band-limited frequency response, that looks just fine.
But we know also that any time that we're sampling-- and here we're sampling the impulse response-- we have an effect in the frequency domain or the potential for an affect an effect that we refer to as aliasing.
So in fact, if instead of the ideal low-pass filter we had taken a filter that was an approximation to a low-pass filter, then the corresponding frequency scale version would look as I've shown here.
And now as we add these together, then what we will have is some potential for distortion corresponding to the fact that these replications overlap, and what that will lead to is aliasing.
So some things that we can say about impulse invariance is that we have an algebraic procedure-- and I'll illustrate with another example shortly-- for taking a continuous time system function and mapping it to a discrete time system function.
It has a very nice property in terms of mapping, the mapping from the frequency axis in continuous time due to the unit circle-- namely, to a first approximation.
As long as there's no aliasing, the mapping is just simply a linear scaling of the frequency axis, although there may be some aliasing.
That means, of course, that this method can only be used if the frequency response that's being mapped, or if the system that's being mapped, has a frequency response that's approximately low-pass-- it has to be approximately band-limited.
Then what we have is some of potential distortion, which comes about because of aliasing.
Also because of the mapping, the fact that poles at s sub k get mapped to poles at e to the s sub k capital T, if the analog or continuous time filter is stable-- meaning that the real part of s sub k is negative-- then the discrete time filter is guaranteed to be stable.
In other words, the magnitude of z sub k will be guaranteed to be less than 1.
I'm assuming, of course, in that discussion that we are always imposing causality on the systems.
To just look at the algebraic mapping a little more carefully, let's take a simple example.
Here is an example of a system, a continuous time system, where I simply have a resident pole pair with an imaginary part along the imaginary axis of omega sub r and a real part of minus alpha.
And so the associated system function then is just the expression which incorporates the two poles, and I've put in a scale factor of 2 omega sub r.
And now to design the discrete time filter using impulse invariance, you would carry out a partial fraction expansion of this, and that partial fraction expansion is shown below.
We have a pole at minus alpha minus j omega r and at minus alpha plus j omega r.
And to determine the discrete time filter based on impulse invariance, we would map the poles and preserve the coefficients a sub k, referred to as the residues.
And so the discrete time filter that we would end up with as a system function, which I indicate here-- and we have, as I said, preserved the residue, and the pole is now at a to the minus alpha T, e to the minus j omega sub r T. That's one term, and the other term in the sum has a pole at the complex conjugate location.
If we were to add these two factors together, then what we would get is both poles and a 0 at the origin.
In fact, then, the pole is defined by its angle, and this angle is e to the j omega sub r capital T, and by its radius, and this radius is e to the minus alpha capital T.
Now we can look at the frequency response associated with that, and let's just do that.
For the original continuous time frequency response, what we have is simply a resonant character, as I've shown here.
And if we map this using impulse invariance, which we just did, the resulting frequency response that we get is the frequency response which I indicate.
We see that that's basically identical to the continuous time frequency response, except for a linear scaling in the frequency axis, if you just compare the dimensions along which the frequency axis is shown except for one minor issue, which is particularly highlighted when we look at the frequency response at higher frequencies.
What's the reason why those two curves don't quite follow each other at higher frequencies?
Well, the reason is aliasing.
In other words, what's happened is that in the process so applying impulse invariance, the frequency response of the original continuous time filter is approximately preserved, except for some distortion, that distortion corresponding to aliasing.
Well, just for comparison, let's look at what would happen if we took the same example-- and we're not going to work it through here carefully.
We're not work it through it all, not even not carefully.
If we took the same example and mapped it to a discrete time filter by replacing derivatives by backward differences, what happens in that case is that we get a frequency response that I indicate here.
Notice that the resonance in the original continuous time filter is totally lost.
In fact, basically the character of the continuous time frequency response is lost.
What's the reason?
Well, the reason goes back to the picture that I drew before.
The continuous time filter had a pair of resident poles close to the j omega axis.
When those get mapped using backward differences, they end up close to this little circle that's inside the unit circle, but in fact for this example, are far away from the unit circle.
So far we have one useful technique for mapping continuous time filters to discrete time filters.
In part to highlight some of the issues, I focused attention also on some not so useful methods-- namely, mapping derivatives to forward or backward differences.
Next time what I would like to do is look at impulse invariance for another example-- namely, the design of a Butterworth filter, and I'll talk more specifically about what Butterworth filters are at the beginning of that lecture.
Then, in addition, what we'll introduce is another very useful technique, which has some difficulties which impulse invariance doesn't have, but avoids the principal difficulty that impulse invariance does have-- namely, aliasing.
That method is referred to the bilinear transformation, which we will define and utilize next time.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Last time, we introduced the notion of mapping continuous time filters to discrete time filters, and we developed impulse invariance as one useful technique for carrying out that type of mapping.
What I'd like to do in this lecture is illustrate impulse invariance as a design procedure in the context of one specific class of filters, namely Butterworth filters.
And then following that, we'll proceed on to discuss another very important and useful mapping or design procedure referred to as the bilinear transformation.
So to begin, let me just discuss briefly what the class of Butterworth filters is.
And specifically, the Butterworth filters are defined through their frequency response or transfer function.
And I'm using capital B to denote a Butterworth filter.
And by definition, the magnitude squared of the frequency response of a Butterworth filter is given by this expression.
And for example, if N were equal to 1, then this would simply correspond to the magnitude squared of the frequency response for a first order differential equation.
Now if you look at the frequency response, B of omega for this class of filters--
I've illustrated that below--
and what we see is that the frequency response starts at unity, because that's the way that it's normalized, and it has a monotonic characteristic in the pass band and in the stop band.
At a frequency equal to the parameter omega sub c, up here, which is referred to as the cutoff frequency, the Butterworth filter frequency response always goes through the same point, namely 0.707.
And as the order of the Butterworth filter, capital N,
increases, the transition from the pass band to the stop band becomes sharper and sharper.
So for higher order filters, then, the frequency response is flatter in the pass band and drops off more quickly, and attenuates more in the stop band.
Now in designing Butterworth filters, what we want to look at is the location of the poles of the system function.
And we can infer those from our definition of what the frequency response for the Butterworth filter is.
In particular, we have this expression for the magnitude squared of the frequency response.
And we recognize that, of course, as B of j omega times B of minus j omega, that's what the magnitude squared will be equal to.
And in order to convert this to an expression describing the system function, or in terms of the more general Laplace transform variable s, what we recognize is that j omega, in the more general setting, simply corresponds to the Laplace transform variable s.
So this product, in fact, is the Laplace transform for s equal to j omega.
More generally, then, this is the result of evaluating B of s times B of minus s at s equals j omega.
Consequently, comparing this expression with this statement leads us to the conclusion that the transfer function B of s of the Butterworth filter, times B of minus s, is given by the expression that I indicate here, simply replacing j omega by s.
Now what we want to look at are the poles of B of s.
That's what we'll want to get as we design a Butterworth filter.
And we can recognize the poles of this product simply by looking at the roots of the denominative polynomial.
And those roots are-- just taking account of this factor 2N-- those roots are at j omega sub c times the 2N roots of minus 1.
Those roots, in fact, all lie on a circle, and the consequence of that is that the poles of this expression are on a circle.
The circle is of radius omega sub c, and the poles are distributed around the circle.
So here I've illustrated the poles of B of s times B of minus s, for this specific case where capital N is equal to 3.
So there are a total of six poles around this circle.
And then for this specific case, the poles are spaced by 60 degrees.
Now we have B of s times B of minus s.
To get the system function for the Butterworth filter, we'd like to get B of s, and the question now is how do we get that?
Well, the thing to recognize is that wherever this factor has a root, this factor has to have a root at the negative location.
So in fact, when we look at these poles, we compare them with this, for example, associated with B of s, this associated with B of minus s.
And likewise, we compare these two together, likewise we compare these two together.
And so we can extract B of s from this product, simply by taking one pole out of each of those pairs.
Now a question, of course, is out of each pair, which one do we associate with B of s, and which do we associate with B of minus s?
And the answer drops out fairly simply if we recognize that if we want to design filters that are stable, then B of s, the transfer function that we're designing, must have all its poles in the left half of the s plane.
So in fact, out of each of these pairs we would associate the left half plane pole with B of s, and so the transfer function for the Butterworth filter for this particular case, where the this designates the parameter omega sub c and capital N is 3, namely a third order Butterworth filter, is this set of pole locations.
Given those, of course, we can figure out, simply through Algebraic means, what the transfer function B of s is.
All right, so that's what Butterworth filters are.
And now what I'd like to do is talk about the design of a digital Butterworth filter using the design technique that we introduced last time, namely impulse invariance.
And the context in which I will phrase the design is the context of mapping a continuous time signal to a discrete time signal, carrying out filtering using the discrete time filter that we're designing, and then mapping back.
So we're talking about now a discrete time filter that we want to design through impulse invariance from continuous time Butterworth filters, and we're going to get our design specifications in the context of having considered discrete time processing of continuous time signals, where we will map from a continuous time signal to a sequence, carry out the filtering with the discrete time filter that we're going to design.
And then we will take the resulting filtered output and map it back to the continuous time signal.
But this discrete time filter is the one that we're talking about designing.
And for a choice of parameter, there's a sampling frequency, of course, involved in this process.
And the value that I'll pick for the sampling frequency is 10 kilohertz.
OK, all fairly straightforward so far.
And so since we have a sampling rate of 10 kilohertz,
we want to first look at our specifications on the desired continuous time filter and then map those to appropriate specifications on the discrete time filter.
And what I'll pick for the desired specifications on the continuous time filter is that at 1 kilohertz, I will ask that the continuous time frequency response be down by no more than 1 db in comparison with its value at omega equals 0.
So that, in effect, specifies the behavior in the pass band, or the specifications on the pass band.
And for the stop band, I'll specify that the filter is down by 15 db by the time we've gotten to 1.5 kilohertz.
So we have, essentially, the beginning of the transition band-- the end of the pass band--at 1 kilohertz, and the beginning of the stop band at 1.5 kilohertz.
And since we're talking about designing a Butterworth filter, we know that the Butterworth filter is monotonic in the pass band and stop band, and so we'll have a filter specification, something as I show here.
This represents the allowable pass band tolerance.
This is the allowable stop band tolerance.
And if I can draw this without getting myself into trouble,
essentially we're looking for a filter, then, that always stays between the specified boundaries here.
Now what we have to figure out is what the corresponding specifications are for the digital filter.
And the strategy, let me emphasize, is that we have a situation where we're doing discrete time processing of continuous time signals, and we have a set of specifications associated with that.
That imposes specifications on our discrete time filter, and then we want to design the discrete time filter using impulse invariance, and that's the discrete time filter that we'll use in the overall system.
All right, now we want specifications on the discrete time filter, and we want the continuous overall equivalent system to meet certain specifications at certain frequencies related to continuous time frequencies.
Recall that when we sample a continuous time signal,
there's a very specific mapping from the continuous time frequency axis to the discrete time frequency axis.
In particular, the sampling frequency gets mapped to 2 pi.
Well, that means that our other critical frequencies get mapped in proportion to that.
So 1 kilohertz, which is a 1/10 of the sampling frequency, would then convert to a discrete time frequency of 0.2 pi.
And 1.5 kilohertz will convert to a discrete time frequency of 0.3 pi.
So what this says is that for the discrete time filter, we would like the same behavior, but at frequencies-- or the same specifications, but at frequencies normalized to the discrete time frequency axis.
That means that we want the discrete time frequency response magnitude to be greater than or equal to minus 1 db at 2/10 pi-- corresponding to the 1 kilohertz in continuous time-- and for the beginning of the stop band, that would occur at 0.3 pi, at which point we want this less than or equal to minus 15.
So those are our discrete time specifications.
And we now want to design the discrete time filter using impulse invariance.
Now in impulse invariance, as you recall, it corresponds to generating an impulse response which is a sampled version of the continuous time impulse response, and there is a temptation naturally to think all of this parameter, capital T, as necessarily identical to the sampling in the system in which the filter is going to be used.
Now this is a fairly subtle, complicated, tongue twisting issue, but the bottom line on it, the essential point, is that the parameter capital T that we use in impulse invariant design is a totally different, unrelated, and in fact, as it turns out, arbitrary parameter, which is not necessarily pegged to the sampling frequency.
And I think it would be difficult for me to totally clarify that during the lecture.
It's discussed more in the book, and certainly, you should take plenty of time to reflect on it.
All right, but let's now look, then, at where we are in our design procedure, and specifically, what it is that we need to do in order to design the digital Butterworth filter.
Now we have a set of specifications that we've generated relating, essentially, to how we want the pass band of the digital filter and the stop band of the digital filter to behave.
Of course, since this isn't an ideal filter, it has some transition from pass band to stop band, and as we discussed last time, there is aliasing, which we need to at least be aware of.
We specified certain frequencies along this axis which are easily converted by relating to the two axes through this mapping, are easily related back to the continuous time frequency axis, as we have here.
And in particular, now if we were to simply pick that parameter in the impulse invariant design, capital T, as equal to unity, and I indicated just a minute ago that we can pick it arbitrarily, if I pick it as unity, then the procedure would consist of designing the continuous time Butterworth filter with meeting or exceeding the appropriate specifications and then going through the impulse invariant procedure.
All right, so let's do that then.
We want the discrete time impulse response to be the continuous time impulse response sampled, and for convenience, I'm going to pick this parameter capital T equal to 1.
That means that the frequency normalization between the discrete time frequency axis and the continuous time frequency axis in fact is-- those axes are scaled identically, because capital T is equal to 1.
And so now we want the analog, or continuous time, specifications.
And so what we need to do then is design a Butterworth filter.
I'm using capital B here again to denote the frequency response of the Butterworth filter.
The Butterworth filter to have a magnitude which is greater than or equal to minus 1 db prior to the frequency 0.2 pi, and less than or equal to minus 15 at a frequency beyond 0.3 pi.
And so now what we need to do is determine capital N and omega sub c in order to meet or exceed those specifications.
Now if you go through the associated algebra in doing that, let's say that you decide that you want to pick capital N and omega sub c to exactly meet those inequalities at the two frequencies, 0.2 pi and 0.3 pi, what you'll find after going through the algebra is that they're exactly met if capital N is 5.88 and omega sub c is 0.7047.
And this obviously isn't satisfactory as parameters for the Butterworth filter.
Why is that?
The reason is that capital N isn't an integer, and in the class of Butterworth filters, to generate a rational transfer function, capital N, this parameter, must be an integer.
So since it can't be 5.88, the natural thing to do is to move it up to the next closest integer, namely 6.
And that means that we'll end up with a filter that does even better than the specifications.
On the other hand, there's something kind of underneath the surface that is inherent in the impulse invariant design procedure, namely the fact that there will always be some aliasing.
So one strategy, and a natural one, often, in impulse invariant design, is to choose N as the next highest integer as I've done, and then choose the parameter omega sub c so that the pass band specifications are exactly met, and the stop band specifications are then slightly exceeded, and that will leave some margin for aliasing.
All right, now continuing this example, then, how would we complete the design?
Well, we know what our two parameters, capital N and omega sub c are.
That means that we can determine B of s times B of minus s.
Those poles are located on a circle in the complex s plane.
And the poles on that circle are paired, some being associated with B of s and some with B of minus s.
And in particular, to determine B of s, we would simply take the poles on the portion of the circle that's in the left half of the s plane.
Now what that gives us is a Butterworth continuous time filter, which we are then mapping through impulse invariance with capital T equal to 1 to a discrete time filter.
That discrete time filter to be used in a system which has an associated sampling frequency which is 10 kilohertz.
So to get the discrete time filter, then, we first determine B of s, as we just did-- or at least I indicated how to do it.
We would then expand that out in a partial fraction expansion, and then apply the impulse invariant procedure, which consists of mapping the poles in the s plane to poles in the z plane at locations e to the s of k, capital T, capital T is equal to 1.
And retaining the residue, or the coefficient, in the expansion.
And this, then, will give us the transfer function for the discrete time filter.
So if in fact we did that, then the resulting frequency response that we would get is what I've indicated here.
And I indicated first on a magnitude scale, linear magnitude scale, and second on a logarithmic scale.
And as we had originally specified, the digital filter is supposed to be greater than or equal to minus 1 db at point 2 pi, and less than or equal to minus 15 db at 0.3 pi.
And in fact, this slightly exceeds those specifications since we had purposely allowed some margin in the stop band.
Now this is an illustration of the impulse invariant procedure, and it has a number of very nice properties, one of which is that it takes a continuous time filter frequency response and it converts it to a discrete time frequency response, which in the absence of aliasing, looks identical, except for a linear frequency scale change.
And in fact, if we picked capital T equal to 1, then there is no scale change.
It has the major disadvantage that there is always aliasing,
and for some problems, for example, if the filter that we're trying to design is not band limited or low pass, then the aliasing will naturally become intolerable.
Well, there's another design procedure which I now want to introduce, which totally avoids the problems of aliasing, but, obviously, then has its own costs associated with it.
And that's procedure is referred to as the bilinear transformation.
The bilinear transformation, which I won't try to derive here in any detail, is a mapping of continuous time filters to discrete time filters, corresponding to taking the Laplace transform variable s in the continuous time filter and replacing it by what is referred to as a bilinear function of z.
And so if I substitute this in here, that will give me the discrete time frequency response.
Again in this procedure, there's a parameter capital T,
which again is totally irrelevant given the approach that we're taking, and which we will generally tend to normalize out to unity.
And let me just say quickly and in passing that although we won't go through this, the notion that the bilinear transformation can be tied to the concept of taking the differential equation for the continuous time filter, converting it to an integral equation by integrating enough times on both sides, and then converting that to a difference equation by approximating the integrals with the trapezoidal rule.
And that, in effect, will correspond to mapping the continuous time filter to a discrete time filter with the bilinear transformation.
Well, we'll just focus on the properties of the mapping.
And in particular, if we were to substitute into the expression z equal to e to the j omega corresponding to the unit circle, we would find that the unit circle in discrete time corresponds to mapping the j omega axis in continuous time, which is exactly what we want.
Now the mapping between the continuous time frequency and the discrete time frequency is a nonlinear mapping, which is given by the algebraic expression that I indicate here.
And if we plot this mapping, what we have is this curve.
And what this corresponds to, then, is a mapping of the j omega axis, or continuous time frequency, to discrete time frequency.
And if we think more generally of the mapping represented by the bilinear transformation in the context of the s-plane and the z-plane, it corresponds to mapping the j omega axis in the s-plane to once around the unit circle in the z-plane.
And you could also convince yourself that the left half of the s-plane maps to the inside of the unit circle.
And so that means that stable continuous time filters will always map to stable discrete time filters, which is exactly what we desire.
Now notice in this that there's no issue of aliasing.
What's happened is that we've replaced s by a function of z, and it corresponds to mapping the s-plane to the z-plane.
In fact, the whole j omega axis has mapped to once around the unit circle, which obviously requires some type of nonlinear mapping, because, look, the j omega axis is infinitely long.
The unit circle has a finite radius.
And so essentially, what has to happen is if you think of walking along the continuous time frequency axis, and simultaneously walking around the unit circle, if you walk at a constant rate around the unit circle and you're simultaneously walking up the j omega axis, if you want to get around to pi by the time, here, you've gotten to infinity along the continuous time frequency axis, you better start walking faster, and faster, and faster, because you've got an infinite distance to cover while you just go over a finite distance here.
Well, what all that says, really, is that that's why, in fact, we're taking the entire-- or we're able to take the entire omega axis and just map it into an interval of length pi.
All right, now that means that there is a nonlinear distortion of the frequency axis if we were to take a continuous time filter and convert it to a discrete time filter with the bilinear transformation.
How do we account for that, or for that matter, when can we really use it?
And we can see how to both take account of it, and what its limitations are, by recognizing the following.
Suppose that I want to design a discrete time filter and what is going to happen is that it will be mapped to a continuous time filter, or the relationship between the two frequency axes will be given by this curve.
So let's suppose that the continuous time frequency response looks as I've shown here, with a pass band cutoff frequency and a stop band cutoff frequency.
If this were mapped through the bilinear transformation to a discrete time filter, then this cutoff frequency would fall over here, which is related through this curve, and this cutoff frequency, the stop band edge would be here.
Again, it's these frequencies reflected through this curve.
So let's suppose now that what I'd like to do is design a discrete time filter, where the discrete time filter has certain frequency specifications, which is what our previous example has.
It has, let's say, a specified stop band edge and a specified pass band edge, and a specified stop band edge.
The design procedure would then correspond to mapping those frequencies to the corresponding continuous time frequencies, designing the continuous time filter to meet the specifications based on those critical frequencies, then taking the continuous time design and mapping it back to a discrete time filter through the bilinear transformation.
All right, now what we want to do is again map a Butterworth filter, continuous time Butterworth filter to a digital filter in such a way that the digital filter approximately meets the specifications that we had before.
And let me just remind you of where we were.
The critical frequencies were at 0.2 pi and 0.3 pi.
And what we had asked for is that the frequency response be down by no more than 1 db up to 0.2 pi, and down by at least 15 db at 0.3 pi.
So we want to design the same discrete time filter as we did before with impulse invariance.
We now want to do it with the bilinear transformation applied to an appropriate Butterworth design.
OK, well, let's see what kind of specifications we have.
We know that the frequency, the critical frequencies, are mapped through this curve, or this equation.
And I indicated again that this parameter capital T is arbitrary in the design procedure.
That may seem confusing, initially, but there's some further discussion of it in the book, and it's true and important to sort out.
So we're going to pick capital T equal to unity.
And that means then that the corresponding critical frequencies of the continuous time filter are at twice the tangent of 0.2 pi over 2, and twice the tangent of 0.3 pi over 2.
So that means then that the specifications on our continuous time filter are given by this, essentially pass band and stop band edges warped through that nonlinear curve-- this nonlinear curve.
And I'm using capital G here to denote the system function for the resulting continuous time filter.
And so now we could think of designing a Butterworth filter that, let's say, exactly meets these specifications.
Well, if you do that, what you'll find is that you get exact equality here if you pick capital N equal to 5.3.
And again, we have the issue that if we want to meet or exceed the specifications, we can't make the filter order lower, we have to make it higher.
And so we would make it, instead of 5.3, we would make the filter order equal to 6.
And now, again, we have several options and trade offs.
Before, with impulse invariance, we essentially decided to meet the pass band specifications and exceed the stop band specifications to provide some margin for aliasing.
Here, we don't have any aliasing, and we can trade things off anyway we'd like.
The way that I've chosen to do it is to exactly meet the stop band cut off, and exceed the pass band specifications.
And the result of doing that is to choose a filter order capital N equal to 6, and the parameter omega sub c in the Butterworth filter is given by 0.76622.
All right, so now we have the parameters for the continuous time Butterworth filter, which when mapped to the discrete time filter through the bilinear transformation, will exceed our requirements in the pass band and just meet the stop band cutoff.
And so now we want to complete the design procedure.
And given the parameters of the Butterworth filter, we can draw our appropriate circle in the s-plane, which happens to be the same circle as before, but with a different radius.
And then associate poles on this circle with B of s and B of minus s.
In particular, take the ones in the left half of the s-plane-- same ones we had before except we have a different value for omega sub c-- and those, then, represent B of s.
So we can now determine B of s.
And once we have the transfer function B of s, we then map that to a discrete time filter by mapping through the bilinear transformation.
And that then is the design procedure which we would follow this example.
All right, well, let's just see what the result looks like when we're done.
Here I show, again, on both a linear magnitude scale and on a logarithmic or DB scale, the frequency response of the resulting filter.
And recall that the specifications were that at 0.2 pi, we should be down by no more than 1 db, and clearly we've exceeded that considerably.
And at 0.3 pi, we should be down by at least 15 db, and we chose the design so that we would exactly meet that edge.
So we've met this point and exceeded this point, and this is our design.
Now let's just compare this with the impulse invariant design that we saw a few minutes ago.
The impulse invariant design is what I indicate here.
The bilinear transformation design is now overlaid on top of it.
It's hard to see much difference on a linear magnitude scale, but let's look on a logarithmic scale.
And this one that I'm lifting is the bilinear transformation, and this is impulse invariance.
And notice, in fact, something very interesting, which is that the filter that we obtained by mapping the Butterworth filter to a digital filter through the bilinear transformation in fact falls off in frequency much more rapidly than the one that we got through impulse invariance.
The question is why?
Now one thought that might come to mind is, well, impulse invariance has aliasing, the bilinear transformation doesn't have aliasing, that must be a consequence of aliasing.
In fact, that's not the reason.
Aliasing, as it turns out in this particular design, was relatively minimal in the impulse invariant design.
The reason has to do with this nonlinear mapping in the bilinear transformation from the continuous time frequency to the discrete time frequency.
Keep in mind that through that mapping, as you start walking around the unit circle and moving up to j omega axis, as you move up the j omega axis, you have to move faster, and faster, and faster, and faster.
And what's in continuous time frequency at infinity is what you get to in the discrete time frequency by the time you get around to pi.
So in fact, what we're looking at as we look out along this frequency axis is we're seeing higher, and higher, and higher frequencies in the continuous time filter.
By the time we get to pi, we should in fact be in the continuous time filter equivalently off to infinity, which sounds like a pretty uncomfortable place to be.
OK, now this was a fairly rapid trip through a number of issues, in particular, some of the issues associated with the bilinear transformation, and also this issue of how you pick this parameter capital T, and how it might be associated with a sampling frequency if you're doing discrete time processing of continuous time signals.
And we don't have time to explore some of those issues more fully in this lecture.
But I'd like to conclude by making a couple of comments.
One comment is that the two techniques that we've talked about, impulse invariance and the bilinear transformation, are the two techniques that are principally used when one thinks of mapping continuous time filters to discrete time filters for whatever application.
And I stress again that you may want to do that mapping whether or not the discrete time filter is eventually going to be used for processing continuous time signals.
Now impulse invariance had the very nice characteristic that it corresponds to a linear mapping between the two frequency axes, except for the issue of aliasing, and that's a problem with it, and in particular, limits its usefulness to filter designs, or for mapping continuous time filters that are band limited.
On the other hand, we have the bilinear transformation as a design procedure, which totally avoids aliasing, but has the disadvantage or difficulty that it represents a nonlinear mapping from the continuous time filter to the discrete time filter.
Now this nonlinear distortion is perfectly acceptable if we're designing or attempting to design filters that have flat frequency characteristics.
It's not acceptable if, for example, we had a linear frequency characteristic that we wanted to map to a discrete time filter and end up with a linear frequency characteristic.
It won't come out to be linear because of this nonlinear mapping of the frequency axis.
Now there are also a number of other design procedures, which we won't go into, for designing discrete time filters, and among them are a variety of techniques, including, for example, computer aided design procedures.
And I invite you, if you're interested and want to dig into that in more detail and more deeply, to explore that topic by making reference to various of the books listed in the bibliography in the text.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
During the course, we've developed a number of very powerful and useful tools.
And we've seen how these can be used in designing and analyzing systems.
For example, for filtering, for modulation, et cetera.
I'd like to conclude this series of lectures with an introduction to one more important topic.
Namely, the analysis of feedback systems.
And one of the principle reasons that we have left this discussion to the last part of the course is so that we can exploit some of the ideas that we've just developed in some of the previous lectures.
Namely, the tools afforded us by Laplace and z-transforms.
Now, as I had indicated in one of the very first lectures, a common example of a feedback system is the problem of, let's say balancing a broom, or in the case of that lecture balancing my son's horse in the palm of your hand.
And kind of the idea there is that what that relies on, in order to make that a stable system, is feedback.
In that particular case, visual feedback.
That specific problem, the one of balancing something, let's say in the palm of your hand, is an example of a problem, which is commonly referred to as the inverted pendulum.
And it's one that we will actually be analyzing in a fair amount of detail not in this lecture, but in the next lecture.
But let me just kind of indicate what some of the issues are.
Let me describe this in the context not of balancing a broom on your hand, but let's say that we have a mechanical system which consists of a cart.
And the cart can move, let's say in one dimension, and it has mounted on it a bar, a rod with a weight on the top, and it pivots around the base.
So that essentially represents the inverted pendulum.
So that system can be, more or less, depicted as I've indicated here.
And this is the cart that can move along the x-axis.
And here we have a pivot point, a rod, a weight at the top.
And then, of course, there are several forces acting on this.
There is an acceleration that can be applied to the cart, and that will be thought of as the external input.
And then on the pendulum itself, on the weight, there is the force of gravity.
And then typically, a set of external disturbances that might represent, for example, air currents, or wind, or whatever, that will attempt to destabilize the system.
Specifically, to have the pendulum fall down.
Now, if we look at this system in, more or less, a straightforward way, what we have then are the system dynamics and several inputs, one of which is the external disturbances and a second is the acceleration, which is the external acceleration that's applied.
And the output of the system can be thought of as the angular displacement of the pendulum.
Which if we want it balanced, we would like that angular displacement to be equal to 0.
Now, if we know exactly what the system dynamics are and if we knew exactly what the external disturbances are, then in principle, we could design an acceleration.
Namely, an input.
That would exactly generate 0 output.
In other words, the angle would be equal to 0.
But as you can imagine, just as it's basically impossible to balance a broom in the palm of your hand with you eyes closed, what is very hard to ascertain in advance are what the various dynamics and disturbances are.
And so more typically what you would think of doing is measuring the output angle, and then using that measurement to somehow influence the applied acceleration or force.
And that, then, is an example of a feedback system.
So we would measure the output angle and generate an input acceleration, which is some function of what that output angle is.
And if we choose the feedback dynamics correctly, then in fact, we can drive this output to 0.
This is one example of a system which is inherently unstable because if we left it to its own devices, the pendulum would simply fall down.
And essentially, by applying feedback, what we're trying to do is stabilize this inherently unstable system.
And we'll talk a little bit more about that specific application of feedback shortly.
Another common example of feedback is in positioning or tracking systems, and I indicate one here which corresponds to the problem of positioning a telescope, which is mounted on a rotating platform.
So in a system of that type, for example, I indicate here the rotating platform and the telescope.
It's driven by a motor.
And again, we could imagine, in principle, the possibility of driving this to the desired angle by choosing an appropriate applied input voltage.
And as long as we know such things as what the disturbances are that influence the telescope mount and what the characteristics of the motor are, in principle we could in fact carry this out in a form which is referred to as open loop.
Namely, we can choose an appropriate input voltage to drive the motor to set the platform angle at the desired angular position.
However, again, there are enough unknowns in a problem like that, so that one is motivated to employ feedback.
Namely, to make a measurement of the output angle and use that in a feedback loop to influence the drive for the motor, so that the telescope platform is positioned appropriately.
So if we look at this in a feedback context, we would then take the measured output angle and the measured output angle would be fed back and compared with the desired angle.
And the difference between those, which essentially is the error between the platform positioning and the desired position would be put perhaps through an appropriate gain or attenuation and used as the excitation to the motor.
So in the mechanical or physical system, that would correspond to measuring the angle, let's say with a potentiometer.
So here we're measuring the angle and we have an output, which is proportional to that measured angle.
And then we would use feedback, comparing the measured angle to some proportionality factor multiplying the desired angle.
So here we have the desired angle, again through some type of potentiometer.
The two are compared.
Out of the comparator, we basically have an indication of what the difference is, and that represents an error between the desired and the true angle.
And then that is used through perhaps an amplifier to control the motor.
And in that case, of course, when the error goes to 0, that means that the actual angle and the desired angle are equal.
And in fact, in that case also with this system, the input to the motor is, likewise, equal to 0.
Now, as I've illustrated it here, it tends to be in the context of a continuous time or analog system.
And in fact, another very common way of doing positioning or tracking is to instead implement the feedback using a discrete-time or digital system.
And so in that case, we would basically take the position output as it's measured, sample it, essentially convert that to a digital discrete-time signal.
And then that is used in conjunction with the desired angle, which both form inputs to this processor.
And the output of that is converted, let's say, back to an analog or continuous-time voltage and used to drive the motor.
Now, you could ask, why would you go to a digital or discrete-time measurement rather than doing it the way I showed on the previous overlay which seemed relatively straightforward?
And the reason, principally, is that in the context of a digital implementation of the feedback process, often you can implement a better controlled and often also, more sophisticated algorithm for the feedback dynamics.
So that you can take a count, perhaps not only of the angle itself, but also of the rate of change of angle.
And in fact, the rate of change of the rate of change of angle.
So the system, as it's shown there then, basically has a discrete-time or digital feedback loop around a continuous time system.
Now, this is an example of, in fact, a more general way in which discrete-time feedback is used with continuous systems.
And let me indicate, in general, what the character or block diagram of such a system might be.
Typically, if we abstract away from the telescope positioning system, we might have a more general continuous-time system.
And around which we want to apply some feedback, which we could do with a continuous-time system or with a discrete-time system by first converting these signals to discrete-time signals.
Then, processing that with a discrete-time system.
And then, through an appropriate interpolation algorithm, we would then convert that back to a continuous-time signal.
And the difference between the input signal and this continuous-time signal which is fed back, then forms the excitation to the system that essentially we're trying to control.
And in many systems of this type, the advantage is that this system can be implemented in a very reproducible way, either with a digital computer or with a microprocessor.
And although we're not going to go into this in any detail in this lecture, there is some discussion of this in the text.
Essentially, if we make certain assumptions about this particular feedback system, we can move the continuous to discrete-time converter up to this point and to this point, and we can move the interpolating system outside the summer.
And what happens in that case is that we end up with what looks like an inherently discrete-time feedback system.
So, in fact, if we take those steps, then what we'll end up with for a feedback system is a system that essentially can be analyzed as a discrete-time system.
Here we have what is, in the forward path, is basically the continuous-time system with the interpolator at one end and the continuous to discrete-time converter at the other end.
And then we have whatever system it was in the feedback loop-- discrete-time-- that shows up in this feedback loop.
Well, I show this mainly to emphasize the fact, although there are some steps there that we obviously left out.
I show that mainly to emphasize the fact that feedback arises not just in the context of continuous-time systems, but also the analysis of discrete-time feedback systems becomes important.
Perhaps because we have used discrete-time feedback around a continuous-time system.
But also perhaps because the feedback system is inherently discrete-time.
And let me just illustrate one, or indicate one example in which that might arise.
This is an example which is also discussed in somewhat more detail in the text.
But basically, population studies, for example,
represent examples of discrete-time feedback systems.
Where let's say that we have some type of model for population growth.
And since people come in integer amounts that represents essentially the output of any population model, essentially or inherently represents a sequence.
Namely, it's indexed on an integer variable.
And typically, models for population growth are unstable systems.
You can kind of imagine that because if you take these simple models of population, what happens is that in any generation, the number of people, or animals, or whatever it is that this is modeling, grows essentially exponentially with the size of the previous generation.
Now, where does the feedback come in?
Well, the feedback typically comes in, in incorporating in the overall model various retarding factors.
For example, as the population increases, the food supply becomes more limited.
And that essentially is a feedback process that acts to retard the population growth.
And so an overall model-- somewhat simplified-- for a population system is the open loop model in the absence of retarding factors.
And then, very often the retarding factors can be described as being related to the size of the population.
And those essentially act to reduce the overall input to the population model.
And so population studies are one very common example of discrete-time feedback systems.
Well, what we want to look at and understand are the basic properties of feedback systems.
And to do that, let's look at the basic block diagram and equations for feedback systems, either continuous-time or discrete-time.
Let's begin with the continuous-time case.
And now what we've done is simply abstract out any of the applications to a fairly general system, in which we have a system H of s in what's referred to as the forward path, and a system G of s in the feedback path.
The input to the system H of s is the difference between the input to the overall system and the output of the feedback loop.
And I draw your attention to the fact that what we illustrate here and what we're analyzing is negative feedback.
Namely, this output is subtracted from the input.
And that's done more for reasons of convention then for any other reasons.
It's typical to do that and appropriate certainly in some feedback systems, but not all.
And the output of the adder is commonly referred to as the error signal, indicating that it's the difference between the signal fed back and the input to the overall system.
Now, if we want to analyze the feedback system, we would do that essentially by writing the appropriate equations.
In generating the equivalent system function for the overall system, it's best done in the frequency or Laplace transform domain rather than in the time domain.
And let me just indicate what the steps are that are involved.
And there are a few steps of algebra that I'll leave in your hands.
But basically, if we look at this feedback system, we can label, of course-- since the output is y of t, we can label the Laplace transform of the output as Y of s.
And we also have Y of s as the input here.
Because this is the system function, the Laplace transform of r of t is simply the Laplace transform of this input, which is Y of s times G of s.
So here we have Y of s times G of s.
At the adder, the input here is x of s.
And so the Laplace transform of the error signal is simply x of s minus r of s, which is Y of s G of s.
So this is minus Y of s G of s.
That's the Laplace transform of the error signal.
The Laplace transform of the output of this system is simply this expression times H of s.
So that's what we have here.
But what we have here we already called Y of s.
So in fact, we can simply say that these two expressions have to be equal.
And so we've essentially done the analysis, saying that those two expressions are equal.
Let's solve for Y of s over x of s, which is the overall system function.
And if we do that, what we end up with for the overall system function is the algebraic expression that I indicate here.
It's H of s divided by 1 plus G of s H of s.
Said another way, it's the system function in the open loop forward path divided by 1 plus, what's referred to as the loop gain, G of s times H of s.
Let's just look back up at the block diagram.
G of s times H of s is simply the gain around the entire loop from this point around to this point.
So the overall system function is the gain in the forward path divided by 1 plus the loop gain, which is H of s times G of s.
Now, none of the equations that we wrote had relied specifically on this being continuous-time.
We just did some algebra and we used the system function property of the systems.
And so, pretty obviously, the same kind of algebraic procedure would work in discrete-time.
And so, in fact, if we carried out a discrete-time analysis rather than a continuous-time analysis, we would simply end up with exactly the same system and exactly the same equation for the overall system function.
The only difference being that here things are a function of z, whereas if I just flip back the other overlay, we simply have-- previously everything is function of t and in the frequency domain s.
In the discrete-time case, we've simply replaced in the time domain, the independent variable by n.
And in the frequency domain, the independent variable by z.
So what we see is that we have a basic feedback equation, and that feedback equation is exactly the same for continuous-time and discrete-time.
Although we have to be careful about what implications we draw, depending on whether we're talking about continuous-time or discrete-time.
Now, to illustrate the importance of feedback, let's look at a number of common applications.
And also, as we talk about these applications, what will see is that while these applications and context in which feedback is used are extremely useful and powerful, they fall out in an almost straightforward way from this very simple feedback equation that we've just derrived.
Well, the examples that I want to just talk about are, first of all, the use of feedback in amplifier design.
And we're not going to design amplifiers in detail, but what I'd like to illustrate is the basic principle behind why feedback is useful in designing amplifiers.
In particular, how it plays a role in compensating for a non-constant frequency response.
So that's one context that we'll talk about.
A second that I'll indicate is the use of feedback for implementing inverse systems.
And the third, which we indicated in the case of the inverted pendulum, is an important context in which feedback is used is in stabilizing unstable systems.
And what we want to see is why or how a feedback system or the basic feedback equation, in fact, let's us do each of these various things.
Well, let's begin with amplifier design.
And let's suppose that we've built somehow without feedback, an amplifier that is terrific in terms of its gain, but has the problem that whereas we might like the amplifier to have a very flat frequency response, in fact the frequency response of this amplifier is not constant.
And what we'd like to do is compensate for that.
Well, it turns out, interestingly, that if we embed the amplifier in a feedback loop where in the feedback path we incorporate an attenuator, then in fact, we can compensate for that non-constant frequency response.
Well, let's see how that works out from the feedback equation.
We have the basic feedback equation that we derived.
And we want to look at frequency response, so we'll look specifically at the Fourier transform.
And of course, the frequency response of the overall system is the frequency response of the Fourier transform of the output divided by the input.
Using the feedback equation that we had just arrived, that has, in the numerator, the frequency response in the forward path divided by 1 plus the loop gain, which is H of j omega times k.
And this is the key.
Because here, if we choose k times H of j omega to be very large, much larger than 1, then what happens is that these two cancel out.
H of j omega here and in the denominator will cancel out as long as this term dominates.
And in that case, under that assumption, the overall system function is approximately 1/k.
Well, if k is constant as a function of frequency, then we somehow magically have ended up with an amplifier that has a flat frequency response.
Well, it seems like we're getting something for nothing.
And actually, we're not.
There's a price that we pay for that.
Because notice the fact that in order to get gain out of the overall system, k must be less than 1.
So this has to correspond to attenuator.
And we also require that k, which is less than 1, times the gain of the original amplifier, that that product be greater than 1.
And the implication of this, without tracking it in detail right now, the implication in this is that whereas we flatten the frequency response, we have in fact paid a price for that.
The price that we've paid is that the gain is somewhat reduced from the gain that we had before the feedback.
Because k times h must be much larger than 1, but the gain is proportional to 1/k.
Now, one last point to make related to that.
One could ask, well, why is it any easier to make k flat with frequency than to build an amplifier with a flat frequency response?
The reason is that the gain in the feedback path is an attenuator, not an amplifier.
And generally, attenuation with a flat frequency response is much easier to get than gain is.
For example, a resistor, which attenuates, would generally have a flatter frequency response than a very high-gain amplifier.
So that's one common example of feedback.
And feedback, in fact, is very often used in high-quality amplifier systems.
Another very common example in which feedback is used is in implementing inverse systems.
Now, what I mean by that is, suppose that we have a system, which I indicate here, P of s-- input and output.
And what we would like to do is implement a system which is the inverse of this system.
Namely, has a Laplace transform or system function which is 1 over P of s.
For example, we may have measured a particular system and what we would like to design is a compensator for it.
And the question is, by putting this in a feedback loop, can we, in fact, implement the inverse of this system?
The answer to that is yes.
And the feedback system, in that case, is as I indicate here.
So here what we choose to do is to put the system whose inverse we're trying to generate in the feedback loop.
And in this case, a high-gain in the forward path.
Now for this situation, k is, again, a constant.
But in fact, it's a high-gain constant.
And now if we look at the feedback equation, then what we see is an equation of this form.
And notice that if k times P of s is large compared with one, then this term dominates.
The gain in the forward path cancels out.
And what we're left with is a system function, which is just 1 over P of s.
And a system of this type is used in a whole variety of contexts.
One very common one is in building what are called logarithmic devices or logarithmic amplifiers.
Ones in which the input-output characteristic is logarithmic.
It's common to do that with a diode that has an exponential characteristic.
And using that with feedback-- as feedback around a high-gain operational amplifier.
And by the way, the logarithmic amplifier is nonlinear.
What I've said here is linear, or the analysis here was linear.
But that example, in fact, suggests something which is true, which is that same basic idea, in fact, can be used often in the context of nonlinear feedback and nonlinear feedback systems.
Well, as a final example, what I'd like to analyze is the context in which we would consider stabilizing unstable systems.
And I had indicated that one context in which that arises and which we will be analyzing in the next lecture is the inverted pendulum.
And in that situation, or in a situation where we're attempting to stabilize an unstable system, we have now in the forward path a system which is unstable.
And in the feedback path, we've put an appropriate system so that the overall system, in fact, is stable.
Now, how can stability arise out of having an initially unstable system?
Well, again, if we look at the basic feedback equation, the overall system function is the system function for the forward path divided by 1 plus the loop gain, G of s times H of s.
And for stability what we want to examine are the roots of 1 plus G of s times H of s.
And in particular, the poles are the zeroes of that factor.
And as long as we choose G of s, so that the poles of this term--
I'm sorry, so that the zeroes of this term are in the left half of the s-plane, then what we'll end up with is stability.
So stability is dependent not just on h of s for the closed-loop system, but on 1 plus G of s times H of s.
And this kind of notion is used in lots of situations.
I indicated the inverted pendulum.
Another very common example is in some very high-performance aircraft where the basic aircraft system is an unstable system.
But in fact, it's stabilized by putting the right kind of feedback dynamics around it.
And those feedback dynamics might, in fact, involve the pilot as well.
Now, for the system that we just talked about, the stability was described in terms of a continuous-time system.
And the stability condition that we end up with, of course, relates to the zeroes of this denominator term.
And we require for stability that the real parts of the associated roots be in the left half of the s-plane.
Exactly the same kind of analysis, in terms of stability, applies in discrete-time.
That is, in discrete-time, as we saw previously, the basic discrete-time feedback system is exactly the same, except that the independent variable is now an integer variable rather than a continuous variable.
The feedback equation is exactly the same.
So to analyze stability of the feedback system, we would want to look at the zeroes of 1 plus G of z times H of z.
So again, it's those zeroes that affect stability.
And the principal difference between the continuous-time and discrete-time cases is the fact that the stability condition in discrete-time is different than it is in continuous-time.
Namely, in continuous-time, we care for stability about poles of the overall system being in the left half of the s-plane or the right half of the s-plane.
In discrete-time, what we care about is whether the poles are inside or outside the unit circle.
So in the discrete-time case, what we would impose for stability is that the zeroes have a magnitude which is less than 1.
So the basic analysis is the same, but the details of the stability condition, of course, are different.
Now, what I've just indicated is that feedback can be used to stabilize an unstable system.
And as you can imagine there's the other side of the coin.
Namely, if you start with a stable system and put feedback around it, if you're not careful what can happen, in fact, is that you can destabilize the system.
So there's always the potential hazard, unless it's something you want to have happen, that feedback around what used to be a stable system now generates a system which is unstable.
And there are lots of examples of that.
One very common example is in audio systems.
And this is probably an example that you're somewhat familiar with.
Basically, an audio system, if you have the speaker and the microphone in any kind of proximity to each other is, in fact, a feedback system.
Well, first of all, the audio input to the microphone consists of the external audio inputs, and the external audio inputs might, for example, be my voice.
It might be the room noise.
And in fact, as we'll illustrate shortly if I'm not careful, might in fact be the output from a speaker, which represents feedback.
That audio, of course, after appropriate amplification drives a speaker.
And if, in fact, the speaker is, let's say has any proximity to the microphone, then there can be a certain amount of the output of the speaker that feeds back around and is fed back into the microphone.
Now, the system function associated with the feedback I indicate here as a constant times e to the minus s times capital T. The e to the minus s times capital T represents the fact that there is, in general, some delay between the time delay between the speaker output and the input that it generates to the microphone.
The reason for that delay of course, being that there may be some distance between the speaker and the microphone.
And then the constant K2 that I have in the feedback path represents the fact that between the speaker and the microphone, there may be some attenuation.
So if I have, for example, a speaker as I happen to have here, and I were to have that speaker putting out what in fact I'm putting into the microphone, or the output of the microphone, then what we have is a feedback path.
And the feedback path is from the microphone, through the speaker, out of the speaker, back into the microphone.
And the feedback path is from here to the microphone.
And the characteristics or frequency response or system function is associated with the characteristics of propagation or transmission.
If I were to move closer to the speaker and I, by the way, don't have the speaker on right now.
And I'm sure you all understand why.
If I move closer, then the constant K2 gets what?
Gets larger.
And if I move further away the constant K2 gets smaller.
Well, let's look at an analysis of this and see what it is, or why it is, that in fact we get an instability in terms-- or that an instability is predicted by the basic feedback equation.
Now, notice first of all, that we're talking about positive feedback here.
And just simply substituting the appropriate system functions into our basic feedback equation, we have an equation that says that the overall system function is given by the forward gain, which is the gain of the amplifier between the microphone and the speaker, divided by 1 minus-- and the minus because we have positive feedback-- the overall loop gain, which is K1, K2, e to the minus s capital T. And these two gains, K1 and K2 are assumed to be positive, and generally are positive.
So in order for us to--
well, if we want to look at the poles of the system, then we want to look at the zeroes of this denominator.
And the zeroes of this denominator occur at values of s such that e to the minus s capital T is equal to 1 over K1 times K2.
And equivalently that says that the poles of the closed loop system occur at 1 over capital T, and capital T is related to the time delay. 1 over capital T times the log to the base e of K1 times K2.
Well, for stability we want these poles to all be in the left half of the s-plane.
And what that means then is that for stability what we require is that K1 times K2 be less than 1.
In other words, we require that the overall loop gain be less-- the magnitude of the loop gain be less than 1.
If it's not, then what we generate is an instability.
And just to illustrate that, let's turn the speaker on.
And what we'll demonstrate is feedback.
Right now the system is stable.
And I'm being careful to keep my distance from the speaker.
As I get closer, K2 will increase.
And as K2 increases, eventually the poles will move into the right half of the s-plane, or they'll try to.
What will happen is that the system will start to oscillate and go into nonlinear distortion.
So as I get closer, you can hear that we get feedback, we get oscillation.
And I guess neither you nor I can take too much of that.
But you can see that what's happening-- if we can just turn the speaker off now.
You can see that what's happening is that as K2 increases, the poles are moving on to the j omega axis, the system starts to oscillate.
They won't actually move into the right half plane because there are nonlinearities that inherently control the system.
OK, so what we've seen in today's lecture is the basic analysis equation and a few of the applications.
And one application, or one both application and hazard that we've talked about, is the application in which we may stabilize unstable systems.
Or if we're not careful, destabilize stable systems.
As I've indicated at several times during the lecture, one common example of an unstable system which feedback can be used to stabilize is the inverted pendulum, which I've referred to several times.
And in the next lecture, what I'd like to do is focus in on a more detailed analysis of this.
And what we'll see, in fact, is that the feedback dynamics,
the form of the feedback dynamics are important with regard to whether you can and can't stabilize the system.
Interestingly enough, for this particular system, as we'll see in the next lecture, if you simply try to measure the angle and feed that back that, in fact, you can't stabilize the system.
What it requires is not only the angle, but some information about the rate of change of angle.
But we'll see that in much more detail in the next lecture.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
Last time, we began a discussion of feedback.
And I briefly introduced a number of applications.
For example, the use of feedback in compensating for non-ideal elements in a system.
And also, as another example, the use of feedback in stabilizing unstable systems.
In this final lecture, what I'd like to do is focus in more detail on the use of feedback to stabilize unstable systems.
And the specific context in which I'd like to do that is in the context of the inverted pendulum.
Now, I've referred to the inverted pendulum in several past lectures.
And, basically, as everyone realizes, a pendulum is essentially a rod with a weight on the bottom.
And, naturally, an inverted pendulum is just that, upside down.
And so it's essentially a rod that's top-heavy.
And the idea with the inverted pendulum is, although by itself it's unstable around a pivot point at the bottom, the idea is to apply an external input, an acceleration, essentially to keep it balanced.
And we've all probably done this type of thing at some point in our lives, either childhood or not.
And so that's the system that I'd like to analyze and then illustrate in this lecture.
Now, the context in which we'll do that is not with my son's horse, but actually with an inverted pendulum which is mounted on a cart.
And first I'd like to go through an analysis of it, and then we'll actually see how the system works.
So basically, then, the system that we're talking about is going to be a system which is a movable cart.
Mounted on the cart is a rod with a weight on the top.
And this then becomes the inverted pendulum.
And the cart has an external acceleration applied to it, which is the input that we can apply to the system.
And then, in general, we can expect some disturbances.
And I'm going to represent the disturbances in terms of an angular acceleration, which shows up around the weight at the top of the rod.
So if we look at the system, then, as I've kind of indicated it here, basically we have two inputs.
We have an input, which are the external disturbances, which we can assume that we have no control over.
And then there is the acceleration that's apply to the cart externally, or in the case of balancing my son's horse, it's the movement of my hand.
And then, through the system dynamics, that ends up influencing the angle of the rod.
And we'll think of the angle of the rod as the system output.
And basically the idea, then, is to try to keep that angle at 0 by applying the appropriate acceleration.
Now, as I've mentioned several times previously, if we know exactly what the system dynamics are, and if we know exactly what the external disturbances are, then theoretically we can choose an acceleration for the cart which will exactly balance the system, or balance the rod.
However, since the system is inherently unstable, any deviation from that model-- in particular, any unanticipated external disturbances-- will excite the instability.
And what that means is that the rod will fall.
So the idea, then, is, as we'll see, to stabilize the system by placing feedback around it.
In particular through a measurement of the output angle, processed through some appropriate feedback dynamics, using that to control the acceleration of the cart.
And if we choose the feedback dynamics correctly, then, in fact, we can end up with a stable system, even though the open-loop system is unstable.
Well, as a first step, let's analyze the system in its open-loop form.
In particular, let's analyze the system without feedback,
demonstrate that, indeed, it is unstable, and then see how we can stabilize it using feedback.
So the system, then, is--
as we've indicated, the variables involved are the measured angle, which represents the output, the angular acceleration due to external disturbances, and then there is the applied external acceleration on the cart.
So these represent the variables, with L representing the length of the rod, and s of t I indicate here, which we won't really be paying attention to, as the position of the cart.
And without going into the details specifically, what we'll do is set up the equation, or write the equation, in terms of balancing the acceleration.
And if you go through that process, then the basic equation that you end up with is the equation that I indicate here.
And this equation, then, tells us how the basic forces, or accelerations, are balanced, where this is the second derivative of the angle.
And then we have the acceleration due to gravity,
reflected through the angular acceleration times the sine of the angle, the angular acceleration due to the external disturbances, and finally, the angular acceleration due to the motion of the cart.
Now, this equation, as it's written here, is a nonlinear equation in the angle theta of t.
And what we'd like to do is linearize the equation.
And we'll linearize the equation by making the assumption that the angle is very small, close to 0.
In other words, what we'll assume is that we're able to keep the rod relatively vertical.
And our analysis, because we're linearizing the equations, will obviously depend on that.
So making the assumption that the angle is small, we then assume that the sine of the angle is approximately equal to the angle, and the cosine of the angle is approximately equal to 1.
That will then linearize this equation.
And the resulting equation, then, in its linearized form, is the one that I indicate here.
So we have an equation of motion, which linearizes the balance of the accelerations.
And what we see on the right-hand side of the equation is the combined inputs due to the angular acceleration of the rod and the acceleration of the cart.
And on the left-hand side of the equation, we have the other forces due to the change in the angle.
So this is, then, the differential equation associated with the open-loop system, the basic dynamics of the system.
And if we apply the Laplace transform to this equation and solve for the system function, then the basic equation that we're left with expresses the Laplace transform of the angle, equal to the system function, the open-loop system function, times the Laplace transform of the combined inputs.
And I remind you again that our assumption is that this is an input that we have no control over.
This is the input that we can control.
And the two together, of course, form the combined input.
Now, we can, of course, solve for the poles and 0's.
There are no 0's.
And there are two poles, since this is a second-order denominator.
And looking at the poles in the s-plane then, we see that we have a pair of poles, one at minus the square root of g over L, and one at plus the square root of g over L.
And the important observation, then, is that while this pole represents a stable pole, the right half-plane pole represents an unstable pole.
And so this system, in fact, is an unstable system.
It's unstable because what we have is a pole in the right half-plane for the open-loop system.
Now, before we see how to stabilize the system, let's,
in fact, look at the mechanical setup, which we won't turn on for now, and just see essentially what this instability means and what the physical orientation of the equipment is.
So what we have, which you just saw a caricature of in the view graph, is an inverted pendulum.
This represents the pendulum.
And it's mounted with the pivot point at the bottom.
It's mounted on a cart.
And here we have the cart.
And this is a pivot point.
And, as you can see, the cart can move back and forth on a track.
And the external acceleration, which is applied to the cart, is applied through this cable.
And that cable is controlled by a motor.
And we have the motor at this end of the track.
And so as the motor turns, then the cart will move back and forth.
OK.
Now, since the system is not turned on and there's no feedback-- in fact, the system, as we just saw in the transparency, is an unstable system.
And what that instability means is that, for example, if I set the angle to 0 and then let it go, then as soon as there's a slight external disturbance, it will start to fall.
Now, you can imagine that not only is the system unstable as it is, but certainly can't accommodate changes in the system dynamics.
For example, if we change, let's say, the weight of the pendulum.
So if we thought, for example, of changing the weight by,
let's say, putting something like a glass with something in it on top, and I think about trying to balance it by itself, obviously that's hard.
In fact, I would say, without turning the system on, guaranteed to be impossible.
So the basic system, as we've just analyzed it, is inherently an unstable system.
The instability reflecting in the fact that the pendulum, following its own natural forces, will tend to fall.
And now what we want to look at is how, through the use of feedback, we can, in fact, stabilize the system.
So let's first, once again, look at the open-loop system.
And the open-loop system function that we saw was a system function of this form, 1 over Ls squared minus g, and that represents the system function associated with the two inputs, one being the external disturbances, the other being the externally applied acceleration.
And the system function here represents the system dynamics.
And the resulting output is the angle.
Now, the feedback, the basic strategy behind the feedback,
is to in some way use the measured angle, make a measurement of the angle, and use that through appropriate feedback dynamics to stabilize the system.
And so with feedback applied around the system then, there would be some measurement of the angle through some appropriately chosen feedback dynamics.
And that then would determine for us the applied external acceleration corresponding to what the motor does by pulling the cable back and forth.
And we would like to choose G of s so that the overall system is stable.
Now, as you recall from the lecture last time, with the feedback around the system and expressed here in terms of negative feedback, the overall transfer function then is the transfer function associated with the basic feedback loop, where H of s is the open-loop system and G of s corresponds to the feedback dynamics.
Now, that's the basic strategy.
We haven't decided yet how to choose the feedback dynamics.
And that becomes the next step.
And we want to choose the dynamics in such a way that the system ends up being stabilized.
Well, I think the thing to do is just simply begin with what would seem to be the most obvious, which is a simple measurement of the angle.
Let's take the angular measurements, feed that back,
let's say through a potentiometer and perhaps an amplifier, so that there's some gain, and see if we can stabilize the system simply through feedback which is proportional to a measurement of the angle, what.
Is typically referred to as "proportional feedback." Well, let's analyze the results of doing that.
Once again we have the basic feedback equation, where the open-loop transfer function is what we had developed previously, given by the second-order expression.
Now, using proportional feedback, we choose the acceleration a of t to be directly proportional through a gain constant or attenuation constant K1, directly proportional to the measured angle theta of t.
And consequently the system function in the feedback path is just simply a gain or attenuate K1.
So this, then, is the system function for the feedback path.
Well, substituting this into the closed-loop expression,
then the overall expression for the Laplace transform of the output angle is, as I indicate here, namely that theta of s is proportional through this system function.
So the Laplace transform of the input X of t-- and let me remind you that X of t, the external disturbances, now represent the only input, since the other input corresponding to the applied acceleration to the cart is now controlled only through the feedback loop.
So we have this system function, then, for the closed-loop system.
We recognize this once again as a second-order system.
And the poles of this second-order system are then given by plus and minus the square root of g minus K1, divided by L. So K1, the feedback constant, clearly influences the position of the poles.
And let's look, in fact, at where the poles are in the s-plane.
Well, first of all, with K1 equal to 0, which, of course,
is no feedback and corresponds to the open-loop system, we have the poles where they were previously.
And if we now choose K1, let's say less than 0, then what will happen as K1 becomes more and more negative, so that this term is more and more positive, is that the left half-plane pole will move further into the left half-plane, the right half-plane pole will move further into the right half-plane.
So clearly with K1 negative, this pole, which represents an instability, becomes even more unstable.
Well, instead of K1 negative, let's try K1 positive and see what happens.
And with K1 positive, what happens in this case is that the left half-plane pole moves closer to the origin, the right half-plane pole moves closer to the origin.
What one would hope is that they both end up in the left half-plane at some point.
But, in fact, they don't.
What happens is that eventually they both reach the origin, split at that point, and travel along the j-omega-axis.
Now, this movement of the poles, as we vary K1 either positive or negative, what we see is that with the open-loop system, as we introduce feedback, those basic poles move in the s-plane, either this way, as K1 becomes more negative, or for this particular case, if K1 is positive, they move in together, split, and move along the j-omega-axis.
And that locus of the poles, in fact, is referred to in feedback terminology as the "root locus." And as is discussed in much more detail in the text, there are lots of ways of determining the root locus for feedback systems without explicitly solving for the roots.
For this particular example, in fact, we can determine the root locus in the most straightforward way simply by solving for the roots of the denominator of the system function.
Now, notice that in this case, these poles, with K1 positive, have moved together.
They move along the j-omega-axis.
And before they come together, the system is clearly unstable.
Even when they come together and split, the system is marginally stable, because, in fact, the system would tend to oscillate.
And what that oscillation means is that with the measurement of the angles, essentially if the poles are operating on the j-omega-axis, what will happen is that things will oscillate back and forth.
Perhaps with the cart moving back and forth trying to compensate, and the rod sort of moving in the opposite direction.
In any case, what's happened is, with just proportional feedback, we apparently are unable to stabilize the system.
Well, you could think that the reason, perhaps, is that we're not responding fast enough.
For example, if the angle starts to change, perhaps, in fact, we should make the feedback proportional to the rate of change of angles, rather than to the angle itself.
And so we could examine the possibility of using what's referred to as "derivative feedback."
In derivative feedback, what we would do is to choose, for the feedback equation, or for the feedback system function, something reflecting a measurement of the derivative of the angle.
And so here again, once again, we have the open-loop system function with derivative feedback.
We'll attempt to-- or we will use this feedback.
And acceleration, which instead of being proportional to the angle, as it was in the previous case, an acceleration which is proportional to the derivative of the angle.
And so the basic feedback dynamics, or feedback system function, is then G of s is equal to K2 times s.
The multiplication by s reflecting the fact that in the time domain it's measuring the derivative of the angle.
The associated system function is then indicated here.
And if we solve again this second-order equation for its roots, that tells us that the location of the poles are given by this equation.
And so now what we would want to look at is how these poles move as we vary the derivative feedback constant K2.
So let's look at the root locus for that case.
And first, once again, we have the basic open-loop poles.
And the open-loop poles consist of a pole on the left half-plane and a pole on the right half-plane, corresponding in this equation to K2 equal to 0.
That is, no feedback in the system.
If K2 is negative, then what you can see is that this real part will become more negative.
Since K2 is squared here, this is still a positive quantity.
And, in fact, then with K2 less than 0, the root locus that we get is indicated by this.
Now, notice, then, that this right half-plane pole is becoming more unstable.
And the left half-plane pole likewise is becoming more unstable.
And this point, by the way, corresponds to where this pole ends up, or where the root locus ends up, when K2 eventually becomes infinite.
So clearly K2 negative is not going to stabilize the system.
Let's try K2 positive.
And the root locus dictated by this equation, then, is what I indicate here.
The left half-plane pole moves further into the left half-plane.
That's good.
That's getting more stable.
The right half-plane pole is moving closer to the left half-plane, but unfortunately never gets there.
And, in fact, it's when K2 eventually becomes infinite that this pole just gets to the point where the system becomes marginally stable.
So what we found is that with proportional feedback, we can't stabilize the system, with the derivative feedback, we can't stabilize the system, by themselves.
And a logical next choice is to see if we can stabilize the system by both measuring the angle and at the same time being careful to be responsive to how fast that angle is changing, so that if it's changing too fast, we can move the cart or our hand under the inverted pendulum more quickly.
Well, now then what we want to examine is the use of proportional plus derivative feedback.
And in that case, we then have a choice for the acceleration,
which is proportional with one constant to the angle and proportional with another constant to the derivative of the angle.
And so the basic system function, then, with proportional plus derivative feedback, is a system function which is K1 plus K2 times s.
We then have an overall closed-loop system function, theta of s, which is given by this equation.
And so the roots of this equation then represent the poles of the closed-loop system.
And those poles involve two parameters.
They involve the parameter K2, which is proportional to the derivative of the angle, and the constant K1, which is proportional to the angle.
And we'll, first of all, examine this just with K2 positive, because what we can see is that as we vary K1, if K2 were negative, that would, more or less immediately, put poles into the right half-plane.
And the more negative K2 got, the larger this term is going to get.
So, in fact, as it will turn out, we can stabilize the system, provided that we choose K2 to be greater than 0.
All right.
Now, with K2 greater than 0, what happens in the location of the poles is that if K2 is greater than 0, and we choose K1 greater than 0--
I'm sorry.
We choose K1 equal to 0, then in effect the influence of K2 is to shift the poles of the open-loop system slightly.
And so with K2 greater than 0 and K1 equal to 0, we have a set of poles, which are indicated here, and so this is just a shift, slight shift, to the left, depending on the value of K2, a shift to the left of the open-loop poles.
Now, as we vary K1, and in particular we're going to choose K1 greater than 0, what happens is that the poles will begin to move together, as they did previously when we looked at the variation of K1.
The poles will move together, reach a point where we have a second-order pole, and then those poles will split and move parallel to the j-omega-axis.
So what's indicated here is the root locus.
And what this represents, then, as long as we make K1 large enough so that this poles moves into the left half-plane, is that it represents now a stable system.
OK.
So what we've seen is that proportional feedback by itself or derivative feedback by itself won't stabilize the system, whereas with the right choice of feedback constants, proportional plus derivative feedback will.
And we saw that basically by examining the root locus in the s-plane.
All right.
Well, let's actually watch the system in action.
And I described it to you previously.
Basically, an inverted pendulum on a cart.
And so I still have it off.
And, of course, we have the pendulum.
And as I indicated, it's pivoted at the base.
And the angle is measured by a potentiometer that we have attached to the pivot point.
And the measurement of the angle is fed back through this wire to a motor that we have at the other end of the table.
And then that motor basically is used to provide the acceleration to drive the cart.
OK.
Well, let's turn it on.
And when we turn it on, it'll take just an instant to stabilize.
And fortunately we have the constants set right, and there we have now the stabilization of an unstable system.
Remember that with the feedback off, the system is unstable because the pendulum will fall, whereas now it's stabilized.
Now, also, as you can see, not only have we stabilized it,
but we're able to compensate through the feedback to changes in the external disturbances.
For example, by tapping it, because of the feedback and the measurement of the angle, it will more or less automatically stabilize.
Now, in addition to being stable in the presence of external disturbances, it also remains stable and remains balanced even if we were to change the system dynamics.
And let me just illustrate that with the glass that we've talked about before.
Let's first not be too bold, and we'll take the liquid out of the glass.
And presumably if it can adjust to changes in the system dynamics, then if I put the glass on, in fact, it will remain balanced.
And indeed it does.
And let me point out, by the way, that I don't have to be very careful about exactly where I position the glass.
And furthermore, I can change the overall system even further by, let's say for example, pouring a liquid in.
And now let me also comment that I've changed the physics of it a little bit.
Because the liquid can slosh around a little bit, it becomes a little more complicated a system.
But as you can see, it still remains balanced.
Now, if we really don't want to be too conservative at all,
we could wonder whether, with the feedback constants we have, we could, in fact, balance the pitcher on the top.
And, well, I guess we may as well give that a try.
And so now we're changing the mass at the top of the pendulum by a considerable amount.
And, again, the system basically can respond to it.
Now, this is a fairly complicated system.
The liquid is sloshing around.
We, in fact, as you can see, have an instability right now, although it's controlled.
And that's because the physics of the dynamics has changed.
And we can put a little bit more mass into the system, and maybe or maybe not that will cut down on the instability.
OK.
Well, in fact, what happened there is that we increased the mass at the top of the pendulum slightly.
And that provided just enough damping to stabilize the system.
OK.
Well, with this lecture and this demonstration, this concludes this entire set of lectures.
It concludes it especially if the pitcher happens to fall.
But, seriously, this concludes the set of lectures as we have put them together.
And let me just comment that, as a professor of mine once said, and which I've never forgotten, the purpose of a set of lectures, or of a course, or, for that matter anything that you study, is not really to cover a subject, but to uncover the subject.
And I hope that, at least to some degree, we were able to uncover the topic of signals and systems through this series of lectures.
There are a lot of topics that we got only a very brief glimpse into.
And I hope that at least what we've been able to do is get you interested enough in them so that you'll pursue some of these on your own.
And so I'd like to conclude by thanking you, both for your patience and your interest.
And I hope that you have enough interest to pursue some of these topics further.
Thank you.
Okay.
That's a wrap.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
In the last lecture, I introduced and illustrated the kinds of signals and systems that we'll be dealing with throughout this course.
In today's lecture I'd like to be a little more specific, and in particular, talk about some of the basic signals, both continuous-time and discrete-time that will form important building blocks as the course progresses.
Let's begin with one signal, the continuous-time sinusoidal signal, which perhaps you're already somewhat familiar with.
Mathematically, the continuous-time sinusoidal signal is expressed as I've indicated here.
There are three parameters, A, omega_0 and phi.
The parameter A is referred to as the amplitude, the parameter omega 0 as the frequency, and the parameter phi as the phase.
And graphically, the continuous-time sinusoidal signal has the form shown here.
Now, the sinusoidal signal has a number of important properties that we'll find it convenient to exploit as the course goes along, one of which is the fact that the sinusoidal signal is what is referred to as periodic.
What I mean by periodic is that under an appropriate time shift, which I indicate here as T_0, the signal replicates or repeats itself.
Or said another way, if we shift the time origin by an appropriate amount T_0, the smallest value T_0 being referred to as the period, then x(t) is equal to itself, shifted.
And we can demonstrate it mathematically by simply substituting into the mathematical expression for the sinusoidal signal t + T_0, in place of t.
When we carry out the expansion we then have, for the argument of the sinusoid, omega_0 t + omega_0 T_0 + phi.
Now, one of the things that we know about sinusoidal functions is that if you change the argument by any integer multiple of 2 pi, then the function has the same value.
And so we can exploit that here, in particular with omega_0 T_0 and integer multiple of 2 pi.
Then the right-hand side of this equation is equal to the left-hand side of the equation.
So with omega_0 T_0 equal to 2 pi times an integer, or T_0 equal to 2 pi times an integer divided by omega_0, the signal repeats.
The period is defined as the smallest value of T_0.
And so the period is 2 pi divided by omega_0.
And going back to our sinusoidal signal, we can see that-- and I've indicated here, then, the period as 2 pi / omega_0.
And that's the value under which the signal repeats.
Now in addition, a useful property of the sinusoidal signal is the fact that a time shift of a sinusoid is equivalent to a phase change.
And we can demonstrate that again mathematically, in particular if we put the sinusoidal signal under a time shift--
I've indicated the time shift that I'm talking about by t_0-- and expand this out, then we see that that is equivalent to a change in phase.
And an important thing to recognize about this statement is that not only is a time shift generating a phase change, but, in fact, if we inserted a phase change, there is always a value of t_0 which would correspond to an equivalent time shift.
Said another way, if we take omega_0 t_0 and think of that as our change in phase, for any change in phase, we can solve this equation for a time shift, or conversely for any value of time shift, that represents an appropriate phase.
So a time shift corresponds to a phase change, and a phase change, likewise, corresponds to time shift.
And so for example, if we look at the general sinusoidal signal that we saw previously, in effect, changing the phase corresponds to moving this signal in time one way or the other.
For example, if we look at the sinusoidal signal with a phase equal to 0 that corresponds to locating the time origin at this peak.
And I've indicated that on the following graph.
So here we have illustrated a sinusoid with 0 phase, or a cosine with 0 phase, corresponding to taking our general picture and shifting it.
Shifting it appropriately as I've indicated here.
This, of course, still has the property that it's a periodic function, since we simply displaced it in time.
And by looking at the graph, what we see is that it has another very important property, a property referred to as even.
And that's a property that we'll find useful, in general, to refer to in relation to signals.
A signal is said to be even if, when we reflect it about the origin, it looks exactly the same.
So it's symmetric about the origin.
And looking at this sinusoid, that, in fact, has that property.
And mathematically, the statement that it's even is equivalent to the statement that if we replace the time argument by its negative, the function itself doesn't change.
Now this corresponded to a phase shift of 0 in our original cosine expression.
If instead, we had chosen a phase shift of, let's say,
-pi/2, then instead of a cosinusoidal signal, what we would regenerate is a sinusoid with the appropriate phase.
Or, said another way, if we take our original cosine and substitute in for the phase -pi/2, then of course we have this mathematical expression.
Using just straightforward trigonometric identities, we can express that alternately as sin(omega_0*t).
The frequency and amplitude, of course, haven't changed.
And that, you can convince yourself, also is equivalent to shifting the cosine by an amount in time that I've indicated here, namely a quarter of a period.
So illustrated below is the graph now, when we have a phase of -pi/2 in our cosine, which is a sinusoidal signal.
Of course, it's still periodic.
It's periodic with a period of 2 pi / omega_0 again, because all that we've done by introducing a phase change is introduced the time shift.
Now, when we look at the sinusoid in comparison with the cosine, namely with this particular choice of phase, this has a different symmetry, and that symmetry is referred to odd.
What odd symmetry means, graphically, is that when we flip the signal about the time origin, we also multiply it by a minus sign.
So that's, in effect, anti-symmetric.
It's not the mirror image, but it's the mirror image flipped over.
And we'll find many occasions, not only to refer to signals more general than sinusoidal signals, as even in some cases and odd in other cases.
And in general, mathematically, an odd signal is one which satisfies the algebraic expression, x(t).
When you replace t by its negative, is equal to -x(-t).
So replacing the argument by its negative corresponds to an algebraic sign reversal.
OK.
So this is the class of continuous-time sinusoids.
We'll have a little more to say about it later.
But I'd now like to turn to discrete-time sinusoids.
What we'll see is that discrete-time sinusoids are very much like continuous-time ones, but also with some very important differences.
And we want to focus, not only on the similarities, but also on the differences.
Well, let's begin with the mathematical expression.
A discrete-time sinusoidal signal, mathematically, is as I've indicated here, A cos(omega_0 n + phi).
And just as in the continuous-time case, the parameter A is what we'll refer to as the amplitude, omega_0 as the frequency, and phi as the phase.
And I've illustrated here several discrete-time sinusoidal signals.
And they kind of look similar.
In fact, if you track what you might think of as the envelope, it looks very much like what a continuous-time sinusoid might look like.
But keep in mind that the independent variable, in this case, is an integer variable.
And so the sequence only takes on values at integer values of the argument.
And we'll see that has a very important implication, and we'll see that shortly.
Now, one of the issues that we addressed in the continuous-time case was periodicity.
And I want to return to that shortly, because that is one of the areas where there is an important distinction.
Let's first, though, examine the statement similar to the one that we examined for continuous time, namely the relationship between a time shift and a phase change.
Now, in continuous time, of course, we saw that a time shift corresponds to a phase change, and vice versa.
Let's first look at the relationship between shifting time and generating a change in phase.
In particular for discrete time, if I implement a time shift that generates a phase change-- and we can see that easily by simply inserting a time shift, n + n_0.
And if we expand out this argument, we have omega_0 n + omega_0 n_0.
And so I've done that on the right-hand side of the equation here.
And the omega_0 n_0, then, simply corresponds to a change in phase.
So clearly, a shift in time generates a change in phase.
And for example, if we take a particular sinusoidal signal,
let's say we take the cosine signal at a particular frequency, and with a phase equal to 0, a sequence that we might generate is one that I've illustrated here.
So what I'm illustrating here is the cosine signal with 0 phase.
And it has a particular behavior to it, which will depend somewhat on the frequency.
If I now take this same sequence and shift it so that the time origin is shifted a quarter of a period away, then you can convince yourself-- and it's straightforward to work out-- that that time shift corresponds to a phase shift of pi/2.
So in that case, with the cosine with a phase of -pi/2, that will correspond to the expression that I have here.
We could alternately write that, using again a trigonometric identity, as a sine function.
And that, I've stated, is equivalent to a time shift.
Namely, this shift of pi/2 is equal to a certain time shift,
and the time shift for this particular example is a quarter of a period.
So here, we have the sinusoid.
Previously we had the cosine.
The cosine was exactly the same sequence, but with the origin located here.
And in fact, that's exactly the way we drew this graph.
Namely, we just simply took the same values and changed the time origin.
Now, looking at this sequence, which is the sinusoidal sequence, the phase of -pi/2, that has a certain symmetry.
And in fact, what we see is that it has an odd symmetry, just as in the continuous-time case.
Namely, if we take that sequence, flip it about the axis, and flip it over in sign, that we get the same sequence back again.
Whereas with 0 phase corresponding to the cosine that I showed previously, that has an even symmetry.
Namely, if I flip it about the time origin and don't do a sign reversal, then the sequence is maintained.
So here, we have an odd symmetry, expressed mathematically as I've indicated.
Namely, replacing the independent variable by its negative attaches a negative sign to the whole sequence.
Whereas in the previous case, what we have is 0 phase and an even symmetry.
And that's expressed mathematically as x[n] = x[-n].
Now, one of the things I've said so far about discrete-time sinusoids is that a time shift corresponds to a phase change.
And we can then ask whether the reverse statement is also true, and we knew that the reverse statement was true in continuous time.
Specifically, is it true that a phase change always corresponds to a time shift?
Now, we know that that is true, namely, that this statement works both ways in continuous time.
Does it in discrete time?
Well, the answer, somewhat interestingly or surprisingly until you sit down and think about it, is no.
It is not necessarily true in discrete time that any phase change can be interpreted as a simple time shift of the sequence.
And let me just indicate what the problem is.
If we look at the relationship between the left side and the right side of this equation, expanding this out as we did previously, we have that omega_0 n + omega_0 n_0 must correspond to omega_0 n + phi.
And so omega_0 n_0 must correspond to the phase change.
Now, what you can see pretty clearly is that depending on the relationship between phi and omega_0, n_0 may or may not come out to be an integer.
Now, in continuous time, the amount of time shift did not have to be an integer amount.
In discrete time, when we talk about a time shift, the amount of time shift-- obviously, because of the nature of discrete time signals-- must be an integer.
So the phase changes related to time shifts must satisfy this particular relationship.
Namely, that omega_0 n_0, where n_0 is an integer, is equal to the change in phase.
OK.
Now, that's one distinction between continuous time and discrete time.
Let's now focus on another one, namely the issue of periodicity.
And what we'll see is that again, whereas in continuous time, all continuous-time sinusoids are periodic, in the discrete-time case that is not necessarily true.
To explore that a little more carefully, let's look at the expression, again, for a general sinusoidal signal with an arbitrary amplitude, frequency, and phase.
And for this to be periodic, what we require is that there be some value, N, under which, when we shift the sequence by that amount, we get the same sequence back again.
And the smallest-value N is what we've defined as the period.
Now, when we try that on a sinusoid, we of course substitute in for n, n + N. And when we expand out the argument here, we'll get the argument that I have on the right-hand side.
And in order for this to repeat, in other words, in order for us to discard this term, omega_0 N, where N is the period, must be an integer multiple of 2 pi.
And in that case, it's periodic as long as omega_0 N,
N being the period, is 2 pi times an integer.
Just simply dividing this out, we have N, the period, is 2 pi m / omega_0.
Well, you could say, OK what's the big deal?
Whatever N happens to come out to be when we do that little bit of algebra, that's the period.
But in fact, N, or 2 pi m / omega_0, may not ever come out to be an integer.
Or it may not come out to be the one that you thought it might.
For example, let's look at some particular sinusoidal signals.
Let's see.
We have the first one here, which is a sinusoid, as I've shown.
And it has a frequency, what I've referred to as the frequency, omega_0 = 2 pi / 12.
And what we'd like to look at is 2 pi / omega_0, then find an integer to multiply that by in order to get another integer.
Let's just try that here.
If we look at 2 pi / omega_0, 2 pi / omega_0, for this case, is equal to 12.
Well, that's fine. 12 is an integer.
So what that says is that this sinusoidal signal is periodic.
And in fact, it's periodic with a period of 12.
Let's look at the next one.
The next one, we would have 2 pi / omega_0 again.
And that's equal to 31/4.
So what that says is that the period is 31/4.
But wait a minute. 31/4 isn't an integer.
We have to multiply that by an integer to get another integer.
Well, we'd multiply that by 4, so (2 pi / omega_0) times 4 is 31, 31 is an integer.
And so what that says is this is periodic, not with a period of 2 pi / omega_0, but with a period of (2 pi / omega_0) times 4, namely with a period of 31.
Finally, let's take the example where omega_0 is equal to 1/6, as I've shown here.
That actually looks, if you track it with your eye, like it's periodic. 2 pi / omega_0, in that case, is equal to 12 pi.
Well, what integer can I multiply 12 pi by and get another integer?
The answer is none, because pi is an irrational number.
So in fact, what that says is that if you look at this sinusoidal signal, it's not periodic at all, even though you might fool yourself into thinking it is simply because the envelope looks periodic.
Namely, the continuous-time equivalent of this is periodic, the discrete-time sequence is not.
OK.
Well, we've seen, then, some important distinctions between continuous-time sinusoidal signals and discrete-time sinusoidal signals.
The first one is the fact that in the continuous-time case, a time shift and phase change are always equivalent.
Whereas in the discrete-time case, in effect, it works one way but not the other way.
We've also seen that for a continuous-time signal, the continuous-time signal is always periodic, whereas the discrete-time signal is not necessarily.
In particular, for the continuous-time case, if we have a general expression for the sinusoidal signal that I've indicated here, that's periodic for any choice of omega_0.
Whereas in the discrete-time case, it's periodic only if 2 pi / omega_0 can be multiplied by an integer to get another integer.
Now, another important and, as it turns out, useful distinction between the continuous-time and discrete-time case is the fact that in the discrete-time case, as we vary what I've called the frequency omega_0, we only see distinct signals as omega_0 varies over a 2 pi interval.
And if we let omega_0 vary outside the range of, let's say, -pi to pi, or 0 to 2 pi, we'll see the same sequences all over again, even though at first glance, the mathematical expression might look different.
So in the discrete-time case, this class of signals is identical for values of omega_0 separated by 2 pi, whereas in the continuous-time case, that is not true.
In particular, if I consider these sinusoidal continuous-time signals, as I vary omega_0, what will happen is that I will always see different sinusoidal signals.
Namely, these won't be equal.
And in effect, we can justify that statement algebraically.
And I won't take the time to do it carefully.
But let's look, first of all, at the discrete-time case.
And the statement that I'm making is that if I have two discrete-time sinusoidal signals at two different frequencies, and if these frequencies are separated by an integer multiple of 2 pi-- namely if omega_2 is equal to omega_1 + 2 pi times an integer m-- when I substitute this into this expression, because of the fact that n is also an integer, I'll have m * n as an integer multiple of 2 pi.
And that term, of course, will disappear because of the periodicity of the sinusoid, and these two sequences will be equal.
On the other hand in the continuous-time case, since t is not restricted to be an integer variable, for different values of omega_1 and omega_2, these sinusoidal signals will always be different.
OK.
Now, many of the issues that I've raised so far, in relation to sinusoidal signals, are elaborated on in more detail in the text.
And of course, you'll have an opportunity to exercise some of this as you work through the video course manual.
Let me stress that sinusoidal signals will play an extremely important role for us as building blocks for general signals and descriptions of systems, and leads to the whole concept Fourier analysis, which is very heavily exploited throughout the course.
What I'd now like to turn to is another class of important building blocks.
And in fact, we'll see that under certain conditions,
these relate strongly to sinusoidal signals, namely the class of real and complex exponentials.
Let me begin, first of all, with the real exponential, and in particular, in the continuous-time case.
A real continuous-time exponential is mathematically expressed, as I indicate here, x(t) = C e ^ (a t), where for the real exponential, C and a are real numbers.
And that's what we mean by the real exponential.
Shortly, we'll also consider complex exponentials, where these numbers can then become complex.
So this is an exponential function.
And for example, if the parameter a is positive, that means that we have a growing exponential function.
If the parameter a is negative, then that means that we have a decaying exponential function.
Now, somewhat as an aside, it's kind of interesting to note that for exponentials, a time shift corresponds to a scale change, which is somewhat different than what happens with sinusoids.
In the sinusoidal case, we saw that a time shift corresponded to a phase change.
With the real exponential, a time shift, as it turns out, corresponds to simply changing the scale.
There's nothing particularly crucial or exciting about that.
And in fact, perhaps stressing it is a little misleading.
For general functions, of course, about all that you can say about what happens when you implement a time shift is that it implements a time shift.
OK.
So here's the real exponential.
Just C e ^ (a t).
Let's look at the real exponential, now, in the discrete-time case.
And in the discrete-time case, we have several alternate ways of expressing it.
We can express the real exponential in the form C e ^ (beta n), or as we'll find more convenient, in part for a reason at I'll indicate shortly, we can rewrite this as C alpha ^ n, where of course, alpha = e ^ beta.
More typically in the discrete-time case, we'll express the exponential as C alpha ^ n.
So for example, this becomes, essentially, a geometric series or progression as n continues for certain values of alpha.
Here for example, we have for alpha greater than 0, first of all on the top, the case where the magnitude of alpha is greater than 1, so that the sequence is exponentially or geometrically growing.
On the bottom, again with alpha positive, but now with its magnitude less than 1, we have a geometric progression that is exponentially or geometrically decaying.
OK.
So this, in both of these cases, is with alpha greater than 0.
Now the function that we're talking about is alpha ^ n.
And of course, what you can see is that if alpha is negative instead of positive, then when n is even, that minus sign is going to disappear.
When n is odd, there will be a minus sign.
And so for alpha negative, the sequence is going to alternate positive and negative values.
So for example, here we have alpha negative, with its magnitude less than 1.
And you can see that, again, its envelope decays geometrically, and the values alternate in sign.
And here we have the magnitude of alpha greater than 1, with alpha negative.
Again, they alternate in sign, and of course it's growing geometrically.
Now, if you think about alpha positive and go back to the expression that I have at the top, namely C alpha ^ n.
With alpha positive, you can see a straightforward relationship between alpha and beta.
Namely, beta is the natural logarithm of alpha.
Something to think about is what happens if alpha is negative?
Which is, of course, a very important and useful class of real discrete-time exponentials also.
Well, it turns out that with alpha negative, if you try to express it as C e ^ (beta n), then beta comes out to be an imaginary number.
And that is one, but not the only reason why, in the discrete-time case, it's often most convenient to phrase real exponentials in the form alpha ^ n, rather than e ^ (beta n).
In other words, to express them in this form rather than in this form.
Those are real exponentials, continuous-time and discrete-time.
Now let's look at the continuous-time complex exponential.
And what I mean by a complex exponential, again, is an exponential of the form C e ^ (a t).
But in this case, we allow the parameters C and a to be complex numbers.
And let's just track this through algebraically.
If C and a are complex numbers, let's write C in polar form, so it has a magnitude and an angle.
Let's write a in rectangular form, so it has a real part and an imaginary part.
And when we substitute these two in here, combine some things together-- well actually, I haven't combined yet.
I have this for the amplitude factor, and this for the exponential factor.
I can now pull out of this the term corresponding to e ^ (r t), and combine the imaginary parts together.
And I come down to the expression that I have here.
So following this further, an exponential of this form, e ^ (j omega) or e ^ (j phi), using Euler's relation, can be expressed as the sum of a cosine plus j times a sine.
And so that corresponds to this factor.
And then there is this time-varying amplitude factor on top of it.
Finally putting those together, we end up with the expression that I show on the bottom.
And what this corresponds to are two sinusoidal signals, 90 degrees out of phase, as indicated by the fact that there's a cosine and a sine.
So there's a real part and an imaginary part, with sinusoidal components 90 degrees out of phase, and a time-varying amplitude factor, which is a real exponential.
So it's a sinusoid multiplied by a real exponential in both the real part and the imaginary part.
And let's just see what one of those terms might look like.
What I've indicated at the top is a sinusoidal signal with a time-varying exponential envelope, or an envelope which is a real exponential, and in particular which is growing, namely with r greater than 0.
And on the bottom, I've indicated the same thing with r less than 0.
And this kind of sinusoidal signal, by the way, is typically referred to as a damped sinusoid.
So with r negative, what we have in the real and imaginary parts are damped sinusoids.
And the sinusoidal components of that are 90 degrees out of phase, in the real part and in the imaginary part.
OK.
Now, in the discrete-time case, we have more or less the same kind of outcome.
In particular we'll make reference to our complex exponentials in the discrete-time case.
The expression for the complex exponential looks very much like the expression for the real exponential, except that now we have complex factors.
So C and alpha are complex numbers.
And again, if we track through the algebra, and get to a point where we have a real exponential multiplied by a factor which is a purely imaginary exponential, apply Euler's relationship to this, we then finally come down to a sequence, which has a real exponential amplitude multiplying one sinusoid in the real part.
And in the imaginary part, exactly the same kind of exponential multiplying a sinusoid that's 90 degrees out of phase from that.
And so if we look at what one of these factors might look like, it's what we would expect given the analogy with the continuous-time case.
Namely, it's a sinusoidal sequence with a real exponential envelope.
In the case where alpha is positive, then it's a growing envelope.
In the case where alpha is negative--
I'm sorry--
where the magnitude of alpha is greater than 1, it's a growing exponential envelope.
Where the magnitude of alpha is less than 1, it's a decaying exponential envelope.
And so I've illustrated that here.
Here we have the magnitude of alpha greater than 1.
And here we have the magnitude of alpha less than 1.
In both cases, sinusoidal sequences underneath the envelope, and then an envelope that is dictated by what the magnitude of alpha is.
OK.
Now, in the discrete-time case, then, we have results similar to the continuous-time case.
Namely, components in a real and imaginary part that have a real exponential factor times a sinusoid.
Of course, if the magnitude of alpha is equal to 1, then this factor disappears, or is equal to 1.
And this factor is equal to 1.
And so we have sinusoids in both the real and imaginary parts.
Now, one can ask whether, in general, the complex exponential with the magnitude of alpha equal to 1 is periodic or not periodic.
And the clue to that can be inferred by examining this expression.
In particular, in the discrete-time case with the magnitude of alpha equal to 1, we have pure sinusoids in the real part and the imaginary part.
And in fact, in a continuous-time case with r equal to 0, we have sinusoids in the real part and the imaginary part.
In a continuous-time case when we have a pure complex exponential, so that the terms aren't exponentially growing or decaying, those exponentials are always periodic.
Because, of course, the real and imaginary sinusoidal components are periodic.
In the discrete-time case, we know that the sinusoids may or may not be periodic, depending on the value of omega_0.
And so in fact, in the discrete-time case, the exponential e ^ (j omega_0 n), that I've indicated here, may or may not be periodic depending on what the value of omega_0 is.
OK.
Now, to summarize, in this lecture I've introduced and discussed a number of important basic signals.
In particular, sinusoids and real and complex exponentials.
One of the important outcomes of the discussion, emphasized further in the text, is that there are some very important similarities between them.
But there are also some very important differences.
And these differences will surface when we exploit sinusoids and complex exponentials as basic building blocks for more general continuous-time and discrete-time signals.
In the next lecture, what I'll discuss are some other very important building blocks, namely, what are referred to as step signals and impulse signals.
And those, together with the sinusoidal signals and exponentials as we've talked about today, will really form the cornerstone for, essentially, all of the signal and system analysis that we'll be dealing with for the remainder of course.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
in the last lecture, we discussed sinusoidal and real and complex exponential signals both for continuous time and discrete time.
And those signals will form very important building blocks when we return to a discussion of Fourier analysis in a later lecture.
In today's lecture, I'd like to introduce some additional basic signals, specifically the unit step and unit impulse signal.
Let's begin with discrete time and the discrete-time unit step and unit impulse.
The discrete-time unit step is a sequence as I've indicated here, specifically a sequence which is 0 for negative values of its argument, and equal to 1 for positive values of its argument and 0.
So mathematically, the unit step sequence is 1 for n greater than or equal to 0 and 0 for n less than 0.
The unit impulse sequence, likewise, is defined in a straightforward way.
The unit impulse sequence is a sequence which is 0 for all values of its argument except for n = 0.
So the unit step and unit impulse sequence are defined in a straightforward way mathematically.
And in fact, they are also related to each other in a straightforward way mathematically.
Specifically, the unit impulse can be related to the unit step through the relationship that I've indicated here-- delta of n, the unit impulse, equal to a unit step minus the unit step delayed.
So mathematically, the relationship is what is referred to as a first difference.
And to see the validity of this expression, we can simply look at the unit step and its delayed version.
So here, we show the unit step, u[n].
Here, we show the unit step delayed by 1.
So it's 0 for n less than or equal to 0.
And clearly, if we subtract the delayed step from the original unit step, everything subtracts out except at n = 0, at which point the difference is equal to 1.
And so the difference between u[n] and u[n-1] is simply the unit impulse, sometimes incidentally also referred to as the unit sample.
Now, in a similar way, we can express the unit step in terms of the unit impulse.
And there are several ways of doing this.
One way is through a relationship referred to as a running sum.
What I mean by that is the following expression.
If we think of forming the sum from minus infinity up to some value n of a unit impulse or unit sample, then this running sum, in fact, is equal to the unit step.
And we can see that in a fairly straightforward way, by simply observing that in this expression, when n is less than 0, there's nothing accumulated in the sum.
And we can see that graphically as I've shown here.
So for n less than 0, so we accumulate no terms in the sum.
Whereas for n greater than 0, we accumulate 1 non-zero value in the sum, namely the value of the unit sample at n = 0.
So we have, then, one expression for the relationship between the unit step and the unit sample.
We can also develop another relationship by observing in essence that if we look at the unit step sequence, as I've returned to here, we can, in effect, think of the unit step sequence as a succession of unit impulses, one following another.
So if we consider forming a sum of delayed impulses, as I indicate mathematically here, and as I indicate graphically down below, we have an impulse here at n = 0 and an impulse here at n = 1, an impulse here at n = 2, et cetera.
And when we continue to add these up, then what they add up to is the unit step sequence.
And so mathematically, then, that would correspond to an impulse at n = 0 plus an impulse at n = 1 plus an impulse at n = 2, et cetera.
Now, in continuous time, we have a very more or less similar situation.
We will find it equally useful to talk about a unit step continuous-time signal and a unit impulse continuous-time signal.
Let's begin with the continuous-time unit step.
The continuous-time unit step function is graphically indicated as I've shown here.
It's a time function which is 0 for t less than 0.
And it's 1 for t greater than 0.
And so mathematically, what it corresponds to is u[t] t defined as a time function which is 0 for t less than 0, 1 for t greater than 0.
Now, an obvious question is, what happens at t = 0?
And the difficulty here-- which is not a difficulty that arises in the discrete-time case-- is that at t = 0, the units step function is in fact discontinuous, which generates a variety of mathematical problems.
And one can define the unit step at t = 0 in a variety of ways, but the essential point is that the unit step function is discontinuous.
So in effect, what we need to do is think of the unit step function as the limit of a continuous function.
And so we can define a function, which I specify here as u_delta(t) (u sub delta of t).
And u_delta(t) is a time function which is 0 for t less than 0, linearly increases to time delta which would correspond to this break point, and then 1 following that.
And so we can think then of the discontinuous unit step as the limiting form of u_delta(t) as delta goes to 0.
Now, we also want to define a unit impulse function.
And it had a fairly straightforward definition in discrete time.
In continuous time, things get slightly more difficult.
And to motivate the definition, let me return to the discrete-time definition-- or rather, the discrete-time relationship between the unit step and the unit impulse function.
In discrete time, we saw that the unit impulse function is the first difference of the unit step function.
Well, similarly in continuous time, we can talk about an impulse function, which is the first derivative of a unit step function.
So the unit impulse, as we want to define it, is the derivative of the unit step.
Of course, we just finished discussing the fact that the unit step function is, in fact, discontinuous at t = 0.
But we can think of its derivative as related to this approximation to the unit step.
And specifically, we will think of the continuous-time impulse as the derivative of u_delta(t) as delta goes to 0.
So to define the unit impulse, we think of the derivative of this approximation to the unit step and then observe what happens as delta goes to 0.
We have, then, the definition of the unit impulse function more or less formally defined as the first derivative of the unit step, or thought of as the limiting form of the derivative of the approximation to the unit step in the limit, as delta, the duration of the discontinuity, goes to 0.
Well, let's look at that.
If we think about the derivative of u_delta(t), the derivative, of course, is 0 for t less than 0.
It's equal to a constant during this linear slope, and then, it's 0 for t greater than delta.
So the derivative of u_delta(t) will then be as I've indicated here.
And it's simply a rectangle with a height, which is 1 / delta, and a width, which is equal to delta.
And observe that no matter what the value of delta is, the area is always equal to 1.
Now, as we let delta go to 0, what happens is that the width of the rectangle gets smaller, the height of the rectangle gets bigger, the area still remains 1.
As delta goes to 0, of course, the width goes to 0, and the height goes to infinity.
And graphically, we choose to depict that as an arrow, where the arrow indicates the fact that we have an impulse occurring at t = 0, and the height of the impulse is used to represent what the area of the impulse is.
And in this case, since we took the derivative of a unit step, the height is equal to 1.
So the impulse has 0 width, infinite height, area 1.
It's mathematically not terribly comfortable because what we've done is taken the derivative of the unit step, which has a discontinuity at the origin, and there's some mathematical difficulties in doing that.
We'll in fact return to another interpretation of the impulse later to emphasize some of the discomfort with the impulse.
I remember something that Sam Mason used to say that his students said about the unit impulse.
His definition was the unit impulse is something that's so small every place I can't see it except at one point where it's so big I can't see it.
In other words, I can't see it at.
Well, accept some informality with the unit impulse function in the continuous time case, and generally, we'll see that we won't get into particular difficulty.
OK, now the impulse is the derivative of the step.
We saw in the discrete-time case that the step could be recovered from the impulse through a running sum.
In continuous time, if the impulse is the derivative of a step, we would more or less reasonably expect that the step would be like an integral of the impulse.
And indeed, that's true.
In fact, mathematically the relationship is that the unit step function in continuous time is the running integral of the unit impulse function.
And so it's the integral from minus infinity up to time t, where t is the argument at which we're examining u(t).
And so, just as with the discrete-time case, if we are looking at t less than 0, there's no area accumulated in the integral.
If we're looking at t greater than 0, then we accumulate the area under the impulse.
OK, now we'll shortly be returning to a further discussion on use of impulses and step functions.
And in particular, what we'll see is that they provide a very convenient and powerful mechanism for describing a particular class of systems referred to as linear time-invariant systems.
To lead up to that discussion, which will be the principal focus of the next lecture, let's for the remainder of this lecture talk about systems in general and then some properties of systems.
And as the lecture proceeds, the specific properties that I want to get to are properties of linearity and time invariance.
So let's first talk about systems in general.
And a system in general, in its most general definition,
is simply a transformation from an input signal to an output signal.
So in a continuous-time case, we would have a continuous-time system, the input, x[t], and the output, y[t].
And the box, in essence, is used to denote a transformation from x[t] to y[t].
And sometimes, we'll also use a shorthand notation along the lines of indicating that the input, x[t], is transformed to the output, y[t].
Now, we have exactly the same kind of definition in the discrete-time case.
In the discrete-time case, of course, the inputs are sequences, and the outputs are sequences.
And our shorthand notation is similar.
Often when we talk about systems, we'll want to talk about interconnections of systems.
And we'll see again in later lectures that interconnections become very important and powerful.
And in the way of introducing terminology, let me introduce the terminology for a few basic and important interconnections of systems.
The first is what's referred to as a cascade of systems, or sometimes as a series interconnection of systems.
And putting two systems in cascade, as I've indicated here, means taking the output of one system-- let's say system 1-- and using that as the input to the second system, which I've denoted as system 2.
So in this cascade with system 1 first and system 2 second,
we have the output of system 1 going into the input of system 2.
Now, we could, of course, take those two systems and cascade them in the reverse order.
Namely, take the output of system 2 and put it into the input of system 1.
And I've indicated that here.
And we have system 2 first in the cascade followed by system 1.
It's important to keep in mind that in general, for general systems, the order in which you cascade the systems is very important.
And in fact, the overall system transformation will be different depending on which system came first and which system came second.
For example, if system 1 was, let's say, a system for which the output doubles the input, and system 2, the output is the square root of the input, clearly doubling first and then taking the square root is different than taking the square root first and then doubling.
Now, kind of amazingly, what we'll see again when we get to this issue of linearity and time invariance-- which are a class of systems that we'll focus in on-- somewhat amazingly, it turns out that for that specific class of systems, the overall system transformation is independent of the order in which the systems are cascaded.
And we'll see that, of course, in more detail later on.
All right, well, that's a cascade or a series interconnection.
Let's look at another interconnection, which is a parallel interconnection of systems.
And here, what's meant by the interconnection is that the input is fed simultaneously into system 1 and into system 2.
And then, the outputs of these two systems are added to give the overall system output.
Now, in this particular case, in contrast to a cascade, no matter what the systems are, it turns out that a parallel combination, the order in which they're put in parallel is-- the overall transformation is independent of the order.
And of course, that follows from the fact that we're simply adding up outputs.
And the outputs can be added in any order because of the fact that addition doesn't care in which order you're adding things up.
OK, so that's the parallel interconnection.
And let's look at one more interconnection, which is what's referred to as a feedback interconnection.
And this, again, is an interconnection that will become a very important topic much later in the course.
But let me just indicate at this point what I mean by it.
What a feedback interconnection means is that we have one system, system 1, with an input and an output.
The output of system 1 is the output of the overall system.
And that output is fed into system 2.
The output of system 2 is then added to the input to the overall system.
So in essence, what happens in a feedback interconnection is we have one system.
The output of that system is fed back through system 2, added to the overall input.
And then, that sum is what forms the input of system 1.
And we'll see that there are lots of uses for feedback and, of course, also lots of ways that feedback gets in the way.
In fact, probably some of you are already familiar with some of the issues in feedback-- for example, in audio systems or whatever.
But this will be a topic that we'll devote a considerable amount of time to later in the course.
And then, of course, there are lots of other interconnections of systems.
And as the course progresses, we'll see lots of ways in which systems get interconnected both in series and in parallel and feedback interconnections, et cetera, to achieve a wide variety of things.
Now, what I've said so far relates to systems in general.
And you can't say much about systems when you try to treat them in their most general form.
So it's useful and important to focus in on properties that a system may or may not have.
So what I'd like to do now is turn our attention to system properties.
And we'll define a number of them.
Some of them, we'll want to impose on a system.
Some of them, we may not want to impose on a system.
But as things progress, we'll tend to find it useful to ask whether a system does or doesn't have a certain property.
Well, let's begin with a property which I refer to here as memoryless.
And what I mean by a system being memoryless is that the output at any given time, t_0, depends only on the input at the same time.
And so what I'm suggesting is that a memoryless system is one for which the output at a specific time depends only on the input at that time.
And that statement is true, or that definition applies, both for continuous time, as I've indicated here, and also for discrete time, as I've indicated here.
And so we have a similar definition in discrete time,
that the system is memoryless if the output at any given time depends only on the input at that time.
Well, I have a number of examples here.
Let's look at the first example in which the output is the square of the input.
And that is a possible system either in continuous time or discrete time.
And that system, as you would expect, is commonly referred to a squarer.
And since the square of a signal at any time depends only on the value of the signal at that time, clearly a squarer is a memoryless system.
So in fact, we can indicate here that this system is memoryless.
Another important system is what is referred to as an integrator.
The output is equal to the integral of the input.
Here, as I've indicated it, it's not just integrating x(t), but it's integrating the square of x(t).
And whether we square before we integrate or not, the essential point is that since we're integrating the input, the value of the output at any time is an accumulation of past history of the input.
Well, an accumulation of past history in essence implies memory.
To get the output at a given time requires the input over an interval, specifically for longer than that time.
So this system, in fact, is not memoryless.
And now, I have a third system defined here.
The third system is a system, a discrete-time system, in which the output is equal to the input but not quite.
The output at some time-- let's say for example at n = 0-- is equal to the input at one time sample, or instant, or value of the index before that.
And so this, in fact, is a system for which the output is simply the input delayed or shifted by one sample.
So this system is referred to as a unit delay.
And now, the question is, is that system memoryless?
Well, the output depends only on the input--
the output at any instant depends only on the input at one instant.
But since it depends on an instant prior to the time at which we're looking, or different than the time at which we're looking, it violates the definition of memoryless that we introduced.
And so, in fact, the unit delay is a system that has memory, and so let's indicate that here.
So this is not a memoryless system because of the fact that there is 1 unit of delay.
And in essence, delay requires memory.
OK, so that's the issue of memory and memoryless systems.
Let's now turn to another property which a system may or may not have, the property referred to as invertibility.
Now, essentially what invertibility means is that given the output of the system, you can figure out uniquely what the input was.
That's one definition for invertibility.
Said another way, invertibility means that given the output, there's only one input that could have caused it.
That's another common definition for invertibility.
Another way of looking at it is, in fact, to look at it in the context of a cascade of systems.
So let's consider a system.
And here is a system which I refer to as system A. And this could be continuous-time or discrete-time.
It has an input, x_1(t) or x_1[n], depending on whether it's continuous-time or discrete-time that we're talking about, and an associated output.
And here, we have system B with its associated input and output.
And now, let's put these two systems in cascade.
So we'll take the output of system 1 and feed it into the input of system 2-- or system B. So the output of system A goes into the input of system B. And if system A is invertible and system B is its inverse, then the consequence is that the output of system B is equal to the input of system A.
Now, I know there are a lot of inputs and outputs and inverses in there.
But essentially, what we mean by what I've just said is that if we have system A and it's invertible, and if we cascade it with its inverse, system B, then the overall cascade of these two systems is simply what's referred to as the identity system.
And the identity system is simply a system which if you put a signal into it, you get the same signal out of it.
In other words, the overall system is no transformation at all.
And clearly, of course, for a system to be able to be cascaded with another system to generate the identity system requires that the first system, system A, be invertible.
So let's look at some examples.
If we had system A as I've indicated here, where now the output is the running integral of the input-- and remember that we saw the running integral when we talked about the relationship between steps and impulses-- if this happened to be an impulse, then the output would be a step.
This system is referred to, of course, as an integrator since the output is the running integral of the input.
And the integrator, in fact, is an invertible system.
And its inverse is a system for which the output is the derivative of the input.
So the inverse of system A, if system A is an integrator, is a system for which the output is equal to the derivative of the input which, not surprisingly, is referred to as a differentiator.
So an integrator is invertible.
Its inverse is a differentiator.
What you might want to think about is the question of whether a differentiator is invertible.
Now, to answer that, what you would ask yourself is, if you always knew what the derivative of the signal is, would you necessarily know what the signal was?
In other words, if you have a differentiator and you have the output of the differentiator, could you always figure out what the input was?
If you could, the system would be invertible.
If you couldn't, the system would not be invertible.
So you might just want to think about that.
I guess I won't tell you right now.
But I'm sure that you'll think about that more, particularly with the guidance of the video manual.
OK, well let's look at one last system.
I've indicated here a system for which the output is related to the input through this curve.
And what I mean by this curve, which wasn't done quite as well as I might have, is that the output is equal to the square of the input.
So this system is our squarer as we talked about before.
We saw previously, or discussed the fact previously, that the squarer is a memoryless system.
And now the question is, is a squarer an invertible system?
Well, the question then is, if you're given the square of a signal, can you figure out what the signal is?
And as I'm sure you've already suspected, the answer to that is no, because obviously if the signal was negative or positive, you wouldn't be able to figure that out after you've squared.
So in fact, the squarer is not invertible.
All right, so we've introduced several properties.
And by the way, as we've gone through it, also introduced some systems that will turn out to be useful and important systems.
And now, let's continue with some other properties.
A property that we'll find useful to make reference to,
from time to time, and will, in fact, play a fairly important role in a variety of discussions during the course, is a property which is referred to as causality.
Now, in essence what causality means is the following.
A system is set to be causal if, as one way of saying it, it only responds when you kick it.
Is another way of saying it, its response at any time only depends on values of the input prior to that time.
So a causal system, both continuous time and discrete time, is defined sometimes as a system which has the property that the output at any time depends only on the input prior or equal to that time.
Essentially what we're saying is that the system can't anticipate future inputs.
And so that, in fact, is another possible definition for causality, that the system is causal if it can't anticipate future inputs.
Finally, an alternative way of saying it mathematically is as follows.
If I have two signals, x_1(t) and x_2(t), with their associated outputs, y_1(t) and y_2(t), then a system is said to be causal if and only if it has the property that if those two inputs are identical up until some time, then the outputs are identical up until the same time.
So if we have two signals that are exactly the same up until some time and perhaps do something different later on, causality requires that the outputs not anticipate the fact that those inputs are at some future time going to do something different.
And that, in fact, is the most useful mathematical definition of causality.
And of course, I've written that here for continuous time.
And the same definition of causality also applies for discrete time.
OK, well let's look at an example.
Let's take an example which is a system which is the following discrete-time system.
The output at any given time is the sum of x[n], x[n] delayed, and x[n] anticipated.
And this is, in fact, a system that's very useful and referred to as a moving average.
And so if we think of a moving average, if we have here a sequence, x[n] and x[n] with other values going off in both directions, for any value, n_0, at which we're computing the output-- and this is y[n], the output-- we take x[n 0], x[n 0-1], and x[n 0+1].
And so to form that moving average, we would take these three values and combine them together, adding them and then dividing by 3 to get that.
Well, is the system causal?
One way to answer that is to determine whether the output at any given time depends on future values of the input.
And clearly, if you look at this, what you see is that the output at time n_0 depends both on past values and on future values.
As opposed to another moving average, which I've indicated here, where I simply shifted the values that I combined together.
And in this case, because of the way in which I picked the values, the output depends on the value at n_0, the value at n_0 - 1, and the value at n_0 - 2.
So y[n 0] would depend here on x[n 0], x[n 0-1], and x[n 0-2].
And so in this case, the system is not causal.
And in this case, the system is causal.
All right, now let's turn to another system property, the property referred to as stability.
Now, there are lots of definitions of stability, and some of them get very mathematical and formal.
But we've chosen, and what we'll use as our definition of stability, is what's called bounded-input bounded-output stability.
And essentially, the definition is that a system is stable if and only if for every bounded input, the output is bounded.
So the notion is if you have a system and the input never gets above some finite value, then stability requires that the output also stay within some bounded values.
And I'm sure that stability and instability are things that you're kind of informally familiar with.
Let me just emphasize the point with something that I borrowed actually from my son with some reluctance on his part.
If we, for example, take a system like this, which is in essence a pendulum, this system as I'm holding it here is stable because if I put in a bounded input, which is a displacement, the output, which is the movement of it, remains bounded.
Now on the other hand, if I put the system like this,
which is, in fact, what's referred to as an inverted pendulum, although we could conceivably get this to balance, just a slight displacement because of the fact that the pendulum is inverted, a slight displacement and the output becomes unbounded.
Now, an interesting thing with the inverted pendulum, by the way, which I'm sure all of you, if you were anything like me, were intrigued with as a kid, was the notion that you could take an inverted pendulum and in effect turn it back into a stable system by using what I'm doing right now, which is feedback.
What I've done in that case is I've stabilized the system by using feedback, visual feedback, from my eye to my hand.
And in fact, one of the very important things that we'll see about feedback when we talk about feedback systems much later in the course is that one of their very important applications is in stabilizing unstable systems.
By the way, one of their problems is that if not used correctly, it can destabilize stable systems.
OK, well let's continue on with our property of stability.
I have here, again, the example of an integrator.
And as I indicate here, if we have an integrator and we put a step function into it or a step signal into it, the output is what's referred to as a ramp signal.
It linearly increases.
Now, the question is, is a ramp unbounded?
The input is bounded.
The step input is bounded.
The ramp is unbounded because if you try to establish any bound on it, you can always go out far enough in time so that the output will exceed that bound.
So in fact, the integrator is not stable.
OK, now finally, I'd like to turn to two properties that we'll make considerable use of as the course goes on, the properties of time invariance and linearity.
Time invariance, in essence, says that the system doesn't really care what you call the origin.
In other words, it says if you take the input and you shift it in time, all that you've done is taken the output and shifted it in time by the same amount.
Somewhat more formally as I've indicated here, if in continuous time we have an input, x(t), which generate an output, y(t), then time invariance requires that if the input is shifted by any amount of time, the output is shifted by the same amount of time.
And exactly the same applies in discrete time.
For example, if we have a system which is a system I've shown here, which by the way is the system that we talked about previously to go from a step sequence--
I'm sorry, from an impulse sequence to a step sequence.
We called it a running sum.
And actually, what it's also often called is an accumulator.
What it does is accumulate past values of the input.
Well, is the accumulator time-invariant?
The best way to establish that is to work through the equations and verify that it either does or doesn't satisfy the formal definition of time invariance.
Informally, if you think about it, it makes intuitive sense that the accumulator is time-invariant because if you're accumulating values and if you delay the values that you're putting into the accumulator, then the associated values that come out will be delayed by the same amount.
The accumulator doesn't care really if you shift the input.
It'll just simply shift the associated output.
But more generally, if you're trying to test time invariance, it's important to return to the definition.
And that's what you're required to do in the examples in the video manual.
OK, well, I indicate another example.
We had the example of an accumulator.
Here's another example which, in fact, as we'll see later is a system which is a modulator.
The output is the input, modulated.
And although you might think at first that this system is time invariant, in fact it is not, because the input shifted generates an output which is the input shifted times the same modulation function.
Whereas if we were to take the output of the system, we have x(t) is the input, then what that would correspond to is sin(t-t_0) * x(t-t_0).
And since these two are not equal, this system is not time invariant.
And this is an example that often causes a slight amount of difficulty because it seems like when you look at it ought to be.
And so I strongly encourage you, in the context of working problems in the manual, that you think very carefully about this and at least believe that what I told you is the right answer.
OK, now the final property that I want to introduce today is the property of linearity.
And linearity is defined in a manner similar for continuous time and discrete time.
And what it says is that if we have some inputs with associated outputs, let's say x_1(t) and x_2(t), then a system is linear if it has the property that the output to a linear combination of those inputs is the same linear combination of the associated outputs.
And so that's what I've indicated here, that if we now put into the system a linear combination of those inputs, then for linearity, we require that the output is the same linear combination.
And exactly the same applies in discrete time.
And you can show from this definition that if a system is linear with two inputs, then it's linear in terms of an arbitrary number of inputs.
I have a number of examples.
And these are examples that, again, I ask you to think about as you look at the video manual.
Just to suggest the answer--
well, not to suggest but to tell you the answer, the integrator as we have here is linear.
This system in which the output is double the input plus a constant, you would kind of think it's linear because it's a straight line.
But one has to be careful.
And in fact, as it turns out, this is not linear.
There is a qualifier attached to it because it has a property referred to as incrementally linear, which is discussed somewhat more in the text.
And finally, we have a system which is the squarer that I've indicated again here.
And squaring it is definitely not a linear operation.
OK, so what we've done, then, is to introduce a number of properties of systems.
And we've also, by the way, as I've stressed previously, as we've gone along introduced also a number of important and useful systems, like the accumulator, the integrator, the differentiator, et cetera.
What we'll see is that the properties of linearity and time invariance in particular become central and important properties.
And in the next lecture, what we'll show is that with systems that are linear and time-invariant, the use of the impulse function, both in continuous time and discrete time, provides an extraordinarily important and useful mechanism for characterizing those systems.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
NOISEEVENT
In the last lecture, we discussed a number of general properties for systems, which, as you recall, applied both to continuous-time and to discrete-time systems.
These properties were the properties associated with a system having memory.
The issue of whether a system is or isn't invertible, we talked about causality and stability, and finally we talked about when linearity and time invariance.
In today's lecture, what I'd like to do is focus specifically on linearity and time invariance, and show how for systems that have those properties, we can exploit them to generate a general representation.
Let me begin by just reviewing the two properties again quickly.
Time invariance, as you recall, is a property that applied both to continuous-time and discrete-time systems, and in essence stated that for any given input and output relationship if we simply shift the input, then the output shifts by the same amount.
And of course, exactly the same kind of statement applied in discrete time.
So time invariance was a property that said that the system didn't care about what the time origin of the signal is.
Linearity was a property related to the fact that if we have a set of outputs associated with a given set of inputs-- as I've indicated here with the inputs as phi_k and the outputs psi_k, then the property of linearity states that if we have an input which is a linear combination of those inputs, then the output is a linear combination of the associated outputs.
So that linear combination of inputs generates, for a linear system, an output which is a linear combination of the associated outputs.
Now the question is, how can we exploit the properties of linearity and time invariance?
There's a basic strategy which will flow more or less through most of this course.
The strategy is to attempt to decompose a signal, either continuous-time or discrete-time, into a set of basic signals.
I've indicated that here.
And the question then is, what basic signal should we pick?
Well, the answer, kind of, is we should pick a set of basic signals that provide a certain degree of analytical convenience.
So we choose a set of inputs for the decomposition that provide outputs that we can easily generate.
Now, as we'll see, when we do this, there are two classes of inputs that are particularly suited to that strategy.
One class is the set of delayed impulses, namely decomposing a signal into a linear combination of these.
And as we'll see, that leads to a representation for linear time-invariant systems, which is referred to as convolution.
The second is a decomposition of inputs into complex exponentials-- a linear combination of complex exponentials-- and that leads to a representation of signals and systems through what we'll refer to as Fourier analysis.
Now, Fourier analysis will be a topic for a set of later lectures.
What I'd like to begin with is the representation in terms of impulses and the associated description of linear time-invariant systems using convolution.
So let's begin with a discussion of discrete-time signals, and in particular the issue of how discrete-time signals can be decomposed as a linear combination of delayed impulses.
Well, in fact, it's relatively straightforward.
What I've shown here is a general sequence with values which I've indicated at the top.
And more or less as we did when we talked about representing a unit step in terms of impulses, we can think of this general sequence as a sequence of impulses-- delayed, namely, occurring at the appropriate time instant, and with the appropriate amplitude.
So we can think of this general sequence and an impulse occurring at n = 0 and with a height of x[0], plus an impulse of height x[1] occurring at time n = 1, and so that's x[1] delta[n-1], an impulse at -1 with an amplitude of x[-1], etc.
So if we continued to generate a set of weighted, delayed unit samples like that, and if we added all these together, then that will generate the total sequence.
Algebraically, then, what that corresponds to is representing the sequence as a sum of individual terms as I've indicated here or in terms of a general sum, the sum of x[k] delta[n-k].
So that's our strategy--
the strategy is to decompose an arbitrary sequence into a linear combination of weighted, delayed impulses.
And here again is the representation, which we just finished generating.
Now, why is this representation useful?
It's useful because we now have a decomposition of the sequence as a linear combination of basic sequences, namely the delayed impulses.
And if we are talking about a linear system, the response to that linear combination is a linear combination of the responses.
So if we denote the response to a delayed impulse as h_k[n], then the response to this general input is what I've indicated here, where y[n], of course, is the output due to the general input x[n]. h_k[n] in is the output due to the delayed impulse, and these are simply the coefficients in the weighting.
So for a linear system, we have this representation.
And if now, in addition, the system is time-invariant, we can, in fact, relate the outputs due to these individual delayed impulses.
Specifically, if the system is time-invariant, then the response to an impulse at time k is exactly the same as the response to an impulse at time 0, shifted over to time k.
Said another way, h_k[n] is simply h_0[n-k], where h_0 is the response of the system to an impulse at n = 0.
And it's generally useful to, rather than carrying around h_0[n], just simply define h_0[n] as h[n], which is the unit sample or unit impulse response of the system.
And so the consequence, then, is for a linear time-invariant system, the output can be expressed as this sum where h[n-k] is the response to an impulse occurring at time n = k.
And this is referred to as the convolution sum.
Now, we can--
just to emphasize how we've gone about this, let me show it from another perspective.
We of course have taken the sequence x[n], we have decomposed it as a linear combination of these weighted, delayed impulses.
When these are added together, those correspond to the original sequence x[n].
If this impulse, for example, generates a response which is x[0] h[n], where h[n] is the response to a unit impulse at n = 0, and the second one generates a delayed weighted response, and the third one similarly, and we generate these individual responses, these are all added together, and it's that linear combination that forms the final output.
So that's really kind of the way we're thinking about it.
We have a general sequence, we're thinking of each individual sample individually, each one of those pops the system, and because of linearity, the response is the sum of those individual responses.
That's what happens in discrete time, and pretty much the same strategy works in continuous time.
In particular, we can begin in continuous time with the notion of decomposing a continuous-time signal into a succession of arbitrarily narrow rectangles.
And as the width of the rectangles goes to 0, the approximation gets better.
Essentially what's going to happen is that each of those individual rectangles, as they get narrower and narrower, correspond more and more to an impulse.
Let me show you what I mean.
Here we have a continuous-time signal, and I've approximated it by a staircase.
So in essence I can think of this as individual rectangles of heights associated with the height of the continuous curve, and so I've indicated that down below.
Here, for example, is the impulse corresponding to the rectangle between t = -2 Delta and t = -Delta.
Here's the one from -Delta to 0, and as we continue on down,
we get impulses, or rather rectangles, from successive parts of the wave form.
Now let's look specifically at the rectangle, for example,
starting at 0 and ending at Delta, and the amplitude of it is x(Delta).
So what we have--
actually, this should be x(0), and so let me just correct that here.
That's x(0).
And so we have a rectangle height x(0), and recall the function that I defined last time as delta_Delta(t), which had height 1 / delta and width delta.
So multiplying finally by this last little Delta, then, this is a representation for the rectangle that I've shown there.
Now there's a little bit of algebra there to kind of track through, but what we're really doing is just simply representing this in terms of rectangles.
What I want to do is describe each rectangle as in terms of that function delta_Delta(t), which in the limit, then, becomes an impulse.
So let's track that through a little further.
When we have that linear combination, then, we're saying that x(t) can be represented by a sum as I indicate here, which I can then write more generally in this form, just indicating that this is an infinite sum.
We now want to take the limit as Delta goes to 0, and as Delta goes to 0, notice that this term becomes arbitrarily narrow.
This goes to our impulse function, and this, of course, goes to x of tau.
And in fact, in the limit, a sum of this form is exactly the way an integral is defined.
So we have an expression for y(t) in terms of an impulse function.
There, I have to admit, is a little bit of detail to kind of focus on at your leisure, but this is the general flow of the strategy.
So what we have now is an integral that tells us that tells us how x(t) can be described as a sum or linear combination involving impulses.
This bottom equation, by the way, is often referred to as the sifting integral.
In essence, what it says is that if I take a time function x(t) and put it through that integral, the impulse as it zips by generates x(t) all over again.
Now, at first glance, what it could look like is that we've taken a time function x(t) and proceeded to represent it in a very complicated way, in terms of itself, and one could ask, why bother doing that?
And the reason, going back to what our strategy was, is that what we want to do is exploit the property of linearity.
So by describing a time function as a linear combination of weighted, delayed impulses, as in effect we've done through this summation that corresponds to a decomposition in terms of impulses, we can now exploit linearity, specifically recognizing that the output of a linear system is the sum of the responses to these individual inputs.
So with h_kDelta(t) corresponding to the response to delta_Delta(t-kDelta) and the rest of this stuff, the x(kDelta) and this little Delta are basically scale factors-- for a linear system, then, if the input is expressed in this form, the output is expressed in this form, and again taking the limit as Delta goes to 0, by definition, this corresponds to an integral.
It's the integral that I indicate here with h_tau(t)
corresponding to the impulse response due to an impulse occurring at time tau.
Now, again, we can do the same thing.
In particular, if the system is time-invariant, then the response to each of these delayed impulses is simply a delayed version of the impulse response, and so we can relate these individual terms.
And in particular, then, the response to an impulse occurring at time t = tau is simply the response to an impulse occurring at time 0 shifted over to the time origin tau.
Again, as we did before, we'll drop this subscript h_0, so h_0(t) we'll simply define as h(t).
What we're left with when we do that is the description of a linear time-invariant system through this integral, which tells us how the output is related to the input and to the impulse response.
Again, let's just quickly look at this from another perspective as we did in discrete time.
Recall that what we've done is to take the continuous function, decompose it in terms of rectangles, and then each of these rectangles generates its individual response, and then these individual responses are added together.
And as we go through that process, of course, there's a process whereby we let the approximation go to the representation of a smooth curve.
Now, again, I stress if there is certainly a fair amount to kind of examine carefully there, but it's important to also reflect on what we've done, which is really pretty significant.
What we've managed to accomplish is to exploit the properties of linearity and time invariance, so that the system could be represented in terms only of its response to an impulse at time 0.
So for a linear time-invariant system-- quite amazingly, actually-- if you know its response to an impulse at t = 0 or n = 0, depending on discrete or continuous time, then in fact, through the convolution sum in discrete time or the convolution integral in continuous time, you can generate the response to an arbitrary input.
Let me just introduce a small amount of notation.
Again, reminding you of the convolution sum in the discrete-time case, which looks as I've indicated here-- the sum of x[k] h[n-k] will have the requirement of making such frequent reference to convolution that it's convenient to notationally represent it as I have here with an asterisk.
So x[n] * h[n] means or denotes the convolution of x[n] with h[n].
And correspondingly in the continuous-time case, we have the convolution integral, which here is the sifting integral as we talked about, representing x(t) in terms of itself as a linear combination of delayed impulses.
Here we have the convolution integral, and again we'll use the asterisk to denote convolution.
Now, there's a lot about convolution that we'll want to talk about.
There are properties of convolution which tell us about properties of linear time-invariant systems.
Also, it's important to focus on the mechanics of implementing a convolution-- in other words, understanding and generating some fluency and insight into what these particular sum and integral expressions mean.
So let's first look at discrete-time convolution and examine more specifically what in essence the sum tells us to do in terms of manipulating the sequences.
So returning to the expression for the convolution sum, as I show here, the sum of x[k] h[n-k]-- let's focus in on an example where we choose x[n] as a unit step and h[n] as a real exponential times the a unit step.
So the sequence x[n] is as I indicate here, and the sequence h[n] is an exponential for positive time and 0 for negative time.
So we have x[n] and h[n], but now let's look back at the equation and let me stress that what we want is not x[n] and h[n]-- we want x[k], because we're going to sum over k, and not h[k] but h[n-k].
So we have then from x[n], it's straightforward to generate x[k].
It's simply changing the index of summation.
And what's h[n-k]?
Well what's h[-k]?
h[-k] is h[k] flipped over.
So if this is what h[k] looks like, then this is what h[n-k] looks like.
In essence, what the operation of convolution or the mechanics of convolution tells us to do is to take the sequence h[n-k], which is h[-k] positioned with its origin at k = n, and multiply this sequence by this sequence and sum the product from k = -infinity to +infinity.
So if we were to compute, for example, the output at n = 0-- as I positioned this sequence here, this is at n = 0--
I would take this and multiply it by this and sum from -infinity to +infinity.
Or, for n = 1, I would position it here, for n = 2, I would position it here.
Well, you can kind of see what the idea is.
Let's look at this a little more dynamically and see, in fact, how one sequence slides past the other, and how the output y[n] builds up to the correct answer.
So the input that we're considering is a step input,
which I show here, and the impulse response that we will convolve this with is a decaying exponential.
Now, to form the convolution, we want the product of x[k]--
not with h[k], but with h[n-k], corresponding to taking h[k] and reflecting it about the origin and then shifting it appropriately.
So here we see h[n-k] for n = 0, namely h[-k], and now h[1-k], which we'll show next, is this shifted to the right by one point.
Here we have h[1-k]--
shifting to the right by one more point is h[2-k], and shifting again to the right we'll have h[3-k].
Now let's shift back to the left until n is negative, and then we'll begin the convolution.
So here's n = 0, n = -1, n = -2, and n = -3.
Now, to form the convolution, we want the product of x[k] with h[n-k] summed from -infinity to +infinity.
For n negative, that product is 0, and therefore the result of the convolution is 0.
As we shift to the right, we'll build up the convolution, and the result of the convolution will be shown on the bottom trace.
So we begin the process with n negative, and here we have n = -1.
At n = 0, we get our first non-zero contribution.
Now as we shift further to the right corresponding to increasing n, we will accumulate more and more terms in the sum, and the convolution will build up.
In particular for this example, the result of the convolution increases monotonically, asymptotically approaching a constant, and that constant, in fact, is just simply the accumulation of the values under the exponential.
Now let's carry out the convolution this time with an input which is a rectangular pulse instead of a step input.
Again, the same impulse response, namely a decaying exponential, and so we want to begin with h[n-k] and again with n negative shown here.
Again, with n negative, there are no non-zero terms in the product, and so the convolution for n negative will be 0 as it was in the previous case.
Again, on the bottom trace we'll show the result of the convolution as the impulse response slides along.
At n = 0, we get our first non-zero term.
As n increases past 0, we will begin to generate an output,
basically the same as the output that we generated with a step input, until the impulse response reaches a point where as we slide further, we slide outside the interval where the rectangle is non-zero.
So when we slide one point further from what's shown here, the output will now decay, corresponding to the fact that the impulse response is sliding outside the interval in which the input is non-zero.
So, on the bottom trace we now see the result of the convolution.
OK.
So what you've seen, then, is an example of discrete-time convolution.
Let's now look at an example of continuous-time convolution.
As you might expect, continuous-time convolution operates in exactly the same way.
Continuous-time convolution--
we have the expression again y(t) is an integral with now x(tau) and h(t-tau).
It has exactly the same kind of form as we had previously for discrete-time convolution-- and in fact, the mechanics of the continuous-time convolution are identical.
So here is our example with x(t) equal to a unit step and h(t) now a real exponential times a unit step.
I show here x(t), which is the unit step function.
Here we have h(t), which is an exponential for positive time and 0 for negative time.
Again, looking back at the expression for convolution, it's not x(t) that we want, it x(tau) that we want.
And it's not h(t) or h(tau) that we want, it's h(t-tau).
We plan to multiply these together and integrate over the variable tau, and that gives us the output at any given time.
If we want it at another time, we change the value of t as an argument inside this integral.
So here we have x(t), and here we have h(t), which isn't quite what we wanted.
Here we have x(tau), and that's fine-- it's just x(t) with t relabeled as tau.
Now, what is h(t-tau)?
Well, here's h(tau), and if we simply turn that over, here is h(t-tau).
And h(t-tau) is positioned, then, at tau equal to t.
As we change the value of t that change the position of this signal, now we multiply these two together and integrate from -infinity to +infinity with h(t-tau) positioned at the appropriate value of t.
Again, it's best really to see this example and get the notion of the signal being flipped and the two signals sliding past each other, multiplying and integrating by looking at it dynamically and observing how the answer builds up.
Again, the input that we consider is a step input.
And again, we use an impulse response which is a decaying exponential.
To form the convolution, we want the product of x(tau)-- not with h(tau), but with h(t-tau).
So we want h(t) time-reversed, and then shifted appropriately depending on the value of t.
Let's first just look at h(t-tau) for t positive corresponding to shifting h(-tau) out to the right, and here we have t increasing.
Here is t decreasing, and we'll want to begin the convolution with t negative, corresponding to shifting h(-tau) to the left.
Now to form the convolution, we want the product of these two.
For t negative, there are no non-zero contributions to the integral, and so the convolution will be 0 for t less than 0.
On the bottom trace, we show the result of the convolution,
here for t negative, and for t less than 0, we will continue to have 0 in the convolution.
Now as t increases past 0, we begin to get some non-zero contribution in the product, indicated by the fact that the convolution-- the result of the convolution-- starts to build up.
As t increases further, we will get more and more non-zero contribution in the integrand.
So, the result of the convolution will be a monotonically increasing function for this particular example, which asymptotically approaches a constant.
That constant will be proportional to the area under the impulse response, because of the fact that we're convolving with a step input.
Now let's carry out the convolution with an input which is a rectangular pulse-- again, an impulse response which is an exponential.
So to form the convolution, we want x(tau) with h(t-tau)-- h(t-tau) shown here for t negative.
To form the convolution, we take the integral of the product of these two, which again will be 0 for t less than 0.
The bottom trace shows the result of the convolution here, shown as 0, and it will continue to be 0 until t becomes positive, at which point we build up some non-zero term in the integrand.
Now as we slide further, until the impulse response shifts outside the interval in which the pulse is non-zero, the output will build up.
But here we've now begun to leave that interval, and so the output will start to decay exponentially.
As the impulse response slides further and further corresponding to increasing t, then the output will decay exponentially, representing the fact that there is less and less area in the product of x(tau) and h(t-tau).
Asymptotically, this output will then approach 0.
OK.
So you've seen convolution, you've seen the derivation of convolution, and kind of the graphical representation of convolution.
Finally, let's work again with these two examples, and let's go through those two examples analytically so that we finally see how, analytically, the result develops for those same examples.
Well, we have first the discrete-time case, and let's take our discrete-time example.
In general, the convolution sum is as I've indicated here.
This is just our expression from before, which is the convolution sum.
If we take our two examples--
the example of an input which is a unit step, and an impulse response, which is a real exponential multiplied by a unit step-- we have then replacing x[k] by what we know the input to be, and h[n-k] by what we know the impulse response to be, the output is the expression that we have here.
Now, in this expression--
and you'll see this very generally and with some more complicated examples when you look at the text-- as you go to evaluate these expressions, generally what happens is that the signals have different analytical forms in different regions.
That's, in fact, what we have here.
In particular, let's look at the sum, and what we observe first of all is that the limits on this sum are going to be modified, depending on where this unit step is 0 and non-zero.
In particular, if we first consider what will turn out to be the simple case-- namely, n less than 0-- for n less than 0, this unit step is 0 for k greater than n.
With n less than 0, that means that this unit step never is non-zero for k positive.
On the other hand, this unit step is never non-zero or always 0 for k negative.
Let me just stress that by looking at the particular graphs, here is the unit step u[k].
Here is the unit step u[n-k], and for n less than 0, so that this point comes before this point, the product of these two is equal to 0.
That means there is no overlap between these two terms, and so it says that y[n], the output, is 0 for n less than 0.
Well, that was an easy one.
For n greater than 0, it's not quite as straightforward as coming out with the answer 0.
So now let's look at what happens when the two unit steps overlap, and this would correspond to what I've labeled here as interval 2, namely for n greater than 0.
If we just look back at the summation that we had, the summation now corresponds to this unit step and this unit step, having some overlap.
So for interval 2, corresponding to n greater than 0, we have u[k], the unit step.
We have u[n-k], which is a unit step going backward in time, but which extends for positive values of n.
If we think about multiplying these two together, we will get in the product unity for what values of k?
Well, for k starting at 0 corresponding to one of the unit steps and ending at n corresponding to the other unit step.
So we have an overlap between these for k equal to 0, et cetera, up through the value n.
Now, that means that in terms of the original sum, we can get rid of the unit steps involved by simply changing the limits on the sum.
The limits now are from 0 to n, of the term alpha^(n-k).
We had before a u[k] and u[n-k], and that disappeared because we dealt with that simply by modifying the limits.
We now pull out the term alpha^n, because the summation is on k, not on n, so we can simply pull that term of the sum.
We now have alpha^(-k), which we can rewrite as (alpha^(-1))^k.
The upshot of all of this is that y[n] now we can reexpress as alpha^n, the sum from 0 to n of (alpha^(-1)^k.
The question is, how do we evaluate that?
It essentially corresponds to a finite number of terms in a geometric series.
That, by the way, is a summation that will recur over and over and over and over again, and it's one that you should write down, write on your back pocket, write on the palm of your hand, or whatever it takes to remember it.
What you'll see is it that will recur more or less throughout the course, and so it's one worth remembering.
In particular, what the sum of a geometric series is, is what I've indicated here.
We have the sum from 0 to r, of beta^k.
It's 1 - beta^(r+1)-- this is one more than the upper limit on the summation-- and in the denominator is 1 - beta.
So, this equation is important.
There's no point in attempting to derive it.
However you get to it, it's important to retain it.
We can now use that summation in the expression that we just developed.
So let's proceed to evaluate that sum in closed form.
We now go back to the expression that we just worked out-- y[n] is alpha^n, the sum from 0 to n, (alpha^(-1))^k.
This plays the role of beta in the term that I just-- in the expression then I just presented.
So, using that result, we can rewrite this summation as I indicate here.
The final result that we end up with after a certain amount of algebra is y[n] equal to (1 - alpha^(n+1)) / (1 - alpha).
Let me just kind of indicate with a few dots here that there is a certain amount of algebra required in going from this step to this step, and I'd like to leave you with the fun and opportunity of doing that at your leisure.
The expression we have now for y[n], is y[n] = (1 - alpha^(n+1)) / (1 - alpha).
That's for n greater than 0.
We had found out previously there it was 0 for n less than 0.
Finally, if we were to plot this, what we would get is the graph that I indicate here.
The first non-zero value occurs at n = 0, and it has a height of 1, and then the next non-zero value at 1, and this has a height of 1 + alpha, and this is 1 + alpha + alpha^2.
The sequence continues on like that and asymptotically approaches, as n goes to infinity, asymptotically approaches 1 / (1 - alpha), which is consistent with the algebraic expression that we have, that we developed, and obviously of course is also consistent with the movie.
That's our discrete-time example, which we kind of went through graphically with the transparencies, and we went through graphically with the movie, and now we've gone through analytically.
Now let's look analytically at the continuous-time example,
which pretty much flows in the same way as we've just gone through.
Again, we have the convolution integral, which is the integral indicated at the top.
Our example, you recall, was with x(t) as a unit step, and h(t) as an exponential times a unit step.
So when we substitute those in, this then corresponds to x(t) and this corresponds to h(t-tau).
Again, we have the same issue more or less, which is that inside that integral, there are two steps, one of them going forward in time and one of them going backward in time, and we need to examine when they overlap and when they don't.
When they don't overlap, the product, of course, is 0, and there's no point doing any integration because the integrand is 0.
So if we track it through, we have again Interval 1, which is t less than 0.
For t less than 0, this unit step, which only begins at tau = 0, and this unit step which is 0, by the time tau gets up to t and beyond.
For t less than 0, there's no overlap between the unit step going forward in time and the unit step going backward in time.
Consequently, the integrand is equal to 0, and consequently, the output is equal to 0.
We can likewise look at the interval where the two unit steps do overlap.
In that case what happens again is that the overlap, in essence of the unit step, tells us, gives us a range on the integration-- in particular, the two steps overlap when t is greater than 0 from tau = 0 to tau = t.
For Interval 2, for t greater than 0-- again, of course, we have this expression.
This product of this unit step and this unit step is equal to 1 in this range, and so that allows us, then, to change the limits on the integral-- instead of from -infinity to +infinity, we know that the integrand is non-zero only over this range.
Looking at this integral, we can now pull out the term which corresponds to e^-at, just as we pulled out a term in the discrete-time case.
We notice in the integral that we have e^(-a*-tau), so that gives us the integral from 0 to t of e^(a*tau).
If we perform that integration, we end up with this expression.
Finally, multiplying this by e^-at, we have for y(t), for t greater than 0, the algebraic expression that I've indicated on the bottom.
So we had worked out t less than 0, and we come out with 0.
We work out t greater than 0, and we come out with this algebraic expression.
If we plot this algebraic expression as a function of time, we find that what it corresponds to is an exponential behavior starting at zero and exponentially heading asymptotically toward the value 1 / a.
We've gone through these examples several ways, and one is analytically.
In order to develop a feel and fluency for convolution, it's absolutely essential to work through a variety of examples, both understanding them graphically and understanding them as we did here analytically.
You'll have an opportunity to do that through the problems that I've suggested in the video course manual.
In the next lecture, what we'll turn to are some general properties of convolution, and show how this rather amazing representation of linear time-invariant systems in fact leads to a variety of properties of linear time-invariant systems.
We'll find that convolution is fairly rich in its properties,
and what this leads to are some very nice and desirable and exploitable properties of linear time-invariant systems.
Thank you.
Last time, we talked about the representation of linear time-invariant systems through the convolution sum in the discrete-time case and the convolution integral in the continuous-time case.
Now, although the derivation was relatively straightforward, in fact, the result is really kind of amazing because what it tells us is that for linear time-invariant systems, if we know the response of the system to a single impulse at t = 0, or in fact, at any other time, then we can determine from that its response to an arbitrary input through the use of convolution.
Furthermore, what we'll see as the course develops is that,
in fact, the class of linear time-invariant systems is a very rich class.
There are lots of systems that have that property.
And in addition, there are lots of very interesting things that you can do with linear time-invariant systems.
In today's lecture, what I'd like to begin with is focusing on convolution as an algebraic operation.
And we'll see that it has a number of algebraic properties that in turn have important implications for linear time-invariant systems.
Then we'll turn to a discussion of what the characterization of linear time-invariant systems through convolution implies, in terms of the relationship, of various other system properties to the impulse response.
Let me begin by reminding you of the basic result that we developed last time, which is the convolution sum in discrete time, as I indicate here, and the convolution integral in continuous time.
And what the convolution sum, or the convolution integral,
tells us is how to relate the output to the input and to the system impulse response.
And the expression looks basically the same in continuous time and discrete time.
And I remind you also that we talked about a graphical interpretation, where essentially, to graphically interpret convolution required, or was developed, in terms of taking the system impulse response, flipping it, sliding it past the input, and positioned appropriately, depending on the value of the independent variable for which we're computing the convolution, and then multiplying and summing in the discrete-time case, or integrating in the continuous-time case.
Now convolution, as an algebraic operation, has a number of important properties.
One of the properties of convolution is that it is what is referred to as commutative.
Commutative means that we can think either of convolving x with h, or h with x, and the order in which that's done doesn't affect the output result.
So summarized here is what the commutative operation is in discrete time, or in continuous time.
And it says, as I just indicated, that x[n] convolved with h[n] is equal to h[n] convolved with x[n].
Or the same, of course, in continuous time.
And in fact, in the lecture last time, we worked an example where we had, in discrete time, an impulse response, which was an exponential, and an input, which is a unit step.
And in the text, what you'll find is the same example worked, except in that case, the input is the exponential.
And the system impulse response is the step.
And that corresponds to the example in the text, which is example 3.1.
And what happens in that example is that, in fact, what you'll see is that the same result occurs in example 3.1 as we generated in the lecture.
And there's a similar comparison in continuous time.
This example was worked in lecture.
And this example is worked in the text.
In other words, the text works the example where the system impulse response is a unit step.
And the input is an exponential.
All right, now the commutative property, as I said, tells us that the order in which we do convolution doesn't affect the result of the convolution.
The same is true for continuous time and discrete time.
And in fact, for the other algebraic properties that I'll talk about, the results are exactly the same for continuous time and discrete time.
So in fact, what we can do is drop the independent variable as an argument so that we suppress any kind of difference between continuous and discrete time.
And suppressing the independent variable, we then state the commutative property as I've rewritten it here.
Just x convolved with h equals h convolved with x.
The same in continuous time and discrete time.
Now, the derivation of the commutative property is, more or less, some algebra which you can follow through in the book.
It involves some changes of variables and some things of that sort.
What I'd like to focus on with that and other properties is not the derivation, which you can see in the text, but rather the interpretation.
So we have the commutative property, and now there are two other important algebraic properties.
One being what is referred to as the associative property,
which tells us that if we have x convolved with the result of convolving h1 with h2, that's exactly the same as x convolved with h1, and that result convolved with h2.
And what this permits is for us to write, for example, x convolved with h1 convolved with h2 without any ambiguity because it doesn't matter from the associative property how we group the terms together.
The third important property is what is referred to as the distributive property, namely the fact that convolution distributes over addition.
And what I mean by that is what I've indicated here on the slide that if I think of x convolved with the sum of h1 and h2, that's identical to first convolving x with h1, also convolving x with h2, and then adding the two together.
And that result will be the same as this result.
So convolution is commutative, associative, and it distributes over addition.
Three very important algebraic properties.
And by the way, there are other algebraic operations that have that same property.
For example, multiplication of numbers is likewise commutative, associative, and distributive.
Now let's look at what these three properties imply specifically for linear time-invariant systems.
And as we'll see, the implications are both very interesting and very important.
Let's begin with the commutative property.
And consider, in particular, a system with an impulse response h.
And I represent that by simply writing the h inside the box.
An input x and an output, then, which is x * h.
Now, since this operation is commutative, I can write instead of x * h, I can write h * x.
And that would correspond to a system with impulse response x, and input h, and output then h * x.
So the commutative property tells us that for a linear time-invariant system, the system output is independent of which function we call the input and which function we call the impulse response.
Kind of amazing actually.
We can interchange the role of input and impulse response.
And from an output point of view, the output or the system doesn't care.
Now furthermore, if we combine the commutative property with the associative property, we get another very interesting result.
Namely that if we have two linear time-invariant systems in cascade, the overall system is independent of the order in which they're cascaded.
And in fact, in either case, the cascade can be collapsed into a single system.
To see this, let's first consider the cascade of two systems, one with impulse response h1, the other with impulse response h2.
And the output of the first system is then x * h1.
And then that is the input to the second system.
And so the output of that is that result convolved with h2.
So this is the result of cascading the two.
And now we can use the associative property to rewrite this as x * (h1 * h2), where we group these two terms together.
And so using the associative property, we now can collapse that into a single system with an input, which is x, and impulse response, which is h1 * h2.
And the output is then x convolved with the result of those two convolved.
Next, we can apply the commutative property.
And the commutative property says we could write this impulse response that way, or we could write it this way.
And since convolution is commutative, the resulting output will be exactly the same.
And so these resulting outputs will be exactly the same.
And now, once again we can use the associative property to group these two terms together.
And x * h2 corresponds to putting x first through the system h2 and then that output through the system h1.
And so finally applying the associative property again, as I just outlined, we can expand that system back into two systems in cascade with h2 first and h1 second,
OK, well that involves a certain amount of algebraic manipulation.
And that is not the algebraic manipulation that is important.
It's the result that it's important.
And what the result says, to reiterate, is if I have two linear time-invariant systems in cascade, I can cascade them in any order, and the result is the same.
Now you might think, well gee, maybe that actually applies to systems in general, whether you put them this way or that way.
But in fact, as we talked about last time, and I illustrated with an example, in general, if the systems are not linear and time-invariant, then the order in which they're cascaded is important to the interpretation of the overall system.
For example, if one system took the square root and the other system doubled the input, taking the square root and then doubling gives us a different answer than doubling first and then taking the square root.
And of course, one can construct much more elaborate examples than that.
So it's a property very particular to linear time-invariant systems.
And also one that we will exploit many, many times as we go through this material.
The final property related to an interconnection of systems that I want to just indicate develops out of the distributive property.
And what it applies to is an interpretation of the interconnection of systems in parallel.
Recall that the parallel combination of systems corresponds, as I indicate here, to a system in which we simultaneously feed the input into h1 and h2, these representing the impulse responses.
And then, the outputs are summed to form the overall output.
And using the fact that convolution distributes over addition, we can rewrite this as x * (h1 + h2).
And when we do that then, we can recognize this as the output of a system with input x and impulse response, which is the sum of these two impulse responses.
So for linear time-invariant systems in parallel, we can,
if we choose, replace that interconnection by a single system whose impulse response is simply the sum of those impulse responses.
OK, now we have this very powerful representation for linear time-invariant systems in terms of convolution.
And we've seen so far in this lecture how convolution and the representation through the impulse response leads to some important implications for system interconnections.
What I'd like to turn to now are other system properties and see how, for linear time-invariant systems in particular, other system properties can be associated with particular properties or characteristics of the system impulse response.
And what we'll talk about are a variety of properties.
We'll talk about the issue of memory, we'll talk about the issue of invertibility, and we'll talk about the issue of causality and also stability.
Well, let's begin with the issue of memory.
And the question now is what are the implications for the system impulse response for a linear time-invariant system?
Remember that we're always imposing that on the system.
What are the implications on the impulse response if the system does or does not have memory?
Well, we can answer that by looking at the convolution property.
And we have here, as a reminder, the convolution integral, which tells us how x(tau) and h(t - tau) are combined to give us y(t).
And what I've illustrated above is a general kind of example.
Here is x(tau).
Here is h(t - tau).
And to compute the output at any time t, we would take these two, multiply them together, and integrate from -infinity to +infinity.
So the question then is what can we say about h(t), the impulse response in order to guarantee, let's say, that the output depends only on the input at time t.
Well, it's pretty much obvious from looking at the graphs.
If we only want the output to depend on x(tau) at tau = t, then h(t - tau) better be non-zero only at tau = t.
And so the implication then is that for the system to be memoryless, what we require is that h(t - tau) be non-zero only at tau = t.
So we want the impulse response to be non-zero at only one point.
We want it to contribute something after we multiply and go through an integral.
And in effect, what that says is the only thing that it can be and meet all those conditions is a scaled impulse.
So if the system is to be memoryless, then that requires that the impulse response be a scaled impulse.
Any other impulse response then, in essence, requires that the system have memory, or implies that the system have memory.
So for the continuous-time case then, memoryless would correspond only to the impulse response being proportional to an impulse.
And in the discrete-time case, a similar statement, in which case, the output is just proportional to the input, again either in the continuous-time or in the discrete-time case.
All right.
Now we can turn our attention to the issue of system invertibility.
And recall that what is meant by invertibility of a system, or the inverse of a system.
The inverse of a system is a system, which when we cascade it with the one that we're inquiring about, the overall cascade is the identity system.
In other words, the output is equal to the input.
So let's consider a system with impulse response h, input is x.
And let's say that the impulse response of the inverse system is h_i, and the output is y.
Then, the output of this system is x * (h * h_i).
And we want this to come out equal to x.
And what that requires than is that this convolution just simply be equal to an impulse, either in the discrete-time case or in the continuous-time case.
And under those conditions then, h_i is equal to the inverse of h.
Notationally, by the way, it's often convenient to write instead of h_i as the impulse response of the inverse, you'll find it convenient often and more typical to write as the inverse, instead of h_i, h^(-1).
And we mean by that the inverse impulse response.
And one has to be careful not to misinterpret this as the reciprocal of h(t) or h(n).
What's meant in this notation is the inverse system.
Now, if h_i is the inverse of h, is h the inverse of h_i?
Well, it seems like that ought to be plausible or perhaps make sense.
The question, if you believe that the answer is yes, is how, in fact, do you verify that?
And I'll leave it to you to think about it.
The answer is yes,
that if h_i is the inverse of h, then h is the inverse of h_i.
And the key to showing that is to exploit the fact that when we take these systems and cascade them, we can cascade them in either order.
All right now let's turn to another system property, the property of stability.
And again, we can tie that property directly to issues related, in particular, to the system impulse response.
Now, stability is defined as we've chosen to define it and as I've defined it previously, as bounded-input bounded-output stability.
In other words, for every bounded input is a bounded output.
What you can show--
and I won't go through the algebra here; it's gone through in the book-- is that a necessary and sufficient condition for a linear time-invariant system to be stable in the bounded-input bounded-output sense is that the impulse response be what is referred to as absolutely summable.
In other words, if you take the absolute values and sum them over infinite limits, that's finite.
Or in the continuous-time case, that the impulse response is absolutely integrable.
In other words, if you take the absolute values of h(t) and integrate, that's finite.
And under those conditions, the system is stable.
If those conditions are violated, then for sure, as you'll see in the text, the system is unstable.
So stability can also be tied to the system impulse response.
Now, the next property that I want to talk about is the property of causality.
And before I do, what I'd like to do is introduce a peripheral result that we'll then use-- when we talked about causality-- namely what's referred to as the zero input response of a linear system.
The basic result, which is a very interesting and useful one, is that for a linear system-- and in fact, it's whether it's time-invariant or not, that this applies-- if you put nothing into it, you get nothing out of it.
So if we have an input x(t) which is 0 for all t, and if the output of that system is y(t), if the input is 0 for all time, then the output likewise is 0 for all time.
That's true for continuous time, and it's also true for discrete time.
And in fact, to show that result is pretty much straightforward.
We could do it either by using convolution, which would, of course, be associated with linearity and time invariance.
But in fact, we can show that property relatively easily by simply using the fact that, for a linear system, what we know is that if we have an input x(t) with an output y(t), then if we scale the input, then the output scales accordingly.
Well, we can simply choose, as the scale factor, a = 0.
And if we do that, it says put nothing in, you get nothing out.
And what we'll see is that has some important implications in terms of causality.
It's important, though, while we're on it, to stress that not every system, obviously, has that property.
That if you put nothing in, you get nothing out.
A simple example is, let's say, a battery, let's say not connected to anything.
The output is six volts no matter what the input is.
And it of course then doesn't have this zero response to a zero input.
It's very particular to linear systems.
All right, well now let's see what this means for causality.
To remind you, causality says, in effect, that the system can't anticipate the input.
That's what, basically, causality means.
When we talked about it previously, we defined it in a variety of ways, one of which was the statement that if two inputs are identical up until some time, then the outputs must be identical up until the same time.
The reason, kind of intuitively, is that if the system is causal-- so it can't anticipate the future-- it can't anticipate whether these two identical inputs are sometime later going to change from each other or not.
So causality, in general, is simply this statement, either continuous-time or discrete-time.
And now, so let's look at what that means for a linear system.
For a linear system, what that corresponds to or could be translated to is a statement that says that if x(t) is 0, for t less than t_0, then y(t) must be 0 for t less than t_0 also.
And so what that, in effect, says, is that the system--
for a linear system to be causal, it must have the property sometimes referred to as initial rest, meaning it doesn't respond until there's some input that happens.
That it's initially at rest until the input becomes non-zero.
Now, why is this true?
Why is this a consequence of causality for linear systems?
Well, the reason is we know that if we put nothing in, we get nothing out.
If we have an input that's 0 for t less than t_0, and the system can't anticipate whether that input is going to change from 0 or not, then the system must generate an output that's 0 up until that time, following the principle that if two inputs are identical up until some time, the outputs must be identical up until the same time.
So this basic result for linear systems is essentially a consequence of the statement that for a linear system, zero in gives us zero out.
Now, that tells us how to interpret causality for linear systems.
Now, let's proceed to linear time-invariant systems.
And in fact, we can carry the point one step further.
In particular, a necessary and sufficient condition for causality in the case of linear time-invariant systems is that the impulse response be 0, for t less than 0 in the continuous-time case, or for n less than 0 in the discrete-time case.
So for linear time-invariant systems, causality is equivalent to the impulse response being 0 up until t or n equal to 0.
Now, to show this essentially follows by first considering why causality would imply that this is true.
And that follows because of the straightforward fact that the impulse itself is 0 for t less than 0.
And what we saw before is that for any linear system, causality requires that if the input is 0 up until some time, the output must be 0 up until the same time.
And so that's showing the result in one direction.
To show the result in the other direction, namely to show that if, in fact, the impulse response satisfies that condition, then the system is causal.
While I won't work through it in detail, it essentially boils down to recognizing that in the convolution sum, or in the convolution integral, if, in fact, that condition is satisfied on the impulse response, then the upper limit on the sum, in the discrete-time case, changes to n.
And in the continuous-time case, changes to t.
And that, in effect, says that values of the input only from -infinity up to time n are used in computing y[n].
And a similar kind of result for the continuous-time case y(t).
OK, so we've seen how the impulse response, or rather how certain system properties in the linear time-invariant case can, be converted into various requirements on the impulse response of a linear time-invariant system, the impulse response being a complete characterization.
Let's look at some particular examples just to kind of cement the ideas further.
And let's begin with a system that you've seen previously, which is an accumulator.
An accumulator, as you recall, has an output y[n], which is the accumulated value of the input from -infinity up to n.
Now, you've seen in the impulse in a previous lecture, or rather in the video course manual for a previous lecture, that an accumulator is a linear time-invariant system.
And in fact, its impulse response is the accumulated values of an impulse.
Namely, the impulse response is equal to a step.
So what we want to answer is, knowing what that impulse response is, what some properties are of the accumulator.
And let's first ask about memory.
Well, we recognize that the impulse response is not simply an impulse.
In fact, it's a step.
And so this implies what?
Well, it implies that the system has memory.
Second, the impulse response is 0 for n less than 0.
That implies that the system is causal.
And finally, if we look at the sum of the absolute values of the impulse response from -infinity to +infinity, this is a step.
If we accumulate those values over infinite limits, then that in fact comes out to be infinite.
And so what that implies, then, is that the accumulator is not stable in the bounded-input bounded-output sense.
Now I want to turn to some other systems.
But while we're on the accumulator, I just want to draw your attention to the fact, which will kind of come up in a variety of ways again later, that we can rewrite the equation for an accumulator, the difference equation, by recognizing that we could, in fact, write the output as the accumulated values up to time n - 1 and then add on the last value.
And in fact, if we do that, this corresponds to y[n-1].
And so we could rewrite this difference equation as y[n] = y[n-1] + x[n].
So the output is the previously-computed output plus the input.
Expressed that way, what that corresponds to is what is called a recursive difference equation.
And different equations will be a topic of considerable emphasis in the next lecture.
Now, does an accumulator have an inverse?
Well, the answer is, in fact, yes.
And let's look at what the inverse of the accumulator is.
The impulse response of the accumulator is a step.
To inquire about its inverse, we inquire about whether there's a system, which when we cascade the accumulator with that system, which I'm calling its inverse, we get an impulse out.
Well, let's see.
The impulse response of the accumulator is a step.
We want to put the step into something and get out an impulse.
And in fact, what you recall from the lecture in which we introduced steps and impulses, the impulse is, in fact, the first difference of the units step.
So we have a difference equation that describes for us how the impulse is related to the step.
And so if this system does this, the output will be that, an impulse.
And so if we think of x_2[n] as the input and y_2[n] as the output, then the difference equation for the inverse system is what I've indicated here.
And if we want to look at the impulse response of that, we can then inquire as to what the response is with an impulse in.
And what develops in a straightforward way then is delta[n], which is our impulse input, minus delta[n-1] is equal to the impulse response of the inverse system.
So I'll write that as h^(-1)[n] (h-inverse of n).
Now, we have then that the accumulator has an inverse.
And this is the inverse.
And you can examine issues of memory, stability, causality, et cetera.
What you'll find is that the system has memory, the inverse accumulator.
It's stable, and it's causal.
And it's interesting to note, by the way, that although the accumulator was an unstable system, the inverse of the accumulator is a stable system.
In general, if the system is stable, its inverse does not have to be stable or vice versa.
And the same thing with causality.
OK now, there are a number of other examples, which, of course, we could discuss.
And let me just quickly point to one example, which is a difference equation, as I've indicated here.
And as we'll talk about in more detail in our next lecture, where we'll get involved in a fairly detailed discussion of linear constant-coefficient difference and differential equations, this falls into that category.
And under the imposition of what's referred to as initial rest, which corresponds to the response being 0 up until the time that the input becomes non-zero, the impulse response is a^n times u[n].
And something that you'll be asked to think about in the video course manual is whether that system has memory, whether it's causal, and whether it's stable.
And likewise, for a linear constant coefficient differential equation, the specific one that I've indicated here, under the assumption of initial rest, the impulse response is e^(-2t) times u(t).
And in the video course manual again, you'll be asked to examine whether the system has memory, whether it's causal, and whether it's stable.
OK well, as I've indicated, in the next lecture we'll return to a much more detailed discussion of linear constant-coefficient differential and difference equations.
Now, what I'd like to finally do in this lecture is use the notion of convolution in a much different way to help us with a problem that I alluded to earlier.
In particular, the issue of how to deal with some of the mathematical difficulties associated with impulses and steps.
Now, let me begin by illustrating kind of what the problem is and an example of the kind of paradox that you sort of run into when dealing with impulse functions and step functions.
Let's consider, first of all, a system, which is the identity system.
And so the output is, of course, equal to the input.
And again, we can talk about that either in continuous time or in discrete time.
Well, we know that the function that you convolve with a signal to retain the signal is an impulse.
And so that means that the impulse response of an identity system is an impulse.
Makes logical sense.
Furthermore, if I take two identity systems and cascade them, I put in an input, get the same thing out of the first system.
That goes into the second system.
Get the same thing out of the second.
In other words, if I have two identity systems in cascade, the cascade, likewise, is an identity system.
In other words, this overall system is also an identity system.
And the implication there is that the impulse response of this is an impulse.
The impulse response of this is an impulse.
And the convolution of those two is also an impulse.
So for continuous time, we require, then, that an impulse convolved with itself is an impulse.
And the same thing for discrete time.
Now, in discrete time, we don't have any particular problem with that.
If you think about convolving these together, it's a straightforward mathematical operation since the impulse in discrete time is very nicely defined.
However, in continuous time, we have to be somewhat careful about the definition of the impulse because it was the derivative of a step.
A step has a discontinuity.
You can't really differentiate at a discontinuity.
And the way that we dealt with that was to expand out the discontinuity so that it had some finite time region in which it happened.
When we did that, we ended up with a definition for the impulse, which was the limiting form of this function, which is a rectangle of width Delta, and height 1 / Delta, and an area equal to 1.
Now, if we think of convolving this signal with itself, the impulse being the limiting form of this, then the convolution of this with itself is a triangle of width 2 Delta, height 1 / Delta, and area 1.
In other words, this triangular function is this approximation delta_Delta(t) convolved with delta_Delta(t).
And since the limit of this would correspond to the impulse response of the identity system convolved with itself, the implication is that not only should the top function, this one, correspond in its limiting form to an impulse, but also this should correspond in its limiting form to an impulse.
So one could wonder well, what is an impulse?
Is it this one in the limit?
Or is it this one in the limit?
Now, beyond that-- so kind of what this suggests is that in the limiting form, you kind of run into a contradiction unless you don't try to distinguish between this rectangle and the triangle.
Things get even worse when you think about what happens when you put an impulse into a differentiator.
And a differentiator is a very commonly occurring system.
In particular, suppose we had a system for which the output was the derivative of the input.
So if we put in x(t), we got out dx(t) / dt.
If I put in an impulse, or if I talked about the impulse response, what is that?
And of course, the problem is that if you think that the impulse itself is very badly behaved, then what about its derivative, which is not only infinitely big, but there's a positive-going one, and a negative-going one, and the difference between there has some area.
And you end up in a lot of difficulty.
Well, the way around this, formally, is through a set of mathematics referred to as generalized functions.
We won't be quite that formal.
But I'd like to, at least, suggest what the essence of that formality is.
And it really helps us in interpreting the impulses in steps and functions of that type.
And what it is is an operational definition of steps, impulses, and their derivatives in the following sense.
Usually when we talk about a function, we talk about what the value of the function is at any instant of time.
And of course, the trouble with an impulse is it's infinitely big, in zero width, and has some area, et cetera.
What we can turn to is what is referred to as an operational definition where the operational definition is related not to what the impulse is, but to what the impulse does under the operation of convolution.
So what is an impulse?
An impulse is something, which under convolution, retains the function.
And that then can serve as a definition of the impulse.
Well, let's see where that gets us.
Suppose that we now want to talk about the derivative of the impulse.
Well, what we ask about is what it is operationally.
And so if we have a system, which is a differentiator, and we inquire about its impulse response, which let's say we define notationally as u_1(t).
What's important about this function u_1(t) is not what it is at each value of time but what it does under convolution.
What does it do under convolution?
Well, the output of the differentiator is the convolution of the input with the impulse response.
And so what u_1(t) does under convolution is to differentiate.
And that is the operational definition.
And now, of course, we can think of extending that.
Not only would we want to think about differentiating an impulse, but we would also want to think about differentiating the derivative of an impulse.
We'll define that as a function u_2(t). u_2(t)-- because we have this impulse response convolved with this one is u_1(t) * u_1(t).
And what is u_2(t) operationally?
It is the operation such that when you convolve that with x(t), what you get is the second derivative.
OK now, we can carry this further and, in fact, talk about the result of convolving u_1(t) with itself more times.
In fact, if we think of the convulution of u_1(t) with itself k times, then logically we would define that as u_k(t).
Again, we would interpret that operationally.
And the operational definition is through convolution, where this corresponds to u_k(t) being the impulse response of k differentiators in cascade.
So what is the operational definition?
Well, it's simply that x(t) * u_k(t) is the k derivative of x(t).
And this now gives us a set of what are referred to as singularity functions.
Very badly behaved mathematically in a sense, but as we've seen, reasonably well defined under an operational definition.
With k = 0, incidentally, that's the same as what we have referred to previously as the impulse.
So with k 0, that's just delta(t).
Now to be complete, we can also go the other way and talk about the impulse response of a string of integrators instead of a string of differentiators.
Of course, the impulse response of a single integrator is a unit step.
Two integrators together is the integral of a unit step, et cetera.
And that, likewise, corresponds to a set of what are called singularity functions.
In particular, if I take a string of m integrators in cascade, then the impulse response of that is denoted as u sub minus m of t.
And for example, with a single integrator, u sub minus 1 of t corresponds to our unit step as we talked about previously. u sub minus 2 of t corresponds to a unit ramp, et cetera.
And there is, in fact, a reason for choosing negative values of the argument when going in one direction near integration as compared with positive values of the argument when going in the other direction, namely differentiation.
In particular, we know that with u sub minus m of t, the operational definition is the mth running integral.
And likewise, u_k(t)-- so with a positive sub script-- has an operational definition, which is the derivative.
So it's the kth derivative of x(t).
And partly as a consequence of that, if we take u_k(t) and convolve it with u_l(t), the result is the singularity function with the subscript, which is the sum of k and l.
And that holds whether this is positive values of the subscript or negative values of the subscript.
So just to summarize this last discussion, we've used an operational definition to talk about derivatives of impulses and integrals of impulses.
This led to a set of singularity functions-- what I've called singularity functions-- of which the impulse and the step are two examples.
But using an operational definition through convolution allows us to define, at least in an operational sense, these functions that otherwise are very badly behaved.
OK now, in this lecture and previous lectures, for the most part, our discussion has been about linear time-invariant systems in fairly general terms.
And we've seen a variety of properties, representation through convolution, and properties as they can be associated with the impulse response.
In the next lecture, we'll turn our attention to a very important subclass of those systems, namely systems that are describable by linear constant-coefficient difference equations in the discrete-time case, and linear constant-coefficient differential equations in the continuous-time case.
Those classes, while not forming all of the class of linear time-invariant systems, are a very important sub class.
And we'll focus in on those specifically next time.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Over the past several lectures, we've developed a representation for linear time-invariant systems.
A particularly important set of systems, which are linear and time-invariant, are those that are represented by linear constant-coefficient differential equations in continuous time or linear constant-coefficient difference equations in discrete time.
For example, electrical circuits that are built, let's say, out of resistors, inductors, and capacitors, perhaps with op-amps, correspond to systems described by differential equations.
Mechanical systems with springs and dashpots, likewise, are described by differential equations.
And in the discrete-time case, things such as moving average filters, digital filters, and most simple kinds of data smoothing are all linear constant-coefficient difference equations.
Now, presumably in a previous course, you've had some exposure to differential equations for continuous time, and their solution using notions like particular solution, and homogeneous solution, initial conditions, et cetera.
Later on in the course, when we've developed the concept of the Fourier transform after that, the Laplace transform, we'll see some very efficient and useful ways of generating solutions, both for differential and difference equations.
At this point, however, I'd like to just introduce linear constant-coefficient differential equations and their discrete-time counterpart.
And address, among other things, the issue of when they do and don't correspond to linear time-invariant systems.
Well, let's first consider what I refer to as an nth-order linear constant-coefficient differential equation, as I've indicated here.
And what it consists of is a linear combination of derivatives of the system output, y(t), equal to a linear combination of derivatives of the system input x(t).
And it's referred to as a constant-coefficient equation, of course, because the coefficients are constant.
In other words, not assumed to be time-varying.
And it's referred to as linear because it corresponds to a linear combination of these derivatives, not because it corresponds to a linear system.
And, in fact, as we'll see, or as I'll indicate, this equation may or may not, in fact, correspond to a linear system.
In the discrete-time case, the corresponding equation is a linear constant-coefficient difference equation.
And that corresponds to, again, a linear combination of delayed versions of the output equal to a linear combination of delayed versions of the input.
This equation is referred to an nth-order difference equation.
The n referring to the number of delays of the output involved, just as an Nth-order differential equation, the n or the order of the equation refers to the number of derivatives of the output.
Now, let's first begin with linear constant-coefficient differential equations.
And the basic point of the solution for the differential equations is the fact that if we've generated some solution, which I refer to here as y_p(t), some solution to the equation for a given input, then, in fact, we can add to that solution any other solution which satisfies what's referred to as the homogeneous equation.
So in fact, this differential equation by itself is not a unique specification of the system.
If I have any solution, then I can add to that solution any other solution which satisfies the homogeneous equation, and the sum of those two will likewise be a solution.
And that's very straightforward to verify.
By simply substituting into the differential equation, the sum of a particular and the homogeneous solution, and what you'll see is that the homogeneous contribution, in fact, goes to zero by definition of what we mean by the homogeneous equation.
Now, the homogeneous solution for a linear constant-coefficient differential equation is of the form that I indicate at the bottom.
And it typically consists of a sum of N complex exponentials.
And the constants are undetermined by the equation itself.
And this form for the homogeneous solution, in essence, drops out of examining the homogeneous equation, where if we assume that the form of the homogeneous solution is a complex exponential with some unspecified amplitude and unspecified exponent.
If we substitute this into our homogeneous equation, we end up with the equation that I've indicated here.
The factor A and the e^(st) can in fact be canceled out,
and we find that that equation is satisfied for N values of s.
And that's true no matter what choice is made for these coefficients.
And the essential consequence of all of that is that the homogeneous solution is of the form that I indicated previously.
Namely it consists of a sum of N complex exponentials, where the coefficients, the N coefficients, attach to each of those complex exponential is undetermined or unspecified.
So what this says is that in order to obtain the solution for a linear constant-coefficient differential equation, we need some kind of auxiliary information that tells us is how to obtain these N undetermined constants.
And there are variety of ways of specifying this auxiliary information or auxiliary conditions.
For example, in addition to the differential equation,
what I can tell you is the value of the output and N - 1 of its derivatives, at some specified time, t_0.
And so the differential equation together with the auxiliary information, the initial conditions, then lets you determine the total solution, which namely lets you determine these previously-unspecified coefficients in the homogeneous solution.
Now, depending on how the auxiliary information is stated or what auxiliary information is available, the system may or may not correspond to a linear system, and may or may not correspond to a linear time-invariant system.
One essential condition for it to correspond to a linear system is that the initial conditions must be 0.
And one can see the reason for that, if we refer back to the previous lecture in which we saw that for a linear system, if we put 0 in, we get 0 out.
So if x(t), the input, is 0, the output must be 0.
And so that, in essence, tells us that, at least for the system to be linear, these initial conditions must be 0.
Now, beyond that, if we want the system to be causal and linear and time-invariant, then what's required on the initial conditions is that they be consistent with what's referred to as initial rest.
Initial rest says that the output must be 0 up until the time that the input becomes non-zero.
And we can see, of course, that that's consistent with the notion of causality, as we talked about in the previous lecture.
And it's relatively straightforward to see that if the system is causal and linear and time-invariant, that will require initial rest.
It's somewhat more difficult to see that if we specify initial rest, then that, in fact, is sufficient to determine that the system is both causal and linear and time invariant.
But the essential point then is that it requires initial rest for both linearity and causality.
OK, well, let's look at an example, and let's take the example of a first-order differential equation, as I've indicated here.
So we have a first-order differential equation, dy(t) / dt + ay(t) is the input x(t).
And let's first look at what the homogeneous solution of this equation is.
And so, we consider the homogeneous equation, namely the equation that specifies solutions, which would correspond to 0 input.
We, in essence, guess, or impose a solution of the form.
The homogeneous solution is an amplitude factor times a complex exponential.
Substituting this into the homogeneous equation, we then get the equation that I've indicated here.
What you can see is that in this equation, I can cancel out the amplitude factor and this complex exponential.
So let's just cancel those out on both sides of the equation.
And what we're left with is an equation that specifies what the complex exponent must be.
In particular, for the homogeneous solution, s must be equal to -a, and so finally, our homogeneous solution is as I've indicated here.
Now, let's look at the solution for a specific input.
Let's consider, for example, an input which is a scaled unit step.
And although I won't work out the solution in detail and perhaps using what you've worked on previously, you know how to carry out the solution for that.
A solution with a step input is what I've indicated here a scalar, 1 minus an exponential, times a unit step.
And you can verify that simply by substituting into the differential equation.
Now, we know that there's a family of solutions.
In other words, any solution with a homogeneous solution added to it, is, again, a solution.
And so if we consider the solution that I just indicated, we generate the entire family of solutions by adding a homogeneous solution to it, and so this then corresponds to the entire family of solutions, where the constant A is unspecified so far, and needs to be specified through some type of auxiliary conditions.
Now, a class of auxiliary conditions is the condition of initial rest, which as I indicated before, is equivalent to the statement that the system is causal and linear and time-invariant.
And in that case, for the initial rest condition, we would then require in this equation above that this constant be equal to 0.
And so finally, the response to a scaled step--
if the system is to correspond to a causal linear time-invariant system, is then just this term, namely, a constant times 1 minus an exponential, times the step.
Now, if the system is a linear time-invariant system, it can, as we know, be described through its impulse response.
And as you've worked out previously in the video course manual, for a linear time-invariant system, the impulse response is the derivative of the step response.
And just to quickly remind you of where that result comes from.
In essence, we can consider two linear time-invariant systems in cascade, one a differentiator, the other the system that we're talking about described by the differential equation.
And a step in here then generates an impulse into our system, and out comes the impulse response.
Well, just using the fact that these are both linear time-invariant systems, and they can be cascaded in either order, then means that if we have the step response to our system, and that goes through the differentiator, what must come out, again, is the impulse response.
So differentiating the step response, we get the impulse response.
Here again, is the step response as we just worked it out, this time for a unit step.
If we differentiate, we have then, since the step response is the product of two terms, the derivative of a product is the sum of the derivatives.
And carrying that algebra through, and using the fact that the derivative of the step is an impulse, finally we come down to this statement for the impulse response.
And then recognizing that this is a time function times an impulse.
And we know that if a time function times an impulse takes on the value at the time that the impulse occurs, then this term is simply 0.
And the impulse response then finally is an exponential of this form.
And this is the decaying exponential for a-positive, it's a growing exponential for a-negative.
And recall that, as we talked about previously, a linear time-invariant system is stable if its impulse response is absolutely integrable.
For this particular case, this impulse response is absolutely integrable provided that the exponential factor a is greater than 0.
Okay, so what we've seen then is the impulse response for a system described by a linear constant-coefficient differential equation, where in addition, we would impose causality, linearity, and time-invariance, essentially, through the initial conditions of initial rest.
Now, pretty much the same kinds of things happen with difference equations as we've gone through with differential equations.
In particular, again, let me remind you of the form of an Nth order linear constant coefficient difference equation.
It's as I indicate here.
And, again, a linear combination, this time of delayed versions of the output equal to a linear combination of delayed versions of the input.
Once again, difference equation is not a complete specification of the system because we can add to the response any homogeneous solution.
In other words, any solution that satisfies the homogeneous equation and the sum of those will also satisfy the original difference equation.
So if we have a particular response that satisfies the difference equation, then adding to that any response that is a solution to the homogeneous equation will also be a solution to the total equation.
The homogeneous solution, again, is of the form of a linear combination of exponentials.
Here, we have the homogeneous equation.
As with differential equations, we can guess or impose solutions of the form A times an exponential.
When we substitute this into the homogeneous equation, we then end up with the equation that I've indicated here.
We recognize again that A, the amplitude, and z^n, this exponential factor, cancel out.
And so this equation is satisfied for any values of z that satisfy this equation.
And there are N roots, z_1 through z_N.
And so finally, the form for the homogeneous solution is a linear combination of capital N exponentials, where capital N is the order of the equation.
With each of those exponentials, the amplitude factor is undetermined and needs to be determined in some way through the imposition of appropriate initial conditions or boundary conditions.
So the general form then for the solution to the difference equation is a sum of exponentials plus any particular solution.
It's through auxiliary conditions that we determine these coefficients.
We have N undetermined coefficients and, so we require N auxiliary conditions.
For example, some set of values of the output at N distinct instance of time.
Now this was the same as with differential equations.
In the case of differential equations, we talked about specifying the value of the output and its derivatives.
And there, we indicated that for linearity, what we required-- for linearity, what we required is that the auxiliary conditions be 0.
And the same thing applies here for the same reason.
Namely, if the system is to be linear, then the response, if there's no input, must be equal to 0.
In addition, what we may want to impose on the system is that it be causal, and in addition to linear time-invariant, and what that requires, again, is that the auxiliary conditions be consistent with initial rest, namely, that if the input is 0 prior to some time, then the output is 0 prior to the same time.
So we've seen a very direct parallel so far between differential equations and difference equations.
In fact, one difference between them that, in some sense, makes difference equations easier to deal with in some situations, is that in contrast to a differential equation, a difference equation, if we assume causality, in fact is an explicit input-output relationship for the system.
Now, let me show you what I mean.
Let's consider the nth-order difference equation, as I've indicated here.
And let's assume that we're imposing causality so that the output can only depend on prior values of the input, and therefore, aren't prior values of the output.
Well, we can simply rearrange this equation solving for y[n] the leading term, with k = 0.
Taking all of the other terms over to the right side of the equation, and we then have a recursive equation, namely, an equation that expresses the output in terms of prior values of the input, which is this term, and prior values of the output.
And so if, in fact, we have this equation running, then once it started, we know how to compute the output for the next time instant.
Well, how do we get it started?
The way we get it started, of course, is through the appropriate set of initial conditions or boundary conditions.
And, if for example, we assume initial rest corresponding to a causal linear time-invariant system, then if the input is 0 up until some time, the output must be 0 up until that time.
And that, in essence, helps us get the equation started.
Well, let's look at this specifically in the context of a first-order difference equation.
So let's take a first-order difference equation, as I've indicated here.
And so, we have an equation that tells us that y[n] - ay[n-1] = x[n].
Now, if we want this to correspond to a causal linear time-invariant system, we impose initial rest on it.
We can rewrite the first-order difference equation by taking the term involving y[n-1] over two the right hand side of the equation.
Now, this gives us a recursive equation that expresses the output in terms of the input and past values of the output.
And since we've imposed causality, and if we're talking about a linear time-invariant system, we can now inquire as to what the impulse response is.
And we know, of course, the impulse response tells us everything that we need to know about the system.
So let's choose an input, which is an impulse.
So the impulse response is delta[n] corresponding to the x[n] up here, plus a delta of-- this should be h[n-1].
And let me just correct that.
This is h[n-1].
So we have the impulse response as delta[n] plus a times h[n-1].
Now, from initial rest, we know that since the input,
namely an impulse, is 0 for n less than 0, the impulse response is likewise 0 for n less than 0.
And now, let's work out what h[0] is.
Well, h of 0, with n = 0, is delta[0] plus a times h[n-1]. h[n-1] is 0.
And so, h[0] is equal to 1.
Now that we have h[0], we can figure out h[1] by running this recursive equation.
So h[1] is delta[1], which is 0, plus a times h[0], which we just figured out is 1.
So, h[1] is equal to a.
And if we carry this through, we'll have h[2] equal to a^2, and this will continue on.
And in fact, what we can recognize by looking at this and how we would expect these terms to build, we would see that the impulse response, h[n], in fact, is of the form a^n times u[n].
And we also can recognize then that this corresponds to a stable system, if and only if the impulse response, which is what we just figured out, if and only if the impulse response is absolutely summable.
And what that will require is that the magnitude of a be less than 1.
Now, we imposed causality, linearity, and time-invariance, and generated a solution recursively.
And now, of course, if we want to generate the more general set of solutions to this difference equation, we can do that by adding all of the homogeneous solutions, namely, all the solutions that satisfy the homogeneous equation.
So here, we have the causal linear time-invariant impulse response.
In fact, with an impulse input, all of the possible solutions are that impulse response plus the homogeneous solutions.
The homogeneous solution is the solution that satisfies the homogeneous equation.
That will, in general, be of the form an amplitude factor times an exponential factor, and if we substitute this into this equation, then we see that the homogeneous equation is satisfied for any values of a and any values of z that satisfy this equation.
Again, as we did with differential equations, the factor A cancels out.
And also, in fact, I can cancel out a z^n there and a z^n here.
And so what we're left with is a statement that tells us then that the homogeneous solution is of the form a(z^n), for any value of A and any value of z that satisfies this equation.
And that value of z, in particular, is z equal to a.
So the homogeneous solution then is any exponential of this form with any amplitude factor.
And so, the family of solutions, with an impulse input is the solution corresponding to the system being causal, linear, and time-invariant, plus the homogeneous term.
If we impose causality, linearity, and time-invariance on the system, then of course, that additional exponential factor will be 0.
In other words, A is equal to 0.
Now, we've seen the differential equations and difference equations in terms of the fact that there are families of solutions, and in order to get causality, linearity, and time-invariance requires imposing a particular set of initial conditions, namely, imposing initial rest on the system.
Let's now look at the difference equation and then later, the differential equation, interpreted in block diagram terms.
Now, the difference equation, as I just simply repeated here, is y[n] = x[n] + ay[n-1], where I've taken the delayed term over to the right hand side of the equation.
So, in effect, what I'm imposing on this system is causality.
I'm assuming that if I know the past history of the input and the output, I can determine the next value of the output.
Well, we can, in fact, draw a block diagram that represents that equation.
The equations says, we take x[n] for any given value of n, say n0, whatever value we're computing the output for.
We take the input at that time, and add to it the factor a times the output value that we calculated last time.
So if we have x[n], which is our input, and if we have y[n], which is our output, we, in fact, can get y of n by taking the last value of y[n], indicated here by putting y[n] through a delay, multiplying that by the factor a, and then adding that result to the input.
And the result of doing that is y[n].
So the way this block diagram might be interpreted, for example, as an algorithm is to say that we take x[n], add to it a times the previous value the output.
That sum gives us the current value of the output, which we then put out of the system, and also put into a delay element, or basically, into a storage register, to use on the next iteration or recursion.
Now, how do we get this started?
Well, we know that the difference equation requires initial conditions.
And, in fact, the initial conditions correspond to what we store in the delay register when this block diagram or equation initially starts up.
OK Now, let's look at this for the case of difference equations more generally.
So, what we've said is that we can calculate the output by having previous values of the input, previous values of the output, and forming the appropriate linear combination.
So let's just build up the more general block diagram that would correspond to this.
And what it says is that we want to have a mechanism for storing past values of the input, and a mechanism for storing past values of the output.
And I've indicated that on this figure, so far, by a chain of delay elements, indicating that what the output of each delay is, is the input delayed by one time instant or interval.
And so what we see down this chain of delays are delayed replications of the input.
And what we see on the other chain is delayed replications of the output.
Now, the difference equation says that we want to take these, and multiply them by the appropriate coefficients, the coefficients in the difference equation, and so, we can do that as I've indicated here.
So now, we have these delay elements, each multiplied by the appropriate coefficients on the input, and by appropriate coefficients on the output.
Those are then summed together, and so we now will sum these and will sum these.
After we've summed these, we want to add those together.
And there's a factor of 1 / a_0 that comes in.
And so that then generates our output.
And so this, in fact, then represents a block diagram,
which is a general block diagram for implementing or representing a linear constant-coefficient difference equation.
Now, if you think about what it means in terms of, let's say, a computer algorithm or a piece of hardware, in fact, this block diagram is a recipe or algorithm for doing the implementation.
But it's important to recognize, even at this point,
that it's only one of many possible algorithms or implementations for this difference equation.
Just for example, I can consider that equation for that block diagram.
And here, I've re-drawn it.
So here, are once again.
I have the same block diagram that we just saw.
And I can recognize, for example, that this, in essence, corresponds to two linear time-invariant systems in cascade.
Now, that assumes, of course, that my initial conditions are such that the system is, in fact, linear.
And that, in turn, requires that we're assuming initial rests, namely, before the input does anything other than 0.
There are just 0 values stored in the registers.
But assuming that it corresponds to a linear time-invariant system, this is a cascade of two linear time-invariant invriant systems.
We know that two linear time-invariant systems can be cascaded in either order.
So, in particular, I can consider breaking this cascade here, and moving this block over to the other side.
And so let's just do that.
And when I do, I then have this combination of systems,
and, of course, you can ask what advantage there is to doing that, and the advantage arises because of the fact that in this form, exactly what is stored in these delays is also stored in these delay registers.
In other words, it's this intermediate variable-- whatever it is-- down this chain of the delays and down this chain of delays, and so, in fact, I can collapse those delays into a single chain of delays.
And the network that I'm left with is the network that I indicate on this view graph, where what I've done is to simply collapse that double chain of delays into a single change of delays.
Now, one can ask, well, what's the advantage to doing that?
And one advantage, simply stated, is that when you think in terms of an implementation of a difference equation, a delay corresponds to a storage register, a memory location, and by simply using the fact that we can interchange the order in which linear time-invariant systems are cascaded, we can reduce the amount of memory by a factor of 2.
Now, an essentially similar procedure can also be used for differential equations, in terms of implementation using block diagrams or the interpretation of implementations using block diagrams.
And let me first do that--
rather than in general-- let me first do it in the context of a specific example.
So let's consider a linear constant-coefficient differential equation, as I've indicated here, and I have terms on the left side and terms on the right side.
And with the differential equation, let's consider taking all the terms over to the right side of the equation, except for the highest derivative in the output.
Next, we integrate both sides of the equation so that when we're done, we end up with on the left side of the equation with y(t).
On the right side of the equation with the appropriate number of integrations.
And so the integral equation that we'll get for this example y(t), the output, is x(t) plus b, the scale factor times the integral of the input, and minus a, that scale factor, times the integral of the output.
So to form the output in the block diagram terms, we form a linear combination of the input, a scaled integral of the input, and a scaled integral of the output, all of that added together.
So we need, in addition to the input, we need the integral of the input.
And so this box indicates an integrator.
In addition to the output, we need the integral of the output.
And now, to form y(t), we multiply the integrated input by the scale factor, b.
And add that to x(t), and we take the integrated output,
multiply it by -a, and add to that the result of the previous addition, and according to the integral equation, then that forms the output.
So just as we did with the difference equation, we've converted the differential equation to an integral equation, and we have a block diagram form very similar to what we had in the case of the difference equation.
Now, the initial conditions, of course, are tied up in, again, how these integrators are initialized.
Assuming that we impose initial rest on the system, we can think of the overall system as a linear time-invariant system, and it's a cascade of one linear time-invariant system with a second.
So we can, in fact, break this, and consider interchanging the order in which these two systems are cascaded.
And so I've indicated that down below.
Here, I've simply taken the top block diagram,
interchanged the order in which the two systems are cascaded.
And here, again, we can ask what the advantages to this, as opposed to the previous one.
And what you can see, just as we saw with the difference equation, is that now, the integrators-- both integrators-- are integrating the same thing.
In particular, the input to this integrator and the input to this integrator are identical.
So in fact, rather than using this one, we can simply tap off from here.
We can, in fact, remove this integrator, break this connection, and tap in at this point.
And so what we've done then, by interchanging the order in which the systems are cascaded, is reduced the implementation to the implementation with a single integrator.
Very much similar to what we talked about in the case of the difference equation.
Now, let's just, again, with the integral equation or the differential equation, look at this somewhat more generally.
Again, if we take the differential equation, the general differential equation, integrate it a sufficient number of times to convert it to an integral equation.
We would then have this cascade of systems.
And again, if we assume initial rest, so that these are both linear time-invariant systems, we can interchange the order in which they're cascaded.
Namely, take the second system, and move it to precede the first system.
And then what we recognize is that the input to this chain of integrators and this chain of integrators is exactly the same.
And so, in fact we can collapse these together using only one chain of integrators.
And the system that we're left with then is a system that looks as I've indicated here.
So we have now just a single chains of integrators instead of the two sets of integrators.
So we've seen that the situation is very similar here as it was in the case of the difference equation.
Again, why do we want to cut the number of integrators in half?
Well, one reason is because integrators, in effect, represent hardware.
And if we have half as many integrators, then we're using half as much hardware.
Well, let me just conclude by summarizing a number of points.
I indicated at the beginning that linear constant-coefficient differential equations and difference equations will play an important role as linear time-invariant systems throughout this course and throughout this set of lectures.
I also stressed the fact that differential or difference equations, by themselves, are not a complete specification of the system because of the fact that we can add to any solution a homogeneous solution.
How do we specify the appropriate initial conditions to ensure-- how do we specify the appropriate initial conditions to ensure that the system is linear and time-invariant?
Well, the auxiliary information, namely, the initial conditions associated with the system being causal, linear, and time-invariant are the conditions of initial rest.
And, in fact, for most of the course, what we'll be interested in are systems that are in fact, causal, linear, and time-invariant.
And so we will, in fact, be assuming initial rest conditions.
Now, as I also indicated, there are a variety of efficient procedures for solving differential and difference equations that we haven't yet addressed.
And beginning with the next set of lectures, we'll be talking about the Fourier Transform and much later in the course, what's referred to as the Laplace Transform for continuous time and the Z-transform for discrete time.
And what we'll see is that with the Fourier Transform and later with the Laplace and Z-transform, we'll have a number of efficient and very useful ways of generating the solution for differential and difference equations under the assumption that the system is causal, linear, and time-invariant.
Also, we'll see in addition to the block diagram implementations of these systems that we've talked about so far, we'll see a number of other useful implementations that exploit a variety of properties associated with Fourier and Laplace Transforms.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Over the last several lectures, we've dealt with the representation of linear time-invariant systems through convolution.
And just to remind you of our basic strategy, essentially,
the idea was to exploit the notion of linearity by decomposing the input into a sum of basic inputs and then using linearity to tell us that the output can be represented as the corresponding linear combination of the associated outputs.
So, if we have a linear system, either continuous-time or discrete-time, for example, with continuous time, if the input is decomposed as a linear combination of basic inputs, with each of these basic inputs generating an associated output, and if the system is linear, then the output of the system is the same linear combination of the associated outputs.
And the same statement is identical both for continuous time and discrete time.
So the strategy is to decompose the input into these basic inputs.
And the inputs were chosen also with some particular strategy in mind.
In particular, for both continuous time or discrete time, in this representation, the basic inputs used in the decomposition are chosen, first of all, so that a broad class of signals could be represented in terms of these basic inputs, and second of all, so that the response to these basic inputs is, in some sense, easy to compute.
Now, in the representation which led us to convolution,
the particular choice that we made in the discrete-time case for our basic inputs was a decomposition of the input in terms of delayed impulses.
And the associated outputs that that generated were delayed versions of the impulse response.
Decomposing the input into a linear combination of these,
the output into the corresponding linear combination of these, then led to the convolution sum in the discrete time case.
And in the continuous-time case, a similar kind of decomposition, in terms of impulses, and associated representation of the output, in terms of the impulse response, led to the convolution integral.
Now, in this lecture, and for a number of the succeeding lectures, we'll want to turn our attention to a very different set of basic building blocks.
And in particular, the signals that we'll be using as the building blocks for our more general signals, rather than impulses, as we've dealt with before, will be, in general, complex exponentials.
So, in a general sense, in the continuous-time case, we'll be thinking in terms of a decomposition of our signals as a linear combination of complex exponentials, continuous-time, or, in the discrete-time case, complex exponentials, where z_k is complex here in discrete time and s sub k is complex here in continuous time.
Now, the basic strategy, of course, requires that we choose a set of inputs, basic building blocks, which have two properties.
One is that the system response be straightforward to compute, or in some sense, easy to compute.
And second is that it be a fairly general set of building blocks so that we can build lots of signals out of them.
What we'll find with complex exponentials, either continuous-time or discrete-time, is that they very nicely have those two properties.
In particular, the notion that the output of a linear time-invariant system is easy to compute is tied to what's referred to as the Eigenfunction function property of complex exponentials, which we'll focus on shortly in a little more detail.
And second of all, the fact that we can, in fact,
represent very broad classes of signals as linear combinations of these will be a topic and an issue that we'll develop in detail over, in fact, the next set of lectures, this lecture, and the next set of lectures.
Now, in doing this, although we could, in fact, begin with our attention focused on, in general, complex exponentials, what we'll choose to do is first focus on the case in which the exponent in the continuous-time case is purely imaginary, as I indicate here, and in the discrete-time case, where the magnitude of the complex number z_k is equal to 1.
So what that corresponds to in the continuous-time case is a set of building blocks of the form e^(j omega_k t), and in the discrete-time case, a set of building blocks of the form e^(j Omega_k n).
What we'll see is a representation in these terms leads to what's referred to as Fourier analysis.
And that's what will be dealing with over the next set of lectures.
We'll then be exploiting this representation actually through most of the course.
And then toward the end of the course, we'll return to generalizing the Fourier representation to a discussion Laplace transforms and Z-transforms.
So for now, we want to restrict ourselves to complex exponentials of a particular form, and in fact, also initially to continuous-time signals and systems.
So let's begin with the continuous-time case and the complex exponentials that we want to deal with and focus, first of all, on what I refer to as the Eigenfunction property of this particular set of building blocks.
We're talking about basic signals of the form e^(j omega_k t).
And the statement is that for a linear time-invariant system, the response to one of these is of exactly the same form, just simply multiplied by a complex factor, that complex factor depending on what the frequency, omega_k, is.
Now more or less, the justification for this, or the proof, follows by simply looking at the response to a complex exponential, using the convolution integral.
So if we put a complex exponentials into a linear time-invariant system with impulse response h(t), then we can express the response as I've indicated here.
We can then recognize that this complex exponentials can be factored into two terms.
And so we can rewrite this complex exponential as this product.
Second, recognize that this term can be taken outside the integral, over here, because of the fact that it depends only on t and not on Tau.
And so what we're left with, when we track this through, is that, with a complex exponential input, we get an output which is the same complex exponential, namely this factor, times this integral.
And this integral is what I refer to above as H(omega_k).
And so, in fact, we put in a complex exponential, we get out a complex exponentials of the same frequency, multiplied by a complex constant.
And that is what's referred to as the Eigenfunction property,
Eigenfunction meaning that an Eigenfunction of a system, or mathematical expression, is a function which, when you put it through the system, comes out looking exactly the same except for a change in amplitude, the change in amplitude being the Eigenvalue.
So in fact, this function is the Eigenfunction.
And this value is the Eigenvalue.
OK, now it's because of the Eigenfunction property that complex exponentials are particularly convenient as building blocks.
Namely you put it through the system, they come out with the same form and simply scale.
The other part to the question, related to the strategy that we've been pursuing, is to hope that these signals can be used as building blocks to represent a very broad class of signals through a linear combination.
And in fact, that turns out to be the case with complex exponentials.
As we work our way through that, we'll first consider the case of periodic signals.
And what that leads to is a representation of periodic signals through what's called the Fourier series.
Following that, we'll turn our attention to non-periodic, or as I refer to it, aperiodic signals.
And the representation that's developed in terms of linear combinations of complex exponentials is what's referred to as the Fourier transform.
So the first thing we want to deal with are periodic signals and the Fourier series.
So what we're talking about then is the continuous-time Fourier series.
And the Fourier series is a representation for periodic continuous-time signals.
We have a signal, then, which is periodic.
And we're choosing T_0 to denote the period.
So it's T_0 that corresponds to the period of our periodic signal. omega_0 is 2 pi / T_0, as you recall from our discussion pf periodic signals and sinusoids before.
And that's 2 pi f_0.
So this is the fundamental frequency.
Now let's examine, first of all, complex exponentials, and recognize, first of all, that there is a complex exponential that has exactly the same period and fundamental frequency as our more general periodic signal, namely the complex exponential e^(j omega_0 t), where omega_0 is 2 pi / T_0, or equivalently, T_0 is 2 pi / omega_0.
Now that's the complex exponential which has T_0 as the fundamental period.
But there are harmonically related complex exponentials that also have T_0 as a period, although in fact, their fundamental period is shorter.
So we can also look at complex exponentials of the form e^(j k omega_0 t).
These likewise are periodic with a period of T_0.
Although, in fact, their fundamental period is T_0 / k,
or equivalently, 2 pi divided by their fundamental frequency, k omega_0.
So as k, an integer, varies, these correspond to harmonically related complex exponentials.
Now what the Fourier series says, and we'll justify this bit by bit as the discussion goes on, what the Fourier series says, and in fact, what Fourier said, which was essentially his brilliant insight, is that, if I have a very general periodic signal, I can represent it as a linear combination of these harmonically-related complex exponentials.
So that representation is what I've indicated here.
And this summation is what will be referred to as the Fourier series.
And as we proceed with the discussion, there are two issues that will develop.
One is, assuming that our periodic signal can be represented this way, how do we determine the Fourier series coefficients, as they're referred to, a_k.
That's one question.
And the second question will be how broad a class of signals, in fact, can be represented this way.
And that's another question that we'll deal with separately.
Now just focusing on this representation for a minute,
this representation of the Fourier series, which I've repeated again here, is what's referred to as the complex exponential form of the Fourier series.
And it's important to note, incidentally, that the summation involves frequencies, k omega_0, that are both positive and negative.
In other words, this index k runs over limits that include both negative values and positive values.
Now that complex exponential form is one representation for the Fourier series.
And in fact, it's the one that we will be principally relying on in this course.
There is another representation that perhaps you've come across previously and that in a variety of other contexts is typically used, which is called the trigonometric form for the Fourier series.
Without really tracking through the algebra,
essentially we can get to the trigonometric form from the complex exponential form by recognizing that if we express the complex coefficient in polar form or in rectangular form and expand the complex exponential term out in terms of cosine plus j sine, using just simply Euler's relation, then we will end up with a representation for the periodic signal, or a re-expression of the Fourier series expression that we had previously, either in the form that I indicate here, where now the periodic signal is expressed in terms of a summation of cosines with appropriate amplitude and phase.
Or another equivalent trigonometric form involves rearranging this in terms of a combination of cosines and sines.
Now in this representation, the frequencies of the sinusoids vary only over positive frequencies.
And typically one thinks of periodic signals as having positive frequencies associated with them.
However, let's look back and the complex exponential form for the Fourier series at the top of the board.
And in that representation, when we use this representation, we'll find it convenient to refer to both positive frequencies and negative frequencies.
So the representation that we will most typically be using is the complex exponential form.
And in that form, what we'll find as we think of decomposing a periodic signal into its components at different frequencies, it will involve both positive frequencies and negative frequencies.
Okay, now we have the Fourier series representation, as I've indicated here.
Again, so far I've sidestepped the issue as to whether this in fact represents all the signals that we'd like to represent.
Let's first address the issue of how we determine these coefficients a_k, assuming that, in fact, this representation is valid.
And again, I'll kind of move through the algebra fairly quickly.
The algebraic steps are ones that you can pursue more leisurely just to kind of verify them and step through them.
But essentially, the algebra develops out of the recognition that if we integrate a complex exponential over one period, T_0-- and I mean by this notation that this is an integral over a period, where I don't particularly care where the period starts and where the period stops, in other words, exactly what period I picked-- that this integral is equal to T_0 when m is equal to 0.
And it's equal to 0 if m is not equal to 0.
That follows simply from the fact that if we substitute in for using or Euler's relation, so that we have the integral of a cosine plus j times the sine, if m is not equal to 0, then both of these integrals over a period are 0.
The integral of a of a periodic of a sinusoid, cosine or sine, over an integral number of periods is 0.
Whereas, if m is equal to 0, this integral will be equal to T_0, the integral of the cosine.
And the integral of the sine is equal to 0.
Okay, well, the next step in developing the expression for the coefficient a_k is to refer back to the Fourier series expression, which was that x(t) is equal to the sum of a_k e^(j k omega_0 t).
If we multiply both sides of that by e^(-j n omega_0 t), and integrate that over a period-- both sides of the equation integrated over a period, so these two equations are equal-- and then in essence, interchange the summation and the integration so that this part of the expression comes outside the sum, and then we combine these two complex exponentials together, where we come out is the expression that I've indicated here.
And then essentially what happens at this point,
algebraically, is that we use the result that we just developed to evaluate this integral.
So multiplying both sides of the Fourier series and then doing the integration leads us, after the appropriate manipulation, to the expression that I have up here.
And this integral is equal to T_0 if k is equal to n, corresponding to 0 up here.
And it's 0 otherwise, which is what we had demonstrated or argued previously.
And the upshot of all that, then, is that the right hand side of this expression disappears except for the term when k is equal to n.
And so finally, we have what I indicate here, taking T_0 and moving it over to the other side of the equation, that then tells us how we determine the Fourier series coefficients a_n, or a_k.
So that, in effect, then is what we refer to as the analysis equation, the equation that begins with x(t) and tells us how to get the Fourier series coefficients.
What I'll refer to as the Fourier series synthesis equation is the equation that tells us how to build x(t) out of these complex exponentials.
So we have the synthesis equation, which is the one we started from.
We have the analysis equation, which is the equation that we just developed.
So we in effect have gone through the issue of, assuming that a Fourier series representation is in fact valid, how we get the coefficients.
We'll want to address somewhat the question of how broad a class of signals are we talking about.
And what's in fact amazing, and was Fourier's amazing insight, was that it's a very broad class of signals.
But let's first look at just some examples in which we take a signal, assume that it has the Fourier series representation, and see what the Fourier series coefficients look like.
So we'll begin with what I refer to as an antisymmetric periodic square wave-- periodic of course, because we're talking about periodic signals; square wave referring to its shape; and antisymmetric referring to the fact that it is an odd time function.
In other words, it is antisymmetric about the origin.
Now the expression for the Fourier series coefficients tells us that we determine a_k by taking 1 / T0 times the integral over a period of x(t), e^(-j k omega_0 t) dt.
The most convenient thing in this case is to choose a period, which let's say goes from -T_0 / 2 to +T_0 / 2.
So here x(t) is -1.
Here x(t) is +1.
And so I've expressed the Fourier series coefficients as this integral, that's from -T_0 / 2 to 0.
And then added to that is the positive part of the cycle.
And so we have these two integrals.
Now, I don't want to track through the details of the algebra again.
I guess I've decided that that's much more fun for you to do on your own.
But the way it comes out when you go through it is the expression that I finally indicate after suggesting that there are few more steps to follow.
And what develops is that those two integrals together, for k not equal to 0, come out to this expression.
And that expression is not valid for k = 0.
For k equal to 0, we can go back to the basic expression for the Fourier series, which is 1 / T_0, the integral over a period, x(t) e^(-j k omega_0 t) dt.
For k = 0, of course this term just simply becomes 1.
And so the zeroth coefficient is 1 / T_0 times the integral of x(t) over a period.
Now, going back to the original function that we have, what we're saying then is that the zeroth coefficient is 1 / T_0 times the integral over one period, which is, in effect, the average value.
And it's straightforward to verify for this case that average value is equal to 0.
Now let's look at these Fourier series coefficients on a bar graph.
And I've indicated that here.
The expression for the Fourier series coefficients we just developed.
And it involves--
it's 0 for k = 0, it's a factor of this form for k =/= 0.
Plotted on a bar graph, then we see values like this, 0 at k = 0 and then associated values.
And there are a number of things to focus on when you look at this.
One is the fact that the Fourier series coefficients for this example are purely imaginary.
A second is that the Fourier series coefficients for this example are an odd sequence.
In other words, if you look at this sequence, what you see are these values for -k flipped over.
So they're imaginary and odd.
And what that results in, when you look at the trigonometric form of the Fourier series, is that in fact, those conditions, if you put the terms all together, lead you to a trigonometric representation, which involves only sine terms-- in other words, no cosine terms.
Let me just draw your attention to the fact that,
since a_k's are imaginary, this j takes care of that fact so that these coefficients are in fact real.
So what this says is that for the antisymmetric square wave, in effect, the Fourier series is a sine series.
The antisymmetric square wave is an odd function.
Sinusoids are odd functions.
And so this is all kind of reasonable, that we're building an odd function out of odd functions.
As an additional aside, which I won't exploit or refer to any further here, but just draw your attention to, is that another aspect of this periodic square wave, the particular one that we chose, is that it is what's referred to as an odd harmonic function.
In other words, for even values of k, the Fourier series coefficients are 0.
They're are only non-zero for odd values of k.
Now let's look at another example.
Another example is the symmetric periodic square wave.
And this is in fact example 4.5, worked out in more detail in the text.
Then I won't bother to work this out in detail here, except to draw your attention to several points.
Here is the symmetric periodic square wave.
And what I mean by symmetric is that it's an even time function.
Now just kind of extrapolating your intuition, what you should expect is that if it's only an even time function, it should be built up or buildable, if it's buildable at all, out of only even sinusoids.
And in fact, that's the case.
So if we look at the Fourier series coefficients for this,
is zeroth coefficient, again, is the average value, which in this case, is 1/2.
Here I've plotted pi times the Fourier series coefficients.
So the zeroth value is pi / 2.
The coefficients are now an even sequence, in other words, symmetric about k = 0.
And the consequence of that is that when you take these coefficients and put together the equivalent trigonometric form, the trigonometric form involves only cosines and no sine terms.
Now you'll see this in other examples, not that we'll do in the lecture, but examples in the text and in the video manual, if in fact the square wave was neither symmetric or antisymmetric, then the trigonometric form would involve both sines and cosines.
And that is, of course, the more general case.
Furthermore, in the two examples I've shown here, in both cases, the signal is odd harmonic.
In other words, for even values of k, the coefficients are equal to 0.
Although I won't justify that here, that's a consequence of the fact that this symmetry is exactly about half a period.
And if you made the on time of the square wave different in relation to the off time, then that property would also disappear.
Now what's kind of amazing, actually, is that if we take a square wave, like I have here or as I had in the antisymmetric case, the implication is that I can build that square wave by adding up enough sines or cosines.
And it really seems kind of amazing because the square wave, after all, is a very discontinuous function.
Sinusoids are very continuous.
And it seems puzzling that in fact you can do that.
Well let's look in a little bit of detail how the sinusoidal terms add up to build a square wave.
And to do that, let's first define what I refer to as a partial sum.
So here we have the expression which is the synthesis equation, telling us how x(t) could be represented as complex exponentials if it can be.
And let's consider just a finite number of terms in this sum.
And so x_n(t), of course, as n goes to infinity, approaches the infinite sum that we're talking about.
And although we could do this more generally, let's not.
Let's focus on the symmetric square wave case, where because of the symmetry of these coefficients, namely that a_k is equal to a_(-k), we can rewrite these terms as cosine terms.
And so this partial sum can be expressed the way I'm expressing it here.
Well let's look at a few of these terms.
On the graph, I have, first of all, x(t), which is our original square wave.
The term that I indicate here is the factor of 1/2, which is this term.
With n = 1, that would correspond to adding one cosine term to that.
And so the sum of those two would be this, which looks a little closer to the square wave, but certainly not very close to it at all.
And in fact, it's somewhat hard to imagine without seeing the terms build up how in fact, by adding more and more terms, we can generate something that is essentially flat, except at the discontinuities.
So let's look at this example.
And what I'd like to show is this example, but now as we add many more terms to it.
And let's see in fact how these individual terms add up to build up the square wave.
So this is the square wave that we want to build up through the Fourier series as a sum of sinusoids.
And the term for k = 0 will be a constant which represents the DC value of this.
And so in the partial sum, as we develop it, the first thing that we'll show is just the term for k = 0.
Now for k = 1, we would add to that one sinusoidal term.
And so the sum of the term for k = 1 and k = 0 is represented here.
Now when we go to k = 2, because of the fact that this is an odd harmonic function, in fact, the term for k = 2 will have zero amplitude and so this won't change.
Here we show the Fourier series with k = 2 and there's no change.
And then we will go to k = 3.
And we will be adding, then, one additional sinusoidal term.
Here is k = 3.
When we go to k = 4, again, there won't be any change.
But there will be another term that's added at k = 5 here.
Then k = 6, again, because it's odd harmonic, no change.
And finally k = 7 is shown here.
And we can begin to see that this starts to look somewhat like the square wave.
But now to really emphasize how this builds up, let's more rapidly add many more terms, and in fact increase the number of terms up to about 100, recognizing that the shape will only change on the inclusion of the odd-numbered terms, not the even-numbered terms, because it's an odd harmonic function.
So now we're increasing and we're building up toward k = 100, 100 terms.
And notice that it is the higher-order terms that tend to build up the discontinuity corresponding to the notion that the discontinuity, or sharp edges in a signal, in fact, are represented through the higher frequencies in the Fourier series.
And here we have a not-too-unreasonable approximation to the original square wave.
There is the artifact of the ripples at the discontinuity.
And in fact, that rippling behavior at the discontinuity is referred to the Gibbs phenomenon.
And it's an inherent part of the Fourier series representation at discontinuities.
Now to emphasize this, let's decrease the number of terms back down.
And we will carry this down to k = 1, again to emphasize how the sinusoids are building up the square wave.
Here we are back at k = 1.
And then finally, we will add back in the sinusoids that we took out.
And let's build this back up to 100 terms, showing the approximation that we generated with 100 terms to the square wave.
Okay, so what you saw is that, in fact, we got awfully close to a square wave.
And the other thing that was kind of interesting about it as it went along was the fact that, with the low frequencies, what we were tending to build was the general behavior.
And as the higher frequencies came in, that tended contribute to the discontinuity.
And in fact, something that will stand out more and more as we go through our discussion of Fourier series and Fourier transforms, is that general statement, that it's the low-frequency terms that represent the broad time behavior, and it's the high-frequency terms that are used to build up the sharp transitions in the time domain.
Now we need to get a little more precise about the question of how, in fact, the Fourier series, or when the Fourier series represents the functions that we're talking about and in what sense they represent them.
And so if we look again at the synthesis equation, what we really want to ask is, if we add up enough of these terms, in what sense does this sum represent this time function?
Well, let's again use the notion of our partial sum.
So we have the partial sum down here.
And we can think of the difference between this partial sum and the original time function as the error.
And I've defined the error here.
And what we would like to know is does this error decrease as we add more and more terms?
And in fact, in what sense, if the error does decrease, in what sense does it decrease?
Now in detail this is a fairly complicated and elaborate topic.
I don't mean to make that sound frightening.
It's mainly a statement that I don't want to explore in a lot of detail.
But it relates to what it's referred to as the issue of convergence of the Fourier series.
And the convergence of the Fourier series, the bottom line on it, the kind of end statement, can be made in several ways.
One statement related to the convergence of the Fourier series is the following.
If I have a time function, which is what is referred to as square integrable, namely its integral, over a period, is finite.
Then what you can show, kind of amazingly, is that the energy in that error, in other words, the energy and the difference between the original function and the partial sum, the energy in that, goes to 0 as n goes to infinity.
A somewhat tighter condition is a condition referred to as it Dirichlet conditions, which says that if the time function is absolutely integrable, not square integrable, but absolutely integrable-- and I've kind of hedged the issue by just simply referring to x(t) as being well behaved-- then the statement is that the error in fact goes to 0 as n increases, except at the discontinuities.
And what well behaved means in that statement is that, as discussed in the book, there are a finite number of maxima and minima in any period and a finite number of finite discontinuities, which is, essentially, always the case.
So under square integrability what we have is the statement not that the partial sum goes to the right value at every point, but that the energy in the error goes to 0.
Under the Dirichlet conditions, it says that, in fact, the signal goes to the right value at every time instant except at the discontinuities.
So going back to the square wave, the square wave satisfies either one of those conditions.
And so what the consequence is is that, with the square wave,
if we looked at the error, then in fact what we would find is that the energy in the error would go to zero as we add more and more terms in the partial sum.
And in fact, since the square wave also satisfies the Dirichlet conditions, the actual value of the error, the difference between the partial sum and the true value, will actually go to 0.
That difference will go to 0 except at the discontinuities.
And that, in fact, is kind of evident as we watch the function build up by adding up these terms.
And so in fact, let's go back and see again the development of the partial sums in relation to the original time function.
Let's observe, this time again, basically what we saw before, which is that it builds up to the right answer.
And furthermore what we'll plot this time, also as a function of time, is the energy in the error.
And what we'll see is that the energy in the error will be tending towards 0 as the number of terms increases.
So once again, we have the square wave.
And we want to again show the buildup of the Fourier series,
this time showing also how the energy in the error decreases as we add more and more terms.
Well, once again, we'll begin with k = 0, corresponding to the constant term.
And what's shown on the bottom trace is the energy in the error between those two.
And we'll then add the term k = 1 to the DC term and we'll see that the energy will decrease when we do that.
Here we have then the sum of k = 0 and k = 1.
Now with k = 2, the energy won't decrease any further because it's an odd harmonic function.
That's what we've just added in.
When we add in the term for k = 3, again, we'll see the energy in the error decrease as reflected in the bottom curve.
So there we are at k = 3.
When we go to k = 4, there again is no change in the error.
At k = 5, again the error decreases. k = 6, there will be no change again.
And at k = 7, the energy decreases.
And now let's show how the error decreases by building up the number of terms much more rapidly.
Already the error has gotten somewhat small on the scale in which we're showing it, so let's expand out the error scale, the vertical axis displaying the energy in the error, so that we could watch how the energy decreases as we add more and more terms.
So here we have the vertical scale expanded.
And now what we'll do is increase the number of terms in the Fourier series and watch the energy in the error decreasing, always decreasing, of course, on the inclusion of the odd-numbered terms and not on the inclusion of the even-numbered terms because of the fact that it's an odd harmonic function.
Now the energy in the error asymptotically will approach 0, although point by point, the Fourier series will never be equal to the square wave.
It will, at every instant of time, except at the discontinuities, where there will always be some ripple corresponding to what's referred to as the Gibbs phenomenon.
So what we've seen, then, is a quick look at the Fourier series representation of periodic signals.
We more broadly want to have a more general representation of signals in terms of complex exponentials.
And so our next step will be to move toward a representation of nonperiodic or aperiodic signals.
Now the details of this, I leave for the next lecture.
The only thought that I want to introduce at this point is the basic strategy which is somewhat amazing and kind of interesting to reflect on in the interim.
The basic strategy with an aperiodic signal is to think of representing this aperiodic signal as a linear combination of complex exponentials by the simple trick of periodically replicating this signal, generating a periodic signal using a Fourier series representation for that periodic signal, and then simply letting the period go to infinity.
As the period goes to infinity, that periodic signal becomes the original aperiodic one that we had before.
And the Fourier series representation then becomes what we'll refer to the Fourier transform.
So that's just a quick look at the basic idea and approach that we'll take.
In the next lecture, we'll develop this a little more carefully and more fully, moving from the Fourier series, which we've used for periodic signals, to develop the Fourier transform, which will then be representation for aperiodic signals.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
Last time, we began to address the issue of building continuous time signals out of a linear combination of complex exponentials.
And for the class of periodic signals specifically, what this led to was the Fourier series representation for periodic signals.
Let me just summarize the results that we developed last time.
For periodic signals, we had the continuous-time Fourier series, where we built the periodic signal out of a linear combination of harmonically related complex exponentials.
And what that led to was what we referred to as the synthesis equation.
And we briefly addressed the issue of when this in fact builds, when this in fact is a complete representation of the periodic signal, and in essence, what we presented was conditions either for x of t being square integrable or x of t being absolutely integrable.
Then, the other side of the Fourier series is what I referred to as the analysis equation.
And the analysis equation was the equation that told us how we get the Fourier series coefficients from x of t.
And so this equation together with the synthesis equation represent the Fourier series description for periodic signals.
Now what we'd like to do is extend this idea to provide a mechanism for building non-periodic signals also out of a linear combination of complex exponentials.
And the basic idea behind doing this is very simple and also very clever as I indicated last time.
Essentially, the thought is the following, if we have a non-periodic signal or aperiodic signal, we can think of constructing a periodic signal by simply periodically replicating that aperiodic signal.
So for example, if I have an aperiodic signal as I've indicated here, I can consider building a periodic signal, where I simply take this original signal and repeat it at multiples of some period t 0.
Now, two things to recognize about this.
One is that the periodic signal is equal to the aperiodic signal over one period.
And the second is that as the period goes to infinity then, in fact, the periodic signal goes to the aperiodic signal.
So the basic idea then is to use the Fourier series to represent the periodic signal, and then examine the Fourier series expression as we let the period go to infinity.
Well, let's quickly see how this develops in terms of the associated equations.
Here, again, we have the periodic signal.
And what we want to inquire into is what happens to the Fourier series expression for this as we let the period go to infinity.
As that happens, whatever Fourier series representation we end up with will correspond also to a representation for this aperiodic signal.
Well, let's see.
The Fourier series synthesis expression for the periodic signal expresses x tilde of t, the periodic signal as a linear combination of harmonically related complex exponentials with the fundamental frequency omega 0 equaled to 2 pi divided by the period.
And the analysis equation tells us what the relationship is for the coefficients in terms of the periodic signal.
Now, I indicated that the periodic signal and the aperiodic signal are equal over one period.
We recognize that this integration, in fact, only occurs over one period.
And so we can re-express this in terms of our original aperiodic signal.
So this tells us the Fourier series coefficients in terms of x of t.
Now, if we look at this expression, which is the expression for the Fourier coefficients of the aperiodic signal, one of the things to recognize is that in effect what this represents are samples of an integral, where we can think of the variable omega taking on values that are integer multiples of omega 0.
Said another way, let's define a function, as I've indicated here, which is this integral, where we may think of omega as being a continuous variable and then the Fourier series coefficients correspond to substituting for omega k omega 0.
Now, one reason for doing that, as we'll see, is that in fact, this will turn out to provide us with a mechanism for a Fourier representation of x of t.
And this, in fact, then, is an envelope of the Fourier series coefficients.
In other words, t 0 the period times the coefficients is equal to this integral add integer multiples of omega 0.
So this, in effect, tells us how to get the Fourier series coefficients of the periodic signal in terms of samples of an envelope.
And that will become a very important notion shortly.
And that, in effect, will correspond to an analysis equation to represent the aperiodic signal.
Now, let's look at the synthesis equation.
Recall that in the synthesis our strategy is to build a periodic signal and let the period go to infinity.
Well, here is the expression for the synthesis of the periodic signal now expressed in terms of samples of this envelope function, and where I've simply used the fact or the substitution that t 0 is 2 pi over omega 0, and so I have an omega 0 here and a 1 over 2 pi.
And the reason for doing that, as we'll see in a minute, is that this then turns into an integral.
Specifically, then, the synthesis equation that we have is what I've indicated here.
We would now want to examine this as the period goes to infinity, which means that omega 0 becomes infinitesimally small.
And without dwelling on the details, and with my suggesting that you give this a fair amount of reflection, in fact, what happens as the period goes to infinity is that this summation approaches an integral over omega, where omega 0 becomes the differential in omega, and the periodic signal, of course, approach is the aperiodic signal.
So the resulting equation that we get out of the original Fourier series synthesis equation is the equation that I indicate down here, x of t synthesized in terms of this integral, which is what the Fourier series approaches as omega 0 goes to 0.
And we had previously that x of omega was in fact an envelope function.
And we have then the corresponding Fourier transform analysis equation, which tells us how we arrive at that envelope in terms of x of t.
So we now have an analysis equation and a synthesis equation, which in effect expresses for us how to build x of t in terms of infinitesimally finely spaced complex exponentials.
The strategy to review it, and which I'd like to illustrate with a succession of overlays, was to begin with our aperiodic signal, as I indicate here, and then we constructed from that a periodic signal.
And this periodic signal has a Fourier series, and we express the Fourier series coefficients of this as samples of an envelope function.
The envelope function is what I indicate on the curve below.
So this is the envelope of the Fourier series coefficients.
For example, if the period t 0 was four times t1, then the Fourier series coefficients that we would end up with is this set of samples of the envelope.
If instead we doubled that period, then the Fourier series coefficients that we end up with are more finely spaced.
And as t 0 continues to increase, we get more and more finely spaced samples of this envelope function, and as t 0 goes to infinity in fact, what we get is every single point on the envelope, and that provides us with the representation for the aperiodic signal.
Let me, just to really emphasize the point, show this example once again.
But now, let's look at it dynamically on the computer display.
So here we have the square wave, and below it, the Fourier series coefficients.
And we now want to look at the Fourier series coefficients as the period of the square wave starts to increase.
And what we see is that these look like samples of an envelope.
And in fact, the envelope of the Fourier series coefficients is shown in the bottom NOISEEVENT and to emphasize in fact that it is the envelope let's superimpose it on top of the Fourier series coefficients that we've generated so far.
OK.
Now, let's increase the period even further, and we'll see the Fourier series coefficients fill in under that envelope function even more.
And in fact, as the period gets large enough, what we begin to get a sense of is that we're sampling more and more finely this envelope.
And in fact, in the limit, as the period goes off to infinity, the samples basically will represent every single point on the envelope.
Well, this is about as far as we want to go.
Let's once again, plot the envelope function, and again, to emphasize that we've generated samples of that, let's superimpose that on the Fourier series coefficients.
So what we have then is now our Fourier transform representation, the continuous time Fourier transform with the synthesis equation expressed as an integral, as I've indicated here, and this integral is what the Fourier series sum went to as we let the period go to infinity or the frequency go to zero.
The corresponding analysis equation, which we have here,
the analysis equation being the expression for the envelope of the Fourier series coefficients for the periodically replicated signal.
And in shorthand notation, we would think of x of t and NOISEEVENT Fourier transform as a pair, as I've indicated here.
And the Fourier transform, as we'll emphasize in several examples, and certainly as is consistent with the Fourier Series, is a complex valued function even when x of t is real.
So with x of t real, we end up with a Fourier transform, which is a complex function.
Just as the Fourier series coefficients were complex for a real value time function.
So we could alternatively, as with the Fourier series,
express the Fourier transform in terms of it's real part and imaginary part, or alternatively, in terms of its magnitude and its angle.
All right, now let's look at an example of a time function in its Fourier transform.
And so let's consider an example, which in fact is an example worked out in the text.
It's example 4.7 in the text.
And this is our old familiar friend the exponential.
It's Fourier transform is the integral from minus infinity to plus infinity, x of t, e to the minus j omega t dt.
And so, if we substitute in x of t and combine these two exponentials together, these two exponentials combined are e to the minus t times a plus j omega.
And if we carry out the integration of this, we end up with the expression indicated here and provided now, and this is important, provided that a is greater than 0, then at the upper limit, this exponential becomes 0.
At the lower limit, of course, it's one.
And so what we have finally is for the Fourier transform expression 1 over a plus j omega.
Now, this Fourier transform as I indicated is a complex valued function.
Let's just take a look at what it looks like graphically.
We have the expression for the Fourier transform pair, e to the minus a t times NOISEEVENT NOISEEVENT .
And its Fourier transform is 1 over a plus j omega.
And I indicated that that's true for a greater than 0.
Now, in the expression that we just worked out, if a is less than 0, in fact, the expression doesn't converge e to the minus a t for a negative as t goes to infinity, blows up, and so in fact the Fourier transform doesn't converge except for the case where a is greater than 0 And in fact, there is a more detailed discussion of convergence issues in the text.
The convergence issues are very much the same for the Fourier transform as they are for the Fourier series.
And in fact, that's not surprising, because we developed the Fourier transform out of a consideration of the Fourier series.
So the convergence conditions as you'll see as you refer in detail to the text relate to whether the time function is absolutely integrable under one set of conditions and square integrable under another set of conditions.
OK, now, if we plot the Fourier transform, let's first consider the shape of the time function.
And as I indicated, we're restricting the time function so that the exponential factor a is positive.
In other words, e to the minus a t decays as t goes to infinity.
The magnitude of the Fourier transform is as I indicate here and the phase below it.
And there are a number of things we can see about the magnitude and phase of the Fourier transform for this example, which in fact we'll see in the next lecture are properties that apply more generally.
For example, the fact that the Fourier transform magnitude is an even function of frequency, and the phase is an odd function of frequency.
Now, let me also draw your attention to the fact that on this curve we have both positive frequencies and negative frequencies.
In other words, in our expression for the Fourier transform, it requires both omega positive and omega negative.
This, of course, was exactly the same in the case of the Fourier series.
And the reason you should recall and keep in mind is related to the fact that we're building our signals out of complex exponentials, which require both positive values of omega and negative values of omega.
Alternatively, if we chosen other representation, which turns out notationally to be much more difficult, namely sines and cosines, then we would in fact only consider positive frequencies.
So it's important to keep in mind that, in our case, both with the Fourier series and the Fourier transform, we deal and require both positive and negative frequencies in order to build our signals.
Now, in the graphical representation that I've shown here, I've chosen a linear amplitude scale and a linear frequency scale.
And that's one graphical representation for the Fourier transform that we'll typically use.
There's another one that very commonly arises, which I'll just briefly indicate for this example.
And that is what's referred to as a bode plot in which the magnitude is displayed on a log amplitude and log frequency scale.
And the phase is displayed on a log frequency scale.
Let me show you what I mean.
Here is the general expression for the bode plot.
The bode plot expresses for us the amplitude in terms of the logarithm to the base 10 of the magnitude.
And it also expresses the angle in both cases expressed as a function of a logarithmic frequency axis.
So here is the amplitude as I've displayed it.
And this is a log magnitude scale, a logarithmic frequency scale as indicated by the fact that as we move in equal increments along this axis, we change frequency by a factor of 10.
And similarly, what we have is a display for the phase again on a log frequency scale.
And I indicated that there is a symmetry to the Fourier transform, and so in fact, we can infer from this particular picture what it looks like for the negative frequencies as well as for the positive frequencies.
Now, what we've done so far is to develop the Fourier transform on the basis, the Fourier transform of an aperiodic signal on the basis of periodically repeating it and recognizing that the Fourier series coefficients are samples of an envelope and that these become more finely spaced as frequency increases.
And in fact, we can go back to our original equation in which we developed an envelope function, and what we had indicated is that the Fourier series coefficients were samples of this envelope.
We then defined this envelope as the Fourier transform of this aperiodic signal.
So that provided us with a way-- and it was a mechanism-- for getting a representation for an aperiodic signal.
Now, suppose that we have instead a periodic signal, are there, in fact, some statements that we can make about how the Fourier series coefficients of that are related to the Fourier transform of something.
Well, in fact, this statement tells us exactly how to do that.
What this statement says is that, in fact, the Fourier series coefficients are samples of the Fourier transform of one period?
So if we now consider a periodic signal, we can in fact get the Fourier series coefficients of that periodic signal by considering the Fourier transform of one period.
Said another way, the Fourier series coefficients are proportional to samples of the Fourier transform of one period.
So if we consider this a periodic signal, computed as Fourier transform, and selected these samples that I indicate here, namely samples equally spaced in omega by integer multiples of omega 0, then in fact, those would be the Fourier series coefficients.
So we can go back to our example previously that involved the square wave.
And now, in this case, we could argue that if in fact it was the periodic signal that we started with, we could get the Fourier series coefficients of that by thinking about the Fourier transform of one period, which I indicate here.
And then the Fourier series coefficients of the periodic signal, in fact, are the appropriate set of samples of this envelope.
All right, now, we have a way of getting the Fourier series coefficients from the Fourier transform of one period.
We originally derived the Fourier transform of one period from the Fourier series.
What would, in fact, be nice is if we could incorporate the Fourier series and the Fourier transform within a common framework.
And in fact, it turns out that there is a very convenient way of doing that almost by definition.
Essentially, if we consider what the equation for the synthesis looks like in both cases, we can in effect define a Fourier transform for the periodic signal, which we know is represented by its Fourier series coefficients.
We can define a Fourier transform, and the definition of the Fourier transform is as an impulse train, where the coefficients in the impulse train are proportional, with a proportionality factor of 2 pi for a more or less a bookkeeping reason, proportional to the Fourier series coefficients.
And the validity of this is, more or less, can be seen essentially by substitution.
Specifically, here is then the synthesis equation for the Fourier transform if we substitute this definition for the Fourier transform of the periodic signal into this expression then when we do the appropriate bookkeeping and interchange the order of summation and integration the impulse integrates out to the exponential factor that we want.
So we have the exponential factor.
We have the Fourier series coefficients.
The 2 pis take care of each other, and what we're left with is the synthesis equation for aperiodic signal in terms of the Fourier transform, or in terms of its Fourier series coefficients.
Now, we can just see this in terms of a simple example.
If we consider the example of a symmetric square wave, then in effect what we're saying is that for this symmetric square wave, this has a set of Fourier series coefficients, which we worked out previously and which I indicate on this figure with a bar graph.
And really all that we're saying is that, whereas these Fourier series coefficients are indexed on an integer variable k, and NOISEEVENT bars not impulses.
If we simply redefine or define the Fourier transform of the periodic signal as an impulse train, where the weights of the impulses are 2 pi times the corresponding Fourier series coefficients, then this, in fact, is what we would use as the Fourier transform of the periodic signal.
Now, we've kind of gone back and forth, and maybe even it might seem like we've gone around in circles.
So let me just try to summarize the various relationships and steps that we've gone through, keeping in mind that one of our objectives was first to develop a representation for aperiodic signals and then attempt to incorporate within one framework both periodic and aperiodic signals.
We began with an aperiodic signal.
And the strategy was to develop a Fourier representation by constructing a periodic signal for which that was one period.
And then we let the period go to infinity, as I indicate here.
So we have an aperiodic signal.
We construct a periodic signal, x tilde of t for which one period is the aperiodic signal.
X tilde of t, the periodic signal, has a Fourier series,
and as its period increases that approaches the aperiodic signal, and the Fourier series of that approaches the Fourier transform of the original aperiodic signal.
So that was the first step we took.
Now, the second thing that we recognize is that once we have the concept of the Fourier transform, we can, in fact, relate the Fourier series coefficients to the Fourier transform of one period.
So the second statement that we made was that if in fact we're trying to represent a periodic signal, we can get the Fourier series coefficients of that by computing the Fourier transform of one period and then samples of that Fourier transform are, in fact, the Fourier series coefficients for the periodic signal.
Then, the third step that we took was to inquire as to whether there is a Fourier transform that can appropriately be defined for the periodic signal, and the mechanism for doing that was to recognize that if we simply defined the Fourier transform of the periodic signal as an impulse train, where the impulse heights or areas were proportional to the Fourier series coefficients, then, in fact, the Fourier transform synthesis equation reduced to the Fourier series synthesis equation.
So the third step, then, was with a periodic signal.
The Fourier transform of that periodic signal, defined as an impulse train, where the heights or areas of the impulses are proportional to the Fourier series coefficients, provides us with a mechanism for combining it together the concepts or notation of the Fourier series and Fourier transform.
So if we just took a very simple example, here is an example in which we have an aperiodic signal, which is just an impulse, and its Fourier transform is just a constant.
We can think of a periodic signal associated with this,
which is this signal periodically replicated with a spacing t 0.
The Fourier transform of this is a constant.
And this, of course, has a Fourier series representation.
So the Fourier transform of the original impulse is just a constant.
The Fourier transform of the periodic signal is an impulse train, where the heights of the impulses are proportional to the Fourier series coefficients.
And, of course, we could previously have computed the Fourier series coefficients for that impulse train, and those Fourier series coefficients are as I've shown here.
So in both of these cases, these in effect represent just a change in notation, where here we have a bar graph, and here we have an impulse train.
And both of these simply represent samples of what we have above, which is the Fourier transform of the original aperiodic signal.
Once again, I suspect that kind of moving back and forth and trying to straighten out when we're talking about periodic and aperiodic signals may require a little mental gymnastics initially.
Basically, what we've tried to do is incorporate within one framework a representation for both aperiodic and periodic signals, and the Fourier transform provides us with a mechanism to do that.
In the next lecture, I'll continue with the discussion of the continuous-time Fourier transform in particular focusing on a number of its properties, some of which we've already seen, namely the symmetry properties.
We'll see lots of other properties that relate, of course, both to the Fourier transform and to the Fourier series.
Thank you.
The following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
In the last two lectures, we saw how periodic and non periodic signals could be represented as linear combinations of complex exponentials.
And this led to the Fourier series representation, in the periodic case, and it led to the Fourier transform representation in the aperiodic case.
And then, in fact, what we did was to incorporate the Fourier series within the framework of the Fourier transform.
What I'd like to do in today's lecture is look at the Fourier transform more closely, in particular with regard to some of its properties.
So let me begin by reminding you of the analysis and synthesis equations for the Fourier transform, as I've summarized them here.
The synthesis equation being an equation the tells us how to build the time function out of, in essence, a linear combination of complex exponentials.
And the analysis equation telling us how to get the amplitudes of those complex exponentials from the associated time function.
So essentially, in the decomposition of x of t as a linear combination of complex exponentials, the complex amplitudes of those are, in effect, the Fourier transform scaled by the differential and scaled by 1 over 2 pi.
As I indicated last time, the Fourier transform is a complex function of frequency.
And in particular, the complex function of frequency has an important and very useful symmetry property.
The symmetry of the Fourier transform, when x of t is real, is what is referred to as conjugate symmetric.
In other words, if we take the complex function, f of omega,
and its complex conjugate, that's equivalent to replacing omega by minus omega.
And a consequence of that, if we think in terms of the real part of the Fourier transform, the real part is an even function of frequency, and the magnitude is an even function of frequency.
Whereas the imaginary part is an odd function of frequency, and the phase angle is an odd function of frequency.
So we have this symmetry relationship that, for x of t real, if we think of either the real part or the magnitude of the Fourier transform, it's even symmetric.
And the imaginary part, or the phase angle, either one, is odd symmetric.
In other words, if we flip it, we multiply by a minus sign.
Let's look at an example, in the context of an example that we worked last time for the Fourier transform.
We took the case of a real exponential of the form e to the minus at times the step.
And the Fourier transform, as we found, was of the algebraic form 1 over a plus j omega.
And, incidentally, the Fourier transform integral only converged for a greater than 0.
In other words, for this exponential decaying.
And I illustrated the magnitude and angle, and what we see is that e to the minus at is a real time function, therefore its magnitude should be an even function of frequency, and indeed it is.
And its phase angle, shown below, is an odd function of frequency.
So in fact, although I stressed last time that the complex exponentials is required to build a time function require exponentials of both positive and negative frequencies, for x of t real what we see is that, because of these symmetry properties, either for the real and imaginary or magnitude and angle, we can specify the Fourier transform for, let's say only positive frequencies,
and the symmetry, then, implies, or tells us, what the Fourier transform, then, would be for the negative frequencies.
This same example, the decaying exponential,
demonstrates another important and often useful property of the Fourier transform.
Specifically, let's rewrite, algebraically, this example as I indicate here.
So we have, again, the exponential, whose Fourier transform is 1 over a plus j omega.
And if I just simply divide numerator and denominator by a, I can rewrite it in the form shown here.
And what we notice is that in the time function we have a term of the form a times t, and in the frequency function we have a term of the form omega divided by a.
Or equivalently, we could think of the time function,
which I show here, and as the parameter a gets smaller, the exponential gets spread out in time.
Whereas its Fourier transform, or the magnitude of its Fourier transform, has the inverse property that as a gets smaller, in fact, this scales down in frequency.
Well, this is a general property of the Fourier transform, namely the fact that a linear scaling in time generates the inverse linear scaling in frequency.
And the general statement of this time frequency scaling is what I show at the top of the transparency, namely the equation that if we scale the time function in time, then we apply an inverse scaling in frequency.
This, in fact, is probably a result that you're already possibly familiar with, in somewhat of a different context.
Essentially, you could think of this as an example, or a generalization, rather, of the notion that if, let's say I had a signal that was recorded on a tape player, and if I play the tape back at, let's say, twice the speed, which means that I'm compressing the time axis linearly by a factor of 2.
Then, in fact, what happens is that the frequencies that we observe get pushed up by a factor of 2.
And, in fact, let me illustrate that.
I have here a glockenspiel, and anyone who loves a parade certainly knows what a glockenspiel is.
And this particular glockenspiel has three a's, separated each by an octave.
There's a middle a, a high a, and a low a.
And what I've done is to record the middle a on a tape at 7 and 1/2 inches per second.
And what I'd like to demonstrate is that as we play that back, either at twice or half speed, the effective note gets moved down or up by an octave.
So let me first play the note at the speed at which it was recorded.
And so what we'll hear is the middle a as I've recorded it.
And let me just start the tape player.
Let me stop it, and hopefully what you heard is the same note from the tape recorder as the note that I played on the glockenspiel.
Now I'll rewind the tape, and we'll go back to the beginning of that portion.
And now, if I change the tape speed to half the speed, so from 7 and 1/2 inches per second, I'll change the tape speed to 3 and 3/4 inches per second.
And now when I play the tape back, because of the inverse relationship between time and frequency scaling, we're now scaling in time by stretching out, we would expect the frequencies to be lowered by a factor of 2.
We should now expect the taped note to be an octave lower, matching this lower a.
So let's just play that.
Let me stop it, and, again, we'll rewind the tape.
Go back to the beginning, and now we'll play this at twice the speed.
So I'll change from 3 and 3/4 to 15 inches per second.
And now when I play it, we would expect that to match the upper note.
And let's just do that.
Although that's a result that, intuitively, probably makes considerable sense, in fact, what that is is an illustration of the inverse relationship between time scaling and frequency scaling.
And also, by the way, it was finally my opportunity to play the glockenspiel on television.
In addition, there is another very important relationship between the time and frequency domains, namely what is referred to as a duality relationship.
And the duality relationship between time and frequency falls out, more or less directly, from the equations, the analysis and synthesis equations.
In particular, if we look at the synthesis equation, which I repeat here, and the analysis equation, which I repeat below it, what we observe is that, in fact, these equations are basically identical, except for the fact that in the top integral we have things as a function of omega, in the bottom integral as a function of t, and there's a factor of 1 over 2 pi, and, by the way, a minus sign.
You can look at the algebra more carefully at your leisure, but essentially what this says is that if x of omega is the Fourier transform of a time function x of t, then, in fact, x of t is very much like the Fourier transform of x of omega.
In fact, it's the Fourier transform of x of minus omega to account for this minus sign.
And, by the way, there's just an additional factor of 1 over 2 pi.
So the duality relationship which follows from these two equations, in fact, says that if x of t and x of omega are a Fourier transform pair, if x and X are a Fourier transform pair, then X, in fact, has a Fourier transform which is proportional to x turned around.
This duality in the continuous time Fourier transform is very important.
It's very useful.
It, by the way, is not a duality that surfaced in the Fourier series, because, as you recall, the Fourier series begins with a continuous time function and in the frequency domain generates a sequence, which would just naturally have problems associated with it if we attempted to interpret a duality.
And we'll see, also, that in the discrete time case, one of the important differences between continuous time and discrete time Fourier transforms is the fact that in continuous time we have duality, in the discrete time Fourier transform we don't.
Let's illustrate this with an example.
Here are, in fact, two examples of Fourier transform pairs taken from examples in the text.
The top one being example 4.11 from the text, and it's a time function which is a sine x over x type of function.
And its Fourier transform corresponds to a rectangular shape in the frequency domain.
There's also another example in the text, the example that precedes this one, which is example 4.10.
And in example 4.10, we begin with a rectangle, and its Fourier transform is of the form of a sine x over x function.
So in fact, if we look at these two examples together, what we see is the duality very evident.
In other words, if we take this time function and instead think of a frequency function that has the same form, then we simply interchange the roles of time and frequency in the other domains.
So the fact that these two correspond means that these two correspond.
Of course in this particular example, because of the fact that we picked a symmetric function, an even function, in fact, the additional twist of the time axis being reversed didn't show up in duality with this example.
One thing this says, of course, is that essentially any time you've calculated the Fourier transform of one time function, then you've actually calculated the Fourier transform of two time functions.
Another one being the dual example to the one that you just calculated.
Also somewhat related to duality is what is referred to as Parseval's relation for the continuous time Fourier transform.
And essentially, what Parseval's relationship says,
as a summary of it, says that the energy in a time function and the energy in its Fourier transform are proportional, the proportionality factor being a factor of 2 pi.
That's summarized here.
What's meant by the energy is, of course, the integral of the magnitude squared of x of t.
And the statement of Parseval's relation is that that integral, the energy in x of t, is proportional to this integral, which is the energy in x of omega.
Although we've incorporated the Fourier series within a framework of the Fourier transform, Parseval's relation needs to be modified slightly for Fourier series, because of the fact that a periodic signal has an infinite amount of energy in it, and, essentially, that form of Parseval's relationship for the periodic case would say infinity equals infinity, which isn't too useful.
However, it can be modified so that Parseval's relationship to the periodic case says, essentially, that the energy in one period of the periodic time function is proportional with, this is the proportionality factor, proportional to the sum of the magnitude squared of the coefficients.
In other words, the energy in one period is proportional to the energy in the sequence that represents the Fourier series coefficients.
There are lots of other properties, and they're developed in the text and in the study guide.
A number of properties that we want to make particular use of during this lecture, and in later lectures, are ones that I summarize here.
And I won't demonstrate the proofs, but principally focus on some of the interpretation as the lecture goes on.
The first property that I have listed here is what's referred to as the time shifting property.
And the time shifting property says, if I have a time function with a Fourier transform x of omega, if I shift that time function in time, then that corresponds to multiplying the Fourier transform by this factor.
As you examine this factor, what you can see is that this factor has magnitude unity and it has a phase, which is linear with frequency, and a slope of minus t0.
So a statement to remember, that will come up many times throughout the course, is that a time shift, or a displacement in time, corresponds to a linear change in phase and frequency.
Another property and, in fact, a pair of properties that we'll make reference to as we turn our attention toward the end of this lecture to solving differential equations using the Fourier transform, is what's referred to as the differentiation property and its companion, which is the integration property.
The differentiation property says, again, if we have a time function with Fourier transform x of omega, the Fourier transform of the time derivative of that corresponds to multiplying the Fourier transform by a linear function of frequency.
So here it's a linear amplitude change that corresponds to differentiation.
At first glance, what you might think is that the integration property is just the reverse of that.
If for the differentiation property you multiply by j omega, then for integration you must divide by j omega.
And that's almost correct, except not quite.
And the reason for the not quite is that recall that if you differentiate, what happens, of course, is that you lose a constant.
And if we have a time function that's some finite energy signal plus a constant, differentiating will destroy the constant.
The integration property, in essence, tries to bring that back.
So the integration property, which is the inverse of the differentiation property, says that we divide the transform by j omega, and then if, in fact, there was a constant added to x of t, we have to account for that by inserting an impulse into the Fourier transform.
And the final property that I want to draw your attention to on this view graph is the linearity property, which is very straightforward to demonstrate from the analysis and synthesis equations, which simply says if x1 of omega is the Fourier transform x1 of t, and x2 of omega is the Fourier transform of x2 of t, then the Fourier transform of a linear combination is a linear combination of the Fourier transforms.
Let me emphasize, also, that these properties, for the most part, apply both to Fourier series and Fourier transforms because, in fact, what we've done is to incorporate the Fourier series within the framework of the Fourier transform.
We'll be using a number of these properties shortly, when we turn our attention to linear constant coefficient differential equations.
However, before we do that I'd like to focus on two additional major properties, and these are what I refer to as the convolution property and the modulation property.
And in fact, the convolution property, as I'm about to introduce it, forms the mathematical and conceptual basis for the whole notion of filtering, which, in fact, will be a topic by itself in a set of lectures, and, in fact, is a chapter by itself in the textbook.
Similarly, what I'll refer to as the modulation property,
again, will occupy its own set of lectures as we go through the course, and, in fact, has its own chapter in the textbook.
Let me just indicate what the convolution property is.
And what the convolution property tells us is that the Fourier transform of the convolution of two time functions is the product of their Fourier transforms.
So it says, for example, that if I have a linear time invariant system, and I have an input x of t, an impulse response h of t, and the output, of course, being the convolution, then, in fact, if I look at this in the frequency domain, the Fourier transform of the output is the Fourier transform of the input times the Fourier transform of the impulse response.
You can demonstrate this property algebraically by essentially taking the convolution integral and applying the Fourier transform and doing the appropriate interchanging of the order of integration, et cetera.
But what I'd like to draw your attention to is a somewhat more intuitive interpretation of the property.
And the intuitive interpretation stems from the relationship between the Fourier transform of the impulse response and what we've referred to as the frequency response.
Recall that one of the things that led us to use complex exponentials as building blocks was the fact that they're eigenfunctions of linear time and variant systems.
In other words, if we have a linear time invariant system,
and I have an input which is a complex exponential, the output is a complex exponential of the same frequency multiplied by what we call the frequency response.
And, in fact, the expression for the frequency response is identical to the expression for the Fourier transform of the impulse response.
In other words, the frequency response is the Fourier transform of the impulse response.
Now in that context, how can we interpret the convolution property?
Well, remember what I said at the beginning of the lecture, when I pointed to the synthesis equation and I said, in essence, the synthesis equation tells us how to decompose x of t as a linear combination of complex exponentials.
What are the complex amplitudes of those complex exponentials?
In terms of our notation here, the complex amplitude of those complex exponentials is x of omega, or proportional to x of omega, in particular it's x of omega, d omega, and then a factor of 2 pi.
As this signal goes through this linear time invariant system, what happens to each of those exponential is each one gets multiplied by the frequency response at the associated frequency.
What comes out is the amplitude of the complex exponentials that are used to build the output.
So in fact, the convolution property simply is telling us that, in terms of the decomposition of the signal, in terms of complex exponentials, as we push that signal through a linear time invariant system, we're separately multiplying by the frequency response, the amplitudes of the exponential components used to build the input.
And that sum, in turn, is the decomposition of the output in terms of complex exponentials.
I understand that going through that involves a little bit of sorting out, and I strongly encourage you to try to understand and interpret the convolution property in those conceptual terms, rather than simply by applying the mathematics to the convolution integral and seeing the terms match up on both sides.
As I indicated, the convolution property forms the basis for what's referred to as filtering.
And this is a topic that we'll be treating in a considerable amount of detail after we've also gone through a discussion of the discrete time Fourier transform in the next several lectures.
However, what I'd like to do is just indicate, now, a little bit of the conceptual ideas involved.
Essentially, conceptually, what filtering, as it's typically referred to, corresponds to is modifying separately the individual frequency components in a signal.
The convolution property told us that if we look at the individual frequency components, they get multiplied by the frequency response, and so what that says is that we can amplify or attenuate any of those components separately using a linear time invariant system.
For example, what I've illustrated here is the frequency response of what is commonly referred to as an ideal low pass filter.
What an ideal low pass filter does is to pass exactly frequencies in one frequency range and eliminate totally frequencies outside that range.
Another filter which is not so ideal might, for example,
attenuate components in this band but not totally eliminate them.
In terms of filtering, we can think back to the differentiation property and, in fact, interpret differentiator as a filter.
Recall that the differentiation property said that the Fourier transform of the differentiated signal is the Fourier transform of the original signal multiplied by j omega.
So what that says, then, is that if we have a differentiator, the frequency response of that is j omega.
In other words, the Fourier transform of the output is j omega times the Fourier transform of the input.
And so the frequency response of the differentiator looks like this, in terms of its magnitude.
And what it does, of course, is it amplifies high frequencies and attenuates low frequencies.
Let me just, to cement some of these ideas, illustrate them in the context of one kind of signal, namely a signal which, in fact, is a spatial signal rather than a time signal.
And this also gives me an opportunity to introduce you to our colleague, J. B. J. Fourier.
So if we could look at our colleague, Mister Fourier, who,
by the way, is not only a person who had tremendously brilliant insights, and his insights, in fact, have led to forming the foundation of the developments that is the basis for this course.
He was also a very interesting, fascinating person.
And there's a certain amount of historical discussion about Fourier and his background, which you might enjoy reading in the text.
In any case, what you're looking at, of course, is a signal.
And the signal is a spatial signal, and it has high frequencies and low frequencies.
High frequencies corresponding to things that are varying rapidly spatially, and low frequencies varying slowly spatially.
And so, for example, we could low pass filter this picture simply by asking the video crew if they could be slightly defocus it.
And what you see as the picture is defocused, if you could hold it there, is that we've lost edges, which is the rapid variation.
And what we've retained is the broader, slow variation.
And now let's take out the defocusing low pass filter and go back to a focused image.
What we can also consider is what would happen if we looked at the differentiated image.
And there are several ways we can think about this.
One is that a differentiator--
Of course the output of a differentiator is larger where the discontinuity, or where the variation, is faster.
And so we would expect the edges to be enhanced if, in fact, we differentiated the image.
Or if we interpret differentiation in the context of our filter, then what we're saying is that, in effect, what's happening is that the differentiator is accentuating the high frequencies because of the frequency shape of the differentiator.
Recall that this all fits together as a nice package.
We expect intuitively that differentiation will enhance edges.
When we talked about square waves and we saw how the Fourier series built up a square wave, we saw that it was the high frequencies that were required in order to build up the sharp edges.
And so either viewed as a filter, or viewed intuitively,
we would expect that the differentiated image would, in fact, attenuate this slowly varying background and amplify the rapidly varying edges.
So let's look again at our original image, just to remind you of the fact that there are edges, of course, and there is a more slowly varying background.
And now let's look at the result of passing that through a differentiator.
And, as I think is very evident in the resulting image, clearly it's the edges that are retained and the slower background variations are destroyed, which is consistent with everything that we've said.
I've emphasized that we'll be returning to a much broader discussion of filtering at a later point in the course.
I'd now like to comment on another property, which is also, as I indicated, a topic in its own right, and which really is the dual property to the convolution property, and, in fact, could be argued directly from duality.
And that is what's referred to as the modulation property.
The convolution property told us that if we convolve in the time domain, we multiply in the frequency domain.
And we know that time and frequency domains are interchangeable because of duality, so what that would suggest is that if we multiply in the time domain, that would correspond to convolution in the frequency domain.
And, in fact, that is exactly what the modulation property is.
I have it summarized here that if we multiply a time function by another time function, then in the frequency domain we convolve.
Whereas the convolution property is just the dual of that, namely convolving in the time domain corresponds to multiplication in the frequency domain.
The convolution property is the basis, as I indicated, for filtering.
The modulation property, as I've summarized it here, in fact, is the entire basis for amplitude modulation systems as used almost universally in communications.
And what the modulation property, as we'll see when we explore it in more detail, tells us is that if we have a signal with a certain spectrum, and we multiply by a sinusoidal signal whose Fourier transform is a set of impulses, then in a frequency domain we convolve.
And that corresponds to taking the original spectrum and translating it, shifting it in frequency up to the frequency of the carrier, namely the sinusoidal signal.
And as I said, we'll come to that in much more detail in a number of lectures.
We've seen a number of properties, and I indicated sometime earlier when we talked about differential equations, that, in fact, it's the properties of the Fourier transform that provide us with a very useful and important mechanism for solving linear constant coefficient differential equations.
And what I'd like to do now is illustrate the procedure, the basis for that, and I think what we'll do is illustrate it simply in the context of several examples.
What I've indicated is a system with an impulse response h of t, or frequency response h of omega.
And, of course, we know that in the time domain it's described through convolution, in the frequency domain it's described through multiplication.
And I, in essence, am assuming that we're talking about a linear time invariant system.
And we're also going to assume then it's characterized by a linear constant coefficient differential equation, where we're going to impose the condition that it's causal linear and time invariant, or equivalently that the initial conditions are consistent with the initial rest.
And it's because of the fact that we're assuming that it's a linear time invariant system that we can describe it in the frequency domain through the convolution property, and we can use the properties of the Fourier transform.
So let's take, as our example, a first order differential equation as I indicate here.
So the derivative of the output plus a times the output is equal to the input.
And now we can use the differentiation property.
If we Fourier transform this entire expression, the differentiation property tells us that the Fourier transform of the derivative of the output is the Fourier transform of the output multiplied by j omega.
And linearity will let us write the Fourier transform of this as a times y of omega.
And since these are added together, and since we have the linearity property, these are added together.
And x of omega is the Fourier transform of x of t.
So what we've used is the differentiation property, and we've used the linearity property.
We can solve this equation for y of omega.
The Fourier transform of the output in terms of x of omega,
the Fourier transform of the input, and a simple algebraic step gets us to this expression.
So the Fourier transform of the output is 1 over j omega plus a times the Fourier transform of the input.
And I've just simply repeated that equation up here.
So far this is algebra, and the question is, now, how do we interpret this?
Well, we know that the Fourier transform of the output is the Fourier transform of the input times the Fourier transform of the impulse response of the system, namely the frequency response.
So, in fact, if we think of h of t and h of omega as a Fourier transform pair, it's the convolution property that lets us equate this term with h of omega.
So here we're using the convolution property.
So we know what the Fourier transform of the impulse response is, namely 1 over j omega plus a.
We may have, for example, wanted in our problem, instead of getting the frequency response, to get the impulse response.
And there are a variety of ways that we can do this.
We can attempt to go through the inverse Fourier transform expression.
But in fact, one of the most useful ways is formally called the inspection method.
Informally it's called, if you worked it out going one way,
then you ought to remember the answer so that you know how to get back method.
So what that says is, remember that we worked an example, and in fact I showed you the example earlier in the lecture, that the Fourier transform of e to the minus at times the step is 1 over j omega plus a?
So what is the inverse Fourier transfer of 1 over j omega plus a?
Well, it's e to the minus at times a unit step.
And that's just simply remembering, in essence, this particular transform pair.
I've drawn, graphically, the magnitude of the Fourier transform here.
And below it we have the impulse response.
The impulse response is, as I just indicated, e to the minus at times u of t.
Now let's go back up and look at the magnitude of the frequency response.
And given just the little bit of discussion that we had previously about filtering, you should be able to infer something about the filtering characteristics of this simple, first order differential equation.
In particular, if you look at that frequency response, the frequency response falls off with frequency and so what it tends to do is attenuate high frequencies and retain low frequencies.
So in fact, you could think of the defocusing that we did on the image of Fourier, you could think of that, approximately, as similar to the kind of filtering action that you would get by passing a signal through a first order differential equation.
Just to illustrate one additional step in both evaluating inverse transforms and using Fourier transform properties to solve linear constant coefficient differential equations, let's take the same example and, rather than finding the impulse response, let's find the response to another exponential input.
We could, of course, do that using the convolution integral.
We've just gotten the impulse response, and we could put that through the convolution integral to get the response to this input.
But let's do it, instead, by going back to the differential equation.
And so here I'm taking a differential equation.
I'll choose, just to have some numbers to work with, a equal to 2.
And now I'll choose an exponential on the right hand side, e to the minus t times u of t.
And again, we Fourier transform the equation.
And we can remember this particular Fourier transform pair.
It's just the one we worked out previously, now with a equal to 1.
Clearly we're getting a lot of mileage out of that example.
And now, if we want to determine what the output y of t is, we can do that by solving for y of omega and then generating the inverse Fourier transform.
Let's solve this algebraically for y of omega, and that gets us to this expression.
And this is not a Fourier transform that we've worked out before.
And this is the second part to the inspection procedure.
What we have is a Fourier transform which is a product of two terms, each of which we can recognize.
And what we can consider doing is expanding that out in a partial fraction expansion, namely as a sum of terms.
Because of the linearity property associated with the Fourier transform, the inverse transform is then the sum of the inverse transform of each of those terms.
So if we expand this out in a partial fraction expansion,
and you can just verify that if you add these two together you'll get back to where we started.
We now have the sum of two terms, and if we now recognize, by inspection, the inverse Fourier transform of this, we see that it's simply minus e to the minus 2t times the unit step.
This one is plus e to the minus t times the unit step.
And so, in fact, it's the sum of these two terms which are the inverse transforms of the individual terms in the partial fraction expansion that then give us the output.
So this is y of t, which is the sum of these two exponentials, and this is the inverse Fourier transform of y of omega as we calculated it previously.
Hopefully you're beginning to get some sense, now, of how powerful and also beautiful the Fourier transform is.
We've seen already a glimpse of how it plays a role in filtering, modulation, how its properties help us with linear constant coefficient differential equations, et cetera.
What we will do, beginning with the next lecture, is develop a similar set of tools for the discrete time case.
And there are some very strong similarities to what we've done in continuous time, also some very important differences.
And then, after we have the continuous time and discrete time Fourier transforms, we'll then see how the concepts involved and the properties involved lead to very important and powerful notions of filtering, modulation, sampling, and other signal processing ideas.
Thank you.
